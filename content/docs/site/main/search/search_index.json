{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/index.html","text":"","title":"Apache Pekko"},{"location":"/index.html#apache-pekko","text":"","title":"Apache Pekko"},{"location":"/security/index.html","text":"","title":"Security Announcements"},{"location":"/security/index.html#security-announcements","text":"","title":"Security Announcements"},{"location":"/security/index.html#receiving-security-advisories","text":"The best way to receive any and all security announcements is to subscribe to the Apache Announce Mailing List.\nThis mailing list has a reasonable level of traffic, and receives notifications only after security reports have been managed by the core Apache teams and fixes are publicly available.\nThis mailing list also has announcements of releases for Apache projects.","title":"Receiving Security Advisories"},{"location":"/security/index.html#reporting-vulnerabilities","text":"We strongly encourage people to report such problems to our private security mailing list first, before disclosing them in a public forum.\nPlease follow the guidelines laid down by the Apache Security team.\nIdeally, any issues affecting Apache Pekko and Akka should be reported to Apache team first. We will share the report with the Lightbend Akka team.","title":"Reporting Vulnerabilities"},{"location":"/security/index.html#security-related-documentation","text":"Akka security fixes Java Serialization Remote deployment allow list Remote Security","title":"Security Related Documentation"},{"location":"/downloads.html","text":"","title":"Downloads"},{"location":"/downloads.html#downloads","text":"","title":"Downloads"},{"location":"/documentation.html","text":"","title":"Documentation"},{"location":"/documentation.html#documentation","text":"Getting Started Guide Introduction to Apache Pekko Why modern systems need a new programming model How the Actor Model Meets the Needs of Modern, Distributed Systems Overview of Apache Pekko libraries and modules Introduction to the Example Part 1: Actor Architecture Part 2: Creating the First Actor Part 3: Working with Device Actors Part 4: Working with Device Groups Part 5: Querying Device Groups General Concepts Terminology, Concepts Actor Systems What is an Actor? Supervision and Monitoring Actor References, Paths and Addresses Location Transparency Apache Pekko and the Java Memory Model Message Delivery Reliability Configuration Default configuration Actors Introduction to Actors Actor lifecycle Interaction Patterns Fault Tolerance Actor discovery Routers Stash Behaviors as finite state machines Coordinated Shutdown Dispatchers Mailboxes Testing Coexistence Style guide Learning Pekko Typed from Classic Cluster Cluster Usage Cluster Specification Cluster Membership Service Phi Accrual Failure Detector Distributed Data Cluster Singleton Cluster Sharding Cluster Sharding concepts Sharded Daemon Process Multi-DC Cluster Distributed Publish Subscribe in Cluster Reliable delivery Serialization Serialization with Jackson Multi JVM Testing Multi Node Testing Artery Remoting Classic Remoting (Deprecated) Split Brain Resolver Coordination Choosing Pekko Cluster Persistence (Event Sourcing) Event Sourcing Replicated Event Sourcing CQRS Style Guide Snapshotting Testing EventSourced behaviors as finite state machines Schema Evolution for Event Sourced Actors Apache Persistence Query Persistence Query for LevelDB Persistence Plugins Persistence - Building a storage backend Replicated Event Sourcing Examples Persistence (Durable State) Durable State Style Guide CQRS Persistence Query Streams Module info Introduction Streams Quickstart Guide Design Principles behind Apache Pekko Streams Basics and working with Flows Working with Graphs Modularity, Composition and Hierarchy Buffers and working with rate Context Propagation Dynamic stream handling Custom stream processing Futures interop Actors interop Reactive Streams Interop Error Handling in Streams Working with streaming IO StreamRefs - Reactive Streams over the network Pipelining and Parallelism Testing streams Substreams Streams Cookbook Configuration Operators Discovery Module info How it works Discovery Method: DNS Discovery Method: Configuration Discovery Method: Aggregate multiple discovery methods Migrating from Pekko Management Discovery (before 1.0.0) Utilities Logging Circuit Breaker Futures patterns Extending Apache Pekko Other Apache Pekko modules Pekko HTTP Pekko gRPC Pekko Connectors Pekko Kafka Connector Pekko Projections Cassandra Plugin for Pekko Persistence JDBC Plugin for Pekko Persistence R2DBC Plugin for Pekko Persistence Google Cloud Spanner Plugin for Pekko Persistence Apache Pekko Management Package, Deploy and Run Packaging Operating a Cluster Deploying Rolling Updates Project Information Binary Compatibility Rules Scala 3 support Downstream upgrade strategy Modules marked “May Change” IDE Tips Immutability using Lombok Apache Pekko in OSGi Migration Guides Rolling Updates and Versions Issue Tracking Licenses Frequently Asked Questions Books and Videos Example projects Project Pekko Classic Classic Actors Classic Clustering Classic Networking Classic Utilities","title":"Documentation"},{"location":"/typed/guide/index.html","text":"","title":"Getting Started Guide"},{"location":"/typed/guide/index.html#getting-started-guide","text":"Introduction to Apache Pekko How to get started Why modern systems need a new programming model The challenge of encapsulation The illusion of shared memory on modern computer architectures The illusion of a call stack How the Actor Model Meets the Needs of Modern, Distributed Systems Usage of message passing avoids locking and blocking Actors handle error situations gracefully Overview of Apache Pekko libraries and modules Actor library Remoting Cluster Cluster Sharding Cluster Singleton Persistence Projections Distributed Data Streams Pekko Connectors HTTP gRPC Example of module use Introduction to the Example Prerequisites IoT example use case What you will learn in this tutorial Part 1: Actor Architecture Dependency Introduction The Pekko actor hierarchy Summary Part 2: Creating the First Actor Introduction What’s next? Part 3: Working with Device Actors Introduction Identifying messages for devices Adding flexibility to device messages Implementing the device actor and its read protocol Testing the actor Adding a write protocol Actor with read and write messages What’s Next? Part 4: Working with Device Groups Introduction Device manager hierarchy The Registration Protocol Adding registration support to device group actors Creating device manager actors What’s next? Part 5: Querying Device Groups Introduction Dealing with possible scenarios Implementing the query Adding query capability to the group Summary What’s Next?","title":"Getting Started Guide"},{"location":"/typed/guide/introduction.html","text":"","title":"Introduction to Apache Pekko"},{"location":"/typed/guide/introduction.html#introduction-to-apache-pekko","text":"Welcome to Apache Pekko, a set of open-source libraries for designing scalable, resilient systems that span processor cores and networks. Pekko allows you to focus on meeting business needs instead of writing low-level code to provide reliable behavior, fault tolerance, and high performance.\nMany common practices and accepted programming models do not address important challenges inherent in designing systems for modern computer architectures. To be successful, distributed systems must cope in an environment where components crash without responding, messages get lost without a trace on the wire, and network latency fluctuates. These problems occur regularly in carefully managed intra-datacenter environments - even more so in virtualized architectures.\nTo help you deal with these realities, Pekko provides:\nMulti-threaded behavior without the use of low-level concurrency constructs like atomics or locks — relieving you from even thinking about memory visibility issues. Transparent remote communication between systems and their components — relieving you from writing and maintaining difficult networking code. A clustered, high-availability architecture that is elastic, scales in or out, on demand — enabling you to deliver a truly reactive system.\nPekko’s use of the actor model provides a level of abstraction that makes it easier to write correct concurrent, parallel and distributed systems. The actor model spans the full set of Pekko libraries, providing you with a consistent way of understanding and using them. Thus, Pekko offers a depth of integration that you cannot achieve by picking libraries to solve individual problems and trying to piece them together.\nBy learning Pekko and how to use the actor model, you will gain access to a vast and deep set of tools that solve difficult distributed/parallel systems problems in a uniform programming model where everything fits together tightly and efficiently.","title":"Introduction to Apache Pekko"},{"location":"/typed/guide/introduction.html#how-to-get-started","text":"If this is your first experience with Pekko, we recommend that you start by running a simple Hello World project. See the Quickstart Guide Quickstart Guide for instructions on downloading and running the Hello World example. The Quickstart guide walks you through example code that introduces how to define actor systems, actors, and messages as well as how to use the test module and logging. Within 30 minutes, you should be able to run the Hello World example and learn how it is constructed.\nThis Getting Started guide provides the next level of information. It covers why the actor model fits the needs of modern distributed systems and includes a tutorial that will help further your knowledge of Pekko. Topics include:\nWhy modern systems need a new programming model How the actor model meets the needs of concurrent, distributed systems Overview of Pekko libraries and modules A more complex example that builds on the Hello World example to illustrate common Pekko patterns.","title":"How to get started"},{"location":"/typed/guide/actors-motivation.html","text":"","title":"Why modern systems need a new programming model"},{"location":"/typed/guide/actors-motivation.html#why-modern-systems-need-a-new-programming-model","text":"The actor model was proposed decades ago by Carl Hewitt as a way to handle parallel processing in a high performance network — an environment that was not available at the time. Today, hardware and infrastructure capabilities have caught up with and exceeded Hewitt’s vision. Consequently, organizations building distributed systems with demanding requirements encounter challenges that cannot fully be solved with a traditional object-oriented programming (OOP) model, but that can benefit from the actor model.\nToday, the actor model is not only recognized as a highly effective solution — it has been proven in production for some of the world’s most demanding applications. To highlight issues that the actor model addresses, this topic discusses the following mismatches between traditional programming assumptions and the reality of modern multi-threaded, multi-CPU architectures:\nThe challenge of encapsulation The illusion of shared memory on modern computer architectures The illusion of a call stack","title":"Why modern systems need a new programming model"},{"location":"/typed/guide/actors-motivation.html#the-challenge-of-encapsulation","text":"A core pillar of OOP is encapsulation. Encapsulation dictates that the internal data of an object is not accessible directly from the outside; it can only be modified by invoking a set of curated methods. The object is responsible for exposing safe operations that protect the invariant nature of its encapsulated data.\nFor example, operations on an ordered binary tree implementation must not allow violation of the tree ordering invariant. Callers expect the ordering to be intact and when querying the tree for a certain piece of data, they need to be able to rely on this constraint.\nWhen we analyze OOP runtime behavior, we sometimes draw a message sequence chart showing the interactions of method calls. For example:\nUnfortunately, the above diagram does not accurately represent the lifelines of the instances during execution. In reality, a thread executes all these calls, and the enforcement of invariants occurs on the same thread from which the method was called. Updating the diagram with the thread of execution, it looks like this:\nThe significance of this clarification becomes clear when you try to model what happens with multiple threads. Suddenly, our neatly drawn diagram becomes inadequate. We can try to illustrate multiple threads accessing the same instance:\nThere is a section of execution where two threads enter the same method. Unfortunately, the encapsulation model of objects does not guarantee anything about what happens in that section. Instructions of the two invocations can be interleaved in arbitrary ways which eliminate any hope for keeping the invariants intact without some type of coordination between two threads. Now, imagine this issue compounded by the existence of many threads.\nThe common approach to solving this problem is to add a lock around these methods. While this ensures that at most one thread will enter the method at any given time, this is a very costly strategy:\nLocks seriously limit concurrency, they are very costly on modern CPU architectures, requiring heavy-lifting from the operating system to suspend the thread and restore it later. The caller thread is now blocked, so it cannot do any other meaningful work. Even in desktop applications this is unacceptable, we want to keep user-facing parts of applications (its UI) to be responsive even when a long background job is running. In the backend, blocking is outright wasteful. One might think that this can be compensated by launching new threads, but threads are also a costly abstraction. Locks introduce a new menace: deadlocks.\nThese realities result in a no-win situation:\nWithout sufficient locks, the state gets corrupted. With many locks in place, performance suffers and very easily leads to deadlocks.\nAdditionally, locks only really work well locally. When it comes to coordinating across multiple machines, the only alternative is distributed locks. Unfortunately, distributed locks are several magnitudes less efficient than local locks and usually impose a hard limit on scaling out. Distributed lock protocols require several communication round-trips over the network across multiple machines, so latency goes through the roof.\nIn Object Oriented languages we rarely think about threads or linear execution paths in general. We often envision a system as a network of object instances that react to method calls, modify their internal state, then communicate with each other via method calls driving the whole application state forward:\nHowever, in a multi-threaded distributed environment, what actually happens is that threads “traverse” this network of object instances by following method calls. As a result, threads are what really drive execution:\nIn summary:\nObjects can only guarantee encapsulation (protection of invariants) in the face of single-threaded access, multi-thread execution almost always leads to corrupted internal state. Every invariant can be violated by having two contending threads in the same code segment. While locks seem to be the natural remedy to uphold encapsulation with multiple threads, in practice they are inefficient and easily lead to deadlocks in any application of real-world scale. Locks work locally, attempts to make them distributed exist, but offer limited potential for scaling out.","title":"The challenge of encapsulation"},{"location":"/typed/guide/actors-motivation.html#the-illusion-of-shared-memory-on-modern-computer-architectures","text":"Programming models of the 80’-90’s conceptualize that writing to a variable means writing to a memory location directly (which somewhat muddies the water that local variables might exist only in registers). On modern architectures - if we simplify things a bit - CPUs are writing to cache lines instead of writing to memory directly. Most of these caches are local to the CPU core, that is, writes by one core are not visible by another. In order to make local changes visible to another core, and hence to another thread, the cache line needs to be shipped to the other core’s cache.\nOn the JVM, we have to explicitly denote memory locations to be shared across threads by using volatile markers or Atomic wrappers. Otherwise, we can access them only in a locked section. Why don’t we just mark all variables as volatile? Because shipping cache lines across cores is a very costly operation! Doing so would implicitly stall the cores involved from doing additional work, and result in bottlenecks on the cache coherence protocol (the protocol CPUs use to transfer cache lines between main memory and other CPUs). The result is magnitudes of slowdown.\nEven for developers aware of this situation, figuring out which memory locations should be marked as volatile, or which atomic structures to use is a dark art.\nIn summary:\nThere is no real shared memory anymore, CPU cores pass chunks of data (cache lines) explicitly to each other just as computers on a network do. Inter-CPU communication and network communication have more in common than many realize. Passing messages is the norm now be it across CPUs or networked computers. Instead of hiding the message passing aspect through variables marked as shared or using atomic data structures, a more disciplined and principled approach is to keep state local to a concurrent entity and propagate data or events between concurrent entities explicitly via messages.","title":"The illusion of shared memory on modern computer architectures"},{"location":"/typed/guide/actors-motivation.html#the-illusion-of-a-call-stack","text":"Today, we often take call stacks for granted. But, they were invented in an era where concurrent programming was not as important because multi-CPU systems were not common. Call stacks do not cross threads and hence, do not model asynchronous call chains.\nThe problem arises when a thread intends to delegate a task to the “background”. In practice, this really means delegating to another thread. This cannot be a simple method/function call because calls are strictly local to the thread. What usually happens, is that the “caller” puts an object into a memory location shared by a worker thread (“callee”), which in turn, picks it up in some event loop. This allows the “caller” thread to move on and do other tasks.\nThe first issue is, how can the “caller” be notified of the completion of the task? But a more serious issue arises when a task fails with an exception. Where does the exception propagate to? It will propagate to the exception handler of the worker thread completely ignoring who the actual “caller” was:\nThis is a serious problem. How does the worker thread deal with the situation? It likely cannot fix the issue as it is usually oblivious of the purpose of the failed task. The “caller” thread needs to be notified somehow, but there is no call stack to unwind with an exception. Failure notification can only be done via a side-channel, for example putting an error code where the “caller” thread otherwise expects the result once ready. If this notification is not in place, the “caller” never gets notified of a failure and the task is lost! This is surprisingly similar to how networked systems work where messages/requests can get lost/fail without any notification.\nThis bad situation gets worse when things go really wrong and a worker backed by a thread encounters a bug and ends up in an unrecoverable situation. For example, an internal exception caused by a bug bubbles up to the root of the thread and makes the thread shut down. This immediately raises the question, who should restart the normal operation of the service hosted by the thread, and how should it be restored to a known-good state? At first glance, this might seem manageable, but we are suddenly faced by a new, unexpected phenomena: the actual task, that the thread was currently working on, is no longer in the shared memory location where tasks are taken from (usually a queue). In fact, due to the exception reaching to the top, unwinding all of the call stack, the task state is fully lost! We have lost a message even though this is local communication with no networking involved (where message losses are to be expected).\nIn summary:\nTo achieve any meaningful concurrency and performance on current systems, threads must delegate tasks among each other in an efficient way without blocking. With this style of task-delegating concurrency (and even more so with networked/distributed computing) call stack-based error handling breaks down and new, explicit error signaling mechanisms need to be introduced. Failures become part of the domain model. Concurrent systems with work delegation need to handle service faults and have principled means to recover from them. Clients of such services need to be aware that tasks/messages might get lost during restarts. Even if loss does not happen, a response might be delayed arbitrarily due to previously enqueued tasks (a long queue), delays caused by garbage collection, etc. In face of these, concurrent systems should handle response deadlines in the form of timeouts, just like networked/distributed systems.\nNext, let’s see how use of the actor model can overcome these challenges.","title":"The illusion of a call stack"},{"location":"/typed/guide/actors-intro.html","text":"","title":"How the Actor Model Meets the Needs of Modern, Distributed Systems"},{"location":"/typed/guide/actors-intro.html#how-the-actor-model-meets-the-needs-of-modern-distributed-systems","text":"As described in the previous topic, common programming practices do not properly address the needs of demanding modern systems. Thankfully, we don’t need to scrap everything we know. Instead, the actor model addresses these shortcomings in a principled way, allowing systems to behave in a way that better matches our mental model. The actor model abstraction allows you to think about your code in terms of communication, not unlike the exchanges that occur between people in a large organization.\nUse of actors allows us to:\nEnforce encapsulation without resorting to locks. Use the model of cooperative entities reacting to signals, changing state, and sending signals to each other to drive the whole application forward. Stop worrying about an executing mechanism which is a mismatch to our world view.","title":"How the Actor Model Meets the Needs of Modern, Distributed Systems"},{"location":"/typed/guide/actors-intro.html#usage-of-message-passing-avoids-locking-and-blocking","text":"Instead of calling methods, actors send messages to each other. Sending a message does not transfer the thread of execution from the sender to the destination. An actor can send a message and continue without blocking. Therefore, it can accomplish more in the same amount of time.\nWith objects, when a method returns, it releases control of its executing thread. In this respect, actors behave much like objects, they react to messages and return execution when they finish processing the current message. In this way, actors actually achieve the execution we imagined for objects:\nAn important difference between passing messages and calling methods is that messages have no return value. By sending a message, an actor delegates work to another actor. As we saw in The illusion of a call stack, if it expected a return value, the sending actor would either need to block or to execute the other actor’s work on the same thread. Instead, the receiving actor delivers the results in a reply message.\nThe second key change we need in our model is to reinstate encapsulation. Actors react to messages just like objects “react” to methods invoked on them. The difference is that instead of multiple threads “protruding” into our actor and wreaking havoc to internal state and invariants, actors execute independently from the senders of a message, and they react to incoming messages sequentially, one at a time. While each actor processes messages sent to it sequentially, different actors work concurrently with each other so that an actor system can process as many messages simultaneously as the hardware will support.\nSince there is always at most one message being processed per actor, the invariants of an actor can be kept without synchronization. This happens automatically without using locks:\nIn summary, this is what happens when an actor receives a message:\nThe actor adds the message to the end of a queue. If the actor was not scheduled for execution, it is marked as ready to execute. A (hidden) scheduler entity takes the actor and starts executing it. Actor picks the message from the front of the queue. Actor modifies internal state, sends messages to other actors. The actor is unscheduled.\nTo accomplish this behavior, actors have:\nA mailbox (the queue where messages end up). A behavior (the state of the actor, internal variables etc.). Messages (pieces of data representing a signal, similar to method calls and their parameters). An execution environment (the machinery that takes actors that have messages to react to and invokes their message handling code). An address (more on this later).\nMessages go into actor mailboxes. The behavior of the actor describes how the actor responds to messages (like sending more messages and/or changing state). An execution environment orchestrates a pool of threads to drive all these actions completely transparently.\nThis is a very simple model and it solves the issues enumerated previously:\nEncapsulation is preserved by decoupling execution from signaling (method calls transfer execution, message passing does not). There is no need for locks. Modifying the internal state of an actor is only possible via messages, which are processed one at a time eliminating races when trying to keep invariants. There are no locks used anywhere, and senders are not blocked. Millions of actors can be efficiently scheduled on a dozen of threads reaching the full potential of modern CPUs. Task delegation is the natural mode of operation for actors. State of actors is local and not shared, changes and data is propagated via messages, which maps to how modern memory hierarchy actually works. In many cases, this means transferring over only the cache lines that contain the data in the message while keeping local state and data cached at the original core. The same model maps exactly to remote communication where the state is kept in the RAM of machines and changes/data is propagated over the network as packets.","title":"Usage of message passing avoids locking and blocking"},{"location":"/typed/guide/actors-intro.html#actors-handle-error-situations-gracefully","text":"Since we no longer have a shared call stack between actors that send messages to each other, we need to handle error situations differently. There are two kinds of errors we need to consider:\nThe first case is when the delegated task on the target actor failed due to an error in the task (typically some validation issue, like a non-existent user ID). In this case, the service encapsulated by the target actor is intact, it is only the task itself that is erroneous. The service actor should reply to the sender with a message, presenting the error case. There is nothing special here, errors are part of the domain and hence become ordinary messages. The second case is when a service itself encounters an internal fault. Pekko enforces that all actors are organized into a tree-like hierarchy, i.e. an actor that creates another actor becomes the parent of that new actor. This is very similar to how operating systems organize processes into a tree. Just like with processes, when an actor fails, its parent actor can decide how to react to the failure. Also, if the parent actor is stopped, all of its children are recursively stopped, too. This service is called supervision and it is central to Pekko.\nA supervisor strategy is typically defined by the parent actor when it is starting a child actor. It can decide to restart the child actor on certain types of failures or stop it completely on others. Children never go silently dead (with the notable exception of entering an infinite loop) instead they are either failing and the supervisor strategy can react to the fault, or they are stopped (in which case interested parties are notified). There is always a responsible entity for managing an actor: its parent. Restarts are not visible from the outside: collaborating actors can keep sending messages while the target actor restarts.\nNow, let’s take a short tour of the functionality Pekko provides.","title":"Actors handle error situations gracefully"},{"location":"/typed/guide/modules.html","text":"","title":"Overview of Apache Pekko libraries and modules"},{"location":"/typed/guide/modules.html#overview-of-apache-pekko-libraries-and-modules","text":"Before delving into some best practices for writing actors, it will be helpful to preview the most commonly used Pekko libraries. This will help you start thinking about the functionality you want to use in your system. All core Pekko functionality is available as Open Source Software (OSS).\nThe following capabilities are included with Pekko OSS and are introduced later on this page:\nActor library Remoting Cluster Cluster Sharding Cluster Singleton Persistence Projections Distributed Data Streams Apache Pekko Connectors HTTP gRPC Other Apache Pekko modules\nThis page does not list all available modules, but overviews the main functionality and gives you an idea of the level of sophistication you can reach when you start building systems on top of Pekko.","title":"Overview of Apache Pekko libraries and modules"},{"location":"/typed/guide/modules.html#actor-library","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}\nThe core Pekko library is pekko-actor-typed, but actors are used across Pekko libraries, providing a consistent, integrated model that relieves you from individually solving the challenges that arise in concurrent or distributed system design. From a birds-eye view, actors are a programming paradigm that takes encapsulation, one of the pillars of OOP, to its extreme. Unlike objects, actors encapsulate not only their state but their execution. Communication with actors is not via method calls but by passing messages. While this difference may seem minor, it is actually what allows us to break clean from the limitations of OOP when it comes to concurrency and remote communication. Don’t worry if this description feels too high level to fully grasp yet, in the next chapter we will explain actors in detail. For now, the important point is that this is a model that handles concurrency and distribution at the fundamental level instead of ad hoc patched attempts to bring these features to OOP.\nChallenges that actors solve include the following:\nHow to build and design high-performance, concurrent applications. How to handle errors in a multi-threaded environment. How to protect my project from the pitfalls of concurrency.","title":"Actor library"},{"location":"/typed/guide/modules.html#remoting","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-remote\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-remote_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-remote_${versions.ScalaBinary}\"\n}\nRemoting enables actors that live on different computers to seamlessly exchange messages. While distributed as a JAR artifact, Remoting resembles a module more than it does a library. You enable it mostly with configuration and it has only a few APIs. Thanks to the actor model, a remote and local message send looks exactly the same. The patterns that you use on local systems translate directly to remote systems. You will rarely need to use Remoting directly, but it provides the foundation on which the Cluster subsystem is built.\nChallenges Remoting solves include the following:\nHow to address actor systems living on remote hosts. How to address individual actors on remote actor systems. How to turn messages to bytes on the wire. How to manage low-level, network connections (and reconnections) between hosts, detect crashed actor systems and hosts, all transparently. How to multiplex communications from an unrelated set of actors on the same network connection, all transparently.","title":"Remoting"},{"location":"/typed/guide/modules.html#cluster","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-typed_${versions.ScalaBinary}\"\n}\nIf you have a set of actor systems that cooperate to solve some business problem, then you likely want to manage these set of systems in a disciplined way. While Remoting solves the problem of addressing and communicating with components of remote systems, Clustering gives you the ability to organize these into a “meta-system” tied together by a membership protocol. In most cases, you want to use the Cluster module instead of using Remoting directly. Clustering provides an additional set of services on top of Remoting that most real world applications need.\nChallenges the Cluster module solves include the following:\nHow to maintain a set of actor systems (a cluster) that can communicate with each other and consider each other as part of the cluster. How to introduce a new system safely to the set of already existing members. How to reliably detect systems that are temporarily unreachable. How to remove failed hosts/systems (or scale down the system) so that all remaining members agree on the remaining subset of the cluster. How to distribute computations among the current set of members. How to designate members of the cluster to a certain role, in other words, to provide certain services and not others.","title":"Cluster"},{"location":"/typed/guide/modules.html#cluster-sharding","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-sharding-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-sharding-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-sharding-typed_${versions.ScalaBinary}\"\n}\nSharding helps to solve the problem of distributing a set of actors among members of a Pekko cluster. Sharding is a pattern that mostly used together with Persistence to balance a large set of persistent entities (backed by actors) to members of a cluster and also migrate them to other nodes when members crash or leave.\nChallenges that Sharding solves include the following:\nHow to model and scale out a large set of stateful entities on a set of systems. How to ensure that entities in the cluster are distributed properly so that load is properly balanced across the machines. How to ensure migrating entities from a crashed system without losing the state. How to ensure that an entity does not exist on multiple systems at the same time and hence keeps consistent.","title":"Cluster Sharding"},{"location":"/typed/guide/modules.html#cluster-singleton","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-singleton\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-singleton_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-singleton_${versions.ScalaBinary}\"\n}\nA common (in fact, a bit too common) use case in distributed systems is to have a single entity responsible for a given task which is shared among other members of the cluster and migrated if the host system fails. While this undeniably introduces a common bottleneck for the whole cluster that limits scaling, there are scenarios where the use of this pattern is unavoidable. Cluster singleton allows a cluster to select an actor system which will host a particular actor while other systems can always access said service independently from where it is.\nThe Singleton module can be used to solve these challenges:\nHow to ensure that only one instance of a service is running in the whole cluster. How to ensure that the service is up even if the system hosting it currently crashes or shuts down during the process of scaling down. How to reach this instance from any member of the cluster assuming that it can migrate to other systems over time.","title":"Cluster Singleton"},{"location":"/typed/guide/modules.html#persistence","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-typed_${versions.ScalaBinary}\"\n}\nJust like objects in OOP, actors keep their state in volatile memory. Once the system is shut down, gracefully or because of a crash, all data that was in memory is lost. Persistence provides patterns to enable actors to persist events that lead to their current state. Upon startup, events can be replayed to restore the state of the entity hosted by the actor. The event stream can be queried and fed into additional processing pipelines (an external Big Data cluster for example) or alternate views (like reports).\nPersistence tackles the following challenges:\nHow to restore the state of an entity/actor when system restarts or crashes. How to implement a CQRS system. How to ensure reliable delivery of messages in face of network errors and system crashes. How to introspect domain events that have led an entity to its current state. How to leverage Event Sourcing in your application to support long-running processes while the project continues to evolve.","title":"Persistence"},{"location":"/typed/guide/modules.html#projections","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-projection-core\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-projection-core_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-projection-core_${versions.ScalaBinary}\"\n}\nProjections provides a simple API for consuming a stream of events for projection into a variety of downstream options. The core dependency provides only the API and other provider dependencies are required for different source and sink implementations.\nChallenges Projections solve include the following:\nConstructing alternate or aggregate views over an event stream. Propagating an event stream onto another downstream medium such as a Kafka topic. A simple way of building read-side projections in the context of Event Sourcing and CQRS system","title":"Projections"},{"location":"/typed/guide/modules.html#distributed-data","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-typed_${versions.ScalaBinary}\"\n}\nIn situations where eventual consistency is acceptable, it is possible to share data between nodes in a Pekko Cluster and accept both reads and writes even in the face of cluster partitions. This can be achieved using Conflict Free Replicated Data Types (CRDTs), where writes on different nodes can happen concurrently and are merged in a predictable way afterward. The Distributed Data module provides infrastructure to share data and a number of useful data types.\nDistributed Data is intended to solve the following challenges:\nHow to accept writes even in the face of cluster partitions. How to share data while at the same time ensuring low-latency local read and write access.","title":"Distributed Data"},{"location":"/typed/guide/modules.html#streams","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}\nActors are a fundamental model for concurrency, but there are common patterns where their use requires the user to implement the same pattern over and over. Very common is the scenario where a chain, or graph, of actors, need to process a potentially large, or infinite, stream of sequential events and properly coordinate resource usage so that faster processing stages do not overwhelm slower ones in the chain or graph. Streams provide a higher-level abstraction on top of actors that simplifies writing such processing networks, handling all the fine details in the background and providing a safe, typed, composable programming model. Streams is also an implementation of the Reactive Streams standard which enables integration with all third party implementations of that standard.\nStreams solve the following challenges:\nHow to handle streams of events or large datasets with high performance, exploiting concurrency and keeping resource usage tight. How to assemble reusable pieces of event/data processing into flexible pipelines. How to connect asynchronous services in a flexible way to each other with high performance. How to provide or consume Reactive Streams compliant interfaces to interface with a third party library.","title":"Streams"},{"location":"/typed/guide/modules.html#pekko-connectors","text":"Pekko Connectors is a separate module from Pekko.\nPekko Connectors is collection of modules built upon the Streams API to provide Reactive Stream connector implementations for a variety of technologies common in the cloud and infrastructure landscape. See the Pekko Connectors overview page for more details on the API and the implementation modules available.\nPekko Connectors help solve the following challenges:\nConnecting various infrastructure or persistence components to Stream based flows. Connecting to legacy systems in a manner that adheres to a Reactive Streams API.","title":"Pekko Connectors"},{"location":"/typed/guide/modules.html#http","text":"Pekko HTTP is a separate module from Pekko.\nThe de facto standard for providing APIs remotely, internal or external, is HTTP. Pekko provides a library to construct or consume such HTTP services by giving a set of tools to create HTTP services (and serve them) and a client that can be used to consume other services. These tools are particularly suited to streaming in and out a large set of data or real-time events by leveraging the underlying model of Pekko Streams.\nSome of the challenges that HTTP tackles:\nHow to expose services of a system or cluster to the external world via an HTTP API in a performant way. How to stream large datasets in and out of a system using HTTP. How to stream live events in and out of a system using HTTP.","title":"HTTP"},{"location":"/typed/guide/modules.html#grpc","text":"Pekko gRPC is a separate module from Pekko.\nThis library provides an implementation of gRPC that integrates nicely with the HTTP and Streams modules. It is capable of generating both client and server-side artifacts from protobuf service definitions, which can then be exposed using Pekko HTTP, and handled using Streams.\nSome of the challenges that Pekko gRPC tackles:\nExposing services with all the benefits of gRPC & protobuf: Schema-first contract Schema evolution support Efficient binary protocol First-class streaming support Wide interoperability Use of HTTP/2 connection multiplexing","title":"gRPC"},{"location":"/typed/guide/modules.html#example-of-module-use","text":"Pekko modules integrate together seamlessly. For example, think of a large set of stateful business objects, such as documents or shopping carts, that website users access. If you model these as sharded entities, using Sharding and Persistence, they will be balanced across a cluster that you can scale out on-demand. They will be available during spikes that come from advertising campaigns or before holidays will be handled, even if some systems crash. You can also take the real-time stream of domain events with Persistence Query and use Streams to pipe them into a streaming Fast Data engine. Then, take the output of that engine as a Stream, manipulate it using Pekko Streams operators and expose it as web socket connections served by a load balanced set of HTTP servers hosted by your cluster to power your real-time business analytics tool.\nWe hope this preview caught your interest! The next topic introduces the example application we will build in the tutorial portion of this guide.","title":"Example of module use"},{"location":"/typed/guide/tutorial.html","text":"","title":"Introduction to the Example"},{"location":"/typed/guide/tutorial.html#introduction-to-the-example","text":"When writing prose, the hardest part is often composing the first few sentences. There is a similar “blank canvas” feeling when starting to build a Pekko system. You might wonder: Which should be the first actor? Where should it live? What should it do? Fortunately — unlike with prose — established best practices can guide us through these initial steps. In the remainder of this guide, we examine the core logic of a simple Pekko application to introduce you to actors and show you how to formulate solutions with them. The example demonstrates common patterns that will help you kickstart your Pekko projects.","title":"Introduction to the Example"},{"location":"/typed/guide/tutorial.html#prerequisites","text":"You should have already followed the instructions in the Pekko Quickstart with Scala guide Pekko Quickstart with Java guide to download and run the Hello World example. You will use this as a seed project and add the functionality described in this tutorial.\nNote Both the Java and Scala DSLs of Pekko modules bundled in the same JAR. For a smooth development experience, when using an IDE such as Eclipse or IntelliJ, you can disable the auto-importer from suggesting javadsl imports when working in Scala, or viceversa. See IDE Tips.","title":"Prerequisites"},{"location":"/typed/guide/tutorial.html#iot-example-use-case","text":"In this tutorial, we’ll use Pekko to build out part of an Internet of Things (IoT) system that reports data from sensor devices installed in customers’ homes. The example focuses on temperature readings. The target use case allows customers to log in and view the last reported temperature from different areas of their homes. You can imagine that such sensors could also collect relative humidity or other interesting data and an application would likely support reading and changing device configuration, maybe even alerting home owners when sensor state falls outside of a particular range.\nIn a real system, the application would be exposed to customers through a mobile app or browser. This guide concentrates only on the core logic for storing temperatures that would be called over a network protocol, such as HTTP. It also includes writing tests to help you get comfortable and proficient with testing actors.\nThe tutorial application consists of two main components:\nDevice data collection: — maintains a local representation of the remote devices. Multiple sensor devices for a home are organized into one device group. User dashboard: — periodically collects data from the devices for a logged in user’s home and presents the results as a report.\nThe following diagram illustrates the example application architecture. Since we are interested in the state of each sensor device, we will model devices as actors. The running application will create as many instances of device actors and device groups as necessary.","title":"IoT example use case"},{"location":"/typed/guide/tutorial.html#what-you-will-learn-in-this-tutorial","text":"This tutorial introduces and illustrates:\nThe actor hierarchy and how it influences actor behavior How to choose the right granularity for actors How to define protocols as messages Typical conversational styles\nLet’s get started by learning more about actors.","title":"What you will learn in this tutorial"},{"location":"/typed/guide/tutorial_1.html","text":"","title":"Part 1: Actor Architecture"},{"location":"/typed/guide/tutorial_1.html#part-1-actor-architecture","text":"","title":"Part 1: Actor Architecture"},{"location":"/typed/guide/tutorial_1.html#dependency","text":"Add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/guide/tutorial_1.html#introduction","text":"Use of Pekko relieves you from creating the infrastructure for an actor system and from writing the low-level code necessary to control basic behavior. To appreciate this, let’s look at the relationships between actors you create in your code and those that Pekko creates and manages for you internally, the actor lifecycle, and failure handling.","title":"Introduction"},{"location":"/typed/guide/tutorial_1.html#the-pekko-actor-hierarchy","text":"An actor in Pekko always belongs to a parent. You create an actor by calling ActorContext.spawn()ActorContext.spawn(). The creator actor becomes the parent of the newly created child actor. You might ask then, who is the parent of the first actor you create?\nAs illustrated below, all actors have a common parent, the user guardian, which is defined and created when you start the ActorSystemActorSystem. As we covered in the Quickstart GuideQuickstart Guide, creation of an actor returns a reference that is a valid URL. So, for example, if we create an actor named someActor from the user guardian with context.spawn(someBehavior, \"someActor\"), its reference will include the path /user/someActor.\nIn fact, before your first actor is started, Pekko has already created two actors in the system. The names of these built-in actors contain guardian. The guardian actors include:\n/ the so-called root guardian. This is the parent of all actors in the system, and the last one to stop when the system itself is terminated. /system the system guardian. Pekko or other libraries built on top of Pekko may create actors in the system namespace. /user the user guardian. This is the top level actor that you provide to start all other actors in your application.\nThe easiest way to see the actor hierarchy in action is to print ActorRefActorRef instances. In this small experiment, we create an actor, print its reference, create a child of this actor, and print the child’s reference.\nIn a new project, create a com.example package and with a a new Scala file called ActorHierarchyExperiments.scala here. Copy and paste the code from the snippet below to this new source filea Java file for each of the classes in the snippet below and copy the respective contents. Save your file and run sbt \"runMain com.example.ActorHierarchyExperiments\"files and run com.example.ActorHierarchyExperiments from your build tool or IDE to observe the output.\nScala copysourcepackage com.example\n\nimport org.apache.pekko\nimport pekko.actor.typed.ActorSystem\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.AbstractBehavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject PrintMyActorRefActor {\n  def apply(): Behavior[String] =\n    Behaviors.setup(context => new PrintMyActorRefActor(context))\n}\n\nclass PrintMyActorRefActor(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n\n  override def onMessage(msg: String): Behavior[String] =\n    msg match {\n      case \"printit\" =>\n        val secondRef = context.spawn(Behaviors.empty[String], \"second-actor\")\n        println(s\"Second: $secondRef\")\n        this\n    }\n}\n\nobject Main {\n  def apply(): Behavior[String] =\n    Behaviors.setup(context => new Main(context))\n\n}\n\nclass Main(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n  override def onMessage(msg: String): Behavior[String] =\n    msg match {\n      case \"start\" =>\n        val firstRef = context.spawn(PrintMyActorRefActor(), \"first-actor\")\n        println(s\"First: $firstRef\")\n        firstRef ! \"printit\"\n        this\n    }\n}\n\nobject ActorHierarchyExperiments extends App {\n  val testSystem = ActorSystem(Main(), \"testSystem\")\n  testSystem ! \"start\"\n} Java copysourcepackage com.example;\n\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\nclass PrintMyActorRefActor extends AbstractBehavior<String> {\n\n  static Behavior<String> create() {\n    return Behaviors.setup(PrintMyActorRefActor::new);\n  }\n\n  private PrintMyActorRefActor(ActorContext<String> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder().onMessageEquals(\"printit\", this::printIt).build();\n  }\n\n  private Behavior<String> printIt() {\n    ActorRef<String> secondRef = getContext().spawn(Behaviors.empty(), \"second-actor\");\n    System.out.println(\"Second: \" + secondRef);\n    return this;\n  }\n}\n\nclass Main extends AbstractBehavior<String> {\n\n  static Behavior<String> create() {\n    return Behaviors.setup(Main::new);\n  }\n\n  private Main(ActorContext<String> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder().onMessageEquals(\"start\", this::start).build();\n  }\n\n  private Behavior<String> start() {\n    ActorRef<String> firstRef = getContext().spawn(PrintMyActorRefActor.create(), \"first-actor\");\n\n    System.out.println(\"First: \" + firstRef);\n    firstRef.tell(\"printit\");\n    return Behaviors.same();\n  }\n}\n\npublic class ActorHierarchyExperiments {\n  public static void main(String[] args) {\n    ActorRef<String> testSystem = ActorSystem.create(Main.create(), \"testSystem\");\n    testSystem.tell(\"start\");\n  }\n}\nNote the way a message asked the first actor to do its work. We sent the message by using the parent’s reference: firstRef ! \"printit\"firstRef.tell(\"printit\", ActorRef.noSender()). When the code executes, the output includes the references for the first actor and the child it created as part of the printit case. Your output should look similar to the following:\nFirst: Actor[pekko://testSystem/user/first-actor#1053618476]\nSecond: Actor[pekko://testSystem/user/first-actor/second-actor#-1544706041]\nNotice the structure of the references:\nBoth paths start with pekko://testSystem/. Since all actor references are valid URLs, pekko:// is the value of the protocol field. Next, just like on the World Wide Web, the URL identifies the system. In this example, the system is named testSystem, but it could be any other name. If remote communication between multiple systems is enabled, this part of the URL includes the hostname so other systems can find it on the network. Because the second actor’s reference includes the path /first-actor/, it identifies it as a child of the first. The last part of the actor reference, #1053618476 or #-1544706041 is a unique identifier that you can ignore in most cases.\nNow that you understand what the actor hierarchy looks like, you might be wondering: Why do we need this hierarchy? What is it used for?\nAn important role of the hierarchy is to safely manage actor lifecycles. Let’s consider this next and see how that knowledge can help us write better code.","title":"The Pekko actor hierarchy"},{"location":"/typed/guide/tutorial_1.html#the-actor-lifecycle","text":"Actors pop into existence when created, then later, at user requests, they are stopped. Whenever an actor is stopped, all of its children are recursively stopped too. This behavior greatly simplifies resource cleanup and helps avoid resource leaks such as those caused by open sockets and files. In fact, a commonly overlooked difficulty when dealing with low-level multi-threaded code is the lifecycle management of various concurrent resources.\nTo stop an actor, the recommended pattern is to return Behaviors.stoppedBehaviors.stopped inside the actor to stop itself, usually as a response to some user defined stop message or when the actor is done with its job. Stopping a child actor is technically possible by calling context.stop(childRef)context.stop(childRef) from the parent, but it’s not possible to stop arbitrary (non-child) actors this way.\nThe Pekko actor API exposes some lifecycle signals, for example PostStopPostStop is sent just after the actor has been stopped. No messages are processed after this point.\nLet’s use the PostStop lifecycle signal in a simple experiment to observe the behavior when we stop an actor. First, add the following 2 actor classes to your project:\nScala copysourceobject StartStopActor1 {\n  def apply(): Behavior[String] =\n    Behaviors.setup(context => new StartStopActor1(context))\n}\n\nclass StartStopActor1(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n  println(\"first started\")\n  context.spawn(StartStopActor2(), \"second\")\n\n  override def onMessage(msg: String): Behavior[String] =\n    msg match {\n      case \"stop\" => Behaviors.stopped\n    }\n\n  override def onSignal: PartialFunction[Signal, Behavior[String]] = {\n    case PostStop =>\n      println(\"first stopped\")\n      this\n  }\n\n}\n\nobject StartStopActor2 {\n  def apply(): Behavior[String] =\n    Behaviors.setup(new StartStopActor2(_))\n}\n\nclass StartStopActor2(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n  println(\"second started\")\n\n  override def onMessage(msg: String): Behavior[String] = {\n    // no messages handled by this actor\n    Behaviors.unhandled\n  }\n\n  override def onSignal: PartialFunction[Signal, Behavior[String]] = {\n    case PostStop =>\n      println(\"second stopped\")\n      this\n  }\n\n} Java copysourceclass StartStopActor1 extends AbstractBehavior<String> {\n\n  static Behavior<String> create() {\n    return Behaviors.setup(StartStopActor1::new);\n  }\n\n  private StartStopActor1(ActorContext<String> context) {\n    super(context);\n    System.out.println(\"first started\");\n\n    context.spawn(StartStopActor2.create(), \"second\");\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder()\n        .onMessageEquals(\"stop\", Behaviors::stopped)\n        .onSignal(PostStop.class, signal -> onPostStop())\n        .build();\n  }\n\n  private Behavior<String> onPostStop() {\n    System.out.println(\"first stopped\");\n    return this;\n  }\n}\n\nclass StartStopActor2 extends AbstractBehavior<String> {\n\n  static Behavior<String> create() {\n    return Behaviors.setup(StartStopActor2::new);\n  }\n\n  private StartStopActor2(ActorContext<String> context) {\n    super(context);\n    System.out.println(\"second started\");\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder().onSignal(PostStop.class, signal -> onPostStop()).build();\n  }\n\n  private Behavior<String> onPostStop() {\n    System.out.println(\"second stopped\");\n    return this;\n  }\n}\nAnd create a ‘main’ class like above to start the actors and then send them a \"stop\" message:\nScala copysourceval first = context.spawn(StartStopActor1(), \"first\")\nfirst ! \"stop\" Java copysourceActorRef<String> first = context.spawn(StartStopActor1.create(), \"first\");\nfirst.tell(\"stop\");\nYou can again use sbt to start this program. The output should look like this:\nfirst started\nsecond started\nsecond stopped\nfirst stopped\nWhen we stopped actor first, it stopped its child actor, second, before stopping itself. This ordering is strict, all PostStopPostStop signals of the children are processed before the PostStop signal of the parent is processed.","title":"The actor lifecycle"},{"location":"/typed/guide/tutorial_1.html#failure-handling","text":"Parents and children are connected throughout their lifecycles. Whenever an actor fails (throws an exception or an unhandled exception bubbles out from onMessageReceive) the failure information is propagated to the supervision strategy, which then decides how to handle the exception caused by the actor. The supervision strategy is typically defined by the parent actor when it spawns a child actor. In this way, parents act as supervisors for their children. The default supervisor strategy is to stop the child. If you don’t define the strategy all failures result in a stop.\nLet’s observe a restart supervision strategy in a simple experiment. Add the following classes to your project, just as you did with the previous ones:\nScala copysourceobject SupervisingActor {\n  def apply(): Behavior[String] =\n    Behaviors.setup(context => new SupervisingActor(context))\n}\n\nclass SupervisingActor(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n  private val child = context.spawn(\n    Behaviors.supervise(SupervisedActor()).onFailure(SupervisorStrategy.restart),\n    name = \"supervised-actor\")\n\n  override def onMessage(msg: String): Behavior[String] =\n    msg match {\n      case \"failChild\" =>\n        child ! \"fail\"\n        this\n    }\n}\n\nobject SupervisedActor {\n  def apply(): Behavior[String] =\n    Behaviors.setup(context => new SupervisedActor(context))\n}\n\nclass SupervisedActor(context: ActorContext[String]) extends AbstractBehavior[String](context) {\n  println(\"supervised actor started\")\n\n  override def onMessage(msg: String): Behavior[String] =\n    msg match {\n      case \"fail\" =>\n        println(\"supervised actor fails now\")\n        throw new Exception(\"I failed!\")\n    }\n\n  override def onSignal: PartialFunction[Signal, Behavior[String]] = {\n    case PreRestart =>\n      println(\"supervised actor will be restarted\")\n      this\n    case PostStop =>\n      println(\"supervised actor stopped\")\n      this\n  }\n\n} Java copysourceclass SupervisingActor extends AbstractBehavior<String> {\n\n  static Behavior<String> create() {\n    return Behaviors.setup(SupervisingActor::new);\n  }\n\n  private final ActorRef<String> child;\n\n  private SupervisingActor(ActorContext<String> context) {\n    super(context);\n    child =\n        context.spawn(\n            Behaviors.supervise(SupervisedActor.create()).onFailure(SupervisorStrategy.restart()),\n            \"supervised-actor\");\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder().onMessageEquals(\"failChild\", this::onFailChild).build();\n  }\n\n  private Behavior<String> onFailChild() {\n    child.tell(\"fail\");\n    return this;\n  }\n}\n\nclass SupervisedActor extends AbstractBehavior<String> {\n\n  static Behavior<String> create() {\n    return Behaviors.setup(SupervisedActor::new);\n  }\n\n  private SupervisedActor(ActorContext<String> context) {\n    super(context);\n    System.out.println(\"supervised actor started\");\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder()\n        .onMessageEquals(\"fail\", this::fail)\n        .onSignal(PreRestart.class, signal -> preRestart())\n        .onSignal(PostStop.class, signal -> postStop())\n        .build();\n  }\n\n  private Behavior<String> fail() {\n    System.out.println(\"supervised actor fails now\");\n    throw new RuntimeException(\"I failed!\");\n  }\n\n  private Behavior<String> preRestart() {\n    System.out.println(\"supervised will be restarted\");\n    return this;\n  }\n\n  private Behavior<String> postStop() {\n    System.out.println(\"supervised stopped\");\n    return this;\n  }\n}\nAnd run with:\nScala copysourceval supervisingActor = context.spawn(SupervisingActor(), \"supervising-actor\")\nsupervisingActor ! \"failChild\" Java copysourceActorRef<String> supervisingActor =\n    context.spawn(SupervisingActor.create(), \"supervising-actor\");\nsupervisingActor.tell(\"failChild\");\nYou should see output similar to the following:\nsupervised actor started\nsupervised actor fails now\nsupervised actor will be restarted\nsupervised actor started\n[ERROR] [11/12/2018 12:03:27.171] [ActorHierarchyExperiments-pekko.actor.default-dispatcher-2] [pekko://ActorHierarchyExperiments/user/supervising-actor/supervised-actor] Supervisor pekko.actor.typed.internal.RestartSupervisor@1c452254 saw failure: I failed!\njava.lang.Exception: I failed!\n\tat typed.tutorial_1.SupervisedActor.onMessage(ActorHierarchyExperiments.scala:113)\n\tat typed.tutorial_1.SupervisedActor.onMessage(ActorHierarchyExperiments.scala:106)\n\tat org.apache.pekko.actor.typed.scaladsl.AbstractBehavior.receive(AbstractBehavior.scala:59)\n\tat org.apache.pekko.actor.typed.Behavior$.interpret(Behavior.scala:395)\n\tat org.apache.pekko.actor.typed.Behavior$.interpretMessage(Behavior.scala:369)\n\tat org.apache.pekko.actor.typed.internal.InterceptorImpl$$anon$2.apply(InterceptorImpl.scala:49)\n\tat org.apache.pekko.actor.typed.internal.SimpleSupervisor.aroundReceive(Supervision.scala:85)\n\tat org.apache.pekko.actor.typed.internal.InterceptorImpl.receive(InterceptorImpl.scala:70)\n\tat org.apache.pekko.actor.typed.Behavior$.interpret(Behavior.scala:395)\n\tat org.apache.pekko.actor.typed.Behavior$.interpretMessage(Behavior.scala:369)\nWe see that after failure the supervised actor is stopped and immediately restarted. We also see a log entry reporting the exception that was handled, in this case, our test exception. In this example we also used the PreRestartPreRestart signal which is processed before restarts.\nFor the impatient, we also recommend looking into the fault tolerance reference page for more in-depth details.","title":"Failure handling"},{"location":"/typed/guide/tutorial_1.html#summary","text":"We’ve learned about how Pekko manages actors in hierarchies where parents supervise their children and handle exceptions. We saw how to create a very simple actor and child. Next, we’ll apply this knowledge to our example use case by modeling the communication necessary to get information from device actors. Later, we’ll deal with how to manage the actors in groups.","title":"Summary"},{"location":"/typed/guide/tutorial_2.html","text":"","title":"Part 2: Creating the First Actor"},{"location":"/typed/guide/tutorial_2.html#part-2-creating-the-first-actor","text":"","title":"Part 2: Creating the First Actor"},{"location":"/typed/guide/tutorial_2.html#introduction","text":"With an understanding of actor hierarchy and behavior, the remaining question is how to map the top-level components of our IoT system to actors. The user guardian can be an actor that represents the whole application. In other words, we will have a single top-level actor in our IoT system. The components that create and manage devices and dashboards will be children of this actor. This allows us to refactor the example use case architecture diagram into a tree of actors:\nWe can define the first actor, the IotSupervisor, with a few lines of code. To start your tutorial application:\nCreate a new IotSupervisor source file in the com.example package. Paste the following code into the new file to define the IotSupervisor.\nScala copysourcepackage com.example\n\nimport org.apache.pekko\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.PostStop\nimport pekko.actor.typed.Signal\nimport pekko.actor.typed.scaladsl.AbstractBehavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject IotSupervisor {\n  def apply(): Behavior[Nothing] =\n    Behaviors.setup[Nothing](context => new IotSupervisor(context))\n}\n\nclass IotSupervisor(context: ActorContext[Nothing]) extends AbstractBehavior[Nothing](context) {\n  context.log.info(\"IoT Application started\")\n\n  override def onMessage(msg: Nothing): Behavior[Nothing] = {\n    // No need to handle any messages\n    Behaviors.unhandled\n  }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Nothing]] = {\n    case PostStop =>\n      context.log.info(\"IoT Application stopped\")\n      this\n  }\n} Java copysourcepackage com.example;\n\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.PostStop;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\npublic class IotSupervisor extends AbstractBehavior<Void> {\n\n  public static Behavior<Void> create() {\n    return Behaviors.setup(IotSupervisor::new);\n  }\n\n  private IotSupervisor(ActorContext<Void> context) {\n    super(context);\n    context.getLog().info(\"IoT Application started\");\n  }\n\n  // No need to handle any messages\n  @Override\n  public Receive<Void> createReceive() {\n    return newReceiveBuilder().onSignal(PostStop.class, signal -> onPostStop()).build();\n  }\n\n  private IotSupervisor onPostStop() {\n    getContext().getLog().info(\"IoT Application stopped\");\n    return this;\n  }\n}\nThe code is similar to the actor examples we used in the previous experiments, but notice that instead of println() we use Pekko’s built in logging facility via context.logcontext.getLog().\nTo provide the main entry point that creates the actor system, add the following code to the new IotApp object IotMain class.\nScala copysourcepackage com.example\n\nimport org.apache.pekko.actor.typed.ActorSystem\n\nobject IotApp {\n\n  def main(args: Array[String]): Unit = {\n    // Create ActorSystem and top level supervisor\n    ActorSystem[Nothing](IotSupervisor(), \"iot-system\")\n  }\n\n} Java copysourcepackage com.example;\n\nimport org.apache.pekko.actor.typed.ActorSystem;\n\npublic class IotMain {\n\n  public static void main(String[] args) {\n    // Create ActorSystem and top level supervisor\n    ActorSystem.create(IotSupervisor.create(), \"iot-system\");\n  }\n}\nThe application does little, other than log that it is started. But, we have the first actor in place and we are ready to add other actors.","title":"Introduction"},{"location":"/typed/guide/tutorial_2.html#whats-next-","text":"In the following chapters we will grow the application gradually, by:\nCreating the representation for a device. Creating the device management component. Adding query capabilities to device groups.","title":"What’s next?"},{"location":"/typed/guide/tutorial_3.html","text":"","title":"Part 3: Working with Device Actors"},{"location":"/typed/guide/tutorial_3.html#part-3-working-with-device-actors","text":"","title":"Part 3: Working with Device Actors"},{"location":"/typed/guide/tutorial_3.html#introduction","text":"In the previous topics we explained how to view actor systems in the large, that is, how components should be represented, how actors should be arranged in the hierarchy. In this part, we will look at actors in the small by implementing the device actor.\nIf we were working with objects, we would typically design the API as interfaces, a collection of abstract methods to be filled out by the actual implementation. In the world of actors, protocols take the place of interfaces. While it is not possible to formalize general protocols in the programming language, we can compose their most basic element, messages. So, we will start by identifying the messages we will want to send to device actors.\nTypically, messages fall into categories, or patterns. By identifying these patterns, you will find that it becomes easier to choose between them and to implement them. The first example demonstrates the request-respond message pattern.","title":"Introduction"},{"location":"/typed/guide/tutorial_3.html#identifying-messages-for-devices","text":"The tasks of a device actor will be simple:\nCollect temperature measurements When asked, report the last measured temperature\nHowever, a device might start without immediately having a temperature measurement. Hence, we need to account for the case where a temperature is not present. This also allows us to test the query part of the actor without the write part present, as the device actor can report an empty result.\nThe protocol for obtaining the current temperature from the device actor is simple. The actor:\nWaits for a request for the current temperature. Responds to the request with a reply that either: contains the current temperature or, indicates that a temperature is not yet available.\nWe need two messages, one for the request, and one for the reply. Our first attempt might look like the following:\nScala copysourcepackage com.example\n\n  import org.apache.pekko.actor.typed.ActorRef\n\n  object Device {\n    sealed trait Command\n    final case class ReadTemperature(replyTo: ActorRef[RespondTemperature]) extends Command\n    final case class RespondTemperature(value: Option[Double])\n  } Java copysourcepackage com.example;\n\nimport org.apache.pekko.actor.typed.ActorRef;\nimport java.util.Optional;\n\npublic class Device {\n\n  public interface Command {}\n\n  public static final class ReadTemperature implements Command {\n    final ActorRef<RespondTemperature> replyTo;\n\n    public ReadTemperature(ActorRef<RespondTemperature> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class RespondTemperature {\n    final Optional<Double> value;\n\n    public RespondTemperature(Optional<Double> value) {\n      this.value = value;\n    }\n  }\n}\nNote that the ReadTemperature message contains the ActorRef[RespondTemperature]ActorRef<RespondTemperature> that the device actor will use when replying to the request.\nThese two messages seem to cover the required functionality. However, the approach we choose must take into account the distributed nature of the application. While the basic mechanism is the same for communicating with an actor on the local JVM as with a remote actor, we need to keep the following in mind:\nThere will be observable differences in the latency of delivery between local and remote messages, because factors like network link bandwidth and the message size also come into play. Reliability is a concern because a remote message send involves more steps, which means that more can go wrong. A local send will pass a reference to the message inside the same JVM, without any restrictions on the underlying object which is sent, whereas a remote transport will place a limit on the message size.\nIn addition, while sending inside the same JVM is significantly more reliable, if an actor fails due to a programmer error while processing the message, the effect is the same as if a remote network request fails due to the remote host crashing while processing the message. Even though in both cases, the service recovers after a while (the actor is restarted by its supervisor, the host is restarted by an operator or by a monitoring system) individual requests are lost during the crash. Therefore, writing your actors such that every message could possibly be lost is the safe, pessimistic bet.\nBut to further understand the need for flexibility in the protocol, it will help to consider Pekko message ordering and message delivery guarantees. Pekko provides the following behavior for message sends:\nAt-most-once delivery, that is, no guaranteed delivery. Message ordering is maintained per sender, receiver pair.\nThe following sections discuss this behavior in more detail:\nMessage delivery Message ordering","title":"Identifying messages for devices"},{"location":"/typed/guide/tutorial_3.html#message-delivery","text":"The delivery semantics provided by messaging subsystems typically fall into the following categories:\nAt-most-once delivery — each message is delivered zero or one time; in more causal terms it means that messages can be lost, but are never duplicated. At-least-once delivery — potentially multiple attempts are made to deliver each message, until at least one succeeds; again, in more causal terms this means that messages can be duplicated but are never lost. Exactly-once delivery — each message is delivered exactly once to the recipient; the message can neither be lost nor be duplicated.\nThe first behavior, the one used by Pekko, is the cheapest and results in the highest performance. It has the least implementation overhead because it can be done in a fire-and-forget fashion without keeping the state at the sending end or in the transport mechanism. The second, at-least-once, requires retries to counter transport losses. This adds the overhead of keeping the state at the sending end and having an acknowledgment mechanism at the receiving end. Exactly-once delivery is most expensive, and results in the worst performance: in addition to the overhead added by at-least-once delivery, it requires the state to be kept at the receiving end in order to filter out duplicate deliveries.\nIn an actor system, we need to determine exact meaning of a guarantee — at which point does the system consider the delivery as accomplished:\nWhen the message is sent out on the network? When the message is received by the target actor’s host? When the message is put into the target actor’s mailbox? When the message target actor starts to process the message? When the target actor has successfully processed the message?\nMost frameworks and protocols that claim guaranteed delivery actually provide something similar to points 4 and 5. While this sounds reasonable, is it actually useful? To understand the implications, consider a simple, practical example: a user attempts to place an order and we only want to claim that it has successfully processed once it is actually on disk in the orders database.\nIf we rely on the successful processing of the message, the actor will report success as soon as the order has been submitted to the internal API that has the responsibility to validate it, process it and put it into the database. Unfortunately, immediately after the API has been invoked any of the following can happen:\nThe host can crash. Deserialization can fail. Validation can fail. The database might be unavailable. A programming error might occur.\nThis illustrates that the guarantee of delivery does not translate to the domain level guarantee. We only want to report success once the order has been actually fully processed and persisted. The only entity that can report success is the application itself, since only it has any understanding of the domain guarantees required. No generalized framework can figure out the specifics of a particular domain and what is considered a success in that domain.\nIn this particular example, we only want to signal success after a successful database write, where the database acknowledged that the order is now safely stored. For these reasons Pekko lifts the responsibilities of guarantees to the application itself, i.e. you have to implement them yourself with the tools that Pekko provides. This gives you full control of the guarantees that you want to provide. Now, let’s consider the message ordering that Pekko provides to make it easy to reason about application logic.","title":"Message delivery"},{"location":"/typed/guide/tutorial_3.html#message-ordering","text":"In Pekko, for a given pair of actors, messages sent directly from the first to the second will not be received out-of-order. The word directly emphasizes that this guarantee only applies when sending with the tell operator directly to the final destination, but not when employing mediators.\nIf:\nActor A1 sends messages M1, M2, M3 to A2. Actor A3 sends messages M4, M5, M6 to A2.\nThis means that, for Pekko messages:\nIf M1 is delivered it must be delivered before M2 and M3. If M2 is delivered it must be delivered before M3. If M4 is delivered it must be delivered before M5 and M6. If M5 is delivered it must be delivered before M6. A2 can see messages from A1 interleaved with messages from A3. Since there is no guaranteed delivery, any of the messages may be dropped, i.e. not arrive at A2.\nThese guarantees strike a good balance: having messages from one actor arrive in-order is convenient for building systems that can be easily reasoned about, while on the other hand allowing messages from different actors to arrive interleaved provides sufficient freedom for an efficient implementation of the actor system.\nFor the full details on delivery guarantees please refer to the reference page.","title":"Message Ordering"},{"location":"/typed/guide/tutorial_3.html#adding-flexibility-to-device-messages","text":"Our first query protocol was correct, but did not take into account distributed application execution. If we want to implement resends in the actor that queries a device actor (because of timed out requests), or if we want to query multiple actors, we need to be able to correlate requests and responses. Hence, we add one more field to our messages, so that an ID can be provided by the requester (we will add this code to our app in a later step):\nScala copysourcesealed trait Command\nfinal case class ReadTemperature(requestId: Long, replyTo: ActorRef[RespondTemperature]) extends Command\nfinal case class RespondTemperature(requestId: Long, value: Option[Double]) Java copysourcepublic class Device extends AbstractBehavior<Device.Command> {\n  public interface Command {}\n\n  public static final class ReadTemperature implements Command {\n    final long requestId;\n    final ActorRef<RespondTemperature> replyTo;\n\n    public ReadTemperature(long requestId, ActorRef<RespondTemperature> replyTo) {\n      this.requestId = requestId;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class RespondTemperature {\n    final long requestId;\n    final Optional<Double> value;\n\n    public RespondTemperature(long requestId, Optional<Double> value) {\n      this.requestId = requestId;\n      this.value = value;\n    }\n  }\n}","title":"Adding flexibility to device messages"},{"location":"/typed/guide/tutorial_3.html#implementing-the-device-actor-and-its-read-protocol","text":"As we learned in the Hello World example, each actor defines the type of messages it will accept. Our device actor has the responsibility to use the same ID parameter for the response of a given query, which would make it look like the following.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.AbstractBehavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.scaladsl.LoggerOps\n\nobject Device {\n  def apply(groupId: String, deviceId: String): Behavior[Command] =\n    Behaviors.setup(context => new Device(context, groupId, deviceId))\n\n  sealed trait Command\n  final case class ReadTemperature(requestId: Long, replyTo: ActorRef[RespondTemperature]) extends Command\n  final case class RespondTemperature(requestId: Long, value: Option[Double])\n}\n\nclass Device(context: ActorContext[Device.Command], groupId: String, deviceId: String)\n    extends AbstractBehavior[Device.Command](context) {\n  import Device._\n\n  var lastTemperatureReading: Option[Double] = None\n\n  context.log.info2(\"Device actor {}-{} started\", groupId, deviceId)\n\n  override def onMessage(msg: Command): Behavior[Command] = {\n    msg match {\n      case ReadTemperature(id, replyTo) =>\n        replyTo ! RespondTemperature(id, lastTemperatureReading)\n        this\n    }\n  }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case PostStop =>\n      context.log.info2(\"Device actor {}-{} stopped\", groupId, deviceId)\n      this\n  }\n\n} Java copysource import org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.PostStop;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\nimport java.util.Optional;\n\npublic class Device extends AbstractBehavior<Device.Command> {\n  public interface Command {}\n\n  public static final class ReadTemperature implements Command {\n    final long requestId;\n    final ActorRef<RespondTemperature> replyTo;\n\n    public ReadTemperature(long requestId, ActorRef<RespondTemperature> replyTo) {\n      this.requestId = requestId;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class RespondTemperature {\n    final long requestId;\n    final Optional<Double> value;\n\n    public RespondTemperature(long requestId, Optional<Double> value) {\n      this.requestId = requestId;\n      this.value = value;\n    }\n  }\n\n  public static Behavior<Command> create(String groupId, String deviceId) {\n    return Behaviors.setup(context -> new Device(context, groupId, deviceId));\n  }\n\n  private final String groupId;\n  private final String deviceId;\n\n  private Optional<Double> lastTemperatureReading = Optional.empty();\n\n  private Device(ActorContext<Command> context, String groupId, String deviceId) {\n    super(context);\n    this.groupId = groupId;\n    this.deviceId = deviceId;\n\n    context.getLog().info(\"Device actor {}-{} started\", groupId, deviceId);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(ReadTemperature.class, this::onReadTemperature)\n        .onSignal(PostStop.class, signal -> onPostStop())\n        .build();\n  }\n\n  private Behavior<Command> onReadTemperature(ReadTemperature r) {\n    r.replyTo.tell(new RespondTemperature(r.requestId, lastTemperatureReading));\n    return this;\n  }\n\n  private Device onPostStop() {\n    getContext().getLog().info(\"Device actor {}-{} stopped\", groupId, deviceId);\n    return this;\n  }\n}\nNote in the code that:\nThe apply method in the companion objectstatic create method defines how to construct the Behavior for the Device actor. The parameters include an ID for the device and the group to which it belongs, which we will use later. The messages we reasoned about previously are defined in the companion object.Device class that was shown earlier. In the Device class, the value of lastTemperatureReading is initially set to NoneOptional.empty(), and the actor will report it back if queried.","title":"Implementing the device actor and its read protocol"},{"location":"/typed/guide/tutorial_3.html#testing-the-actor","text":"Based on the actor above, we could write a test. In the com.example package in the test tree of your project, add the following code to a DeviceSpec.scalaDeviceTest.java file. (We use ScalaTest but any other test framework can be used with the Pekko Testkit).\nYou can run this test by running test at the sbt promptby running mvn test.\nScala copysourceimport org.apache.pekko.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit\nimport org.scalatest.wordspec.AnyWordSpecLike\n\nclass DeviceSpec extends ScalaTestWithActorTestKit with AnyWordSpecLike {\n  import Device._\n\n  \"Device actor\" must {\n\n    \"reply with empty reading if no temperature is known\" in {\n      val probe = createTestProbe[RespondTemperature]()\n      val deviceActor = spawn(Device(\"group\", \"device\"))\n\n      deviceActor ! Device.ReadTemperature(requestId = 42, probe.ref)\n      val response = probe.receiveMessage()\n      response.requestId should ===(42)\n      response.value should ===(None)\n    }\n} Java copysourceimport org.apache.pekko.actor.testkit.typed.javadsl.TestKitJunitResource;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestProbe;\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.junit.ClassRule;\nimport org.junit.Test;\nimport java.util.Optional;\n\nimport static org.junit.Assert.assertEquals;\n\n\npublic class DeviceTest {\n\n  @ClassRule public static final TestKitJunitResource testKit = new TestKitJunitResource();\n\n  @Test\n  public void testReplyWithEmptyReadingIfNoTemperatureIsKnown() {\n    TestProbe<Device.RespondTemperature> probe =\n        testKit.createTestProbe(Device.RespondTemperature.class);\n    ActorRef<Device.Command> deviceActor = testKit.spawn(Device.create(\"group\", \"device\"));\n    deviceActor.tell(new Device.ReadTemperature(42L, probe.getRef()));\n    Device.RespondTemperature response = probe.receiveMessage();\n    assertEquals(42L, response.requestId);\n    assertEquals(Optional.empty(), response.value);\n  }\n}\nNow, the actor needs a way to change the state of the temperature when it receives a message from the sensor.","title":"Testing the actor"},{"location":"/typed/guide/tutorial_3.html#adding-a-write-protocol","text":"The purpose of the write protocol is to update the currentTemperature field when the actor receives a message that contains the temperature. Again, it is tempting to define the write protocol as a very simple message, something like this:\nScala copysourcesealed trait Command\nfinal case class RecordTemperature(value: Double) extends Command Java copysourcepublic static final class RecordTemperature implements Command {\n  final double value;\n\n  public RecordTemperature(double value) {\n    this.value = value;\n  }\n}\nHowever, this approach does not take into account that the sender of the record temperature message can never be sure if the message was processed or not. We have seen that Pekko does not guarantee delivery of these messages and leaves it to the application to provide success notifications. In our case, we would like to send an acknowledgment to the sender once we have updated our last temperature recording, e.g. replying with a TemperatureRecorded message. Just like in the case of temperature queries and responses, it is also a good idea to include an ID field to provide maximum flexibility.\nScala copysourcefinal case class RecordTemperature(requestId: Long, value: Double, replyTo: ActorRef[TemperatureRecorded])\n    extends Command\nfinal case class TemperatureRecorded(requestId: Long) Java copysourcepublic static final class RecordTemperature implements Command {\n  final long requestId;\n  final double value;\n  final ActorRef<TemperatureRecorded> replyTo;\n\n  public RecordTemperature(long requestId, double value, ActorRef<TemperatureRecorded> replyTo) {\n    this.requestId = requestId;\n    this.value = value;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static final class TemperatureRecorded {\n  final long requestId;\n\n  public TemperatureRecorded(long requestId) {\n    this.requestId = requestId;\n  }\n}","title":"Adding a write protocol"},{"location":"/typed/guide/tutorial_3.html#actor-with-read-and-write-messages","text":"Putting the read and write protocol together, the device actor looks like the following example:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.PostStop\nimport pekko.actor.typed.Signal\nimport pekko.actor.typed.scaladsl.AbstractBehavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.scaladsl.LoggerOps\n\nobject Device {\n  def apply(groupId: String, deviceId: String): Behavior[Command] =\n    Behaviors.setup(context => new Device(context, groupId, deviceId))\n\n  sealed trait Command\n\n  final case class ReadTemperature(requestId: Long, replyTo: ActorRef[RespondTemperature]) extends Command\n  final case class RespondTemperature(requestId: Long, value: Option[Double])\n\n  final case class RecordTemperature(requestId: Long, value: Double, replyTo: ActorRef[TemperatureRecorded])\n      extends Command\n  final case class TemperatureRecorded(requestId: Long)\n}\n\nclass Device(context: ActorContext[Device.Command], groupId: String, deviceId: String)\n    extends AbstractBehavior[Device.Command](context) {\n  import Device._\n\n  var lastTemperatureReading: Option[Double] = None\n\n  context.log.info2(\"Device actor {}-{} started\", groupId, deviceId)\n\n  override def onMessage(msg: Command): Behavior[Command] = {\n    msg match {\n      case RecordTemperature(id, value, replyTo) =>\n        context.log.info2(\"Recorded temperature reading {} with {}\", value, id)\n        lastTemperatureReading = Some(value)\n        replyTo ! TemperatureRecorded(id)\n        this\n\n      case ReadTemperature(id, replyTo) =>\n        replyTo ! RespondTemperature(id, lastTemperatureReading)\n        this\n    }\n  }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case PostStop =>\n      context.log.info2(\"Device actor {}-{} stopped\", groupId, deviceId)\n      this\n  }\n\n} Java copysource import java.util.Optional;\n\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.PostStop;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\npublic class Device extends AbstractBehavior<Device.Command> {\n\n  public interface Command {}\n\n  public static final class RecordTemperature implements Command {\n    final long requestId;\n    final double value;\n    final ActorRef<TemperatureRecorded> replyTo;\n\n    public RecordTemperature(long requestId, double value, ActorRef<TemperatureRecorded> replyTo) {\n      this.requestId = requestId;\n      this.value = value;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class TemperatureRecorded {\n    final long requestId;\n\n    public TemperatureRecorded(long requestId) {\n      this.requestId = requestId;\n    }\n  }\n\n  public static final class ReadTemperature implements Command {\n    final long requestId;\n    final ActorRef<RespondTemperature> replyTo;\n\n    public ReadTemperature(long requestId, ActorRef<RespondTemperature> replyTo) {\n      this.requestId = requestId;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class RespondTemperature {\n    final long requestId;\n    final Optional<Double> value;\n\n    public RespondTemperature(long requestId, Optional<Double> value) {\n      this.requestId = requestId;\n      this.value = value;\n    }\n  }\n\n  public static Behavior<Command> create(String groupId, String deviceId) {\n    return Behaviors.setup(context -> new Device(context, groupId, deviceId));\n  }\n\n  private final String groupId;\n  private final String deviceId;\n\n  private Optional<Double> lastTemperatureReading = Optional.empty();\n\n  private Device(ActorContext<Command> context, String groupId, String deviceId) {\n    super(context);\n    this.groupId = groupId;\n    this.deviceId = deviceId;\n\n    context.getLog().info(\"Device actor {}-{} started\", groupId, deviceId);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(RecordTemperature.class, this::onRecordTemperature)\n        .onMessage(ReadTemperature.class, this::onReadTemperature)\n        .onSignal(PostStop.class, signal -> onPostStop())\n        .build();\n  }\n\n  private Behavior<Command> onRecordTemperature(RecordTemperature r) {\n    getContext().getLog().info(\"Recorded temperature reading {} with {}\", r.value, r.requestId);\n    lastTemperatureReading = Optional.of(r.value);\n    r.replyTo.tell(new TemperatureRecorded(r.requestId));\n    return this;\n  }\n\n  private Behavior<Command> onReadTemperature(ReadTemperature r) {\n    r.replyTo.tell(new RespondTemperature(r.requestId, lastTemperatureReading));\n    return this;\n  }\n\n  private Behavior<Command> onPostStop() {\n    getContext().getLog().info(\"Device actor {}-{} stopped\", groupId, deviceId);\n    return Behaviors.stopped();\n  }\n}\nWe should also write a new test case now, exercising both the read/query and write/record functionality together:\nScala copysource\"reply with latest temperature reading\" in {\n  val recordProbe = createTestProbe[TemperatureRecorded]()\n  val readProbe = createTestProbe[RespondTemperature]()\n  val deviceActor = spawn(Device(\"group\", \"device\"))\n\n  deviceActor ! Device.RecordTemperature(requestId = 1, 24.0, recordProbe.ref)\n  recordProbe.expectMessage(Device.TemperatureRecorded(requestId = 1))\n\n  deviceActor ! Device.ReadTemperature(requestId = 2, readProbe.ref)\n  val response1 = readProbe.receiveMessage()\n  response1.requestId should ===(2)\n  response1.value should ===(Some(24.0))\n\n  deviceActor ! Device.RecordTemperature(requestId = 3, 55.0, recordProbe.ref)\n  recordProbe.expectMessage(Device.TemperatureRecorded(requestId = 3))\n\n  deviceActor ! Device.ReadTemperature(requestId = 4, readProbe.ref)\n  val response2 = readProbe.receiveMessage()\n  response2.requestId should ===(4)\n  response2.value should ===(Some(55.0))\n} Java copysource@Test\npublic void testReplyWithLatestTemperatureReading() {\n  TestProbe<Device.TemperatureRecorded> recordProbe =\n      testKit.createTestProbe(Device.TemperatureRecorded.class);\n  TestProbe<Device.RespondTemperature> readProbe =\n      testKit.createTestProbe(Device.RespondTemperature.class);\n  ActorRef<Device.Command> deviceActor = testKit.spawn(Device.create(\"group\", \"device\"));\n\n  deviceActor.tell(new Device.RecordTemperature(1L, 24.0, recordProbe.getRef()));\n  assertEquals(1L, recordProbe.receiveMessage().requestId);\n\n  deviceActor.tell(new Device.ReadTemperature(2L, readProbe.getRef()));\n  Device.RespondTemperature response1 = readProbe.receiveMessage();\n  assertEquals(2L, response1.requestId);\n  assertEquals(Optional.of(24.0), response1.value);\n\n  deviceActor.tell(new Device.RecordTemperature(3L, 55.0, recordProbe.getRef()));\n  assertEquals(3L, recordProbe.receiveMessage().requestId);\n\n  deviceActor.tell(new Device.ReadTemperature(4L, readProbe.getRef()));\n  Device.RespondTemperature response2 = readProbe.receiveMessage();\n  assertEquals(4L, response2.requestId);\n  assertEquals(Optional.of(55.0), response2.value);\n}","title":"Actor with read and write messages"},{"location":"/typed/guide/tutorial_3.html#whats-next-","text":"So far, we have started designing our overall architecture, and we wrote the first actor that directly corresponds to the domain. We now have to create the component that is responsible for maintaining groups of devices and the device actors themselves.","title":"What’s Next?"},{"location":"/typed/guide/tutorial_4.html","text":"","title":"Part 4: Working with Device Groups"},{"location":"/typed/guide/tutorial_4.html#part-4-working-with-device-groups","text":"","title":"Part 4: Working with Device Groups"},{"location":"/typed/guide/tutorial_4.html#introduction","text":"Let’s take a closer look at the main functionality required by our use case. In a complete IoT system for monitoring home temperatures, the steps for connecting a device sensor to our system might look like this:\nA sensor device in the home connects through some protocol. The component managing network connections accepts the connection. The sensor provides its group and device ID to register with the device manager component of our system. The device manager component handles registration by looking up or creating the actor responsible for keeping sensor state. The actor responds with an acknowledgement, exposing its ActorRefActorRef. The networking component now uses the ActorRef for communication between the sensor and device actor without going through the device manager.\nSteps 1 and 2 take place outside the boundaries of our tutorial system. In this chapter, we will start addressing steps 3-6 and create a way for sensors to register with our system and to communicate with actors. But first, we have another architectural decision — how many levels of actors should we use to represent device groups and device sensors?\nOne of the main design challenges for Pekko programmers is choosing the best granularity for actors. In practice, depending on the characteristics of the interactions between actors, there are usually several valid ways to organize a system. In our use case, for example, it would be possible to have a single actor maintain all the groups and devices — perhaps using hash maps. It would also be reasonable to have an actor for each group that tracks the state of all devices in the same home.\nThe following guidelines help us choose the most appropriate actor hierarchy:\nIn general, prefer larger granularity. Introducing more fine-grained actors than needed causes more problems than it solves. Add finer granularity when the system requires: Higher concurrency. Complex conversations between actors that have many states. We will see a very good example for this in the next chapter. Sufficient state that it makes sense to divide into smaller actors. Multiple unrelated responsibilities. Using separate actors allows individuals to fail and be restored with little impact on others.","title":"Introduction"},{"location":"/typed/guide/tutorial_4.html#device-manager-hierarchy","text":"Considering the principles outlined in the previous section, We will model the device manager component as an actor tree with three levels:\nThe top level supervisor actor represents the system component for devices. It is also the entry point to look up and create device group and device actors. At the next level, group actors each supervise the device actors for one group id (e.g. one home). They also provide services, such as querying temperature readings from all of the available devices in their group. Device actors manage all the interactions with the actual device sensors, such as storing temperature readings.\nWe chose this three-layered architecture for these reasons:\nHaving groups of individual actors: Isolates failures that occur in a group. If a single actor managed all device groups, an error in one group that causes a restart would wipe out the state of groups that are otherwise non-faulty. Simplifies the problem of querying all the devices belonging to a group. Each group actor only contains state related to its group. Increases parallelism in the system. Since each group has a dedicated actor, they run concurrently and we can query multiple groups concurrently. Having sensors modeled as individual device actors: Isolates failures of one device actor from the rest of the devices in the group. Increases the parallelism of collecting temperature readings. Network connections from different sensors communicate with their individual device actors directly, reducing contention points.\nWith the architecture defined, we can start working on the protocol for registering sensors.","title":"Device manager hierarchy"},{"location":"/typed/guide/tutorial_4.html#the-registration-protocol","text":"As the first step, we need to design the protocol both for registering a device and for creating the group and device actors that will be responsible for it. This protocol will be provided by the DeviceManager component itself because that is the only actor that is known and available up front: device groups and device actors are created on-demand.\nLooking at registration in more detail, we can outline the necessary functionality:\nWhen a DeviceManager receives a request with a group and device id: If the manager already has an actor for the device group, it forwards the request to it. Otherwise, it creates a new device group actor and then forwards the request. The DeviceGroup actor receives the request to register an actor for the given device: If the group already has an actor for the device it replies with the ActorRefActorRef of the existing device actor. Otherwise, the DeviceGroup actor first creates a device actor and replies with the ActorRef of the newly created device actor. The sensor will now have the ActorRef of the device actor to send messages directly to it.\nThe messages that we will use to communicate registration requests and their acknowledgement have the definition:\nScala copysourcefinal case class RequestTrackDevice(groupId: String, deviceId: String, replyTo: ActorRef[DeviceRegistered])\n    extends DeviceManager.Command\n    with DeviceGroup.Command\n\nfinal case class DeviceRegistered(device: ActorRef[Device.Command]) Java copysourcepublic class DeviceManager extends AbstractBehavior<DeviceManager.Command> {\n\n  public interface Command {}\n\n  public static final class RequestTrackDevice\n      implements DeviceManager.Command, DeviceGroup.Command {\n    public final String groupId;\n    public final String deviceId;\n    public final ActorRef<DeviceRegistered> replyTo;\n\n    public RequestTrackDevice(String groupId, String deviceId, ActorRef<DeviceRegistered> replyTo) {\n      this.groupId = groupId;\n      this.deviceId = deviceId;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class DeviceRegistered {\n    public final ActorRef<Device.Command> device;\n\n    public DeviceRegistered(ActorRef<Device.Command> device) {\n      this.device = device;\n    }\n  }\n}\nIn this case we have not included a request ID field in the messages. Since registration happens once, when the component connects the system to some network protocol, the ID is not important. However, it is usually a best practice to include a request ID.\nNow, we’ll start implementing the protocol from the bottom up. In practice, both a top-down and bottom-up approach can work, but in our case, we benefit from the bottom-up approach as it allows us to immediately write tests for the new features without mocking out parts that we will need to build later.","title":"The Registration Protocol"},{"location":"/typed/guide/tutorial_4.html#adding-registration-support-to-device-group-actors","text":"A group actor has some work to do when it comes to registrations, including:\nHandling the registration request for existing device actor or by creating a new actor. Tracking which device actors exist in the group and removing them from the group when they are stopped.","title":"Adding registration support to device group actors"},{"location":"/typed/guide/tutorial_4.html#handling-the-registration-request","text":"A device group actor must either reply to the request with the ActorRefActorRef of an existing child, or it should create one. To look up child actors by their device IDs we will use a Map.\nAdd the following to your source file:\nScala copysourceobject DeviceGroup {\n  def apply(groupId: String): Behavior[Command] =\n    Behaviors.setup(context => new DeviceGroup(context, groupId))\n\n  trait Command\n\n  private final case class DeviceTerminated(device: ActorRef[Device.Command], groupId: String, deviceId: String)\n      extends Command\n\n}\n\nclass DeviceGroup(context: ActorContext[DeviceGroup.Command], groupId: String)\n    extends AbstractBehavior[DeviceGroup.Command](context) {\n  import DeviceGroup._\n  import DeviceManager.{ DeviceRegistered, ReplyDeviceList, RequestDeviceList, RequestTrackDevice }\n\n  private var deviceIdToActor = Map.empty[String, ActorRef[Device.Command]]\n\n  context.log.info(\"DeviceGroup {} started\", groupId)\n\n  override def onMessage(msg: Command): Behavior[Command] =\n    msg match {\n      case trackMsg @ RequestTrackDevice(`groupId`, deviceId, replyTo) =>\n        deviceIdToActor.get(deviceId) match {\n          case Some(deviceActor) =>\n            replyTo ! DeviceRegistered(deviceActor)\n          case None =>\n            context.log.info(\"Creating device actor for {}\", trackMsg.deviceId)\n            val deviceActor = context.spawn(Device(groupId, deviceId), s\"device-$deviceId\")\n            deviceIdToActor += deviceId -> deviceActor\n            replyTo ! DeviceRegistered(deviceActor)\n        }\n        this\n\n      case RequestTrackDevice(gId, _, _) =>\n        context.log.warn2(\"Ignoring TrackDevice request for {}. This actor is responsible for {}.\", gId, groupId)\n        this\n    }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case PostStop =>\n      context.log.info(\"DeviceGroup {} stopped\", groupId)\n      this\n  }\n} Java copysourcepublic class DeviceGroup extends AbstractBehavior<DeviceGroup.Command> {\n\n  public interface Command {}\n\n  private class DeviceTerminated implements Command {\n    public final ActorRef<Device.Command> device;\n    public final String groupId;\n    public final String deviceId;\n\n    DeviceTerminated(ActorRef<Device.Command> device, String groupId, String deviceId) {\n      this.device = device;\n      this.groupId = groupId;\n      this.deviceId = deviceId;\n    }\n  }\n\n  public static Behavior<Command> create(String groupId) {\n    return Behaviors.setup(context -> new DeviceGroup(context, groupId));\n  }\n\n  private final String groupId;\n  private final Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n\n  private DeviceGroup(ActorContext<Command> context, String groupId) {\n    super(context);\n    this.groupId = groupId;\n    context.getLog().info(\"DeviceGroup {} started\", groupId);\n  }\n\n  private DeviceGroup onTrackDevice(DeviceManager.RequestTrackDevice trackMsg) {\n    if (this.groupId.equals(trackMsg.groupId)) {\n      ActorRef<Device.Command> deviceActor = deviceIdToActor.get(trackMsg.deviceId);\n      if (deviceActor != null) {\n        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));\n      } else {\n        getContext().getLog().info(\"Creating device actor for {}\", trackMsg.deviceId);\n        deviceActor =\n            getContext()\n                .spawn(Device.create(groupId, trackMsg.deviceId), \"device-\" + trackMsg.deviceId);\n        deviceIdToActor.put(trackMsg.deviceId, deviceActor);\n        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));\n      }\n    } else {\n      getContext()\n          .getLog()\n          .warn(\n              \"Ignoring TrackDevice request for {}. This actor is responsible for {}.\",\n              groupId,\n              this.groupId);\n    }\n    return this;\n  }\n\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(DeviceManager.RequestTrackDevice.class, this::onTrackDevice)\n        .build();\n  }\n\n  private DeviceGroup onPostStop() {\n    getContext().getLog().info(\"DeviceGroup {} stopped\", groupId);\n    return this;\n  }\n}\nJust as we did with the device, we test this new functionality. We also test that the actors returned for the two different IDs are actually different, and we also attempt to record a temperature reading for each of the devices to see if the actors are responding.\nScala copysource\"be able to register a device actor\" in {\n  val probe = createTestProbe[DeviceRegistered]()\n  val groupActor = spawn(DeviceGroup(\"group\"))\n\n  groupActor ! RequestTrackDevice(\"group\", \"device1\", probe.ref)\n  val registered1 = probe.receiveMessage()\n  val deviceActor1 = registered1.device\n\n  // another deviceId\n  groupActor ! RequestTrackDevice(\"group\", \"device2\", probe.ref)\n  val registered2 = probe.receiveMessage()\n  val deviceActor2 = registered2.device\n  deviceActor1 should !==(deviceActor2)\n\n  // Check that the device actors are working\n  val recordProbe = createTestProbe[TemperatureRecorded]()\n  deviceActor1 ! RecordTemperature(requestId = 0, 1.0, recordProbe.ref)\n  recordProbe.expectMessage(TemperatureRecorded(requestId = 0))\n  deviceActor2 ! Device.RecordTemperature(requestId = 1, 2.0, recordProbe.ref)\n  recordProbe.expectMessage(Device.TemperatureRecorded(requestId = 1))\n}\n\n\"ignore requests for wrong groupId\" in {\n  val probe = createTestProbe[DeviceRegistered]()\n  val groupActor = spawn(DeviceGroup(\"group\"))\n\n  groupActor ! RequestTrackDevice(\"wrongGroup\", \"device1\", probe.ref)\n  probe.expectNoMessage(500.milliseconds)\n} Java copysource@Test\npublic void testReplyToRegistrationRequests() {\n  TestProbe<DeviceRegistered> probe = testKit.createTestProbe(DeviceRegistered.class);\n  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create(\"group\"));\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device\", probe.getRef()));\n  DeviceRegistered registered1 = probe.receiveMessage();\n\n  // another deviceId\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device3\", probe.getRef()));\n  DeviceRegistered registered2 = probe.receiveMessage();\n  assertNotEquals(registered1.device, registered2.device);\n\n  // Check that the device actors are working\n  TestProbe<Device.TemperatureRecorded> recordProbe =\n      testKit.createTestProbe(Device.TemperatureRecorded.class);\n  registered1.device.tell(new Device.RecordTemperature(0L, 1.0, recordProbe.getRef()));\n  assertEquals(0L, recordProbe.receiveMessage().requestId);\n  registered2.device.tell(new Device.RecordTemperature(1L, 2.0, recordProbe.getRef()));\n  assertEquals(1L, recordProbe.receiveMessage().requestId);\n}\n\n@Test\npublic void testIgnoreWrongRegistrationRequests() {\n  TestProbe<DeviceRegistered> probe = testKit.createTestProbe(DeviceRegistered.class);\n  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create(\"group\"));\n  groupActor.tell(new RequestTrackDevice(\"wrongGroup\", \"device1\", probe.getRef()));\n  probe.expectNoMessage();\n}\nIf a device actor already exists for the registration request, we would like to use the existing actor instead of a new one. We have not tested this yet, so we need to fix this:\nScala copysource\"return same actor for same deviceId\" in {\n  val probe = createTestProbe[DeviceRegistered]()\n  val groupActor = spawn(DeviceGroup(\"group\"))\n\n  groupActor ! RequestTrackDevice(\"group\", \"device1\", probe.ref)\n  val registered1 = probe.receiveMessage()\n\n  // registering same again should be idempotent\n  groupActor ! RequestTrackDevice(\"group\", \"device1\", probe.ref)\n  val registered2 = probe.receiveMessage()\n\n  registered1.device should ===(registered2.device)\n} Java copysource@Test\npublic void testReturnSameActorForSameDeviceId() {\n  TestProbe<DeviceRegistered> probe = testKit.createTestProbe(DeviceRegistered.class);\n  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create(\"group\"));\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device\", probe.getRef()));\n  DeviceRegistered registered1 = probe.receiveMessage();\n\n  // registering same again should be idempotent\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device\", probe.getRef()));\n  DeviceRegistered registered2 = probe.receiveMessage();\n  assertEquals(registered1.device, registered2.device);\n}","title":"Handling the registration request"},{"location":"/typed/guide/tutorial_4.html#keeping-track-of-the-device-actors-in-the-group","text":"So far, we have implemented logic for registering device actors in the group. Devices come and go, however, so we will need a way to remove device actors from the Map[String, ActorRef[DeviceMessage]]Map<String, ActorRef<DeviceMessage>>. We will assume that when a device is removed, its corresponding device actor is stopped. Supervision, as we discussed earlier, only handles error scenarios — not graceful stopping. So we need to notify the parent when one of the device actors is stopped.\nPekko provides a Death Watch feature that allows an actor to watch another actor and be notified if the other actor is stopped. Unlike supervision, watching is not limited to parent-child relationships, any actor can watch any other actor as long as it knows the ActorRefActorRef. After a watched actor stops, the watcher receives a Terminated(actorRef)Terminated(actorRef) signal which also contains the reference to the watched actor. The watcher can either handle this message explicitly or will fail with a DeathPactExceptionDeathPactException. This latter is useful if the actor can no longer perform its own duties after the watched actor has been stopped. In our case, the group should still function after one device have been stopped, so we need to handle the Terminated(actorRef) signal.\nOur device group actor needs to include functionality that:\nStarts watching new device actors when they are created. Removes a device actor from the Map[String, ActorRef[DeviceMessage]]Map<String, ActorRef<DeviceMessage>> — which maps devices to device actors — when the notification indicates it has stopped.\nUnfortunately, the Terminated signal only contains the ActorRef of the child actor. We need the actor’s ID to remove it from the map of existing device to device actor mappings. An alternative to the Terminated signal is to define a custom message that will be sent when the watched actor is stopped. We will use that here because it gives us the possibility to carry the device ID in that message.\nAdding the functionality to identify the actor results in this:\nScala copysource class DeviceGroup(context: ActorContext[DeviceGroup.Command], groupId: String)\n    extends AbstractBehavior[DeviceGroup.Command](context) {\n  import DeviceGroup._\n  import DeviceManager.{ DeviceRegistered, ReplyDeviceList, RequestDeviceList, RequestTrackDevice }\n\n  private var deviceIdToActor = Map.empty[String, ActorRef[Device.Command]]\n\n  context.log.info(\"DeviceGroup {} started\", groupId)\n\n  override def onMessage(msg: Command): Behavior[Command] =\n    msg match {\n      case trackMsg @ RequestTrackDevice(`groupId`, deviceId, replyTo) =>\n        deviceIdToActor.get(deviceId) match {\n          case Some(deviceActor) =>\n            replyTo ! DeviceRegistered(deviceActor)\n          case None =>\n            context.log.info(\"Creating device actor for {}\", trackMsg.deviceId)\n            val deviceActor = context.spawn(Device(groupId, deviceId), s\"device-$deviceId\")\n            context.watchWith(deviceActor, DeviceTerminated(deviceActor, groupId, deviceId))\n            deviceIdToActor += deviceId -> deviceActor\n            replyTo ! DeviceRegistered(deviceActor)\n        }\n        this\n\n      case RequestTrackDevice(gId, _, _) =>\n        context.log.warn2(\"Ignoring TrackDevice request for {}. This actor is responsible for {}.\", gId, groupId)\n        this\n\n      case DeviceTerminated(_, _, deviceId) =>\n        context.log.info(\"Device actor for {} has been terminated\", deviceId)\n        deviceIdToActor -= deviceId\n        this\n\n    }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case PostStop =>\n      context.log.info(\"DeviceGroup {} stopped\", groupId)\n      this\n  }\n} Java copysourcepublic class DeviceGroup extends AbstractBehavior<DeviceGroup.Command> {\n\n  public interface Command {}\n\n  private class DeviceTerminated implements Command {\n    public final ActorRef<Device.Command> device;\n    public final String groupId;\n    public final String deviceId;\n\n    DeviceTerminated(ActorRef<Device.Command> device, String groupId, String deviceId) {\n      this.device = device;\n      this.groupId = groupId;\n      this.deviceId = deviceId;\n    }\n  }\n\n  public static Behavior<Command> create(String groupId) {\n    return Behaviors.setup(context -> new DeviceGroup(context, groupId));\n  }\n\n  private final String groupId;\n  private final Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n\n  private DeviceGroup(ActorContext<Command> context, String groupId) {\n    super(context);\n    this.groupId = groupId;\n    context.getLog().info(\"DeviceGroup {} started\", groupId);\n  }\n\n  private DeviceGroup onTrackDevice(DeviceManager.RequestTrackDevice trackMsg) {\n    if (this.groupId.equals(trackMsg.groupId)) {\n      ActorRef<Device.Command> deviceActor = deviceIdToActor.get(trackMsg.deviceId);\n      if (deviceActor != null) {\n        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));\n      } else {\n        getContext().getLog().info(\"Creating device actor for {}\", trackMsg.deviceId);\n        deviceActor =\n            getContext()\n                .spawn(Device.create(groupId, trackMsg.deviceId), \"device-\" + trackMsg.deviceId);\n        getContext()\n            .watchWith(deviceActor, new DeviceTerminated(deviceActor, groupId, trackMsg.deviceId));\n        deviceIdToActor.put(trackMsg.deviceId, deviceActor);\n        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));\n      }\n    } else {\n      getContext()\n          .getLog()\n          .warn(\n              \"Ignoring TrackDevice request for {}. This actor is responsible for {}.\",\n              groupId,\n              this.groupId);\n    }\n    return this;\n  }\n\n\n  private DeviceGroup onTerminated(DeviceTerminated t) {\n    getContext().getLog().info(\"Device actor for {} has been terminated\", t.deviceId);\n    deviceIdToActor.remove(t.deviceId);\n    return this;\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(DeviceManager.RequestTrackDevice.class, this::onTrackDevice)\n        .onMessage(DeviceTerminated.class, this::onTerminated)\n        .onSignal(PostStop.class, signal -> onPostStop())\n        .build();\n  }\n\n  private DeviceGroup onPostStop() {\n    getContext().getLog().info(\"DeviceGroup {} stopped\", groupId);\n    return this;\n  }\n}\nSo far we have no means to get which devices the group device actor keeps track of and, therefore, we cannot test our new functionality yet. To make it testable, we add a new query capability (message RequestDeviceList) that lists the currently active device IDs:\nScala copysourcefinal case class RequestDeviceList(requestId: Long, groupId: String, replyTo: ActorRef[ReplyDeviceList])\n    extends DeviceManager.Command\n    with DeviceGroup.Command\n\nfinal case class ReplyDeviceList(requestId: Long, ids: Set[String]) Java copysourcepublic static final class RequestDeviceList\n    implements DeviceManager.Command, DeviceGroup.Command {\n  final long requestId;\n  final String groupId;\n  final ActorRef<ReplyDeviceList> replyTo;\n\n  public RequestDeviceList(long requestId, String groupId, ActorRef<ReplyDeviceList> replyTo) {\n    this.requestId = requestId;\n    this.groupId = groupId;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static final class ReplyDeviceList {\n  final long requestId;\n  final Set<String> ids;\n\n  public ReplyDeviceList(long requestId, Set<String> ids) {\n    this.requestId = requestId;\n    this.ids = ids;\n  }\n}\nScala copysourceobject DeviceGroup {\n  def apply(groupId: String): Behavior[Command] =\n    Behaviors.setup(context => new DeviceGroup(context, groupId))\n\n  trait Command\n\n  private final case class DeviceTerminated(device: ActorRef[Device.Command], groupId: String, deviceId: String)\n      extends Command\n\n}\n\nclass DeviceGroup(context: ActorContext[DeviceGroup.Command], groupId: String)\n    extends AbstractBehavior[DeviceGroup.Command](context) {\n  import DeviceGroup._\n  import DeviceManager.{ DeviceRegistered, ReplyDeviceList, RequestDeviceList, RequestTrackDevice }\n\n  private var deviceIdToActor = Map.empty[String, ActorRef[Device.Command]]\n\n  context.log.info(\"DeviceGroup {} started\", groupId)\n\n  override def onMessage(msg: Command): Behavior[Command] =\n    msg match {\n      case trackMsg @ RequestTrackDevice(`groupId`, deviceId, replyTo) =>\n        deviceIdToActor.get(deviceId) match {\n          case Some(deviceActor) =>\n            replyTo ! DeviceRegistered(deviceActor)\n          case None =>\n            context.log.info(\"Creating device actor for {}\", trackMsg.deviceId)\n            val deviceActor = context.spawn(Device(groupId, deviceId), s\"device-$deviceId\")\n            context.watchWith(deviceActor, DeviceTerminated(deviceActor, groupId, deviceId))\n            deviceIdToActor += deviceId -> deviceActor\n            replyTo ! DeviceRegistered(deviceActor)\n        }\n        this\n\n      case RequestTrackDevice(gId, _, _) =>\n        context.log.warn2(\"Ignoring TrackDevice request for {}. This actor is responsible for {}.\", gId, groupId)\n        this\n\n      case RequestDeviceList(requestId, gId, replyTo) =>\n        if (gId == groupId) {\n          replyTo ! ReplyDeviceList(requestId, deviceIdToActor.keySet)\n          this\n        } else\n          Behaviors.unhandled\n\n      case DeviceTerminated(_, _, deviceId) =>\n        context.log.info(\"Device actor for {} has been terminated\", deviceId)\n        deviceIdToActor -= deviceId\n        this\n\n    }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case PostStop =>\n      context.log.info(\"DeviceGroup {} stopped\", groupId)\n      this\n  }\n} Java copysourcepublic class DeviceGroup extends AbstractBehavior<DeviceGroup.Command> {\n\n  public interface Command {}\n\n  private class DeviceTerminated implements Command {\n    public final ActorRef<Device.Command> device;\n    public final String groupId;\n    public final String deviceId;\n\n    DeviceTerminated(ActorRef<Device.Command> device, String groupId, String deviceId) {\n      this.device = device;\n      this.groupId = groupId;\n      this.deviceId = deviceId;\n    }\n  }\n\n  public static Behavior<Command> create(String groupId) {\n    return Behaviors.setup(context -> new DeviceGroup(context, groupId));\n  }\n\n  private final String groupId;\n  private final Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n\n  private DeviceGroup(ActorContext<Command> context, String groupId) {\n    super(context);\n    this.groupId = groupId;\n    context.getLog().info(\"DeviceGroup {} started\", groupId);\n  }\n\n  private DeviceGroup onTrackDevice(DeviceManager.RequestTrackDevice trackMsg) {\n    if (this.groupId.equals(trackMsg.groupId)) {\n      ActorRef<Device.Command> deviceActor = deviceIdToActor.get(trackMsg.deviceId);\n      if (deviceActor != null) {\n        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));\n      } else {\n        getContext().getLog().info(\"Creating device actor for {}\", trackMsg.deviceId);\n        deviceActor =\n            getContext()\n                .spawn(Device.create(groupId, trackMsg.deviceId), \"device-\" + trackMsg.deviceId);\n        getContext()\n            .watchWith(deviceActor, new DeviceTerminated(deviceActor, groupId, trackMsg.deviceId));\n        deviceIdToActor.put(trackMsg.deviceId, deviceActor);\n        trackMsg.replyTo.tell(new DeviceManager.DeviceRegistered(deviceActor));\n      }\n    } else {\n      getContext()\n          .getLog()\n          .warn(\n              \"Ignoring TrackDevice request for {}. This actor is responsible for {}.\",\n              groupId,\n              this.groupId);\n    }\n    return this;\n  }\n\n\n  private DeviceGroup onDeviceList(DeviceManager.RequestDeviceList r) {\n    r.replyTo.tell(new DeviceManager.ReplyDeviceList(r.requestId, deviceIdToActor.keySet()));\n    return this;\n  }\n\n  private DeviceGroup onTerminated(DeviceTerminated t) {\n    getContext().getLog().info(\"Device actor for {} has been terminated\", t.deviceId);\n    deviceIdToActor.remove(t.deviceId);\n    return this;\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(DeviceManager.RequestTrackDevice.class, this::onTrackDevice)\n        .onMessage(\n            DeviceManager.RequestDeviceList.class,\n            r -> r.groupId.equals(groupId),\n            this::onDeviceList)\n        .onMessage(DeviceTerminated.class, this::onTerminated)\n        .onSignal(PostStop.class, signal -> onPostStop())\n        .build();\n  }\n\n  private DeviceGroup onPostStop() {\n    getContext().getLog().info(\"DeviceGroup {} stopped\", groupId);\n    return this;\n  }\n}\nWe are almost ready to test the removal of devices. But, we still need the following capabilities:\nTo stop a device actor from our test case, from the outside, we must send a message to it. We add a Passivate message which instructs the actor to stop. To be notified once the device actor is stopped. We can use the Death Watch facility for this purpose, too.\nScala copysourcecase object Passivate extends Command Java copysourcestatic enum Passivate implements Command {\n  INSTANCE\n}\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.PostStop\nimport pekko.actor.typed.Signal\nimport pekko.actor.typed.scaladsl.AbstractBehavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.scaladsl.LoggerOps\n\nobject Device {\n  def apply(groupId: String, deviceId: String): Behavior[Command] =\n    Behaviors.setup(context => new Device(context, groupId, deviceId))\n\n  sealed trait Command\n\n  final case class ReadTemperature(requestId: Long, replyTo: ActorRef[RespondTemperature]) extends Command\n  final case class RespondTemperature(requestId: Long, value: Option[Double])\n\n  final case class RecordTemperature(requestId: Long, value: Double, replyTo: ActorRef[TemperatureRecorded])\n      extends Command\n  final case class TemperatureRecorded(requestId: Long)\n\n  case object Passivate extends Command\n}\n\nclass Device(context: ActorContext[Device.Command], groupId: String, deviceId: String)\n    extends AbstractBehavior[Device.Command](context) {\n  import Device._\n\n  var lastTemperatureReading: Option[Double] = None\n\n  context.log.info2(\"Device actor {}-{} started\", groupId, deviceId)\n\n  override def onMessage(msg: Command): Behavior[Command] = {\n    msg match {\n      case RecordTemperature(id, value, replyTo) =>\n        context.log.info2(\"Recorded temperature reading {} with {}\", value, id)\n        lastTemperatureReading = Some(value)\n        replyTo ! TemperatureRecorded(id)\n        this\n\n      case ReadTemperature(id, replyTo) =>\n        replyTo ! RespondTemperature(id, lastTemperatureReading)\n        this\n\n      case Passivate =>\n        Behaviors.stopped\n    }\n  }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case PostStop =>\n      context.log.info2(\"Device actor {}-{} stopped\", groupId, deviceId)\n      this\n  }\n\n} Java copysource import java.util.Optional;\n\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.PostStop;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\npublic class Device extends AbstractBehavior<Device.Command> {\n\n  public interface Command {}\n\n  public static final class RecordTemperature implements Command {\n    final long requestId;\n    final double value;\n    final ActorRef<TemperatureRecorded> replyTo;\n\n    public RecordTemperature(long requestId, double value, ActorRef<TemperatureRecorded> replyTo) {\n      this.requestId = requestId;\n      this.value = value;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class TemperatureRecorded {\n    final long requestId;\n\n    public TemperatureRecorded(long requestId) {\n      this.requestId = requestId;\n    }\n  }\n\n  public static final class ReadTemperature implements Command {\n    final long requestId;\n    final ActorRef<RespondTemperature> replyTo;\n\n    public ReadTemperature(long requestId, ActorRef<RespondTemperature> replyTo) {\n      this.requestId = requestId;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class RespondTemperature {\n    final long requestId;\n    final Optional<Double> value;\n\n    public RespondTemperature(long requestId, Optional<Double> value) {\n      this.requestId = requestId;\n      this.value = value;\n    }\n  }\n\n  static enum Passivate implements Command {\n    INSTANCE\n  }\n\n  public static Behavior<Command> create(String groupId, String deviceId) {\n    return Behaviors.setup(context -> new Device(context, groupId, deviceId));\n  }\n\n  private final String groupId;\n  private final String deviceId;\n\n  private Optional<Double> lastTemperatureReading = Optional.empty();\n\n  private Device(ActorContext<Command> context, String groupId, String deviceId) {\n    super(context);\n    this.groupId = groupId;\n    this.deviceId = deviceId;\n\n    context.getLog().info(\"Device actor {}-{} started\", groupId, deviceId);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(RecordTemperature.class, this::onRecordTemperature)\n        .onMessage(ReadTemperature.class, this::onReadTemperature)\n        .onMessage(Passivate.class, m -> Behaviors.stopped())\n        .onSignal(PostStop.class, signal -> onPostStop())\n        .build();\n  }\n\n  private Behavior<Command> onRecordTemperature(RecordTemperature r) {\n    getContext().getLog().info(\"Recorded temperature reading {} with {}\", r.value, r.requestId);\n    lastTemperatureReading = Optional.of(r.value);\n    r.replyTo.tell(new TemperatureRecorded(r.requestId));\n    return this;\n  }\n\n  private Behavior<Command> onReadTemperature(ReadTemperature r) {\n    r.replyTo.tell(new RespondTemperature(r.requestId, lastTemperatureReading));\n    return this;\n  }\n\n  private Behavior<Command> onPostStop() {\n    getContext().getLog().info(\"Device actor {}-{} stopped\", groupId, deviceId);\n    return Behaviors.stopped();\n  }\n}\nWe add two more test cases now. In the first, we test that we get back the list of proper IDs once we have added a few devices. The second test case makes sure that the device ID is properly removed after the device actor has been stopped. The TestProbeTestProbe has a expectTerminated method that we can easily use to assert that the device actor has been terminated.\nScala copysource\"be able to list active devices\" in {\n  val registeredProbe = createTestProbe[DeviceRegistered]()\n  val groupActor = spawn(DeviceGroup(\"group\"))\n\n  groupActor ! RequestTrackDevice(\"group\", \"device1\", registeredProbe.ref)\n  registeredProbe.receiveMessage()\n\n  groupActor ! RequestTrackDevice(\"group\", \"device2\", registeredProbe.ref)\n  registeredProbe.receiveMessage()\n\n  val deviceListProbe = createTestProbe[ReplyDeviceList]()\n  groupActor ! RequestDeviceList(requestId = 0, groupId = \"group\", deviceListProbe.ref)\n  deviceListProbe.expectMessage(ReplyDeviceList(requestId = 0, Set(\"device1\", \"device2\")))\n}\n\n\"be able to list active devices after one shuts down\" in {\n  val registeredProbe = createTestProbe[DeviceRegistered]()\n  val groupActor = spawn(DeviceGroup(\"group\"))\n\n  groupActor ! RequestTrackDevice(\"group\", \"device1\", registeredProbe.ref)\n  val registered1 = registeredProbe.receiveMessage()\n  val toShutDown = registered1.device\n\n  groupActor ! RequestTrackDevice(\"group\", \"device2\", registeredProbe.ref)\n  registeredProbe.receiveMessage()\n\n  val deviceListProbe = createTestProbe[ReplyDeviceList]()\n  groupActor ! RequestDeviceList(requestId = 0, groupId = \"group\", deviceListProbe.ref)\n  deviceListProbe.expectMessage(ReplyDeviceList(requestId = 0, Set(\"device1\", \"device2\")))\n\n  toShutDown ! Passivate\n  registeredProbe.expectTerminated(toShutDown, registeredProbe.remainingOrDefault)\n\n  // using awaitAssert to retry because it might take longer for the groupActor\n  // to see the Terminated, that order is undefined\n  registeredProbe.awaitAssert {\n    groupActor ! RequestDeviceList(requestId = 1, groupId = \"group\", deviceListProbe.ref)\n    deviceListProbe.expectMessage(ReplyDeviceList(requestId = 1, Set(\"device2\")))\n  }\n} Java copysource@Test\npublic void testListActiveDevices() {\n  TestProbe<DeviceRegistered> registeredProbe = testKit.createTestProbe(DeviceRegistered.class);\n  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create(\"group\"));\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device1\", registeredProbe.getRef()));\n  registeredProbe.receiveMessage();\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device2\", registeredProbe.getRef()));\n  registeredProbe.receiveMessage();\n\n  TestProbe<ReplyDeviceList> deviceListProbe = testKit.createTestProbe(ReplyDeviceList.class);\n\n  groupActor.tell(new RequestDeviceList(0L, \"group\", deviceListProbe.getRef()));\n  ReplyDeviceList reply = deviceListProbe.receiveMessage();\n  assertEquals(0L, reply.requestId);\n  assertEquals(Stream.of(\"device1\", \"device2\").collect(Collectors.toSet()), reply.ids);\n}\n\n@Test\npublic void testListActiveDevicesAfterOneShutsDown() {\n  TestProbe<DeviceRegistered> registeredProbe = testKit.createTestProbe(DeviceRegistered.class);\n  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create(\"group\"));\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device1\", registeredProbe.getRef()));\n  DeviceRegistered registered1 = registeredProbe.receiveMessage();\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device2\", registeredProbe.getRef()));\n  DeviceRegistered registered2 = registeredProbe.receiveMessage();\n\n  ActorRef<Device.Command> toShutDown = registered1.device;\n\n  TestProbe<ReplyDeviceList> deviceListProbe = testKit.createTestProbe(ReplyDeviceList.class);\n\n  groupActor.tell(new RequestDeviceList(0L, \"group\", deviceListProbe.getRef()));\n  ReplyDeviceList reply = deviceListProbe.receiveMessage();\n  assertEquals(0L, reply.requestId);\n  assertEquals(Stream.of(\"device1\", \"device2\").collect(Collectors.toSet()), reply.ids);\n\n  toShutDown.tell(Device.Passivate.INSTANCE);\n  registeredProbe.expectTerminated(toShutDown, registeredProbe.getRemainingOrDefault());\n\n  // using awaitAssert to retry because it might take longer for the groupActor\n  // to see the Terminated, that order is undefined\n  registeredProbe.awaitAssert(\n      () -> {\n        groupActor.tell(new RequestDeviceList(1L, \"group\", deviceListProbe.getRef()));\n        ReplyDeviceList r = deviceListProbe.receiveMessage();\n        assertEquals(1L, r.requestId);\n        assertEquals(Stream.of(\"device2\").collect(Collectors.toSet()), r.ids);\n        return null;\n      });\n}","title":"Keeping track of the device actors in the group"},{"location":"/typed/guide/tutorial_4.html#creating-device-manager-actors","text":"Going up to the next level in our hierarchy, we need to create the entry point for our device manager component in the DeviceManager source file. This actor is very similar to the device group actor, but creates device group actors instead of device actors:\nScala copysourceobject DeviceManager {\n  def apply(): Behavior[Command] =\n    Behaviors.setup(context => new DeviceManager(context))\n\n\n  sealed trait Command\n\n  final case class RequestTrackDevice(groupId: String, deviceId: String, replyTo: ActorRef[DeviceRegistered])\n      extends DeviceManager.Command\n      with DeviceGroup.Command\n\n  final case class DeviceRegistered(device: ActorRef[Device.Command])\n\n  final case class RequestDeviceList(requestId: Long, groupId: String, replyTo: ActorRef[ReplyDeviceList])\n      extends DeviceManager.Command\n      with DeviceGroup.Command\n\n  final case class ReplyDeviceList(requestId: Long, ids: Set[String])\n\n  private final case class DeviceGroupTerminated(groupId: String) extends DeviceManager.Command\n}\n\nclass DeviceManager(context: ActorContext[DeviceManager.Command])\n    extends AbstractBehavior[DeviceManager.Command](context) {\n  import DeviceManager._\n\n  var groupIdToActor = Map.empty[String, ActorRef[DeviceGroup.Command]]\n\n  context.log.info(\"DeviceManager started\")\n\n  override def onMessage(msg: Command): Behavior[Command] =\n    msg match {\n      case trackMsg @ RequestTrackDevice(groupId, _, replyTo) =>\n        groupIdToActor.get(groupId) match {\n          case Some(ref) =>\n            ref ! trackMsg\n          case None =>\n            context.log.info(\"Creating device group actor for {}\", groupId)\n            val groupActor = context.spawn(DeviceGroup(groupId), \"group-\" + groupId)\n            context.watchWith(groupActor, DeviceGroupTerminated(groupId))\n            groupActor ! trackMsg\n            groupIdToActor += groupId -> groupActor\n        }\n        this\n\n      case req @ RequestDeviceList(requestId, groupId, replyTo) =>\n        groupIdToActor.get(groupId) match {\n          case Some(ref) =>\n            ref ! req\n          case None =>\n            replyTo ! ReplyDeviceList(requestId, Set.empty)\n        }\n        this\n\n      case DeviceGroupTerminated(groupId) =>\n        context.log.info(\"Device group actor for {} has been terminated\", groupId)\n        groupIdToActor -= groupId\n        this\n    }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case PostStop =>\n      context.log.info(\"DeviceManager stopped\")\n      this\n  }\n\n} Java copysourcepublic class DeviceManager extends AbstractBehavior<DeviceManager.Command> {\n\n  public interface Command {}\n\n  public static final class RequestTrackDevice\n      implements DeviceManager.Command, DeviceGroup.Command {\n    public final String groupId;\n    public final String deviceId;\n    public final ActorRef<DeviceRegistered> replyTo;\n\n    public RequestTrackDevice(String groupId, String deviceId, ActorRef<DeviceRegistered> replyTo) {\n      this.groupId = groupId;\n      this.deviceId = deviceId;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class DeviceRegistered {\n    public final ActorRef<Device.Command> device;\n\n    public DeviceRegistered(ActorRef<Device.Command> device) {\n      this.device = device;\n    }\n  }\n\n  public static final class RequestDeviceList\n      implements DeviceManager.Command, DeviceGroup.Command {\n    final long requestId;\n    final String groupId;\n    final ActorRef<ReplyDeviceList> replyTo;\n\n    public RequestDeviceList(long requestId, String groupId, ActorRef<ReplyDeviceList> replyTo) {\n      this.requestId = requestId;\n      this.groupId = groupId;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class ReplyDeviceList {\n    final long requestId;\n    final Set<String> ids;\n\n    public ReplyDeviceList(long requestId, Set<String> ids) {\n      this.requestId = requestId;\n      this.ids = ids;\n    }\n  }\n\n  private static class DeviceGroupTerminated implements DeviceManager.Command {\n    public final String groupId;\n\n    DeviceGroupTerminated(String groupId) {\n      this.groupId = groupId;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(DeviceManager::new);\n  }\n\n  private final Map<String, ActorRef<DeviceGroup.Command>> groupIdToActor = new HashMap<>();\n\n  private DeviceManager(ActorContext<Command> context) {\n    super(context);\n    context.getLog().info(\"DeviceManager started\");\n  }\n\n  private DeviceManager onTrackDevice(RequestTrackDevice trackMsg) {\n    String groupId = trackMsg.groupId;\n    ActorRef<DeviceGroup.Command> ref = groupIdToActor.get(groupId);\n    if (ref != null) {\n      ref.tell(trackMsg);\n    } else {\n      getContext().getLog().info(\"Creating device group actor for {}\", groupId);\n      ActorRef<DeviceGroup.Command> groupActor =\n          getContext().spawn(DeviceGroup.create(groupId), \"group-\" + groupId);\n      getContext().watchWith(groupActor, new DeviceGroupTerminated(groupId));\n      groupActor.tell(trackMsg);\n      groupIdToActor.put(groupId, groupActor);\n    }\n    return this;\n  }\n\n  private DeviceManager onRequestDeviceList(RequestDeviceList request) {\n    ActorRef<DeviceGroup.Command> ref = groupIdToActor.get(request.groupId);\n    if (ref != null) {\n      ref.tell(request);\n    } else {\n      request.replyTo.tell(new ReplyDeviceList(request.requestId, Collections.emptySet()));\n    }\n    return this;\n  }\n\n  private DeviceManager onTerminated(DeviceGroupTerminated t) {\n    getContext().getLog().info(\"Device group actor for {} has been terminated\", t.groupId);\n    groupIdToActor.remove(t.groupId);\n    return this;\n  }\n\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(RequestTrackDevice.class, this::onTrackDevice)\n        .onMessage(RequestDeviceList.class, this::onRequestDeviceList)\n        .onMessage(DeviceGroupTerminated.class, this::onTerminated)\n        .onSignal(PostStop.class, signal -> onPostStop())\n        .build();\n  }\n\n  private DeviceManager onPostStop() {\n    getContext().getLog().info(\"DeviceManager stopped\");\n    return this;\n  }\n}\nWe leave tests of the device manager as an exercise for you since it is very similar to the tests we have already written for the group actor.","title":"Creating device manager actors"},{"location":"/typed/guide/tutorial_4.html#whats-next-","text":"We have now a hierarchical component for registering and tracking devices and recording measurements. We have seen how to implement different types of conversation patterns, such as:\nRequest-respond (for temperature recordings) Create-on-demand (for registration of devices) Create-watch-terminate (for creating the group and device actor as children)\nIn the next chapter, we will introduce group query capabilities, which will establish a new conversation pattern of scatter-gather. In particular, we will implement the functionality that allows users to query the status of all the devices belonging to a group.","title":"What’s next?"},{"location":"/typed/guide/tutorial_5.html","text":"","title":"Part 5: Querying Device Groups"},{"location":"/typed/guide/tutorial_5.html#part-5-querying-device-groups","text":"","title":"Part 5: Querying Device Groups"},{"location":"/typed/guide/tutorial_5.html#introduction","text":"The conversational patterns that we have seen so far are simple in the sense that they require the actor to keep little or no state. Specifically:\nDevice actors return a reading, which requires no state change Record a temperature, which updates a single field Device Group actors maintain group membership by adding or removing entries from a map\nIn this part, we will use a more complex example. Since homeowners will be interested in the temperatures throughout their home, our goal is to be able to query all of the device actors in a group. Let us start by investigating how such a query API should behave.","title":"Introduction"},{"location":"/typed/guide/tutorial_5.html#dealing-with-possible-scenarios","text":"The very first issue we face is that the membership of a group is dynamic. Each sensor device is represented by an actor that can stop at any time. At the beginning of the query, we can ask all of the existing device actors for the current temperature. However, during the lifecycle of the query:\nA device actor might stop and not be able to respond back with a temperature reading. A new device actor might start up and not be included in the query because we weren’t aware of it.\nThese issues can be addressed in many different ways, but the important point is to settle on the desired behavior. The following works well for our use case:\nWhen a query arrives, the group actor takes a snapshot of the existing device actors and will only ask those actors for the temperature. Actors that start up after the query arrives are ignored. If an actor in the snapshot stops during the query without answering, we will report the fact that it stopped to the sender of the query message.\nApart from device actors coming and going dynamically, some actors might take a long time to answer. For example, they could be stuck in an accidental infinite loop, or fail due to a bug and drop our request. We don’t want the query to continue indefinitely, so we will consider it complete in either of the following cases:\nAll actors in the snapshot have either responded or have confirmed being stopped. We reach a pre-defined deadline.\nGiven these decisions, along with the fact that a device in the snapshot might have just started and not yet received a temperature to record, we can define four states for each device actor, with respect to a temperature query:\nIt has a temperature available: Temperature. It has responded, but has no temperature available yet: TemperatureNotAvailable. It has stopped before answering: DeviceNotAvailable. It did not respond before the deadline: DeviceTimedOut.\nSummarizing these in message types we can add the following to the message protocol:\nScala copysource final case class RequestAllTemperatures(requestId: Long, groupId: String, replyTo: ActorRef[RespondAllTemperatures])\n    extends DeviceGroupQuery.Command\n    with DeviceGroup.Command\n    with DeviceManager.Command\n\nfinal case class RespondAllTemperatures(requestId: Long, temperatures: Map[String, TemperatureReading])\n\nsealed trait TemperatureReading\nfinal case class Temperature(value: Double) extends TemperatureReading\ncase object TemperatureNotAvailable extends TemperatureReading\ncase object DeviceNotAvailable extends TemperatureReading\ncase object DeviceTimedOut extends TemperatureReading Java copysource public static final class RequestAllTemperatures\n    implements DeviceGroupQuery.Command, DeviceGroup.Command, Command {\n\n  final long requestId;\n  final String groupId;\n  final ActorRef<RespondAllTemperatures> replyTo;\n\n  public RequestAllTemperatures(\n      long requestId, String groupId, ActorRef<RespondAllTemperatures> replyTo) {\n    this.requestId = requestId;\n    this.groupId = groupId;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static final class RespondAllTemperatures {\n  final long requestId;\n  final Map<String, TemperatureReading> temperatures;\n\n  public RespondAllTemperatures(long requestId, Map<String, TemperatureReading> temperatures) {\n    this.requestId = requestId;\n    this.temperatures = temperatures;\n  }\n}\n\npublic interface TemperatureReading {}\n\npublic static final class Temperature implements TemperatureReading {\n  public final double value;\n\n  public Temperature(double value) {\n    this.value = value;\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n    if (o == null || getClass() != o.getClass()) return false;\n\n    Temperature that = (Temperature) o;\n\n    return Double.compare(that.value, value) == 0;\n  }\n\n  @Override\n  public int hashCode() {\n    long temp = Double.doubleToLongBits(value);\n    return (int) (temp ^ (temp >>> 32));\n  }\n\n  @Override\n  public String toString() {\n    return \"Temperature{\" + \"value=\" + value + '}';\n  }\n}\n\npublic enum TemperatureNotAvailable implements TemperatureReading {\n  INSTANCE\n}\n\npublic enum DeviceNotAvailable implements TemperatureReading {\n  INSTANCE\n}\n\npublic enum DeviceTimedOut implements TemperatureReading {\n  INSTANCE\n}","title":"Dealing with possible scenarios"},{"location":"/typed/guide/tutorial_5.html#implementing-the-query","text":"One approach for implementing the query involves adding code to the device group actor. However, in practice this can be very cumbersome and error-prone. Remember that when we start a query, we need to take a snapshot of the devices present and start a timer so that we can enforce the deadline. In the meantime, another query can arrive. For the second query we need to keep track of the exact same information but in isolation from the previous query. This would require us to maintain separate mappings between queries and device actors.\nInstead, we will implement a simpler, and superior approach. We will create an actor that represents a single query and that performs the tasks needed to complete the query on behalf of the group actor. So far we have created actors that belonged to classical domain objects, but now, we will create an actor that represents a process or a task rather than an entity. We benefit by keeping our group device actor simple and being able to better test query capability in isolation.","title":"Implementing the query"},{"location":"/typed/guide/tutorial_5.html#defining-the-query-actor","text":"First, we need to design the lifecycle of our query actor. This consists of identifying its initial state, the first action it will take, and the cleanup — if necessary. The query actor will need the following information:\nThe snapshot and IDs of active device actors to query. The ID of the request that started the query (so that we can include it in the reply). The reference of the actor who sent the query. We will send the reply to this actor directly. A deadline that indicates how long the query should wait for replies. Making this a parameter will simplify testing.","title":"Defining the query actor"},{"location":"/typed/guide/tutorial_5.html#scheduling-the-query-timeout","text":"Since we need a way to indicate how long we are willing to wait for responses, it is time to introduce a new Pekko feature that we have not used yet, the built-in scheduler facility. Using Behaviors.withTimersBehaviors.withTimers and startSingleTimerstartSingleTimer to schedule a message that will be sent after a given delay.\nWe need to create a message that represents the query timeout. We create a simple message CollectionTimeout without any parameters for this purpose.\nAt the start of the query, we need to ask each of the device actors for the current temperature. To be able to quickly detect devices that stopped before they got the ReadTemperature message we will also watch each of the actors. This way, we get DeviceTerminated messages for those that stop during the lifetime of the query, so we don’t need to wait until the timeout to mark these as not available.\nPutting this together, the outline of our DeviceGroupQuery actor looks like this:\nScala copysourceobject DeviceGroupQuery {\n\n  def apply(\n      deviceIdToActor: Map[String, ActorRef[Device.Command]],\n      requestId: Long,\n      requester: ActorRef[DeviceManager.RespondAllTemperatures],\n      timeout: FiniteDuration): Behavior[Command] = {\n    Behaviors.setup { context =>\n      Behaviors.withTimers { timers =>\n        new DeviceGroupQuery(deviceIdToActor, requestId, requester, timeout, context, timers)\n      }\n    }\n  }\n\n  trait Command\n\n  private case object CollectionTimeout extends Command\n\n  final case class WrappedRespondTemperature(response: Device.RespondTemperature) extends Command\n\n  private final case class DeviceTerminated(deviceId: String) extends Command\n}\n\nclass DeviceGroupQuery(\n    deviceIdToActor: Map[String, ActorRef[Device.Command]],\n    requestId: Long,\n    requester: ActorRef[DeviceManager.RespondAllTemperatures],\n    timeout: FiniteDuration,\n    context: ActorContext[DeviceGroupQuery.Command],\n    timers: TimerScheduler[DeviceGroupQuery.Command])\n    extends AbstractBehavior[DeviceGroupQuery.Command](context) {\n\n  import DeviceGroupQuery._\n  import DeviceManager.DeviceNotAvailable\n  import DeviceManager.DeviceTimedOut\n  import DeviceManager.RespondAllTemperatures\n  import DeviceManager.Temperature\n  import DeviceManager.TemperatureNotAvailable\n  import DeviceManager.TemperatureReading\n\n  timers.startSingleTimer(CollectionTimeout, CollectionTimeout, timeout)\n\n  private val respondTemperatureAdapter = context.messageAdapter(WrappedRespondTemperature.apply)\n\n\n  deviceIdToActor.foreach {\n    case (deviceId, device) =>\n      context.watchWith(device, DeviceTerminated(deviceId))\n      device ! Device.ReadTemperature(0, respondTemperatureAdapter)\n  }\n\n} Java copysourcepublic class DeviceGroupQuery extends AbstractBehavior<DeviceGroupQuery.Command> {\n\n  public interface Command {}\n\n  private static enum CollectionTimeout implements Command {\n    INSTANCE\n  }\n\n  static class WrappedRespondTemperature implements Command {\n    final Device.RespondTemperature response;\n\n    WrappedRespondTemperature(Device.RespondTemperature response) {\n      this.response = response;\n    }\n  }\n\n  private static class DeviceTerminated implements Command {\n    final String deviceId;\n\n    private DeviceTerminated(String deviceId) {\n      this.deviceId = deviceId;\n    }\n  }\n\n  public static Behavior<Command> create(\n      Map<String, ActorRef<Device.Command>> deviceIdToActor,\n      long requestId,\n      ActorRef<DeviceManager.RespondAllTemperatures> requester,\n      Duration timeout) {\n    return Behaviors.setup(\n        context ->\n            Behaviors.withTimers(\n                timers ->\n                    new DeviceGroupQuery(\n                        deviceIdToActor, requestId, requester, timeout, context, timers)));\n  }\n\n  private final long requestId;\n  private final ActorRef<DeviceManager.RespondAllTemperatures> requester;\n\n  private DeviceGroupQuery(\n      Map<String, ActorRef<Device.Command>> deviceIdToActor,\n      long requestId,\n      ActorRef<DeviceManager.RespondAllTemperatures> requester,\n      Duration timeout,\n      ActorContext<Command> context,\n      TimerScheduler<Command> timers) {\n    super(context);\n    this.requestId = requestId;\n    this.requester = requester;\n\n    timers.startSingleTimer(CollectionTimeout.INSTANCE, timeout);\n\n    ActorRef<Device.RespondTemperature> respondTemperatureAdapter =\n        context.messageAdapter(Device.RespondTemperature.class, WrappedRespondTemperature::new);\n\n    for (Map.Entry<String, ActorRef<Device.Command>> entry : deviceIdToActor.entrySet()) {\n      context.watchWith(entry.getValue(), new DeviceTerminated(entry.getKey()));\n      entry.getValue().tell(new Device.ReadTemperature(0L, respondTemperatureAdapter));\n    }\n    stillWaiting = new HashSet<>(deviceIdToActor.keySet());\n  }\n\n\n}\nNote that we have to convert the RespondTemperature replies from the device actor to the message protocol that the DeviceGroupQuery actor understands, i.e. DeviceGroupQuery.Command. For this we use a messageAdapter that wraps the RespondTemperature in a WrappedRespondTemperature, which extendsimplements DeviceGroupQuery.Command.","title":"Scheduling the query timeout"},{"location":"/typed/guide/tutorial_5.html#tracking-actor-state","text":"The query actor, apart from the pending timer, has one stateful aspect, tracking the set of actors that: have replied, have stopped, or have not replied. We track this state in a var field of an immutable Mapin a mutable HashMap in the actor.\nFor our use case:\nWe keep track of the state with: a Map of already received replies a Set of actors that we still wait on We have three events to act on: We can receive a RespondTemperature message from one of the devices. We can receive a DeviceTerminated message for a device actor that has been stopped in the meantime. We can reach the deadline and receive a CollectionTimeout.\nTo accomplish this, add the following to your DeviceGroupQuery source file:\nScala copysourceprivate var repliesSoFar = Map.empty[String, TemperatureReading]\nprivate var stillWaiting = deviceIdToActor.keySet\n\noverride def onMessage(msg: Command): Behavior[Command] =\n  msg match {\n    case WrappedRespondTemperature(response) => onRespondTemperature(response)\n    case DeviceTerminated(deviceId)          => onDeviceTerminated(deviceId)\n    case CollectionTimeout                   => onCollectionTimout()\n  }\n\nprivate def onRespondTemperature(response: Device.RespondTemperature): Behavior[Command] = {\n  val reading = response.value match {\n    case Some(value) => Temperature(value)\n    case None        => TemperatureNotAvailable\n  }\n\n  val deviceId = response.deviceId\n  repliesSoFar += (deviceId -> reading)\n  stillWaiting -= deviceId\n\n  respondWhenAllCollected()\n}\n\nprivate def onDeviceTerminated(deviceId: String): Behavior[Command] = {\n  if (stillWaiting(deviceId)) {\n    repliesSoFar += (deviceId -> DeviceNotAvailable)\n    stillWaiting -= deviceId\n  }\n  respondWhenAllCollected()\n}\n\nprivate def onCollectionTimout(): Behavior[Command] = {\n  repliesSoFar ++= stillWaiting.map(deviceId => deviceId -> DeviceTimedOut)\n  stillWaiting = Set.empty\n  respondWhenAllCollected()\n} Java copysourceprivate Map<String, DeviceManager.TemperatureReading> repliesSoFar = new HashMap<>();\nprivate final Set<String> stillWaiting;\n\n@Override\npublic Receive<Command> createReceive() {\n  return newReceiveBuilder()\n      .onMessage(WrappedRespondTemperature.class, this::onRespondTemperature)\n      .onMessage(DeviceTerminated.class, this::onDeviceTerminated)\n      .onMessage(CollectionTimeout.class, this::onCollectionTimeout)\n      .build();\n}\n\nprivate Behavior<Command> onRespondTemperature(WrappedRespondTemperature r) {\n  DeviceManager.TemperatureReading reading =\n      r.response\n          .value\n          .map(v -> (DeviceManager.TemperatureReading) new DeviceManager.Temperature(v))\n          .orElse(DeviceManager.TemperatureNotAvailable.INSTANCE);\n\n  String deviceId = r.response.deviceId;\n  repliesSoFar.put(deviceId, reading);\n  stillWaiting.remove(deviceId);\n\n  return respondWhenAllCollected();\n}\n\nprivate Behavior<Command> onDeviceTerminated(DeviceTerminated terminated) {\n  if (stillWaiting.contains(terminated.deviceId)) {\n    repliesSoFar.put(terminated.deviceId, DeviceManager.DeviceNotAvailable.INSTANCE);\n    stillWaiting.remove(terminated.deviceId);\n  }\n  return respondWhenAllCollected();\n}\n\nprivate Behavior<Command> onCollectionTimeout(CollectionTimeout timeout) {\n  for (String deviceId : stillWaiting) {\n    repliesSoFar.put(deviceId, DeviceManager.DeviceTimedOut.INSTANCE);\n  }\n  stillWaiting.clear();\n  return respondWhenAllCollected();\n}\nFor RespondTemperature and DeviceTerminated we keep track of the replies by updating repliesSoFar and remove the actor from stillWaiting. For this, we can use the actor’s identifier already present in the DeviceTerminated message. For our RespondTemperature message we will need to add this information as follows:\nScala copysourcefinal case class RespondTemperature(requestId: Long, deviceId: String, value: Option[Double]) Java copysourcepublic static final class RespondTemperature {\n  final long requestId;\n  final String deviceId;\n  final Optional<Double> value;\n\n  public RespondTemperature(long requestId, String deviceId, Optional<Double> value) {\n    this.requestId = requestId;\n    this.deviceId = deviceId;\n    this.value = value;\n  }\n}\nAnd:\nScala copysourcecase ReadTemperature(id, replyTo) =>\n  replyTo ! RespondTemperature(id, deviceId, lastTemperatureReading)\n  this Java copysourceprivate Behavior<Command> onReadTemperature(ReadTemperature r) {\n  r.replyTo.tell(new RespondTemperature(r.requestId, deviceId, lastTemperatureReading));\n  return this;\n}\nAfter processing each message we delegate to a method respondWhenAllCollected, which we will discuss soon.\nIn the case of timeout, we need to take all the actors that have not yet replied (the members of the set stillWaiting) and put a DeviceTimedOut as the status in the final reply.\nWe now have to figure out what to do in respondWhenAllCollected. First, we need to record the new result in the map repliesSoFar and remove the actor from stillWaiting. The next step is to check if there are any remaining actors we are waiting for. If there is none, we send the result of the query to the original requester and stop the query actor. Otherwise, we need to update the repliesSoFar and stillWaiting structures and wait for more messages.\nWith all this knowledge, we can create the respondWhenAllCollected method:\nScala copysourceprivate def respondWhenAllCollected(): Behavior[Command] = {\n  if (stillWaiting.isEmpty) {\n    requester ! RespondAllTemperatures(requestId, repliesSoFar)\n    Behaviors.stopped\n  } else {\n    this\n  }\n} Java copysourceprivate Behavior<Command> respondWhenAllCollected() {\n  if (stillWaiting.isEmpty()) {\n    requester.tell(new DeviceManager.RespondAllTemperatures(requestId, repliesSoFar));\n    return Behaviors.stopped();\n  } else {\n    return this;\n  }\n}\nOur query actor is now done:\nScala copysourceobject DeviceGroupQuery {\n\n  def apply(\n      deviceIdToActor: Map[String, ActorRef[Device.Command]],\n      requestId: Long,\n      requester: ActorRef[DeviceManager.RespondAllTemperatures],\n      timeout: FiniteDuration): Behavior[Command] = {\n    Behaviors.setup { context =>\n      Behaviors.withTimers { timers =>\n        new DeviceGroupQuery(deviceIdToActor, requestId, requester, timeout, context, timers)\n      }\n    }\n  }\n\n  trait Command\n\n  private case object CollectionTimeout extends Command\n\n  final case class WrappedRespondTemperature(response: Device.RespondTemperature) extends Command\n\n  private final case class DeviceTerminated(deviceId: String) extends Command\n}\n\nclass DeviceGroupQuery(\n    deviceIdToActor: Map[String, ActorRef[Device.Command]],\n    requestId: Long,\n    requester: ActorRef[DeviceManager.RespondAllTemperatures],\n    timeout: FiniteDuration,\n    context: ActorContext[DeviceGroupQuery.Command],\n    timers: TimerScheduler[DeviceGroupQuery.Command])\n    extends AbstractBehavior[DeviceGroupQuery.Command](context) {\n\n  import DeviceGroupQuery._\n  import DeviceManager.DeviceNotAvailable\n  import DeviceManager.DeviceTimedOut\n  import DeviceManager.RespondAllTemperatures\n  import DeviceManager.Temperature\n  import DeviceManager.TemperatureNotAvailable\n  import DeviceManager.TemperatureReading\n\n  timers.startSingleTimer(CollectionTimeout, CollectionTimeout, timeout)\n\n  private val respondTemperatureAdapter = context.messageAdapter(WrappedRespondTemperature.apply)\n\n  private var repliesSoFar = Map.empty[String, TemperatureReading]\n  private var stillWaiting = deviceIdToActor.keySet\n\n\n  deviceIdToActor.foreach {\n    case (deviceId, device) =>\n      context.watchWith(device, DeviceTerminated(deviceId))\n      device ! Device.ReadTemperature(0, respondTemperatureAdapter)\n  }\n\n  override def onMessage(msg: Command): Behavior[Command] =\n    msg match {\n      case WrappedRespondTemperature(response) => onRespondTemperature(response)\n      case DeviceTerminated(deviceId)          => onDeviceTerminated(deviceId)\n      case CollectionTimeout                   => onCollectionTimout()\n    }\n\n  private def onRespondTemperature(response: Device.RespondTemperature): Behavior[Command] = {\n    val reading = response.value match {\n      case Some(value) => Temperature(value)\n      case None        => TemperatureNotAvailable\n    }\n\n    val deviceId = response.deviceId\n    repliesSoFar += (deviceId -> reading)\n    stillWaiting -= deviceId\n\n    respondWhenAllCollected()\n  }\n\n  private def onDeviceTerminated(deviceId: String): Behavior[Command] = {\n    if (stillWaiting(deviceId)) {\n      repliesSoFar += (deviceId -> DeviceNotAvailable)\n      stillWaiting -= deviceId\n    }\n    respondWhenAllCollected()\n  }\n\n  private def onCollectionTimout(): Behavior[Command] = {\n    repliesSoFar ++= stillWaiting.map(deviceId => deviceId -> DeviceTimedOut)\n    stillWaiting = Set.empty\n    respondWhenAllCollected()\n  }\n\n  private def respondWhenAllCollected(): Behavior[Command] = {\n    if (stillWaiting.isEmpty) {\n      requester ! RespondAllTemperatures(requestId, repliesSoFar)\n      Behaviors.stopped\n    } else {\n      this\n    }\n  }\n} Java copysourcepublic class DeviceGroupQuery extends AbstractBehavior<DeviceGroupQuery.Command> {\n\n  public interface Command {}\n\n  private static enum CollectionTimeout implements Command {\n    INSTANCE\n  }\n\n  static class WrappedRespondTemperature implements Command {\n    final Device.RespondTemperature response;\n\n    WrappedRespondTemperature(Device.RespondTemperature response) {\n      this.response = response;\n    }\n  }\n\n  private static class DeviceTerminated implements Command {\n    final String deviceId;\n\n    private DeviceTerminated(String deviceId) {\n      this.deviceId = deviceId;\n    }\n  }\n\n  public static Behavior<Command> create(\n      Map<String, ActorRef<Device.Command>> deviceIdToActor,\n      long requestId,\n      ActorRef<DeviceManager.RespondAllTemperatures> requester,\n      Duration timeout) {\n    return Behaviors.setup(\n        context ->\n            Behaviors.withTimers(\n                timers ->\n                    new DeviceGroupQuery(\n                        deviceIdToActor, requestId, requester, timeout, context, timers)));\n  }\n\n  private final long requestId;\n  private final ActorRef<DeviceManager.RespondAllTemperatures> requester;\n  private Map<String, DeviceManager.TemperatureReading> repliesSoFar = new HashMap<>();\n  private final Set<String> stillWaiting;\n\n\n  private DeviceGroupQuery(\n      Map<String, ActorRef<Device.Command>> deviceIdToActor,\n      long requestId,\n      ActorRef<DeviceManager.RespondAllTemperatures> requester,\n      Duration timeout,\n      ActorContext<Command> context,\n      TimerScheduler<Command> timers) {\n    super(context);\n    this.requestId = requestId;\n    this.requester = requester;\n\n    timers.startSingleTimer(CollectionTimeout.INSTANCE, timeout);\n\n    ActorRef<Device.RespondTemperature> respondTemperatureAdapter =\n        context.messageAdapter(Device.RespondTemperature.class, WrappedRespondTemperature::new);\n\n    for (Map.Entry<String, ActorRef<Device.Command>> entry : deviceIdToActor.entrySet()) {\n      context.watchWith(entry.getValue(), new DeviceTerminated(entry.getKey()));\n      entry.getValue().tell(new Device.ReadTemperature(0L, respondTemperatureAdapter));\n    }\n    stillWaiting = new HashSet<>(deviceIdToActor.keySet());\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(WrappedRespondTemperature.class, this::onRespondTemperature)\n        .onMessage(DeviceTerminated.class, this::onDeviceTerminated)\n        .onMessage(CollectionTimeout.class, this::onCollectionTimeout)\n        .build();\n  }\n\n  private Behavior<Command> onRespondTemperature(WrappedRespondTemperature r) {\n    DeviceManager.TemperatureReading reading =\n        r.response\n            .value\n            .map(v -> (DeviceManager.TemperatureReading) new DeviceManager.Temperature(v))\n            .orElse(DeviceManager.TemperatureNotAvailable.INSTANCE);\n\n    String deviceId = r.response.deviceId;\n    repliesSoFar.put(deviceId, reading);\n    stillWaiting.remove(deviceId);\n\n    return respondWhenAllCollected();\n  }\n\n  private Behavior<Command> onDeviceTerminated(DeviceTerminated terminated) {\n    if (stillWaiting.contains(terminated.deviceId)) {\n      repliesSoFar.put(terminated.deviceId, DeviceManager.DeviceNotAvailable.INSTANCE);\n      stillWaiting.remove(terminated.deviceId);\n    }\n    return respondWhenAllCollected();\n  }\n\n  private Behavior<Command> onCollectionTimeout(CollectionTimeout timeout) {\n    for (String deviceId : stillWaiting) {\n      repliesSoFar.put(deviceId, DeviceManager.DeviceTimedOut.INSTANCE);\n    }\n    stillWaiting.clear();\n    return respondWhenAllCollected();\n  }\n\n  private Behavior<Command> respondWhenAllCollected() {\n    if (stillWaiting.isEmpty()) {\n      requester.tell(new DeviceManager.RespondAllTemperatures(requestId, repliesSoFar));\n      return Behaviors.stopped();\n    } else {\n      return this;\n    }\n  }\n\n}","title":"Tracking actor state"},{"location":"/typed/guide/tutorial_5.html#testing-the-query-actor","text":"Now let’s verify the correctness of the query actor implementation. There are various scenarios we need to test individually to make sure everything works as expected. To be able to do this, we need to simulate the device actors somehow to exercise various normal or failure scenarios. Thankfully we took the list of collaborators (actually a Map) as a parameter to the query actor, so we can pass in TestProbeTestProbe references. In our first test, we try out the case when there are two devices and both report a temperature:\nScala copysource\"return temperature value for working devices\" in {\n  val requester = createTestProbe[RespondAllTemperatures]()\n\n  val device1 = createTestProbe[Command]()\n  val device2 = createTestProbe[Command]()\n\n  val deviceIdToActor = Map(\"device1\" -> device1.ref, \"device2\" -> device2.ref)\n\n  val queryActor =\n    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 3.seconds))\n\n  device1.expectMessageType[Device.ReadTemperature]\n  device2.expectMessageType[Device.ReadTemperature]\n\n  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, \"device1\", Some(1.0)))\n  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, \"device2\", Some(2.0)))\n\n  requester.expectMessage(\n    RespondAllTemperatures(\n      requestId = 1,\n      temperatures = Map(\"device1\" -> Temperature(1.0), \"device2\" -> Temperature(2.0))))\n} Java copysource@Test\npublic void testReturnTemperatureValueForWorkingDevices() {\n  TestProbe<RespondAllTemperatures> requester =\n      testKit.createTestProbe(RespondAllTemperatures.class);\n  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);\n  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);\n\n  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n  deviceIdToActor.put(\"device1\", device1.getRef());\n  deviceIdToActor.put(\"device2\", device2.getRef());\n\n  ActorRef<DeviceGroupQuery.Command> queryActor =\n      testKit.spawn(\n          DeviceGroupQuery.create(\n              deviceIdToActor, 1L, requester.getRef(), Duration.ofSeconds(3)));\n\n  device1.expectMessageClass(Device.ReadTemperature.class);\n  device2.expectMessageClass(Device.ReadTemperature.class);\n\n  queryActor.tell(\n      new DeviceGroupQuery.WrappedRespondTemperature(\n          new Device.RespondTemperature(0L, \"device1\", Optional.of(1.0))));\n\n  queryActor.tell(\n      new DeviceGroupQuery.WrappedRespondTemperature(\n          new Device.RespondTemperature(0L, \"device2\", Optional.of(2.0))));\n\n  RespondAllTemperatures response = requester.receiveMessage();\n  assertEquals(1L, response.requestId);\n\n  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();\n  expectedTemperatures.put(\"device1\", new Temperature(1.0));\n  expectedTemperatures.put(\"device2\", new Temperature(2.0));\n\n  assertEquals(expectedTemperatures, response.temperatures);\n}\nThat was the happy case, but we know that sometimes devices cannot provide a temperature measurement. This scenario is just slightly different from the previous:\nScala copysource\"return TemperatureNotAvailable for devices with no readings\" in {\n  val requester = createTestProbe[RespondAllTemperatures]()\n\n  val device1 = createTestProbe[Command]()\n  val device2 = createTestProbe[Command]()\n\n  val deviceIdToActor = Map(\"device1\" -> device1.ref, \"device2\" -> device2.ref)\n\n  val queryActor =\n    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 3.seconds))\n\n  device1.expectMessageType[Device.ReadTemperature]\n  device2.expectMessageType[Device.ReadTemperature]\n\n  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, \"device1\", None))\n  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, \"device2\", Some(2.0)))\n\n  requester.expectMessage(\n    RespondAllTemperatures(\n      requestId = 1,\n      temperatures = Map(\"device1\" -> TemperatureNotAvailable, \"device2\" -> Temperature(2.0))))\n} Java copysource@Test\npublic void testReturnTemperatureNotAvailableForDevicesWithNoReadings() {\n  TestProbe<RespondAllTemperatures> requester =\n      testKit.createTestProbe(RespondAllTemperatures.class);\n  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);\n  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);\n\n  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n  deviceIdToActor.put(\"device1\", device1.getRef());\n  deviceIdToActor.put(\"device2\", device2.getRef());\n\n  ActorRef<DeviceGroupQuery.Command> queryActor =\n      testKit.spawn(\n          DeviceGroupQuery.create(\n              deviceIdToActor, 1L, requester.getRef(), Duration.ofSeconds(3)));\n\n  assertEquals(0L, device1.expectMessageClass(Device.ReadTemperature.class).requestId);\n  assertEquals(0L, device2.expectMessageClass(Device.ReadTemperature.class).requestId);\n\n  queryActor.tell(\n      new DeviceGroupQuery.WrappedRespondTemperature(\n          new Device.RespondTemperature(0L, \"device1\", Optional.empty())));\n\n  queryActor.tell(\n      new DeviceGroupQuery.WrappedRespondTemperature(\n          new Device.RespondTemperature(0L, \"device2\", Optional.of(2.0))));\n\n  RespondAllTemperatures response = requester.receiveMessage();\n  assertEquals(1L, response.requestId);\n\n  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();\n  expectedTemperatures.put(\"device1\", TemperatureNotAvailable.INSTANCE);\n  expectedTemperatures.put(\"device2\", new Temperature(2.0));\n\n  assertEquals(expectedTemperatures, response.temperatures);\n}\nWe also know, that sometimes device actors stop before answering:\nScala copysource\"return DeviceNotAvailable if device stops before answering\" in {\n  val requester = createTestProbe[RespondAllTemperatures]()\n\n  val device1 = createTestProbe[Command]()\n  val device2 = createTestProbe[Command]()\n\n  val deviceIdToActor = Map(\"device1\" -> device1.ref, \"device2\" -> device2.ref)\n\n  val queryActor =\n    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 3.seconds))\n\n  device1.expectMessageType[Device.ReadTemperature]\n  device2.expectMessageType[Device.ReadTemperature]\n\n  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, \"device1\", Some(2.0)))\n\n  device2.stop()\n\n  requester.expectMessage(\n    RespondAllTemperatures(\n      requestId = 1,\n      temperatures = Map(\"device1\" -> Temperature(2.0), \"device2\" -> DeviceNotAvailable)))\n} Java copysource@Test\npublic void testReturnDeviceNotAvailableIfDeviceStopsBeforeAnswering() {\n  TestProbe<RespondAllTemperatures> requester =\n      testKit.createTestProbe(RespondAllTemperatures.class);\n  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);\n  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);\n\n  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n  deviceIdToActor.put(\"device1\", device1.getRef());\n  deviceIdToActor.put(\"device2\", device2.getRef());\n\n  ActorRef<DeviceGroupQuery.Command> queryActor =\n      testKit.spawn(\n          DeviceGroupQuery.create(\n              deviceIdToActor, 1L, requester.getRef(), Duration.ofSeconds(3)));\n\n  assertEquals(0L, device1.expectMessageClass(Device.ReadTemperature.class).requestId);\n  assertEquals(0L, device2.expectMessageClass(Device.ReadTemperature.class).requestId);\n\n  queryActor.tell(\n      new DeviceGroupQuery.WrappedRespondTemperature(\n          new Device.RespondTemperature(0L, \"device1\", Optional.of(1.0))));\n\n  device2.stop();\n\n  RespondAllTemperatures response = requester.receiveMessage();\n  assertEquals(1L, response.requestId);\n\n  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();\n  expectedTemperatures.put(\"device1\", new Temperature(1.0));\n  expectedTemperatures.put(\"device2\", DeviceNotAvailable.INSTANCE);\n\n  assertEquals(expectedTemperatures, response.temperatures);\n}\nIf you remember, there is another case related to device actors stopping. It is possible that we get a normal reply from a device actor, but then receive a TerminatedTerminated for the same actor later. In this case, we would like to keep the first reply and not mark the device as DeviceNotAvailable. We should test this, too:\nScala copysource\"return temperature reading even if device stops after answering\" in {\n  val requester = createTestProbe[RespondAllTemperatures]()\n\n  val device1 = createTestProbe[Command]()\n  val device2 = createTestProbe[Command]()\n\n  val deviceIdToActor = Map(\"device1\" -> device1.ref, \"device2\" -> device2.ref)\n\n  val queryActor =\n    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 3.seconds))\n\n  device1.expectMessageType[Device.ReadTemperature]\n  device2.expectMessageType[Device.ReadTemperature]\n\n  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, \"device1\", Some(1.0)))\n  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, \"device2\", Some(2.0)))\n\n  device2.stop()\n\n  requester.expectMessage(\n    RespondAllTemperatures(\n      requestId = 1,\n      temperatures = Map(\"device1\" -> Temperature(1.0), \"device2\" -> Temperature(2.0))))\n} Java copysource@Test\npublic void testReturnTemperatureReadingEvenIfDeviceStopsAfterAnswering() {\n  TestProbe<RespondAllTemperatures> requester =\n      testKit.createTestProbe(RespondAllTemperatures.class);\n  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);\n  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);\n\n  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n  deviceIdToActor.put(\"device1\", device1.getRef());\n  deviceIdToActor.put(\"device2\", device2.getRef());\n\n  ActorRef<DeviceGroupQuery.Command> queryActor =\n      testKit.spawn(\n          DeviceGroupQuery.create(\n              deviceIdToActor, 1L, requester.getRef(), Duration.ofSeconds(3)));\n\n  assertEquals(0L, device1.expectMessageClass(Device.ReadTemperature.class).requestId);\n  assertEquals(0L, device2.expectMessageClass(Device.ReadTemperature.class).requestId);\n\n  queryActor.tell(\n      new DeviceGroupQuery.WrappedRespondTemperature(\n          new Device.RespondTemperature(0L, \"device1\", Optional.of(1.0))));\n\n  queryActor.tell(\n      new DeviceGroupQuery.WrappedRespondTemperature(\n          new Device.RespondTemperature(0L, \"device2\", Optional.of(2.0))));\n\n  device2.stop();\n\n  RespondAllTemperatures response = requester.receiveMessage();\n  assertEquals(1L, response.requestId);\n\n  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();\n  expectedTemperatures.put(\"device1\", new Temperature(1.0));\n  expectedTemperatures.put(\"device2\", new Temperature(2.0));\n\n  assertEquals(expectedTemperatures, response.temperatures);\n}\nThe final case is when not all devices respond in time. To keep our test relatively fast, we will construct the DeviceGroupQuery actor with a smaller timeout:\nScala copysource\"return DeviceTimedOut if device does not answer in time\" in {\n  val requester = createTestProbe[RespondAllTemperatures]()\n\n  val device1 = createTestProbe[Command]()\n  val device2 = createTestProbe[Command]()\n\n  val deviceIdToActor = Map(\"device1\" -> device1.ref, \"device2\" -> device2.ref)\n\n  val queryActor =\n    spawn(DeviceGroupQuery(deviceIdToActor, requestId = 1, requester = requester.ref, timeout = 200.millis))\n\n  device1.expectMessageType[Device.ReadTemperature]\n  device2.expectMessageType[Device.ReadTemperature]\n\n  queryActor ! WrappedRespondTemperature(Device.RespondTemperature(requestId = 0, \"device1\", Some(1.0)))\n\n  // no reply from device2\n\n  requester.expectMessage(\n    RespondAllTemperatures(\n      requestId = 1,\n      temperatures = Map(\"device1\" -> Temperature(1.0), \"device2\" -> DeviceTimedOut)))\n} Java copysource@Test\npublic void testReturnDeviceTimedOutIfDeviceDoesNotAnswerInTime() {\n  TestProbe<RespondAllTemperatures> requester =\n      testKit.createTestProbe(RespondAllTemperatures.class);\n  TestProbe<Device.Command> device1 = testKit.createTestProbe(Device.Command.class);\n  TestProbe<Device.Command> device2 = testKit.createTestProbe(Device.Command.class);\n\n  Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n  deviceIdToActor.put(\"device1\", device1.getRef());\n  deviceIdToActor.put(\"device2\", device2.getRef());\n\n  ActorRef<DeviceGroupQuery.Command> queryActor =\n      testKit.spawn(\n          DeviceGroupQuery.create(\n              deviceIdToActor, 1L, requester.getRef(), Duration.ofMillis(200)));\n\n  assertEquals(0L, device1.expectMessageClass(Device.ReadTemperature.class).requestId);\n  assertEquals(0L, device2.expectMessageClass(Device.ReadTemperature.class).requestId);\n\n  queryActor.tell(\n      new DeviceGroupQuery.WrappedRespondTemperature(\n          new Device.RespondTemperature(0L, \"device1\", Optional.of(1.0))));\n\n  // no reply from device2\n\n  RespondAllTemperatures response = requester.receiveMessage();\n  assertEquals(1L, response.requestId);\n\n  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();\n  expectedTemperatures.put(\"device1\", new Temperature(1.0));\n  expectedTemperatures.put(\"device2\", DeviceTimedOut.INSTANCE);\n\n  assertEquals(expectedTemperatures, response.temperatures);\n}\nOur query works as expected now, it is time to include this new functionality in the DeviceGroup actor now.","title":"Testing the query actor"},{"location":"/typed/guide/tutorial_5.html#adding-query-capability-to-the-group","text":"Including the query feature in the group actor is fairly simple now. We did all the heavy lifting in the query actor itself, the group actor only needs to create it with the right initial parameters and nothing else.\nScala copysourceclass DeviceGroup(context: ActorContext[DeviceGroup.Command], groupId: String)\n    extends AbstractBehavior[DeviceGroup.Command](context) {\n  import DeviceGroup._\n  import DeviceManager.{\n    DeviceRegistered,\n    ReplyDeviceList,\n    RequestAllTemperatures,\n    RequestDeviceList,\n    RequestTrackDevice\n  }\n\n  private var deviceIdToActor = Map.empty[String, ActorRef[Device.Command]]\n\n  context.log.info(\"DeviceGroup {} started\", groupId)\n\n  override def onMessage(msg: Command): Behavior[Command] =\n    msg match {\n      // ... other cases omitted\n\n      case RequestAllTemperatures(requestId, gId, replyTo) =>\n        if (gId == groupId) {\n          context.spawnAnonymous(\n            DeviceGroupQuery(deviceIdToActor, requestId = requestId, requester = replyTo, 3.seconds))\n          this\n        } else\n          Behaviors.unhandled\n    }\n\n  override def onSignal: PartialFunction[Signal, Behavior[Command]] = {\n    case PostStop =>\n      context.log.info(\"DeviceGroup {} stopped\", groupId)\n      this\n  }\n} Java copysourcepublic class DeviceGroup extends AbstractBehavior<DeviceGroup.Command> {\n\n  public interface Command {}\n\n  private class DeviceTerminated implements Command {\n    public final ActorRef<Device.Command> device;\n    public final String groupId;\n    public final String deviceId;\n\n    DeviceTerminated(ActorRef<Device.Command> device, String groupId, String deviceId) {\n      this.device = device;\n      this.groupId = groupId;\n      this.deviceId = deviceId;\n    }\n  }\n\n  public static Behavior<Command> create(String groupId) {\n    return Behaviors.setup(context -> new DeviceGroup(context, groupId));\n  }\n\n  private final String groupId;\n  private final Map<String, ActorRef<Device.Command>> deviceIdToActor = new HashMap<>();\n\n  private DeviceGroup(ActorContext<Command> context, String groupId) {\n    super(context);\n    this.groupId = groupId;\n    context.getLog().info(\"DeviceGroup {} started\", groupId);\n  }\n\n\n  private DeviceGroup onAllTemperatures(DeviceManager.RequestAllTemperatures r) {\n    // since Java collections are mutable, we want to avoid sharing them between actors (since\n    // multiple Actors (threads)\n    // modifying the same mutable data-structure is not safe), and perform a defensive copy of the\n    // mutable map:\n    //\n    // Feel free to use your favourite immutable data-structures library with Pekko in Java\n    // applications!\n    Map<String, ActorRef<Device.Command>> deviceIdToActorCopy = new HashMap<>(this.deviceIdToActor);\n\n    getContext()\n        .spawnAnonymous(\n            DeviceGroupQuery.create(\n                deviceIdToActorCopy, r.requestId, r.replyTo, Duration.ofSeconds(3)));\n\n    return this;\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        // ... other cases omitted\n        .onMessage(\n            DeviceManager.RequestAllTemperatures.class,\n            r -> r.groupId.equals(groupId),\n            this::onAllTemperatures)\n        .build();\n  }\n}\nIt is probably worth restating what we said at the beginning of the chapter. By keeping the temporary state that is only relevant to the query itself in a separate actor we keep the group actor implementation very simple. It delegates everything to child actors and therefore does not have to keep state that is not relevant to its core business. Also, multiple queries can now run parallel to each other, in fact, as many as needed. In our case querying an individual device actor is a fast operation, but if this were not the case, for example, because the remote sensors need to be contacted over the network, this design would significantly improve throughput.\nWe close this chapter by testing that everything works together. This test is a variant of the previous ones, now exercising the group query feature:\nScala copysource\"be able to collect temperatures from all active devices\" in {\n  val registeredProbe = createTestProbe[DeviceRegistered]()\n  val groupActor = spawn(DeviceGroup(\"group\"))\n\n  groupActor ! RequestTrackDevice(\"group\", \"device1\", registeredProbe.ref)\n  val deviceActor1 = registeredProbe.receiveMessage().device\n\n  groupActor ! RequestTrackDevice(\"group\", \"device2\", registeredProbe.ref)\n  val deviceActor2 = registeredProbe.receiveMessage().device\n\n  groupActor ! RequestTrackDevice(\"group\", \"device3\", registeredProbe.ref)\n  registeredProbe.receiveMessage()\n\n  // Check that the device actors are working\n  val recordProbe = createTestProbe[TemperatureRecorded]()\n  deviceActor1 ! RecordTemperature(requestId = 0, 1.0, recordProbe.ref)\n  recordProbe.expectMessage(TemperatureRecorded(requestId = 0))\n  deviceActor2 ! RecordTemperature(requestId = 1, 2.0, recordProbe.ref)\n  recordProbe.expectMessage(TemperatureRecorded(requestId = 1))\n  // No temperature for device3\n\n  val allTempProbe = createTestProbe[RespondAllTemperatures]()\n  groupActor ! RequestAllTemperatures(requestId = 0, groupId = \"group\", allTempProbe.ref)\n  allTempProbe.expectMessage(\n    RespondAllTemperatures(\n      requestId = 0,\n      temperatures =\n        Map(\"device1\" -> Temperature(1.0), \"device2\" -> Temperature(2.0), \"device3\" -> TemperatureNotAvailable)))\n} Java copysource@Test\npublic void testCollectTemperaturesFromAllActiveDevices() {\n  TestProbe<DeviceRegistered> registeredProbe = testKit.createTestProbe(DeviceRegistered.class);\n  ActorRef<DeviceGroup.Command> groupActor = testKit.spawn(DeviceGroup.create(\"group\"));\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device1\", registeredProbe.getRef()));\n  ActorRef<Device.Command> deviceActor1 = registeredProbe.receiveMessage().device;\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device2\", registeredProbe.getRef()));\n  ActorRef<Device.Command> deviceActor2 = registeredProbe.receiveMessage().device;\n\n  groupActor.tell(new RequestTrackDevice(\"group\", \"device3\", registeredProbe.getRef()));\n  ActorRef<Device.Command> deviceActor3 = registeredProbe.receiveMessage().device;\n\n  // Check that the device actors are working\n  TestProbe<Device.TemperatureRecorded> recordProbe =\n      testKit.createTestProbe(Device.TemperatureRecorded.class);\n  deviceActor1.tell(new Device.RecordTemperature(0L, 1.0, recordProbe.getRef()));\n  assertEquals(0L, recordProbe.receiveMessage().requestId);\n  deviceActor2.tell(new Device.RecordTemperature(1L, 2.0, recordProbe.getRef()));\n  assertEquals(1L, recordProbe.receiveMessage().requestId);\n  // No temperature for device 3\n\n  TestProbe<RespondAllTemperatures> allTempProbe =\n      testKit.createTestProbe(RespondAllTemperatures.class);\n  groupActor.tell(new RequestAllTemperatures(0L, \"group\", allTempProbe.getRef()));\n  RespondAllTemperatures response = allTempProbe.receiveMessage();\n  assertEquals(0L, response.requestId);\n\n  Map<String, TemperatureReading> expectedTemperatures = new HashMap<>();\n  expectedTemperatures.put(\"device1\", new Temperature(1.0));\n  expectedTemperatures.put(\"device2\", new Temperature(2.0));\n  expectedTemperatures.put(\"device3\", TemperatureNotAvailable.INSTANCE);\n\n  assertEquals(expectedTemperatures, response.temperatures);\n}","title":"Adding query capability to the group"},{"location":"/typed/guide/tutorial_5.html#summary","text":"In the context of the IoT system, this guide introduced the following concepts, among others. You can follow the links to review them if necessary:\nThe hierarchy of actors and their lifecycle The importance of designing messages for flexibility How to watch and stop actors, if necessary","title":"Summary"},{"location":"/typed/guide/tutorial_5.html#whats-next-","text":"To continue your journey with Pekko, we recommend:\nFor some additional background, and detail, read the rest of the reference documentation and check out some of the books and videos on Pekko. If you are interested in functional programming, read how actors can be defined in a functional style. In this guide the object-oriented style was used, but you can mix both as you like.\nTo get from this guide to a complete application you would likely need to provide either an UI or an API. For this we recommend that you look at the following technologies and see what fits you:\nMicroservices with Pekko tutorial illustrates how to implement an Event Sourced CQRS application with Pekko Persistence and Pekko Projections Pekko HTTP is a HTTP server and client library, making it possible to publish and consume HTTP endpoints","title":"What’s Next?"},{"location":"/general/index.html","text":"","title":"General Concepts"},{"location":"/general/index.html#general-concepts","text":"Terminology, Concepts Concurrency vs. Parallelism Asynchronous vs. Synchronous Non-blocking vs. Blocking Deadlock vs. Starvation vs. Live-lock Race Condition Non-blocking Guarantees (Progress Conditions) Recommended literature Actor Systems Hierarchical Structure Configuration Container Actor Best Practices What you should not concern yourself with Terminating ActorSystem What is an Actor? Actor Reference State Behavior Mailbox Child Actors Supervisor Strategy When an Actor Terminates Supervision and Monitoring What Supervision Means The Top-Level actors What Restarting Means What Lifecycle Monitoring Means Actors and exceptions Actor References, Paths and Addresses What is an Actor Reference What is an Actor Path? How are Actor References obtained? Actor Reference and Path Equality Reusing Actor Paths What is the Address part used for? Top-Level Scopes for Actor Paths Location Transparency Distributed by Default Ways in which Transparency is Broken Peer-to-Peer vs. Client-Server Marking Points for Scaling Up with Routers Apache Pekko and the Java Memory Model The Java Memory Model Actors and the Java Memory Model Futures and the Java Memory Model Actors and shared mutable state Message Delivery Reliability The General Rules The Rules for In-JVM (Local) Message Sends Higher-level abstractions Dead Letters Configuration Where configuration is read from When using JarJar, OneJar, Assembly or any jar-bundler Custom application.conf Including files Logging of Configuration A Word About ClassLoaders Application specific settings Configuring multiple ActorSystem Reading configuration from a custom location Listing of the Reference Configuration Default configuration pekko-actor pekko-actor-typed pekko-cluster-typed pekko-cluster pekko-discovery pekko-coordination pekko-multi-node-testkit pekko-persistence-typed pekko-persistence pekko-persistence-query pekko-persistence-testkit pekko-remote artery pekko-remote classic (deprecated) pekko-testkit pekko-cluster-metrics pekko-cluster-tools pekko-cluster-sharding-typed pekko-cluster-sharding pekko-distributed-data pekko-stream pekko-stream-testkit","title":"General Concepts"},{"location":"/general/terminology.html","text":"","title":"Terminology, Concepts"},{"location":"/general/terminology.html#terminology-concepts","text":"In this chapter we attempt to establish a common terminology to define a solid ground for communicating about concurrent, distributed systems which Pekko targets. Please note that, for many of these terms, there is no single agreed definition. We seek to give working definitions that will be used in the scope of the Pekko documentation.","title":"Terminology, Concepts"},{"location":"/general/terminology.html#concurrency-vs-parallelism","text":"Concurrency and parallelism are related concepts, but there are small differences. Concurrency means that two or more tasks are making progress even though they might not be executing simultaneously. This can for example be realized with time slicing where parts of tasks are executed sequentially and mixed with parts of other tasks. Parallelism on the other hand arise when the execution can be truly simultaneous.","title":"Concurrency vs. Parallelism"},{"location":"/general/terminology.html#asynchronous-vs-synchronous","text":"A method call is considered synchronous if the caller cannot make progress until the method returns a value or throws an exception. On the other hand, an asynchronous call allows the caller to progress after a finite number of steps, and the completion of the method may be signalled via some additional mechanism (it might be a registered callback, a Future, or a message).\nA synchronous API may use blocking to implement synchrony, but this is not a necessity. A very CPU intensive task might give a similar behavior as blocking. In general, it is preferred to use asynchronous APIs, as they guarantee that the system is able to progress. Actors are asynchronous by nature: an actor can progress after a message send without waiting for the actual delivery to happen.","title":"Asynchronous vs. Synchronous"},{"location":"/general/terminology.html#non-blocking-vs-blocking","text":"We talk about blocking if the delay of one thread can indefinitely delay some of the other threads. A good example is a resource which can be used exclusively by one thread using mutual exclusion. If a thread holds on to the resource indefinitely (for example accidentally running an infinite loop) other threads waiting on the resource can not progress. In contrast, non-blocking means that no thread is able to indefinitely delay others.\nNon-blocking operations are preferred to blocking ones, as the overall progress of the system is not trivially guaranteed when it contains blocking operations.\nIt’s not always possible to avoid using blocking APIs, please see Blocking Needs Careful Management to use those safely within the actor code.","title":"Non-blocking vs. Blocking"},{"location":"/general/terminology.html#deadlock-vs-starvation-vs-live-lock","text":"Deadlock arises when several participants are waiting on each other to reach a specific state to be able to progress. As none of them can progress without some other participant to reach a certain state (a “Catch-22” problem) all affected subsystems stall. Deadlock is closely related to blocking, as it is necessary that a participant thread be able to delay the progression of other threads indefinitely.\nIn the case of deadlock, no participants can make progress, while in contrast Starvation happens, when there are participants that can make progress, but there might be one or more that cannot. Typical scenario is the case of a naive scheduling algorithm that always selects high-priority tasks over low-priority ones. If the number of incoming high-priority tasks is constantly high enough, no low-priority ones will be ever finished.\nLivelock is similar to deadlock as none of the participants make progress. The difference though is that instead of being frozen in a state of waiting for others to progress, the participants continuously change their state. An example scenario when two participants have two identical resources available. They each try to get the resource, but they also check if the other needs the resource, too. If the resource is requested by the other participant, they try to get the other instance of the resource. In the unfortunate case it might happen that the two participants “bounce” between the two resources, never acquiring it, but always yielding to the other.","title":"Deadlock vs. Starvation vs. Live-lock"},{"location":"/general/terminology.html#race-condition","text":"We call it a Race condition when an assumption about the ordering of a set of events might be violated by external non-deterministic effects. Race conditions often arise when multiple threads have a shared mutable state, and the operations of thread on the state might be interleaved causing unexpected behavior. While this is a common case, shared state is not necessary to have race conditions. One example could be a client sending unordered packets (e.g. UDP datagrams) P1, P2 to a server. As the packets might potentially travel via different network routes, it is possible that the server receives P2 first and P1 afterwards. If the messages contain no information about their sending order it is impossible to determine by the server that they were sent in a different order. Depending on the meaning of the packets this can cause race conditions.\nNote The only guarantee that Pekko provides about messages sent between a given pair of actors is that their order is always preserved. see Message Delivery Reliability","title":"Race Condition"},{"location":"/general/terminology.html#non-blocking-guarantees-progress-conditions-","text":"As discussed in the previous sections blocking is undesirable for several reasons, including the dangers of deadlocks and reduced throughput in the system. In the following sections we discuss various non-blocking properties with different strength.","title":"Non-blocking Guarantees (Progress Conditions)"},{"location":"/general/terminology.html#wait-freedom","text":"A method is wait-free if every call is guaranteed to finish in a finite number of steps. If a method is bounded wait-free then the number of steps has a finite upper bound.\nFrom this definition it follows that wait-free methods are never blocking, therefore deadlock can not happen. Additionally, as each participant can progress after a finite number of steps (when the call finishes), wait-free methods are free of starvation.","title":"Wait-freedom"},{"location":"/general/terminology.html#lock-freedom","text":"Lock-freedom is a weaker property than wait-freedom. In the case of lock-free calls, infinitely often some method finishes in a finite number of steps. This definition implies that no deadlock is possible for lock-free calls. On the other hand, the guarantee that some call finishes in a finite number of steps is not enough to guarantee that all of them eventually finish. In other words, lock-freedom is not enough to guarantee the lack of starvation.","title":"Lock-freedom"},{"location":"/general/terminology.html#obstruction-freedom","text":"Obstruction-freedom is the weakest non-blocking guarantee discussed here. A method is called obstruction-free if there is a point in time after which it executes in isolation (other threads make no steps, e.g.: become suspended), it finishes in a bounded number of steps. All lock-free objects are obstruction-free, but the opposite is generally not true.\nOptimistic concurrency control (OCC) methods are usually obstruction-free. The OCC approach is that every participant tries to execute its operation on the shared object, but if a participant detects conflicts from others, it rolls back the modifications, and tries again according to some schedule. If there is a point in time, where one of the participants is the only one trying, the operation will succeed.","title":"Obstruction-freedom"},{"location":"/general/terminology.html#recommended-literature","text":"The Art of Multiprocessor Programming, M. Herlihy and N Shavit, 2008. ISBN 978-0123705914 Java Concurrency in Practice, B. Goetz, T. Peierls, J. Bloch, J. Bowbeer, D. Holmes and D. Lea, 2006. ISBN 978-0321349606","title":"Recommended literature"},{"location":"/general/actor-systems.html","text":"","title":"Actor Systems"},{"location":"/general/actor-systems.html#actor-systems","text":"Actors are objects which encapsulate state and behavior, they communicate exclusively by exchanging messages which are placed into the recipient’s mailbox. In a sense, actors are the most stringent form of object-oriented programming, but it serves better to view them as persons: while modeling a solution with actors, envision a group of people and assign sub-tasks to them, arrange their functions into an organizational structure and think about how to escalate failure (all with the benefit of not actually dealing with people, which means that we need not concern ourselves with their emotional state or moral issues). The result can then serve as a mental scaffolding for building the software implementation.\nNote An ActorSystem is a heavyweight structure that will allocate 1…N Threads, so create one per logical application.","title":"Actor Systems"},{"location":"/general/actor-systems.html#hierarchical-structure","text":"Like in an economic organization, actors naturally form hierarchies. One actor, which is to oversee a certain function in the program might want to split up its task into smaller, more manageable pieces. For this purpose, it starts child actors.\nThe quintessential feature of actor systems is that tasks are split up and delegated until they become small enough to be handled in one piece. In doing so, not only is the task itself clearly structured, but the resulting actors can be reasoned about in terms of which messages they should process, how they should react normally and how failure should be handled.\nCompare this to layered software design which easily devolves into defensive programming with the aim of not leaking any failure out: if the problem is communicated to the right person, a better solution can be found than if trying to keep everything “under the carpet”.\nNow, the difficulty in designing such a system is how to decide how to structure the work. There is no single best solution, but there are a few guidelines which might be helpful:\nIf one actor carries very important data (i.e. its state shall not be lost if avoidable), this actor should source out any possibly dangerous sub-tasks to children and handle failures of these children as appropriate. Depending on the nature of the requests, it may be best to create a new child for each request, which simplifies state management for collecting the replies. This is known as the “Error Kernel Pattern” from Erlang. If one actor depends on another actor for carrying out its duty, it should watch that other actor’s liveness and act upon receiving a termination notice. If one actor has multiple responsibilities each responsibility can often be pushed into a separate child to make the logic and state more simple.","title":"Hierarchical Structure"},{"location":"/general/actor-systems.html#configuration-container","text":"The actor system as a collaborating ensemble of actors is the natural unit for managing shared facilities like scheduling services, configuration, logging, etc. Several actor systems with different configurations may co-exist within the same JVM without problems, there is no global shared state within Pekko itself, however the most common scenario will only involve a single actor system per JVM.\nCouple this with the transparent communication between actor systems — within one node or across a network connection — and actor systems are a perfect fit to form a distributed application.","title":"Configuration Container"},{"location":"/general/actor-systems.html#actor-best-practices","text":"Actors should be like nice co-workers: do their job efficiently without bothering everyone else needlessly and avoid hogging resources. Translated to programming this means to process events and generate responses (or more requests) in an event-driven manner. Actors should not block (i.e. passively wait while occupying a Thread) on some external entity—which might be a lock, a network socket, etc.—unless it is unavoidable; in the latter case see Blocking Needs Careful Management. Do not pass mutable objects between actors. In order to ensure that, prefer immutable messages. If the encapsulation of actors is broken by exposing their mutable state to the outside, you are back in normal Java concurrency land with all the drawbacks. Actors are made to be containers for behavior and state, embracing this means to not routinely send behavior within messages (which may be tempting using Scala closures). One of the risks is to accidentally share mutable state between actors, and this violation of the actor model unfortunately breaks all the properties which make programming in actors such a nice experience. The top-level actor of the actor system is the innermost part of your Error Kernel, it should only be responsible for starting the various sub systems of your application, and not contain much logic in itself, prefer truly hierarchical systems. This has benefits with respect to fault-handling (both considering the granularity of configuration and the performance) and it also reduces the strain on the guardian actor, which is a single point of contention if over-used.","title":"Actor Best Practices"},{"location":"/general/actor-systems.html#what-you-should-not-concern-yourself-with","text":"An actor system manages the resources it is configured to use in order to run the actors which it contains. There may be millions of actors within one such system, after all the mantra is to view them as abundant and they weigh in at an overhead of only roughly 300 bytes per instance. Naturally, the exact order in which messages are processed in large systems is not controllable by the application author, but this is also not intended. Take a step back and relax while Pekko does the heavy lifting under the hood.","title":"What you should not concern yourself with"},{"location":"/general/actor-systems.html#terminating-actorsystem","text":"When you know everything is done for your application, you can have the user guardian actor stop, or call the terminateterminate() method of ActorSystemActorSystem. That will run CoordinatedShutdown stopping all running actors.\nIf you want to execute some operations while terminating ActorSystemActorSystem, look at CoordinatedShutdown.","title":"Terminating ActorSystem"},{"location":"/general/actors.html","text":"","title":"What is an Actor?"},{"location":"/general/actors.html#what-is-an-actor-","text":"The previous section about Actor Systems explained how actors form hierarchies and are the smallest unit when building an application. This section looks at one such actor in isolation, explaining the concepts you encounter while implementing it. For a more in depth reference with all the details, please refer to Introduction to Actors.\nThe Actor Model as defined by Hewitt, Bishop and Steiger in 1973 is a computational model that expresses exactly what it means for computation to be distributed. The processing units—Actors—can only communicate by exchanging messages and upon reception of a message an Actor can do the following three fundamental actions:\nsend a finite number of messages to Actors it knows create a finite number of new Actors designate the behavior to be applied to the next message\nAn actor is a container for State, Behavior, a Mailbox, Child Actors and a Supervisor Strategy. All of this is encapsulated behind an Actor Reference. One noteworthy aspect is that actors have an explicit lifecycle, they are not automatically destroyed when no longer referenced; after having created one, it is your responsibility to make sure that it will eventually be terminated as well—which also gives you control over how resources are released When an Actor Terminates.","title":"What is an Actor?"},{"location":"/general/actors.html#actor-reference","text":"As detailed below, an actor object needs to be shielded from the outside in order to benefit from the actor model. Therefore, actors are represented to the outside using actor references, which are objects that can be passed around freely and without restriction. This split into inner and outer object enables transparency for all the desired operations: restarting an actor without needing to update references elsewhere, placing the actual actor object on remote hosts, sending messages to actors independent of where they are running. But the most important aspect is that it is not possible to look inside an actor and get hold of its state from the outside, unless the actor unwisely publishes this information itself.\nActor references are parameterized and only messages that are of the specified type can be sent to them.","title":"Actor Reference"},{"location":"/general/actors.html#state","text":"Actor objects will typically contain some variables which reflect possible states the actor may be in. This can be an explicit state machine, or it could be a counter, set of listeners, pending requests, etc. These data are what make an actor valuable, and they must be protected from corruption by other actors. The good news is that Pekko actors conceptually each have their own light-weight thread, which is completely shielded from the rest of the system. This means that instead of having to synchronize access using locks you can write your actor code without worrying about concurrency at all.\nBehind the scenes Pekko will run sets of actors on sets of real threads, where typically many actors share one thread, and subsequent invocations of one actor may end up being processed on different threads. Pekko ensures that this implementation detail does not affect the single-threadedness of handling the actor’s state.\nBecause the internal state is vital to an actor’s operations, having inconsistent state is fatal. Thus, when the actor fails and is restarted by its supervisor, the state will be created from scratch, like upon first creating the actor. This is to enable the ability of self-healing of the system.\nOptionally, an actor’s state can be automatically recovered to the state before a restart by persisting received messages and replaying them after restart (see Event Sourcing).","title":"State"},{"location":"/general/actors.html#behavior","text":"Every time a message is processed, it is matched against the current behavior of the actor. Behavior means a function which defines the actions to be taken in reaction to the message at that point in time, say forward a request if the client is authorized, deny it otherwise. This behavior may change over time, e.g. because different clients obtain authorization over time, or because the actor may go into an “out-of-service” mode and later come back. These changes are achieved by either encoding them in state variables which are read from the behavior logic, or the function itself may be swapped out at runtime, by returning a different behavior to be used for next message. However, the initial behavior defined during construction of the actor object is special in the sense that a restart of the actor will reset its behavior to this initial one.\nMessages can be sent to an actor Reference and behind this façade there is a behavior that receives the message and acts upon it. The binding between Actor reference and behavior can change over time, but that is not visible on the outside.\nActor references are parameterized and only messages that are of the specified type can be sent to them. The association between an actor reference and its type parameter must be made when the actor reference (and its Actor) is created. For this purpose each behavior is also parameterized with the type of messages it is able to process. Since the behavior can change behind the actor reference façade, designating the next behavior is a constrained operation: the successor must handle the same type of messages as its predecessor. This is necessary in order to not invalidate the actor references that refer to this Actor.\nWhat this enables is that whenever a message is sent to an Actor we can statically ensure that the type of the message is one that the Actor declares to handle—we can avoid the mistake of sending completely pointless messages. What we cannot statically ensure, though, is that the behavior behind the actor reference will be in a given state when our message is received. The fundamental reason is that the association between actor reference and behavior is a dynamic runtime property, the compiler cannot know it while it translates the source code.\nThis is the same as for normal Java objects with internal variables: when compiling the program we cannot know what their value will be, and if the result of a method call depends on those variables then the outcome is uncertain to a degree—we can only be certain that the returned value is of a given type.\nThe reply message type of an Actor command is described by the type of the actor reference for the reply-to that is contained within the message. This allows a conversation to be described in terms of its types: the reply will be of type A, but it might also contain an address of type B, which then allows the other Actor to continue the conversation by sending a message of type B to this new actor reference. While we cannot statically express the “current” state of an Actor, we can express the current state of a protocol between two Actors, since that is just given by the last message type that was received or sent.","title":"Behavior"},{"location":"/general/actors.html#mailbox","text":"An actor’s purpose is the processing of messages, and these messages were sent to the actor from other actors (or from outside the actor system). The piece which connects sender and receiver is the actor’s mailbox: each actor has exactly one mailbox to which all senders enqueue their messages. Enqueuing happens in the time-order of send operations, which means that messages sent from different actors may not have a defined order at runtime due to the apparent randomness of distributing actors across threads. Sending multiple messages to the same target from the same actor, on the other hand, will enqueue them in the same order.\nThere are different mailbox implementations to choose from, the default being a FIFO: the order of the messages processed by the actor matches the order in which they were enqueued. This is usually a good default, but applications may need to prioritize some messages over others. In this case, a priority mailbox will enqueue not always at the end but at a position as given by the message priority, which might even be at the front. While using such a queue, the order of messages processed will naturally be defined by the queue’s algorithm and in general not be FIFO.\nAn important feature in which Pekko differs from some other actor model implementations is that the current behavior must always handle the next dequeued message, there is no scanning the mailbox for the next matching one. Failure to handle a message will typically be treated as a failure, unless this behavior is overridden.","title":"Mailbox"},{"location":"/general/actors.html#child-actors","text":"Each actor is potentially a parent: if it creates children for delegating sub-tasks, it will automatically supervise them. The list of children is maintained within the actor’s context and the actor has access to it. Modifications to the list are done by spawning or stopping children and these actions are reflected immediately. The actual creation and termination actions happen behind the scenes in an asynchronous way, so they do not “block” their parent.","title":"Child Actors"},{"location":"/general/actors.html#supervisor-strategy","text":"The final piece of an actor is its strategy for handling unexpected exceptions - failures. Fault handling is then done transparently by Pekko, applying one of the strategies described in Fault Tolerance for each failure.","title":"Supervisor Strategy"},{"location":"/general/actors.html#when-an-actor-terminates","text":"Once an actor terminates, i.e. fails in a way which is not handled by a restart, stops itself or is stopped by its supervisor, it will free up its resources, draining all remaining messages from its mailbox into the system’s “dead letter mailbox” which will forward them to the EventStream as DeadLetters. The mailbox is then replaced within the actor reference with a system mailbox, redirecting all new messages to the EventStream as DeadLetters. This is done on a best effort basis, though, so do not rely on it in order to construct “guaranteed delivery”.","title":"When an Actor Terminates"},{"location":"/general/supervision.html","text":"","title":"Supervision and Monitoring"},{"location":"/general/supervision.html#supervision-and-monitoring","text":"This chapter outlines the concept behind supervision, the primitives offered and their semantics. For details on how that translates into real code, please refer to supervision.\nSupervision has changed since classic, for details on classic supervision see Classic Supervision","title":"Supervision and Monitoring"},{"location":"/general/supervision.html#what-supervision-means","text":"There are two categories of exception that can happen in an actor:\nInput validation errors, expected exceptions which can be handled with a regular try-catch or other language and standard library tools. Unexpected failures, for example a network resource being unavailable, a disk write failing or perhaps a bug in the application logic.\nSupervision deals with failures and should be separated from the business logic while validating data and handling of expected exceptions is a vital part of the business logic. Therefore supervision is added to an actor as decoration rather than something that is intermingled with the message processing logic of the actor.\nDepending on the nature of the work to be supervised and the nature of the failure, supervision provides the following three strategies:\nResume the actor, keeping its accumulated internal state Restart the actor, clearing out its accumulated internal state, with a potential delay starting again Stop the actor permanently\nSince actors are part of a hierarchy it can often make sense to propagate the permanent failures upwards, if all children of an actor has stopped unexpectedly it may make sense for the actor itself to restart or stop to get back to a functional state. This can be achieved through a combination of supervision and watching the children to get notified when they terminate. An example of this can be found in Bubble failures up through the hierarchy.","title":"What Supervision Means"},{"location":"/general/supervision.html#the-top-level-actors","text":"An actor system will during its creation start at least two actors.","title":"The Top-Level actors"},{"location":"/general/supervision.html#user-the-user-guardian-actor","text":"This is the top level user provided actor, meant to bootstrap the application by spawning subsystems as children. When the user guardian stops the entire actor system is shut down.","title":"/user: the user guardian actor"},{"location":"/general/supervision.html#system-the-system-guardian-actor","text":"This special guardian has been introduced in order to achieve an orderly shut-down sequence where logging remains active while all normal actors terminate, even though logging itself is implemented using actors. This is realized by having the system guardian watch the user guardian and initiate its own shut-down upon having seen the user guardian stop.","title":"/system: the system guardian actor"},{"location":"/general/supervision.html#what-restarting-means","text":"When presented with an actor which failed while processing a certain message, causes for the failure fall into three categories:\nSystematic (i.e. programming) error for the specific message received (Transient) failure of some external resource used during processing the message Corrupt internal state of the actor\nUnless the failure is specifically recognizable, the third cause cannot be ruled out, which leads to the conclusion that the internal state needs to be cleared out. If the supervisor decides that its other children or itself is not affected by the corruption—e.g. because of conscious application of the error kernel pattern—it is therefore best to restart the actor. This is carried out by creating a new instance of the underlying BehaviorBehavior class and replacing the failed instance with the fresh one inside the child’s ActorRefActorRef; the ability to do this is one of the reasons for encapsulating actors within special references. The new actor then resumes processing its mailbox, meaning that the restart is not visible outside of the actor itself with the notable exception that the message during which the failure occurred is not re-processed.","title":"What Restarting Means"},{"location":"/general/supervision.html#what-lifecycle-monitoring-means","text":"Note Lifecycle Monitoring in Pekko is usually referred to as DeathWatch\nIn contrast to the special relationship between parent and child described above, each actor may monitor any other actor. Since actors emerge from creation fully alive and restarts are not visible outside of the affected supervisors, the only state change available for monitoring is the transition from alive to dead. Monitoring is thus used to tie one actor to another so that it may react to the other actor’s termination, in contrast to supervision which reacts to failure.\nLifecycle monitoring is implemented using a TerminatedTerminated message to be received by the monitoring actor, where the default behavior is to throw a special DeathPactExceptionDeathPactException if not otherwise handled. In order to start listening for TerminatedTerminated messages, invoke ActorContext.watch(targetActorRef)ActorContext.watch(targetActorRef). To stop listening, invoke ActorContext.unwatch(targetActorRef)ActorContext.unwatch(targetActorRef). One important property is that the message will be delivered irrespective of the order in which the monitoring request and target’s termination occur, i.e. you still get the message even if at the time of registration the target is already dead.","title":"What Lifecycle Monitoring Means"},{"location":"/general/supervision.html#actors-and-exceptions","text":"It can happen that while a message is being processed by an actor, that some kind of exception is thrown, e.g. a database exception.","title":"Actors and exceptions"},{"location":"/general/supervision.html#what-happens-to-the-message","text":"If an exception is thrown while a message is being processed (i.e. taken out of its mailbox and handed over to the current behavior), then this message will be lost. It is important to understand that it is not put back on the mailbox. So if you want to retry processing of a message, you need to deal with it yourself by catching the exception and retry your flow. Make sure that you put a bound on the number of retries since you don’t want a system to livelock (so consuming a lot of cpu cycles without making progress).","title":"What happens to the Message"},{"location":"/general/supervision.html#what-happens-to-the-mailbox","text":"If an exception is thrown while a message is being processed, nothing happens to the mailbox. If the actor is restarted, the same mailbox will be there. So all messages on that mailbox will be there as well.","title":"What happens to the mailbox"},{"location":"/general/supervision.html#what-happens-to-the-actor","text":"If code within an actor throws an exception, that actor is suspended and the supervision process is started. Depending on the supervisor’s decision the actor is resumed (as if nothing happened), restarted (wiping out its internal state and starting from scratch) or terminated.","title":"What happens to the actor"},{"location":"/general/addressing.html","text":"","title":"Actor References, Paths and Addresses"},{"location":"/general/addressing.html#actor-references-paths-and-addresses","text":"This chapter describes how actors are identified and located within a possibly distributed Pekko application.\nThe above image displays the relationship between the most important entities within an actor system, please read on for the details.","title":"Actor References, Paths and Addresses"},{"location":"/general/addressing.html#what-is-an-actor-reference","text":"An actor reference is a subtype of ActorRefActorRef, whose foremost purpose is to support sending messages to the actor it represents. Each actor has access to its canonical (local) reference through the ActorContext.selfActorContext.self field; this reference can be included in messages to other actors to get replies back.\nThere are several different types of actor references that are supported depending on the configuration of the actor system:\nPurely local actor references are used by actor systems which are not configured to support networking functions. These actor references will not function if sent across a network connection to a remote JVM. Local actor references when remoting is enabled are used by actor systems which support networking functions for those references which represent actors within the same JVM. In order to also be reachable when sent to other network nodes, these references include protocol and remote addressing information. Remote actor references represent actors which are reachable using remote communication, i.e. sending messages to them will serialize the messages transparently and send them to the remote JVM. There are several special types of actor references which behave like local actor references for all practical purposes: PromiseActorRef is the special representation of a Promise for the purpose of being completed by the response from an actor. org.apache.pekko.pattern.ask creates this actor reference. DeadLetterActorRef is the default implementation of the dead letters service to which Pekko routes all messages whose destinations are shut down or non-existent. EmptyLocalActorRef is what Pekko returns when looking up a non-existent local actor path: it is equivalent to a DeadLetterActorRef, but it retains its path so that Pekko can send it over the network and compare it to other existing actor references for that path, some of which might have been obtained before the actor died. And then there are some one-off internal implementations which you should never really see: There is an actor reference which does not represent an actor but acts only as a pseudo-supervisor for the root guardian, we call it “the one who walks the bubbles of space-time”. The first logging service started before actually firing up actor creation facilities is a fake actor reference which accepts log events and prints them directly to standard output; it is Logging.StandardOutLoggerLogging.StandardOutLogger.","title":"What is an Actor Reference"},{"location":"/general/addressing.html#what-is-an-actor-path-","text":"Since actors are created in a strictly hierarchical fashion, there exists a unique sequence of actor names given by recursively following the supervision links between child and parent down towards the root of the actor system. This sequence can be seen as enclosing folders in a file system, hence we adopted the name “path” to refer to it, although actor hierarchy has some fundamental difference from file system hierarchy.\nAn actor path consists of an anchor, which identifies the actor system, followed by the concatenation of the path elements, from root guardian to the designated actor; the path elements are the names of the traversed actors and are separated by slashes.","title":"What is an Actor Path?"},{"location":"/general/addressing.html#what-is-the-difference-between-actor-reference-and-path-","text":"An actor reference designates a single actor and the life-cycle of the reference matches that actor’s life-cycle; an actor path represents a name which may or may not be inhabited by an actor and the path itself does not have a life-cycle, it never becomes invalid. You can create an actor path without creating an actor, but you cannot create an actor reference without creating a corresponding actor.\nYou can create an actor, terminate it, and then create a new actor with the same actor path. The newly created actor is a new incarnation of the actor. It is not the same actor. An actor reference to the old incarnation is not valid for the new incarnation. Messages sent to the old actor reference will not be delivered to the new incarnation even though they have the same path.","title":"What is the Difference Between Actor Reference and Path?"},{"location":"/general/addressing.html#actor-path-anchors","text":"Each actor path has an address component, describing the protocol and location by which the corresponding actor is reachable, followed by the names of the actors in the hierarchy from the root up. Examples are:\n\"pekko://my-sys/user/service-a/worker1\"               // purely local\n\"pekko://my-sys@host.example.com:5678/user/service-b\" // remote\nThe interpretation of the host and port part (i.e. host.example.com:5678 in the example) depends on the transport mechanism used, but it must abide by the URI structural rules.","title":"Actor Path Anchors"},{"location":"/general/addressing.html#logical-actor-paths","text":"The unique path obtained by following the parental supervision links towards the root guardian is called the logical actor path. This path matches exactly the creation ancestry of an actor, so it is completely deterministic as soon as the actor system’s remoting configuration (and with it the address component of the path) is set.","title":"Logical Actor Paths"},{"location":"/general/addressing.html#actor-path-alias-or-symbolic-link-","text":"As in some real file-systems you might think of a “path alias” or “symbolic link” for an actor, i.e. one actor may be reachable using more than one path. However, you should note that actor hierarchy is different from file system hierarchy. You cannot freely create actor paths like symbolic links to refer to arbitrary actors.","title":"Actor path alias or symbolic link?"},{"location":"/general/addressing.html#how-are-actor-references-obtained-","text":"There are two general categories to how actor references may be obtained: by creating actors or by looking them up through the Receptionist.","title":"How are Actor References obtained?"},{"location":"/general/addressing.html#actor-reference-and-path-equality","text":"Equality of ActorRefActorRef match the intention that an ActorRefActorRef corresponds to the target actor incarnation. Two actor references are compared equal when they have the same path and point to the same actor incarnation. A reference pointing to a terminated actor does not compare equal to a reference pointing to another (re-created) actor with the same path. Note that a restart of an actor caused by a failure still means that it is the same actor incarnation, i.e. a restart is not visible for the consumer of the ActorRefActorRef.\nIf you need to keep track of actor references in a collection and do not care about the exact actor incarnation you can use the ActorPathActorPath as key, because the identifier of the target actor is not taken into account when comparing actor paths.","title":"Actor Reference and Path Equality"},{"location":"/general/addressing.html#reusing-actor-paths","text":"When an actor is terminated, its reference will point to the dead letter mailbox, DeathWatch will publish its final transition and in general it is not expected to come back to life again (since the actor life cycle does not allow this).","title":"Reusing Actor Paths"},{"location":"/general/addressing.html#what-is-the-address-part-used-for-","text":"When sending an actor reference across the network, it is represented by its path. Hence, the path must fully encode all information necessary to send messages to the underlying actor. This is achieved by encoding protocol, host and port in the address part of the path string. When an actor system receives an actor path from a remote node, it checks whether that path’s address matches the address of this actor system, in which case it will be resolved to the actor’s local reference. Otherwise, it will be represented by a remote actor reference.","title":"What is the Address part used for?"},{"location":"/general/addressing.html#top-level-scopes-for-actor-paths","text":"At the root of the path hierarchy resides the root guardian above which all other actors are found; its name is \"/\". The next level consists of the following:\n\"/user\" is the guardian actor for all user-created top-level actors; actors created using ActorSystem.actorOfActorSystem.actorOf are found below this one. \"/system\" is the guardian actor for all system-created top-level actors, e.g. logging listeners or actors automatically deployed by configuration at the start of the actor system. \"/deadLetters\" is the dead letter actor, which is where all messages sent to stopped or non-existing actors are re-routed (on a best-effort basis: messages may be lost even within the local JVM). \"/temp\" is the guardian for all short-lived system-created actors, e.g. those which are used in the implementation of ActorRef.askPatterns.ask. \"/remote\" is an artificial path below which all actors reside whose supervisors are remote actor references\nThe need to structure the name space for actors like this arises from a central and very simple design goal: everything in the hierarchy is an actor, and all actors function in the same way.","title":"Top-Level Scopes for Actor Paths"},{"location":"/general/remoting.html","text":"","title":"Location Transparency"},{"location":"/general/remoting.html#location-transparency","text":"The previous section describes how actor paths are used to enable location transparency. This special feature deserves some extra explanation, because the related term “transparent remoting” was used quite differently in the context of programming languages, platforms and technologies.","title":"Location Transparency"},{"location":"/general/remoting.html#distributed-by-default","text":"Everything in Pekko is designed to work in a distributed setting: all interactions of actors use purely message passing and everything is asynchronous. This effort has been undertaken to ensure that all functions are available equally when running within a single JVM or on a cluster of hundreds of machines. The key for enabling this is to go from remote to local by way of optimization instead of trying to go from local to remote by way of generalization. See this classic paper for a detailed discussion on why the second approach is bound to fail.","title":"Distributed by Default"},{"location":"/general/remoting.html#ways-in-which-transparency-is-broken","text":"What is true of Pekko need not be true of the application which uses it, since designing for distributed execution poses some restrictions on what is possible. The most obvious one is that all messages sent over the wire must be serializable.\nAnother consequence is that everything needs to be aware of all interactions being fully asynchronous, which in a computer network might mean that it may take several minutes for a message to reach its recipient (depending on configuration). It also means that the probability for a message to be lost is much higher than within one JVM, where it is close to zero (still: no hard guarantee!).","title":"Ways in which Transparency is Broken"},{"location":"/general/remoting.html#peer-to-peer-vs-client-server","text":"Pekko Remoting is a communication module for connecting actor systems in a peer-to-peer fashion, and it is the foundation for Pekko Clustering. The design of remoting is driven by two (related) design decisions:\nCommunication between involved systems is symmetric: if a system A can connect to a system B then system B must also be able to connect to system A independently. The role of the communicating systems are symmetric in regards to connection patterns: there is no system that only accepts connections, and there is no system that only initiates connections.\nThe consequence of these decisions is that it is not possible to safely create pure client-server setups with predefined roles (violates assumption 2). For client-server setups it is better to use HTTP or Pekko I/O.\nImportant: Using setups involving Network Address Translation, Load Balancers or Docker containers violates assumption 1, unless additional steps are taken in the network configuration to allow symmetric communication between involved systems. In such situations Pekko can be configured to bind to a different network address than the one used for establishing connections between Pekko nodes. See Pekko behind NAT or in a Docker container.","title":"Peer-to-Peer vs. Client-Server"},{"location":"/general/remoting.html#marking-points-for-scaling-up-with-routers","text":"In addition to being able to run different parts of an actor system on different nodes of a cluster, it is also possible to scale up onto more cores by multiplying actor sub-trees which support parallelization (think for example a search engine processing different queries in parallel). The clones can then be routed to in different fashions, e.g. round-robin. See Routing for more details.","title":"Marking Points for Scaling Up with Routers"},{"location":"/general/jmm.html","text":"","title":"Apache Pekko and the Java Memory Model"},{"location":"/general/jmm.html#apache-pekko-and-the-java-memory-model","text":"A major benefit of using Pekko is that it simplifies the process of writing concurrent software. This article discusses how the Pekko Platform approaches shared memory in concurrent applications.","title":"Apache Pekko and the Java Memory Model"},{"location":"/general/jmm.html#the-java-memory-model","text":"Prior to Java 5, the Java Memory Model (JMM) was ill defined. It was possible to get all kinds of strange results when shared memory was accessed by multiple threads, such as:\na thread not seeing values written by other threads: a visibility problem a thread observing ‘impossible’ behavior of other threads, caused by instructions not being executed in the order expected: an instruction reordering problem.\nWith the implementation of JSR 133 in Java 5, a lot of these issues have been resolved. The JMM is a set of rules based on the “happens-before” relation, which constrain when one memory access must happen before another, and conversely, when they are allowed to happen out of order. Two examples of these rules are:\nThe monitor lock rule: a release of a lock happens before every subsequent acquire of the same lock. The volatile variable rule: a write of a volatile variable happens before every subsequent read of the same volatile variable\nAlthough the JMM can seem complicated, the specification tries to find a balance between ease of use and the ability to write performant and scalable concurrent data structures.","title":"The Java Memory Model"},{"location":"/general/jmm.html#actors-and-the-java-memory-model","text":"With the Actors implementation in Pekko, there are two ways multiple threads can execute actions on shared memory:\nif a message is sent to an actor (e.g. by another actor). In most cases messages are immutable, but if that message is not a properly constructed immutable object, without a “happens before” rule, it would be possible for the receiver to see partially initialized data structures and possibly even values out of thin air (longs/doubles). if an actor makes changes to its internal state while processing a message, and accesses that state while processing another message moments later. It is important to realize that with the actor model you don’t get any guarantee that the same thread will be executing the same actor for different messages.\nTo prevent visibility and reordering problems on actors, Pekko guarantees the following two “happens before” rules:\nThe actor send rule: the send of the message to an actor happens before the receive of that message by the same actor. The actor subsequent processing rule: processing of one message happens before processing of the next message by the same actor.\nNote In layman’s terms this means that changes to internal fields of the actor are visible when the next message is processed by that actor. So fields in your actor need not be volatile or equivalent.\nBoth rules only apply for the same actor instance and are not valid if different actors are used.","title":"Actors and the Java Memory Model"},{"location":"/general/jmm.html#futures-and-the-java-memory-model","text":"The completion of a Future “happens before” the invocation of any callbacks registered to it are executed.\nWe recommend not to close over non-final fields (final in Java and val in Scala), and if you do choose to close over non-final fields, they must be marked volatile in order for the current value of the field to be visible to the callback.\nIf you close over a reference, you must also ensure that the instance that is referred to is thread safe. We highly recommend staying away from objects that use locking, since it can introduce performance problems and in the worst case, deadlocks. Such are the perils of synchronized.","title":"Futures and the Java Memory Model"},{"location":"/general/jmm.html#actors-and-shared-mutable-state","text":"Since Pekko runs on the JVM there are still some rules to be followed.\nMost importantly, you must not close over internal Actor state and exposing it to other threads:\nScala copysourceclass MyActor(context: ActorContext[MyActor.Command]) extends AbstractBehavior[MyActor.Command](context) {\n  import MyActor._\n\n  var state = \"\"\n  val mySet = mutable.Set[String]()\n\n  def onMessage(cmd: MyActor.Command) = cmd match {\n    case Message(text, otherActor) =>\n      // Very bad: shared mutable object allows\n      // the other actor to mutate your own state,\n      // or worse, you might get weird race conditions\n      otherActor ! mySet\n\n      implicit val ec = context.executionContext\n\n      // Example of incorrect approach\n      // Very bad: shared mutable state will cause your\n      // application to break in weird ways\n      Future { state = \"This will race\" }\n\n      // Example of incorrect approach\n      // Very bad: shared mutable state will cause your\n      // application to break in weird ways\n      expensiveCalculation().foreach { result =>\n        state = s\"new state: $result\"\n      }\n\n      // Example of correct approach\n      // Turn the future result into a message that is sent to\n      // self when future completes\n      val futureResult = expensiveCalculation()\n      context.pipeToSelf(futureResult) {\n        case Success(result) => UpdateState(result)\n        case Failure(ex)     => throw ex\n      }\n\n      // Another example of incorrect approach\n      // mutating actor state from ask future callback\n      import org.apache.pekko.actor.typed.scaladsl.AskPattern._\n      implicit val timeout: Timeout = 5.seconds // needed for `ask` below\n      implicit val scheduler = context.system.scheduler\n      val future: Future[String] = otherActor.ask(Query(_))\n      future.foreach { result =>\n        state = result\n      }\n\n      // use context.ask instead, turns the completion\n      // into a message sent to self\n      context.ask[Query, String](otherActor, Query(_)) {\n        case Success(result) => UpdateState(result)\n        case Failure(ex)     => throw ex\n      }\n      this\n\n    case UpdateState(newState) =>\n      // safe as long as `newState` is immutable, if it is mutable we'd need to\n      // make a defensive copy\n      state = newState\n      this\n  }\n} Java copysourceclass MyActor extends AbstractBehavior<MyActor.Command> {\n\n  interface Command {}\n\n  class Message implements Command {\n    public final ActorRef<Object> otherActor;\n\n    public Message(ActorRef<Object> replyTo) {\n      this.otherActor = replyTo;\n    }\n  }\n\n  class UpdateState implements Command {\n    public final String newState;\n\n    public UpdateState(String newState) {\n      this.newState = newState;\n    }\n  }\n\n  private String state = \"\";\n  private Set<String> mySet = new HashSet<>();\n\n  public MyActor(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Message.class, this::onMessage)\n        .onMessage(UpdateState.class, this::onUpdateState)\n        .build();\n  }\n\n  private Behavior<Command> onMessage(Message message) {\n    // Very bad: shared mutable object allows\n    // the other actor to mutate your own state,\n    // or worse, you might get weird race conditions\n    message.otherActor.tell(mySet);\n\n    // Example of incorrect approach\n    // Very bad: shared mutable state will cause your\n    // application to break in weird ways\n    CompletableFuture.runAsync(\n        () -> {\n          state = \"This will race\";\n        });\n\n    // Example of incorrect approach\n    // Very bad: shared mutable state will cause your\n    // application to break in weird ways\n    expensiveCalculation()\n        .whenComplete(\n            (result, failure) -> {\n              if (result != null) state = \"new state: \" + result;\n            });\n\n    // Example of correct approach\n    // Turn the future result into a message that is sent to\n    // self when future completes\n    CompletableFuture<String> futureResult = expensiveCalculation();\n    getContext()\n        .pipeToSelf(\n            futureResult,\n            (result, failure) -> {\n              if (result != null) return new UpdateState(result);\n              else throw new RuntimeException(failure);\n            });\n\n    // Another example of incorrect approach\n    // mutating actor state from ask future callback\n    CompletionStage<String> response =\n        AskPattern.ask(\n            message.otherActor,\n            Query::new,\n            Duration.ofSeconds(3),\n            getContext().getSystem().scheduler());\n    response.whenComplete(\n        (result, failure) -> {\n          if (result != null) state = \"new state: \" + result;\n        });\n\n    // use context.ask instead, turns the completion\n    // into a message sent to self\n    getContext()\n        .ask(\n            String.class,\n            message.otherActor,\n            Duration.ofSeconds(3),\n            Query::new,\n            (result, failure) -> {\n              if (result != null) return new UpdateState(result);\n              else throw new RuntimeException(failure);\n            });\n    return this;\n  }\n\n  private Behavior<Command> onUpdateState(UpdateState command) {\n    // safe as long as `newState` is immutable, if it is mutable we'd need to\n    // make a defensive copy\n    this.state = command.newState;\n    return this;\n  }\n}\nMessages should be immutable, this is to avoid the shared mutable state trap.","title":"Actors and shared mutable state"},{"location":"/general/message-delivery-reliability.html","text":"","title":"Message Delivery Reliability"},{"location":"/general/message-delivery-reliability.html#message-delivery-reliability","text":"Pekko helps you build reliable applications which make use of multiple processor cores in one machine (“scaling up”) or distributed across a computer network (“scaling out”). The key abstraction to make this work is that all interactions between your code units—actors—happen via message passing, which is why the precise semantics of how messages are passed between actors deserve their own chapter.\nIn order to give some context to the discussion below, consider an application which spans multiple network hosts. The basic mechanism for communication is the same whether sending to an actor on the local JVM or to a remote actor, but there will be observable differences in the latency of delivery (possibly also depending on the bandwidth of the network link and the message size) and the reliability. In case of a remote message send there are more steps involved which means that more can go wrong. Another aspect is that local sending will pass a reference to the message inside the same JVM, without any restrictions on the underlying object which is sent, whereas a remote transport will place a limit on the message size.\nWriting your actors such that every interaction could possibly be remote is the safe, pessimistic bet. It means to only rely on those properties which are always guaranteed and which are discussed in detail below. This has some overhead in the actor’s implementation. If you are willing to sacrifice full location transparency—for example in case of a group of closely collaborating actors—you can place them always on the same JVM and enjoy stricter guarantees on message delivery. The details of this trade-off are discussed further below.\nAs a supplementary part we give a few pointers at how to build stronger reliability on top of the built-in ones. The chapter closes by discussing the role of the “Dead Letter Office”.","title":"Message Delivery Reliability"},{"location":"/general/message-delivery-reliability.html#the-general-rules","text":"These are the rules for message sends (i.e. the tell or !tell method, which also underlies the ask pattern):\nat-most-once delivery, i.e. no guaranteed delivery message ordering per sender–receiver pair\nThe first rule is typically found also in other actor implementations while the second is specific to Pekko.","title":"The General Rules"},{"location":"/general/message-delivery-reliability.html#discussion-what-does-at-most-once-mean-","text":"When it comes to describing the semantics of a delivery mechanism, there are three basic categories:\nat-most-once delivery means that for each message handed to the mechanism, that message is delivered once or not at all; in more casual terms it means that messages may be lost. at-least-once delivery means that for each message handed to the mechanism potentially multiple attempts are made at delivering it, such that at least one succeeds; again, in more casual terms this means that messages may be duplicated but not lost. exactly-once delivery means that for each message handed to the mechanism exactly one delivery is made to the recipient; the message can neither be lost nor duplicated.\nThe first one is the cheapest—highest performance, least implementation overhead—because it can be done in a fire-and-forget fashion without keeping state at the sending end or in the transport mechanism. The second one requires retries to counter transport losses, which means keeping state at the sending end and having an acknowledgement mechanism at the receiving end. The third is most expensive—and has consequently worst performance—because in addition to the second it requires state to be kept at the receiving end in order to filter out duplicate deliveries.","title":"Discussion: What does “at-most-once” mean?"},{"location":"/general/message-delivery-reliability.html#discussion-why-no-guaranteed-delivery-","text":"At the core of the problem lies the question what exactly this guarantee shall mean:\nThe message is sent out on the network? The message is received by the other host? The message is put into the target actor’s mailbox? The message is starting to be processed by the target actor? The message is processed successfully by the target actor?\nEach one of these have different challenges and costs, and it is obvious that there are conditions under which any message passing library would be unable to comply; think for example about configurable mailbox types and how a bounded mailbox would interact with the third point, or even what it would mean to decide upon the “successfully” part of point five.\nAlong those same lines goes the reasoning in Nobody Needs Reliable Messaging. The only meaningful way for a sender to know whether an interaction was successful is by receiving a business-level acknowledgement message, which is not something Pekko could make up on its own (neither are we writing a “do what I mean” framework nor would you want us to).\nPekko embraces distributed computing and makes the fallibility of communication explicit through message passing, therefore it does not try to lie and emulate a leaky abstraction. This is a model that has been used with great success in Erlang and requires the users to design their applications around it. You can read more about this approach in the Erlang documentation (section 10.8 and 10.9), Pekko follows it closely.\nAnother angle on this issue is that by providing only basic guarantees those use cases which do not need stronger reliability do not pay the cost of their implementation; it is always possible to add stronger reliability on top of basic ones, but it is not possible to retro-actively remove reliability in order to gain more performance.","title":"Discussion: Why No Guaranteed Delivery?"},{"location":"/general/message-delivery-reliability.html#discussion-message-ordering","text":"The rule more specifically is that for a given pair of actors, messages sent directly from the first to the second will not be received out-of-order. The word directly emphasizes that this guarantee only applies when sending with the tell operator to the final destination, not when employing mediators or other message dissemination features (unless stated otherwise).\nThe guarantee is illustrated in the following:\nActor A1 sends messages M1, M2, M3 to A2 Actor A3 sends messages M4, M5, M6 to A2\nThis means that:\nIf M1 is delivered it must be delivered before M2 and M3 If M2 is delivered it must be delivered before M3 If M4 is delivered it must be delivered before M5 and M6 If M5 is delivered it must be delivered before M6 A2 can see messages from A1 interleaved with messages from A3 Since there is no guaranteed delivery, any of the messages may be dropped, i.e. not arrive at A2\nNote It is important to note that Pekko’s guarantee applies to the order in which messages are enqueued into the recipient’s mailbox. If the mailbox implementation does not respect FIFO order (e.g. a PriorityMailbox), then the order of processing by the actor can deviate from the enqueueing order.\nPlease note that this rule is not transitive:\nActor A sends message M1 to actor C Actor A then sends message M2 to actor B Actor B forwards message M2 to actor C Actor C may receive M1 and M2 in any order\nCausal transitive ordering would imply that M2 is never received before M1 at actor C (though any of them might be lost). This ordering can be violated due to different message delivery latencies when A, B and C reside on different network hosts, see more below.\nNote Actor creation is treated as a message sent from the parent to the child, with the same semantics as discussed above. Sending a message to an actor in a way which could be reordered with this initial creation message means that the message might not arrive because the actor does not exist yet. An example where the message might arrive too early would be to create a remote-deployed actor R1, send its reference to another remote actor R2 and have R2 send a message to R1. An example of well-defined ordering is a parent which creates an actor and immediately sends a message to it.","title":"Discussion: Message Ordering"},{"location":"/general/message-delivery-reliability.html#communication-of-failure","text":"Please note, that the ordering guarantees discussed above only hold for user messages between actors. Failure of a child of an actor is communicated by special system messages that are not ordered relative to ordinary user messages. In particular:\nChild actor C sends message M to its parent P Child actor fails with failure F Parent actor P might receive the two events either in order M, F or F, M\nThe reason for this is that internal system messages have their own mailboxes therefore the ordering of enqueue calls of a user and system messages cannot guarantee the ordering of their dequeue times.","title":"Communication of failure"},{"location":"/general/message-delivery-reliability.html#the-rules-for-in-jvm-local-message-sends","text":"","title":"The Rules for In-JVM (Local) Message Sends"},{"location":"/general/message-delivery-reliability.html#be-careful-what-you-do-with-this-section-","text":"Relying on the stronger reliability in this section is not recommended since it will bind your application to local-only deployment: an application may have to be designed differently (as opposed to just employing some message exchange patterns local to some actors) in order to be fit for running on a cluster of machines. Our credo is “design once, deploy any way you wish”, and to achieve this you should only rely on The General Rules.","title":"Be careful what you do with this section!"},{"location":"/general/message-delivery-reliability.html#reliability-of-local-message-sends","text":"The Pekko test suite relies on not losing messages in the local context (and for non-error condition tests also for remote deployment), meaning that we actually do apply the best effort to keep our tests stable. A local telltell operation can however fail for the same reasons as a normal method call can on the JVM:\nStackOverflowError OutOfMemoryError other VirtualMachineError\nIn addition, local sends can fail in Pekko-specific ways:\nif the mailbox does not accept the message (e.g. full BoundedMailboxBoundedMailbox) if the receiving actor fails while processing the message or is already terminated\nWhile the first is a matter of configuration the second deserves some thought: the sender of a message does not get feedback if there was an exception while processing, that notification goes to the supervisor instead. This is in general not distinguishable from a lost message for an outside observer.","title":"Reliability of Local Message Sends"},{"location":"/general/message-delivery-reliability.html#ordering-of-local-message-sends","text":"Assuming strict FIFO mailboxes the aforementioned caveat of non-transitivity of the message ordering guarantee is eliminated under certain conditions. As you will note, these are quite subtle as it stands, and it is even possible that future performance optimizations will invalidate this whole paragraph. The possibly non-exhaustive list of counter-indications is:\nBefore receiving the first reply from a top-level actor, there is a lock which protects an internal interim queue, and this lock is not fair; the implication is that enqueue requests from different senders which arrive during the actor’s construction (figuratively, the details are more involved) may be reordered depending on low-level thread scheduling. Since completely fair locks do not exist on the JVM this is unfixable. The same mechanism is used during the construction of a Router, more precisely the routed ActorRef, hence the same problem exists for actors deployed with Routers. As mentioned above, the problem occurs anywhere a lock is involved during enqueueing, which may also apply to custom mailboxes.\nThis list has been compiled carefully, but other problematic scenarios may have escaped our analysis.","title":"Ordering of Local Message Sends"},{"location":"/general/message-delivery-reliability.html#how-does-local-ordering-relate-to-network-ordering","text":"The rule that for a given pair of actors, messages sent directly from the first to the second will not be received out-of-order holds for messages sent over the network with the TCP based Pekko remote transport protocol.\nAs explained in the previous section local message sends obey transitive causal ordering under certain conditions. This ordering can be violated due to different message delivery latencies. For example:\nActor A on node-1 sends message M1 to actor C on node-3 Actor A on node-1 then sends message M2 to actor B on node-2 Actor B on node-2 forwards message M2 to actor C on node-3 Actor C may receive M1 and M2 in any order\nIt might take longer time for M1 to “travel” to node-3 than it takes for M2 to “travel” to node-3 via node-2.","title":"How does Local Ordering relate to Network Ordering"},{"location":"/general/message-delivery-reliability.html#higher-level-abstractions","text":"Based on a small and consistent tool set in Pekko’s core, Pekko also provides powerful, higher-level abstractions on top of it.","title":"Higher-level abstractions"},{"location":"/general/message-delivery-reliability.html#messaging-patterns","text":"As discussed above a straight-forward answer to the requirement of reliable delivery is an explicit ACK–RETRY protocol. In its simplest form this requires\na way to identify individual messages to correlate message with acknowledgement a retry mechanism which will resend messages if not acknowledged in time a way for the receiver to detect and discard duplicates\nThe third becomes necessary by virtue of the acknowledgements not being guaranteed to arrive either.\nAn ACK-RETRY protocol with business-level acknowledgements and de-duplication using identifiers is supported by the Reliable Delivery feature.\nAnother way of implementing the third part would be to make processing the messages idempotent on the level of the business logic.","title":"Messaging Patterns"},{"location":"/general/message-delivery-reliability.html#event-sourcing","text":"Event Sourcing (and sharding) is what makes large websites scale to billions of users, and the idea is quite simple: when a component (think actor) processes a command it will generate a list of events representing the effect of the command. These events are stored in addition to being applied to the component’s state. The nice thing about this scheme is that events only ever are appended to the storage, nothing is ever mutated; this enables perfect replication and scaling of consumers of this event stream (i.e. other components may consume the event stream as a means to replicate the component’s state on a different continent or to react to changes). If the component’s state is lost—due to a machine failure or by being pushed out of a cache—it can be reconstructed by replaying the event stream (usually employing snapshots to speed up the process). Event Sourcing is supported by Pekko Persistence.","title":"Event Sourcing"},{"location":"/general/message-delivery-reliability.html#mailbox-with-explicit-acknowledgement","text":"By implementing a custom mailbox type it is possible to retry message processing at the receiving actor’s end in order to handle temporary failures. This pattern is mostly useful in the local communication context where delivery guarantees are otherwise sufficient to fulfill the application’s requirements.\nPlease note that the caveats for The Rules for In-JVM (Local) Message Sends do apply.","title":"Mailbox with Explicit Acknowledgement"},{"location":"/general/message-delivery-reliability.html#dead-letters","text":"Messages which cannot be delivered (and for which this can be ascertained) will be delivered to a synthetic actor called /deadLetters. This delivery happens on a best-effort basis; it may fail even within the local JVM (e.g. during actor termination). Messages sent via unreliable network transports will be lost without turning up as dead letters.","title":"Dead Letters"},{"location":"/general/message-delivery-reliability.html#what-should-i-use-dead-letters-for-","text":"The main use of this facility is for debugging, especially if an actor send does not arrive consistently (where usually inspecting the dead letters will tell you that the sender or recipient was set wrong somewhere along the way). In order to be useful for this purpose it is good practice to avoid sending to deadLetters where possible, i.e. run your application with a suitable dead letter logger (see more below) from time to time and clean up the log output. This exercise—like all else—requires judicious application of common sense: it may well be that avoiding to send to a terminated actor complicates the sender’s code more than is gained in debug output clarity.\nThe dead letter service follows the same rules with respect to delivery guarantees as all other message sends, hence it cannot be used to implement guaranteed delivery.","title":"What Should I Use Dead Letters For?"},{"location":"/general/message-delivery-reliability.html#how-do-i-receive-dead-letters-","text":"An actor can subscribe to class actor.DeadLetteractor.DeadLetter on the event stream, see Event Stream for how to do that. The subscribed actor will then receive all dead letters published in the (local) system from that point onwards. Dead letters are not propagated over the network, if you want to collect them in one place you will have to subscribe one actor per network node and forward them manually. Also consider that dead letters are generated at that node which can determine that a send operation is failed, which for a remote send can be the local system (if no network connection can be established) or the remote one (if the actor you are sending to does not exist at that point in time).","title":"How do I Receive Dead Letters?"},{"location":"/general/message-delivery-reliability.html#dead-letters-which-are-usually-not-worrisome","text":"Every time an actor does not terminate by its own decision, there is a chance that some messages which it sends to itself are lost. There is one which happens quite easily in complex shutdown scenarios that is usually benign: seeing instances of a graceful stop command for an actor being dropped means that two stop requests were given, but only one can succeed. In the same vein, you might see actor.Terminatedactor.Terminated messages from children while stopping a hierarchy of actors turning up in dead letters if the parent is still watching the child when the parent terminates.","title":"Dead Letters Which are (Usually) not Worrisome"},{"location":"/general/configuration.html","text":"","title":"Configuration"},{"location":"/general/configuration.html#configuration","text":"You can start using Pekko without defining any configuration, since sensible default values are provided. Later on you might need to amend the settings to change the default behavior or adapt for specific runtime environments. Typical examples of settings that you might amend:\nlog level and logger backend enable Cluster message serializers tuning of dispatchers\nPekko uses the Typesafe Config Library, which might also be a good choice for the configuration of your own application or library built with or without Pekko. This library is implemented in Java with no external dependencies; This is only a summary of the most important parts for more details see the config library docs.","title":"Configuration"},{"location":"/general/configuration.html#where-configuration-is-read-from","text":"All configuration for Pekko is held within instances of ActorSystemActorSystem, or put differently, as viewed from the outside, ActorSystemActorSystem is the only consumer of configuration information. While constructing an actor system, you can either pass in a Config object or not, where the second case is equivalent to passing ConfigFactory.load() (with the right class loader). This means roughly that the default is to parse all application.conf, application.json and application.properties found at the root of the class path—please refer to the aforementioned documentation for details. The actor system then merges in all reference.conf resources found at the root of the class path to form the fallback configuration, i.e. it internally uses\nappConfig.withFallback(ConfigFactory.defaultReference(classLoader))\nThe philosophy is that code never contains default values, but instead relies upon their presence in the reference.conf supplied with the library in question.\nHighest precedence is given to overrides given as system properties, see the HOCON specification (near the bottom). Also noteworthy is that the application configuration—which defaults to application—may be overridden using the config.resource property (there are more, please refer to the Config docs).\nNote If you are writing a Pekko application, keep your configuration in application.conf at the root of the class path. If you are writing an Pekko-based library, keep its configuration in reference.conf at the root of the JAR file. It’s not supported to override a config property owned by one library in a reference.conf of another library.","title":"Where configuration is read from"},{"location":"/general/configuration.html#when-using-jarjar-onejar-assembly-or-any-jar-bundler","text":"Warning Pekko’s configuration approach relies heavily on the notion of every module/jar having its own reference.conf file. All of these will be discovered by the configuration and loaded. Unfortunately this also means that if you put/merge multiple jars into the same jar, you need to merge all the reference.conf files as well: otherwise all defaults will be lost.\nSee the deployment documentation for information on how to merge the reference.conf resources while bundling.","title":"When using JarJar, OneJar, Assembly or any jar-bundler"},{"location":"/general/configuration.html#custom-application-conf","text":"A custom application.conf might look like this:\n# In this file you can override any option defined in the reference files.\n# Copy in parts of the reference files and modify as you please.\n\npekko {\n\n  # Logger config for Pekko internals and classic actors, the new API relies\n  # directly on SLF4J and your config for the logger backend.\n\n  # Loggers to register at boot time (org.apache.pekko.event.Logging$DefaultLogger logs\n  # to STDOUT)\n  loggers = [\"org.apache.pekko.event.slf4j.Slf4jLogger\"]\n\n  # Log level used by the configured loggers (see \"loggers\") as soon\n  # as they have been started; before that, see \"stdout-loglevel\"\n  # Options: OFF, ERROR, WARNING, INFO, DEBUG\n  loglevel = \"DEBUG\"\n\n  # Log level for the very basic logger activated during ActorSystem startup.\n  # This logger prints the log messages to stdout (System.out).\n  # Options: OFF, ERROR, WARNING, INFO, DEBUG\n  stdout-loglevel = \"DEBUG\"\n\n  # Filter of log events that is used by the LoggingAdapter before\n  # publishing log events to the eventStream.\n  logging-filter = \"org.apache.pekko.event.slf4j.Slf4jLoggingFilter\"\n\n  actor {\n    provider = \"cluster\"\n\n    default-dispatcher {\n      # Throughput for default Dispatcher, set to 1 for as fair as possible\n      throughput = 10\n    }\n  }\n\n  remote.artery {\n    # The port clients should connect to.\n    canonical.port = 4711\n  }\n}","title":"Custom application.conf"},{"location":"/general/configuration.html#including-files","text":"Sometimes, it can be useful to include another configuration file, for example if you have one application.conf with all environment independent settings and then override some settings for specific environments.\nSpecifying system property with -Dconfig.resource=/dev.conf will load the dev.conf file, which includes the application.conf","title":"Including files"},{"location":"/general/configuration.html#dev-conf","text":"include \"application\"\n\npekko {\n  loglevel = \"DEBUG\"\n}\nMore advanced include and substitution mechanisms are explained in the HOCON specification.","title":"dev.conf"},{"location":"/general/configuration.html#logging-of-configuration","text":"If the system or config property pekko.log-config-on-start is set to on, then the complete configuration is logged at INFO level when the actor system is started. This is useful when you are uncertain of what configuration is used.\nIf in doubt, you can inspect your configuration objects before or after using them to construct an actor system: Welcome to Scala 2.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import com.typesafe.config._\nimport com.typesafe.config._\n\nscala> ConfigFactory.parseString(\"a.b=12\")\nres0: com.typesafe.config.Config = Config(SimpleConfigObject({\"a\" : {\"b\" : 12}}))\n\nscala> res0.root.render\nres1: java.lang.String =\n{\n    # String: 1\n    \"a\" : {\n        # String: 1\n        \"b\" : 12\n    }\n}\nThe comments preceding every item give detailed information about the origin of the setting (file & line number) plus possible comments which were present, e.g. in the reference configuration. The settings as merged with the reference and parsed by the actor system can be displayed like this:\nScala copysourceval system = ActorSystem(rootBehavior, \"MySystem\")\nsystem.logConfiguration() Java copysourceActorSystem<Void> system = ActorSystem.create(rootBehavior, \"MySystem\");\nsystem.logConfiguration();","title":"Logging of Configuration"},{"location":"/general/configuration.html#a-word-about-classloaders","text":"In several places of the configuration file it is possible to specify the fully-qualified class name of something to be instantiated by Pekko. This is done using Java reflection, which in turn uses a ClassLoader. Getting the right one in challenging environments like application containers or OSGi bundles is not always trivial, the current approach of Pekko is that each ActorSystemActorSystem implementation stores the current thread’s context class loader (if available, otherwise just its own loader as in this.getClass.getClassLoader) and uses that for all reflective accesses. This implies that putting Pekko on the boot class path will yield NullPointerException from strange places: this is not supported.","title":"A Word About ClassLoaders"},{"location":"/general/configuration.html#application-specific-settings","text":"The configuration can also be used for application specific settings. A good practice is to place those settings in an Extension.","title":"Application specific settings"},{"location":"/general/configuration.html#configuring-multiple-actorsystem","text":"If you have more than one ActorSystemActorSystem (or you’re writing a library and have an ActorSystemActorSystem that may be separate from the application’s) you may want to separate the configuration for each system.\nGiven that ConfigFactory.load() merges all resources with matching name from the whole class path, it is easiest to utilize that functionality and differentiate actor systems within the hierarchy of the configuration:\nmyapp1 {\n  pekko.loglevel = \"WARNING\"\n  my.own.setting = 43\n}\nmyapp2 {\n  pekko.loglevel = \"ERROR\"\n  app2.setting = \"appname\"\n}\nmy.own.setting = 42\nmy.other.setting = \"hello\"\nScala copysourceval config = ConfigFactory.load()\nval app1 = ActorSystem(rootBehavior, \"MyApp1\", config.getConfig(\"myapp1\").withFallback(config))\nval app2 =\n  ActorSystem(rootBehavior, \"MyApp2\", config.getConfig(\"myapp2\").withOnlyPath(\"pekko\").withFallback(config)) Java copysourceConfig config = ConfigFactory.load();\nActorSystem<Void> app1 =\n    ActorSystem.create(rootBehavior, \"MyApp1\", config.getConfig(\"myapp1\").withFallback(config));\nActorSystem<Void> app2 =\n    ActorSystem.create(\n        rootBehavior,\n        \"MyApp2\",\n        config.getConfig(\"myapp2\").withOnlyPath(\"pekko\").withFallback(config));\nThese two samples demonstrate different variations of the “lift-a-subtree” trick: in the first case, the configuration accessible from within the actor system is this\npekko.loglevel = \"WARNING\"\nmy.own.setting = 43\nmy.other.setting = \"hello\"\n// plus myapp1 and myapp2 subtrees\nwhile in the second one, only the “pekko” subtree is lifted, with the following result\npekko.loglevel = \"ERROR\"\nmy.own.setting = 42\nmy.other.setting = \"hello\"\n// plus myapp1 and myapp2 subtrees\nNote The configuration library is really powerful, explaining all features exceeds the scope affordable here. In particular not covered are how to include other configuration files within other files (see a small example at Including files) and copying parts of the configuration tree by way of path substitutions.\nYou may also specify and parse the configuration programmatically in other ways when instantiating the ActorSystemActorSystem.\nScala copysourceimport org.apache.pekko.actor.typed.ActorSystem\nimport com.typesafe.config.ConfigFactory\nval customConf = ConfigFactory.parseString(\"\"\"\n  pekko.log-config-on-start = on\n\"\"\")\n// ConfigFactory.load sandwiches customConfig between default reference\n// config and default overrides, and then resolves it.\nval system = ActorSystem(rootBehavior, \"MySystem\", ConfigFactory.load(customConf)) Java copysourceimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport com.typesafe.config.Config;\nimport com.typesafe.config.ConfigFactory;\n\nConfig customConf = ConfigFactory.parseString(\"pekko.log-config-on-start = on\");\n// ConfigFactory.load sandwiches customConfig between default reference\n// config and default overrides, and then resolves it.\nActorSystem<Void> system =\n    ActorSystem.create(rootBehavior, \"MySystem\", ConfigFactory.load(customConf));","title":"Configuring multiple ActorSystem"},{"location":"/general/configuration.html#reading-configuration-from-a-custom-location","text":"You can replace or supplement application.conf either in code or using system properties.\nIf you’re using ConfigFactory.load() (which Pekko does by default) you can replace application.conf by defining -Dconfig.resource=whatever, -Dconfig.file=whatever, or -Dconfig.url=whatever.\nFrom inside your replacement file specified with -Dconfig.resource and friends, you can include \"application\" if you still want to use application.{conf,json,properties} as well. Settings specified before include \"application\" would be overridden by the included file, while those after would override the included file.\nIn code, there are many customization options.\nThere are several overloads of ConfigFactory.load(); these allow you to specify something to be sandwiched between system properties (which override) and the defaults (from reference.conf), replacing the usual application.{conf,json,properties} and replacing -Dconfig.file and friends.\nThe simplest variant of ConfigFactory.load() takes a resource basename (instead of application); myname.conf, myname.json, and myname.properties would then be used instead of application.{conf,json,properties}.\nThe most flexible variant takes a Config object, which you can load using any method in ConfigFactory. For example you could put a config string in code using ConfigFactory.parseString() or you could make a map and ConfigFactory.parseMap(), or you could load a file.\nYou can also combine your custom config with the usual config, that might look like:\nScala copysource// make a Config with just your special setting\nval myConfig = ConfigFactory.parseString(\"something=somethingElse\");\n// load the normal config stack (system props,\n// then application.conf, then reference.conf)\nval regularConfig = ConfigFactory.load();\n// override regular stack with myConfig\nval combined = myConfig.withFallback(regularConfig);\n// put the result in between the overrides\n// (system props) and defaults again\nval complete = ConfigFactory.load(combined);\n// create ActorSystem\nval system = ActorSystem(rootBehavior, \"myname\", complete); Java copysource// make a Config with just your special setting\nConfig myConfig = ConfigFactory.parseString(\"something=somethingElse\");\n// load the normal config stack (system props,\n// then application.conf, then reference.conf)\nConfig regularConfig = ConfigFactory.load();\n// override regular stack with myConfig\nConfig combined = myConfig.withFallback(regularConfig);\n// put the result in between the overrides\n// (system props) and defaults again\nConfig complete = ConfigFactory.load(combined);\n// create ActorSystem\nActorSystem system = ActorSystem.create(rootBehavior, \"myname\", complete);\nWhen working with Config objects, keep in mind that there are three “layers” in the cake:\nConfigFactory.defaultOverrides() (system properties) the app’s settings ConfigFactory.defaultReference() (reference.conf)\nThe normal goal is to customize the middle layer while leaving the other two alone.\nConfigFactory.load() loads the whole stack the overloads of ConfigFactory.load() let you specify a different middle layer the ConfigFactory.parse variations load single files or resources\nTo stack two layers, use override.withFallback(fallback); try to keep system props (defaultOverrides()) on top and reference.conf (defaultReference()) on the bottom.\nDo keep in mind, you can often just add another include statement in application.conf rather than writing code. Includes at the top of application.conf will be overridden by the rest of application.conf, while those at the bottom will override the earlier stuff.","title":"Reading configuration from a custom location"},{"location":"/general/configuration.html#listing-of-the-reference-configuration","text":"Each Pekko module has a reference configuration file with the default values. Those reference.conf files are listed in Default configuration","title":"Listing of the Reference Configuration"},{"location":"/general/configuration-reference.html","text":"","title":"Default configuration"},{"location":"/general/configuration-reference.html#default-configuration","text":"Each Pekko module has a reference.conf file with the default values.\nMake your edits/overrides in your application.conf. Don’t override default values if you are not sure of the implications.\nThe purpose of reference.conf files is for libraries, like Pekko, to define default values that are used if an application doesn’t define a more specific value. It’s also a good place to document the existence and meaning of the configuration properties. One library must not try to override properties in its own reference.conf for properties originally defined by another library’s reference.conf, because the effective value would be nondeterministic when loading the configuration.`","title":"Default configuration"},{"location":"/general/configuration-reference.html#pekko-actor","text":"copysource####################################\n# Pekko Actor Reference Config File #\n####################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\n# Pekko version, checked against the runtime version of Pekko. Loaded from generated conf file.\ninclude \"version\"\n\npekko {\n  # Home directory of Pekko, modules in the deploy directory will be loaded\n  home = \"\"\n\n  # Loggers to register at boot time (org.apache.pekko.event.Logging$DefaultLogger logs\n  # to STDOUT)\n  loggers = [\"org.apache.pekko.event.Logging$DefaultLogger\"]\n\n  # Filter of log events that is used by the LoggingAdapter before\n  # publishing log events to the eventStream. It can perform\n  # fine grained filtering based on the log source. The default\n  # implementation filters on the `loglevel`.\n  # FQCN of the LoggingFilter. The Class of the FQCN must implement\n  # org.apache.pekko.event.LoggingFilter and have a public constructor with\n  # (org.apache.pekko.actor.ActorSystem.Settings, org.apache.pekko.event.EventStream) parameters.\n  logging-filter = \"org.apache.pekko.event.DefaultLoggingFilter\"\n\n  # Specifies the default loggers dispatcher\n  loggers-dispatcher = \"pekko.actor.default-dispatcher\"\n\n  # Loggers are created and registered synchronously during ActorSystem\n  # start-up, and since they are actors, this timeout is used to bound the\n  # waiting time\n  logger-startup-timeout = 5s\n\n  # Log level used by the configured loggers (see \"loggers\") as soon\n  # as they have been started; before that, see \"stdout-loglevel\"\n  # Options: OFF, ERROR, WARNING, INFO, DEBUG\n  loglevel = \"INFO\"\n\n  # Log level for the very basic logger activated during ActorSystem startup.\n  # This logger prints the log messages to stdout (System.out).\n  # Options: OFF, ERROR, WARNING, INFO, DEBUG\n  stdout-loglevel = \"WARNING\"\n\n  # Log the complete configuration at INFO level when the actor system is started.\n  # This is useful when you are uncertain of what configuration is used.\n  log-config-on-start = off\n\n  # Log at info level when messages are sent to dead letters, or published to\n  # eventStream as `DeadLetter`, `Dropped` or `UnhandledMessage`.\n  # Possible values:\n  # on: all dead letters are logged\n  # off: no logging of dead letters\n  # n: positive integer, number of dead letters that will be logged\n  log-dead-letters = 10\n\n  # Possibility to turn off logging of dead letters while the actor system\n  # is shutting down. Logging is only done when enabled by 'log-dead-letters'\n  # setting.\n  log-dead-letters-during-shutdown = off\n\n  # When log-dead-letters is enabled, this will re-enable the logging after configured duration.\n  # infinite: suspend the logging forever;\n  # or a duration (eg: 5 minutes), after which the logging will be re-enabled.\n  log-dead-letters-suspend-duration = 5 minutes\n\n  # List FQCN of extensions which shall be loaded at actor system startup.\n  # Library extensions are regular extensions that are loaded at startup and are\n  # available for third party library authors to enable auto-loading of extensions when\n  # present on the classpath. This is done by appending entries:\n  # 'library-extensions += \"Extension\"' in the library `reference.conf`.\n  #\n  # Should not be set by end user applications in 'application.conf', use the extensions property for that\n  #\n  library-extensions = ${?pekko.library-extensions} [\"org.apache.pekko.serialization.SerializationExtension$\"]\n\n  # List FQCN of extensions which shall be loaded at actor system startup.\n  # Should be on the format: 'extensions = [\"foo\", \"bar\"]' etc.\n  # See the Pekko Documentation for more info about Extensions\n  extensions = []\n\n  # Toggles whether threads created by this ActorSystem should be daemons or not\n  daemonic = off\n\n  # JVM shutdown, System.exit(-1), in case of a fatal error,\n  # such as OutOfMemoryError\n  jvm-exit-on-fatal-error = on\n\n  # Pekko installs JVM shutdown hooks by default, e.g. in CoordinatedShutdown and Artery. This property will\n  # not disable user-provided hooks registered using `CoordinatedShutdown#addCancellableJvmShutdownHook`.\n  # This property is related to `pekko.coordinated-shutdown.run-by-jvm-shutdown-hook` below.\n  # This property makes it possible to disable all such hooks if the application itself\n  # or a higher level framework such as Play prefers to install the JVM shutdown hook and\n  # terminate the ActorSystem itself, with or without using CoordinatedShutdown.\n  jvm-shutdown-hooks = on\n\n  # Version must be the same across all modules and if they are different the startup\n  # will fail. It's possible but not recommended, to disable this check, and only log a warning,\n  # by setting this property to `off`.\n  fail-mixed-versions = on\n\n  # Some modules (remoting only right now) can emit custom events to the Java Flight Recorder if running\n  # on JDK 11 or later. If you for some reason do not want that, it can be disabled and switched to no-ops\n  # with this toggle.\n  java-flight-recorder {\n    enabled = true\n  }\n\n  actor {\n\n    # Either one of \"local\", \"remote\" or \"cluster\" or the\n    # FQCN of the ActorRefProvider to be used; the below is the built-in default,\n    # note that \"remote\" and \"cluster\" requires the pekko-remote and pekko-cluster\n    # artifacts to be on the classpath.\n    provider = \"local\"\n\n    # The guardian \"/user\" will use this class to obtain its supervisorStrategy.\n    # It needs to be a subclass of org.apache.pekko.actor.SupervisorStrategyConfigurator.\n    # In addition to the default there is org.apache.pekko.actor.StoppingSupervisorStrategy.\n    guardian-supervisor-strategy = \"org.apache.pekko.actor.DefaultSupervisorStrategy\"\n\n    # Timeout for Extension creation and a few other potentially blocking\n    # initialization tasks.\n    creation-timeout = 20s\n\n    # Serializes and deserializes (non-primitive) messages to ensure immutability,\n    # this is only intended for testing.\n    serialize-messages = off\n\n    # Serializes and deserializes creators (in Props) to ensure that they can be\n    # sent over the network, this is only intended for testing. Purely local deployments\n    # as marked with deploy.scope == LocalScope are exempt from verification.\n    serialize-creators = off\n\n    # If serialize-messages or serialize-creators are enabled classes that starts with\n    # a prefix listed here are not verified.\n    no-serialization-verification-needed-class-prefix = [\"org.apache.pekko.\"]\n\n    # Timeout for send operations to top-level actors which are in the process\n    # of being started. This is only relevant if using a bounded mailbox or the\n    # CallingThreadDispatcher for a top-level actor.\n    unstarted-push-timeout = 10s\n\n    # TypedActor deprecated since 2.6.0.\n    typed {\n      # Default timeout for the deprecated TypedActor (not the new actor APIs in 2.6)\n      # methods with non-void return type.\n      timeout = 5s\n    }\n\n    # Mapping between ´deployment.router' short names to fully qualified class names\n    router.type-mapping {\n      from-code = \"org.apache.pekko.routing.NoRouter\"\n      round-robin-pool = \"org.apache.pekko.routing.RoundRobinPool\"\n      round-robin-group = \"org.apache.pekko.routing.RoundRobinGroup\"\n      random-pool = \"org.apache.pekko.routing.RandomPool\"\n      random-group = \"org.apache.pekko.routing.RandomGroup\"\n      balancing-pool = \"org.apache.pekko.routing.BalancingPool\"\n      smallest-mailbox-pool = \"org.apache.pekko.routing.SmallestMailboxPool\"\n      broadcast-pool = \"org.apache.pekko.routing.BroadcastPool\"\n      broadcast-group = \"org.apache.pekko.routing.BroadcastGroup\"\n      scatter-gather-pool = \"org.apache.pekko.routing.ScatterGatherFirstCompletedPool\"\n      scatter-gather-group = \"org.apache.pekko.routing.ScatterGatherFirstCompletedGroup\"\n      tail-chopping-pool = \"org.apache.pekko.routing.TailChoppingPool\"\n      tail-chopping-group = \"org.apache.pekko.routing.TailChoppingGroup\"\n      consistent-hashing-pool = \"org.apache.pekko.routing.ConsistentHashingPool\"\n      consistent-hashing-group = \"org.apache.pekko.routing.ConsistentHashingGroup\"\n    }\n\n    deployment {\n\n      # deployment id pattern - on the format: /parent/child etc.\n      default {\n\n        # The id of the dispatcher to use for this actor.\n        # If undefined or empty the dispatcher specified in code\n        # (Props.withDispatcher) is used, or default-dispatcher if not\n        # specified at all.\n        dispatcher = \"\"\n\n        # The id of the mailbox to use for this actor.\n        # If undefined or empty the default mailbox of the configured dispatcher\n        # is used or if there is no mailbox configuration the mailbox specified\n        # in code (Props.withMailbox) is used.\n        # If there is a mailbox defined in the configured dispatcher then that\n        # overrides this setting.\n        mailbox = \"\"\n\n        # routing (load-balance) scheme to use\n        # - available: \"from-code\", \"round-robin\", \"random\", \"smallest-mailbox\",\n        #              \"scatter-gather\", \"broadcast\"\n        # - or:        Fully qualified class name of the router class.\n        #              The class must extend org.apache.pekko.routing.CustomRouterConfig and\n        #              have a public constructor with com.typesafe.config.Config\n        #              and optional org.apache.pekko.actor.DynamicAccess parameter.\n        # - default is \"from-code\";\n        # Whether or not an actor is transformed to a Router is decided in code\n        # only (Props.withRouter). The type of router can be overridden in the\n        # configuration; specifying \"from-code\" means that the values specified\n        # in the code shall be used.\n        # In case of routing, the actors to be routed to can be specified\n        # in several ways:\n        # - nr-of-instances: will create that many children\n        # - routees.paths: will route messages to these paths using ActorSelection,\n        #   i.e. will not create children\n        # - resizer: dynamically resizable number of routees as specified in\n        #   resizer below\n        router = \"from-code\"\n\n        # number of children to create in case of a router;\n        # this setting is ignored if routees.paths is given\n        nr-of-instances = 1\n\n        # within is the timeout used for routers containing future calls\n        within = 5 seconds\n\n        # number of virtual nodes per node for consistent-hashing router\n        virtual-nodes-factor = 10\n\n        tail-chopping-router {\n          # interval is duration between sending message to next routee\n          interval = 10 milliseconds\n        }\n\n        routees {\n          # Alternatively to giving nr-of-instances you can specify the full\n          # paths of those actors which should be routed to. This setting takes\n          # precedence over nr-of-instances\n          paths = []\n        }\n\n        # To use a dedicated dispatcher for the routees of the pool you can\n        # define the dispatcher configuration inline with the property name\n        # 'pool-dispatcher' in the deployment section of the router.\n        # For example:\n        # pool-dispatcher {\n        #   fork-join-executor.parallelism-min = 5\n        #   fork-join-executor.parallelism-max = 5\n        # }\n\n        # Routers with dynamically resizable number of routees; this feature is\n        # enabled by including (parts of) this section in the deployment\n        resizer {\n\n          enabled = off\n\n          # The fewest number of routees the router should ever have.\n          lower-bound = 1\n\n          # The most number of routees the router should ever have.\n          # Must be greater than or equal to lower-bound.\n          upper-bound = 10\n\n          # Threshold used to evaluate if a routee is considered to be busy\n          # (under pressure). Implementation depends on this value (default is 1).\n          # 0:   number of routees currently processing a message.\n          # 1:   number of routees currently processing a message has\n          #      some messages in mailbox.\n          # > 1: number of routees with at least the configured pressure-threshold\n          #      messages in their mailbox. Note that estimating mailbox size of\n          #      default UnboundedMailbox is O(N) operation.\n          pressure-threshold = 1\n\n          # Percentage to increase capacity whenever all routees are busy.\n          # For example, 0.2 would increase 20% (rounded up), i.e. if current\n          # capacity is 6 it will request an increase of 2 more routees.\n          rampup-rate = 0.2\n\n          # Minimum fraction of busy routees before backing off.\n          # For example, if this is 0.3, then we'll remove some routees only when\n          # less than 30% of routees are busy, i.e. if current capacity is 10 and\n          # 3 are busy then the capacity is unchanged, but if 2 or less are busy\n          # the capacity is decreased.\n          # Use 0.0 or negative to avoid removal of routees.\n          backoff-threshold = 0.3\n\n          # Fraction of routees to be removed when the resizer reaches the\n          # backoffThreshold.\n          # For example, 0.1 would decrease 10% (rounded up), i.e. if current\n          # capacity is 9 it will request an decrease of 1 routee.\n          backoff-rate = 0.1\n\n          # Number of messages between resize operation.\n          # Use 1 to resize before each message.\n          messages-per-resize = 10\n        }\n\n        # Routers with dynamically resizable number of routees based on\n        # performance metrics.\n        # This feature is enabled by including (parts of) this section in\n        # the deployment, cannot be enabled together with default resizer.\n        optimal-size-exploring-resizer {\n\n          enabled = off\n\n          # The fewest number of routees the router should ever have.\n          lower-bound = 1\n\n          # The most number of routees the router should ever have.\n          # Must be greater than or equal to lower-bound.\n          upper-bound = 10\n\n          # probability of doing a ramping down when all routees are busy\n          # during exploration.\n          chance-of-ramping-down-when-full = 0.2\n\n          # Interval between each resize attempt\n          action-interval = 5s\n\n          # If the routees have not been fully utilized (i.e. all routees busy)\n          # for such length, the resizer will downsize the pool.\n          downsize-after-underutilized-for = 72h\n\n          # Duration exploration, the ratio between the largest step size and\n          # current pool size. E.g. if the current pool size is 50, and the\n          # explore-step-size is 0.1, the maximum pool size change during\n          # exploration will be +- 5\n          explore-step-size = 0.1\n\n          # Probability of doing an exploration v.s. optimization.\n          chance-of-exploration = 0.4\n\n          # When downsizing after a long streak of underutilization, the resizer\n          # will downsize the pool to the highest utiliziation multiplied by a\n          # a downsize ratio. This downsize ratio determines the new pools size\n          # in comparison to the highest utilization.\n          # E.g. if the highest utilization is 10, and the down size ratio\n          # is 0.8, the pool will be downsized to 8\n          downsize-ratio = 0.8\n\n          # When optimizing, the resizer only considers the sizes adjacent to the\n          # current size. This number indicates how many adjacent sizes to consider.\n          optimization-range = 16\n\n          # The weight of the latest metric over old metrics when collecting\n          # performance metrics.\n          # E.g. if the last processing speed is 10 millis per message at pool\n          # size 5, and if the new processing speed collected is 6 millis per\n          # message at pool size 5. Given a weight of 0.3, the metrics\n          # representing pool size 5 will be 6 * 0.3 + 10 * 0.7, i.e. 8.8 millis\n          # Obviously, this number should be between 0 and 1.\n          weight-of-latest-metric = 0.5\n        }\n      }\n\n      \"/IO-DNS/inet-address\" {\n        mailbox = \"unbounded\"\n        router = \"consistent-hashing-pool\"\n        nr-of-instances = 4\n      }\n\n      \"/IO-DNS/inet-address/*\" {\n        dispatcher = \"pekko.actor.default-blocking-io-dispatcher\"\n      }\n\n      \"/IO-DNS/async-dns\" {\n        mailbox = \"unbounded\"\n        router = \"round-robin-pool\"\n        nr-of-instances = 1\n      }\n    }\n\n    default-dispatcher {\n      # Must be one of the following\n      # Dispatcher, PinnedDispatcher, or a FQCN to a class inheriting\n      # MessageDispatcherConfigurator with a public constructor with\n      # both com.typesafe.config.Config parameter and\n      # org.apache.pekko.dispatch.DispatcherPrerequisites parameters.\n      # PinnedDispatcher must be used together with executor=thread-pool-executor.\n      type = \"Dispatcher\"\n\n      # Which kind of ExecutorService to use for this dispatcher\n      # Valid options:\n      #  - \"default-executor\" requires a \"default-executor\" section\n      #  - \"fork-join-executor\" requires a \"fork-join-executor\" section\n      #  - \"thread-pool-executor\" requires a \"thread-pool-executor\" section\n      #  - \"affinity-pool-executor\" requires an \"affinity-pool-executor\" section\n      #  - A FQCN of a class extending ExecutorServiceConfigurator\n      executor = \"default-executor\"\n\n      # This will be used if you have set \"executor = \"default-executor\"\".\n      # If an ActorSystem is created with a given ExecutionContext, this\n      # ExecutionContext will be used as the default executor for all\n      # dispatchers in the ActorSystem configured with\n      # executor = \"default-executor\". Note that \"default-executor\"\n      # is the default value for executor, and therefore used if not\n      # specified otherwise. If no ExecutionContext is given,\n      # the executor configured in \"fallback\" will be used.\n      default-executor {\n        fallback = \"fork-join-executor\"\n      }\n\n      # This will be used if you have set \"executor = \"affinity-pool-executor\"\"\n      # Underlying thread pool implementation is org.apache.pekko.dispatch.affinity.AffinityPool.\n      # This executor is classified as \"ApiMayChange\".\n      affinity-pool-executor {\n        # Min number of threads to cap factor-based parallelism number to\n        parallelism-min = 4\n\n        # The parallelism factor is used to determine thread pool size using the\n        # following formula: ceil(available processors * factor). Resulting size\n        # is then bounded by the parallelism-min and parallelism-max values.\n        parallelism-factor = 0.8\n\n        # Max number of threads to cap factor-based parallelism number to.\n        parallelism-max = 64\n\n        # Each worker in the pool uses a separate bounded MPSC queue. This value\n        # indicates the upper bound of the queue. Whenever an attempt to enqueue\n        # a task is made and the queue does not have capacity to accommodate\n        # the task, the rejection handler created by the rejection handler specified\n        # in \"rejection-handler\" is invoked.\n        task-queue-size = 512\n\n        # FQCN of the Rejection handler used in the pool.\n        # Must have an empty public constructor and must\n        # implement org.apache.pekko.actor.affinity.RejectionHandlerFactory.\n        rejection-handler = \"org.apache.pekko.dispatch.affinity.ThrowOnOverflowRejectionHandler\"\n\n        # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.\n        # The tradeoff is that to have low latency more CPU time must be used to be\n        # able to react quickly on incoming messages or send as fast as possible after\n        # backoff backpressure.\n        # Level 1 strongly prefer low CPU consumption over low latency.\n        # Level 10 strongly prefer low latency over low CPU consumption.\n        idle-cpu-level = 5\n\n        # FQCN of the org.apache.pekko.dispatch.affinity.QueueSelectorFactory.\n        # The Class of the FQCN must have a public constructor with a\n        # (com.typesafe.config.Config) parameter.\n        # A QueueSelectorFactory create instances of org.apache.pekko.dispatch.affinity.QueueSelector,\n        # that is responsible for determining which task queue a Runnable should be enqueued in.\n        queue-selector = \"org.apache.pekko.dispatch.affinity.FairDistributionHashCache\"\n\n        # When using the \"org.apache.pekko.dispatch.affinity.FairDistributionHashCache\" queue selector\n        # internally the AffinityPool uses two methods to determine which task\n        # queue to allocate a Runnable to:\n        # - map based - maintains a round robin counter and a map of Runnable\n        # hashcodes to queues that they have been associated with. This ensures\n        # maximum fairness in terms of work distribution, meaning that each worker\n        # will get approximately equal amount of mailboxes to execute. This is suitable\n        # in cases where we have a small number of actors that will be scheduled on\n        # the pool and we want to ensure the maximum possible utilization of the\n        # available threads.\n        # - hash based - the task - queue in which the runnable should go is determined\n        # by using an uniformly distributed int to int hash function which uses the\n        # hash code of the Runnable as an input. This is preferred in situations where we\n        # have enough number of distinct actors to ensure statistically uniform\n        # distribution of work across threads or we are ready to sacrifice the\n        # former for the added benefit of avoiding map look-ups.\n        fair-work-distribution {\n          # The value serves as a threshold which determines the point at which the\n          # pool switches from the first to the second work distribution schemes.\n          # For example, if the value is set to 128, the pool can observe up to\n          # 128 unique actors and schedule their mailboxes using the map based\n          # approach. Once this number is reached the pool switches to hash based\n          # task distribution mode. If the value is set to 0, the map based\n          # work distribution approach is disabled and only the hash based is\n          # used irrespective of the number of unique actors. Valid range is\n          # 0 to 2048 (inclusive)\n          threshold = 128\n        }\n      }\n\n      # This will be used if you have set \"executor = \"fork-join-executor\"\"\n      # Underlying thread pool implementation is java.util.concurrent.ForkJoinPool\n      fork-join-executor {\n        # Min number of threads to cap factor-based parallelism number to\n        parallelism-min = 8\n\n        # The parallelism factor is used to determine thread pool size using the\n        # following formula: ceil(available processors * factor). Resulting size\n        # is then bounded by the parallelism-min and parallelism-max values.\n        parallelism-factor = 1.0\n\n        # Max number of threads to cap factor-based parallelism number to\n        parallelism-max = 64\n\n        # Setting to \"FIFO\" to use queue like peeking mode which \"poll\" or \"LIFO\" to use stack\n        # like peeking mode which \"pop\".\n        task-peeking-mode = \"FIFO\"\n      }\n\n      # This will be used if you have set \"executor = \"thread-pool-executor\"\"\n      # Underlying thread pool implementation is java.util.concurrent.ThreadPoolExecutor\n      thread-pool-executor {\n        # Keep alive time for threads\n        keep-alive-time = 60s\n\n        # Define a fixed thread pool size with this property. The corePoolSize\n        # and the maximumPoolSize of the ThreadPoolExecutor will be set to this\n        # value, if it is defined. Then the other pool-size properties will not\n        # be used.\n        #\n        # Valid values are: `off` or a positive integer.\n        fixed-pool-size = off\n\n        # Min number of threads to cap factor-based corePoolSize number to\n        core-pool-size-min = 8\n\n        # The core-pool-size-factor is used to determine corePoolSize of the\n        # ThreadPoolExecutor using the following formula:\n        # ceil(available processors * factor).\n        # Resulting size is then bounded by the core-pool-size-min and\n        # core-pool-size-max values.\n        core-pool-size-factor = 3.0\n\n        # Max number of threads to cap factor-based corePoolSize number to\n        core-pool-size-max = 64\n\n        # Minimum number of threads to cap factor-based maximumPoolSize number to\n        max-pool-size-min = 8\n\n        # The max-pool-size-factor is used to determine maximumPoolSize of the\n        # ThreadPoolExecutor using the following formula:\n        # ceil(available processors * factor)\n        # The maximumPoolSize will not be less than corePoolSize.\n        # It is only used if using a bounded task queue.\n        max-pool-size-factor  = 3.0\n\n        # Max number of threads to cap factor-based maximumPoolSize number to\n        max-pool-size-max = 64\n\n        # Specifies the bounded capacity of the task queue (< 1 == unbounded)\n        task-queue-size = -1\n\n        # Specifies which type of task queue will be used, can be \"array\" or\n        # \"linked\" (default)\n        task-queue-type = \"linked\"\n\n        # Allow core threads to time out\n        allow-core-timeout = on\n      }\n\n      # How long time the dispatcher will wait for new actors until it shuts down\n      shutdown-timeout = 1s\n\n      # Throughput defines the number of messages that are processed in a batch\n      # before the thread is returned to the pool. Set to 1 for as fair as possible.\n      throughput = 5\n\n      # Throughput deadline for Dispatcher, set to 0 or negative for no deadline\n      throughput-deadline-time = 0ms\n\n      # For BalancingDispatcher: If the balancing dispatcher should attempt to\n      # schedule idle actors using the same dispatcher when a message comes in,\n      # and the dispatchers ExecutorService is not fully busy already.\n      attempt-teamwork = on\n\n      # If this dispatcher requires a specific type of mailbox, specify the\n      # fully-qualified class name here; the actually created mailbox will\n      # be a subtype of this type. The empty string signifies no requirement.\n      mailbox-requirement = \"\"\n    }\n\n    # Default separate internal dispatcher to run Pekko internal tasks and actors on\n    # protecting them against starvation because of accidental blocking in user actors (which run on the\n    # default dispatcher)\n    internal-dispatcher {\n      type = \"Dispatcher\"\n      executor = \"fork-join-executor\"\n      throughput = 5\n      fork-join-executor {\n        parallelism-min = 4\n        parallelism-factor = 1.0\n        parallelism-max = 64\n      }\n    }\n\n    default-blocking-io-dispatcher {\n      type = \"Dispatcher\"\n      executor = \"thread-pool-executor\"\n      throughput = 1\n\n      thread-pool-executor {\n        fixed-pool-size = 16\n      }\n    }\n\n    default-mailbox {\n      # FQCN of the MailboxType. The Class of the FQCN must have a public\n      # constructor with\n      # (org.apache.pekko.actor.ActorSystem.Settings, com.typesafe.config.Config) parameters.\n      mailbox-type = \"org.apache.pekko.dispatch.UnboundedMailbox\"\n\n      # If the mailbox is bounded then it uses this setting to determine its\n      # capacity. The provided value must be positive.\n      # NOTICE:\n      # Up to version 2.1 the mailbox type was determined based on this setting;\n      # this is no longer the case, the type must explicitly be a bounded mailbox.\n      mailbox-capacity = 1000\n\n      # If the mailbox is bounded then this is the timeout for enqueueing\n      # in case the mailbox is full. Negative values signify infinite\n      # timeout, which should be avoided as it bears the risk of dead-lock.\n      mailbox-push-timeout-time = 10s\n\n      # For Actor with Stash: The default capacity of the stash.\n      # If negative (or zero) then an unbounded stash is used (default)\n      # If positive then a bounded stash is used and the capacity is set using\n      # the property\n      stash-capacity = -1\n    }\n\n    mailbox {\n      # Mapping between message queue semantics and mailbox configurations.\n      # Used by org.apache.pekko.dispatch.RequiresMessageQueue[T] to enforce different\n      # mailbox types on actors.\n      # If your Actor implements RequiresMessageQueue[T], then when you create\n      # an instance of that actor its mailbox type will be decided by looking\n      # up a mailbox configuration via T in this mapping\n      requirements {\n        \"org.apache.pekko.dispatch.UnboundedMessageQueueSemantics\" =\n          pekko.actor.mailbox.unbounded-queue-based\n        \"org.apache.pekko.dispatch.BoundedMessageQueueSemantics\" =\n          pekko.actor.mailbox.bounded-queue-based\n        \"org.apache.pekko.dispatch.DequeBasedMessageQueueSemantics\" =\n          pekko.actor.mailbox.unbounded-deque-based\n        \"org.apache.pekko.dispatch.UnboundedDequeBasedMessageQueueSemantics\" =\n          pekko.actor.mailbox.unbounded-deque-based\n        \"org.apache.pekko.dispatch.BoundedDequeBasedMessageQueueSemantics\" =\n          pekko.actor.mailbox.bounded-deque-based\n        \"org.apache.pekko.dispatch.MultipleConsumerSemantics\" =\n          pekko.actor.mailbox.unbounded-queue-based\n        \"org.apache.pekko.dispatch.ControlAwareMessageQueueSemantics\" =\n          pekko.actor.mailbox.unbounded-control-aware-queue-based\n        \"org.apache.pekko.dispatch.UnboundedControlAwareMessageQueueSemantics\" =\n          pekko.actor.mailbox.unbounded-control-aware-queue-based\n        \"org.apache.pekko.dispatch.BoundedControlAwareMessageQueueSemantics\" =\n          pekko.actor.mailbox.bounded-control-aware-queue-based\n        \"org.apache.pekko.event.LoggerMessageQueueSemantics\" =\n          pekko.actor.mailbox.logger-queue\n      }\n\n      unbounded-queue-based {\n        # FQCN of the MailboxType, The Class of the FQCN must have a public\n        # constructor with (org.apache.pekko.actor.ActorSystem.Settings,\n        # com.typesafe.config.Config) parameters.\n        mailbox-type = \"org.apache.pekko.dispatch.UnboundedMailbox\"\n      }\n\n      bounded-queue-based {\n        # FQCN of the MailboxType, The Class of the FQCN must have a public\n        # constructor with (org.apache.pekko.actor.ActorSystem.Settings,\n        # com.typesafe.config.Config) parameters.\n        mailbox-type = \"org.apache.pekko.dispatch.BoundedMailbox\"\n      }\n\n      unbounded-deque-based {\n        # FQCN of the MailboxType, The Class of the FQCN must have a public\n        # constructor with (org.apache.pekko.actor.ActorSystem.Settings,\n        # com.typesafe.config.Config) parameters.\n        mailbox-type = \"org.apache.pekko.dispatch.UnboundedDequeBasedMailbox\"\n      }\n\n      bounded-deque-based {\n        # FQCN of the MailboxType, The Class of the FQCN must have a public\n        # constructor with (org.apache.pekko.actor.ActorSystem.Settings,\n        # com.typesafe.config.Config) parameters.\n        mailbox-type = \"org.apache.pekko.dispatch.BoundedDequeBasedMailbox\"\n      }\n\n      unbounded-control-aware-queue-based {\n        # FQCN of the MailboxType, The Class of the FQCN must have a public\n        # constructor with (org.apache.pekko.actor.ActorSystem.Settings,\n        # com.typesafe.config.Config) parameters.\n        mailbox-type = \"org.apache.pekko.dispatch.UnboundedControlAwareMailbox\"\n      }\n\n      bounded-control-aware-queue-based {\n        # FQCN of the MailboxType, The Class of the FQCN must have a public\n        # constructor with (org.apache.pekko.actor.ActorSystem.Settings,\n        # com.typesafe.config.Config) parameters.\n        mailbox-type = \"org.apache.pekko.dispatch.BoundedControlAwareMailbox\"\n      }\n\n      # The LoggerMailbox will drain all messages in the mailbox\n      # when the system is shutdown and deliver them to the StandardOutLogger.\n      # Do not change this unless you know what you are doing.\n      logger-queue {\n        mailbox-type = \"org.apache.pekko.event.LoggerMailboxType\"\n      }\n    }\n\n    debug {\n      # enable function of Actor.loggable(), which is to log any received message\n      # at DEBUG level, see the “Testing Actor Systems” section of the Pekko\n      # Documentation at https://pekko.apache.org/docs/pekko/current/\n      receive = off\n\n      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill etc.)\n      autoreceive = off\n\n      # enable DEBUG logging of actor lifecycle changes\n      lifecycle = off\n\n      # enable DEBUG logging of all LoggingFSMs for events, transitions and timers\n      fsm = off\n\n      # enable DEBUG logging of subscription changes on the eventStream\n      event-stream = off\n\n      # enable DEBUG logging of unhandled messages\n      unhandled = off\n\n      # enable WARN logging of misconfigured routers\n      router-misconfiguration = off\n    }\n\n    # SECURITY BEST-PRACTICE is to disable java serialization for its multiple\n    # known attack surfaces.\n    #\n    # This setting is a short-cut to\n    # - using DisabledJavaSerializer instead of JavaSerializer\n    #\n    # Completely disable the use of `org.apache.pekko.serialization.JavaSerialization` by the\n    # Pekko Serialization extension, instead DisabledJavaSerializer will\n    # be inserted which will fail explicitly if attempts to use java serialization are made.\n    #\n    # The log messages emitted by such serializer SHOULD be treated as potential\n    # attacks which the serializer prevented, as they MAY indicate an external operator\n    # attempting to send malicious messages intending to use java serialization as attack vector.\n    # The attempts are logged with the SECURITY marker.\n    #\n    # Please note that this option does not stop you from manually invoking java serialization\n    #\n    allow-java-serialization = off\n\n    # Log warnings when the Java serialization is used to serialize messages.\n    # Java serialization is not very performant and should not be used in production\n    # environments unless you don't care about performance and security. In that case\n    # you can turn this off.\n    warn-about-java-serializer-usage = on\n\n    # To be used with the above warn-about-java-serializer-usage\n    # When warn-about-java-serializer-usage = on, and this warn-on-no-serialization-verification = off,\n    # warnings are suppressed for classes extending NoSerializationVerificationNeeded\n    # to reduce noise.\n    warn-on-no-serialization-verification = on\n\n    # Entries for pluggable serializers and their bindings.\n    serializers {\n      java = \"org.apache.pekko.serialization.JavaSerializer\"\n      bytes = \"org.apache.pekko.serialization.ByteArraySerializer\"\n      primitive-long = \"org.apache.pekko.serialization.LongSerializer\"\n      primitive-int = \"org.apache.pekko.serialization.IntSerializer\"\n      primitive-string = \"org.apache.pekko.serialization.StringSerializer\"\n      primitive-bytestring = \"org.apache.pekko.serialization.ByteStringSerializer\"\n      primitive-boolean = \"org.apache.pekko.serialization.BooleanSerializer\"\n    }\n\n    # Class to Serializer binding. You only need to specify the name of an\n    # interface or abstract base class of the messages. In case of ambiguity it\n    # is using the most specific configured class, or giving a warning and\n    # choosing the “first” one.\n    #\n    # To disable one of the default serializers, assign its class to \"none\", like\n    # \"java.io.Serializable\" = none\n    serialization-bindings {\n      \"[B\" = bytes\n      \"java.io.Serializable\" = java\n\n      \"java.lang.String\" = primitive-string\n      \"org.apache.pekko.util.ByteString$ByteString1C\" = primitive-bytestring\n      \"org.apache.pekko.util.ByteString$ByteString1\" = primitive-bytestring\n      \"org.apache.pekko.util.ByteString$ByteStrings\" = primitive-bytestring\n      \"java.lang.Long\" = primitive-long\n      \"scala.Long\" = primitive-long\n      \"java.lang.Integer\" = primitive-int\n      \"scala.Int\" = primitive-int\n      \"java.lang.Boolean\" = primitive-boolean\n      \"scala.Boolean\" = primitive-boolean\n    }\n\n    # Configuration namespace of serialization identifiers.\n    # Each serializer implementation must have an entry in the following format:\n    # `org.apache.pekko.actor.serialization-identifiers.\"FQCN\" = ID`\n    # where `FQCN` is fully qualified class name of the serializer implementation\n    # and `ID` is globally unique serializer identifier number.\n    # Identifier values from 0 to 40 are reserved for Pekko internal usage.\n    serialization-identifiers {\n      \"org.apache.pekko.serialization.JavaSerializer\" = 1\n      \"org.apache.pekko.serialization.ByteArraySerializer\" = 4\n\n      primitive-long = 18\n      primitive-int = 19\n      primitive-string = 20\n      primitive-bytestring = 21\n      primitive-boolean = 35\n    }\n\n  }\n\n  serialization.protobuf {\n    # deprecated, use `allowed-classes` instead\n    whitelist-class = [\n      \"com.google.protobuf.GeneratedMessage\",\n      \"com.google.protobuf.GeneratedMessageV3\",\n      \"scalapb.GeneratedMessageCompanion\",\n      \"org.apache.pekko.protobuf.GeneratedMessage\",\n      \"org.apache.pekko.protobufv3.internal.GeneratedMessageV3\"\n    ]\n\n    # Additional classes that are allowed even if they are not defined in `serialization-bindings`.\n    # It can be exact class name or name of super class or interfaces (one level).\n    # This is useful when a class is not used for serialization any more and therefore removed\n    # from `serialization-bindings`, but should still be possible to deserialize.\n    allowed-classes = ${pekko.serialization.protobuf.whitelist-class}\n\n  }\n\n  # Used to set the behavior of the scheduler.\n  # Changing the default values may change the system behavior drastically so make\n  # sure you know what you're doing! See the Scheduler section of the Pekko\n  # Documentation for more details.\n  scheduler {\n    # The LightArrayRevolverScheduler is used as the default scheduler in the\n    # system. It does not execute the scheduled tasks on exact time, but on every\n    # tick, it will run everything that is (over)due. You can increase or decrease\n    # the accuracy of the execution timing by specifying smaller or larger tick\n    # duration. If you are scheduling a lot of tasks you should consider increasing\n    # the ticks per wheel.\n    # Note that it might take up to 1 tick to stop the Timer, so setting the\n    # tick-duration to a high value will make shutting down the actor system\n    # take longer.\n    tick-duration = 10ms\n\n    # The timer uses a circular wheel of buckets to store the timer tasks.\n    # This should be set such that the majority of scheduled timeouts (for high\n    # scheduling frequency) will be shorter than one rotation of the wheel\n    # (ticks-per-wheel * ticks-duration)\n    # THIS MUST BE A POWER OF TWO!\n    ticks-per-wheel = 512\n\n    # This setting selects the timer implementation which shall be loaded at\n    # system start-up.\n    # The class given here must implement the org.apache.pekko.actor.Scheduler interface\n    # and offer a public constructor which takes three arguments:\n    #  1) com.typesafe.config.Config\n    #  2) org.apache.pekko.event.LoggingAdapter\n    #  3) java.util.concurrent.ThreadFactory\n    implementation = org.apache.pekko.actor.LightArrayRevolverScheduler\n\n    # When shutting down the scheduler, there will typically be a thread which\n    # needs to be stopped, and this timeout determines how long to wait for\n    # that to happen. In case of timeout the shutdown of the actor system will\n    # proceed without running possibly still enqueued tasks.\n    shutdown-timeout = 5s\n  }\n\n  io {\n\n    # By default the select loops run on dedicated threads, hence using a\n    # PinnedDispatcher\n    pinned-dispatcher {\n      type = \"PinnedDispatcher\"\n      executor = \"thread-pool-executor\"\n      thread-pool-executor.allow-core-timeout = off\n    }\n\n    tcp {\n\n      # The number of selectors to stripe the served channels over; each of\n      # these will use one select loop on the selector-dispatcher.\n      nr-of-selectors = 1\n\n      # Maximum number of open channels supported by this TCP module; there is\n      # no intrinsic general limit, this setting is meant to enable DoS\n      # protection by limiting the number of concurrently connected clients.\n      # Also note that this is a \"soft\" limit; in certain cases the implementation\n      # will accept a few connections more or a few less than the number configured\n      # here. Must be an integer > 0 or \"unlimited\".\n      max-channels = 256000\n\n      # When trying to assign a new connection to a selector and the chosen\n      # selector is at full capacity, retry selector choosing and assignment\n      # this many times before giving up\n      selector-association-retries = 10\n\n      # The maximum number of connection that are accepted in one go,\n      # higher numbers decrease latency, lower numbers increase fairness on\n      # the worker-dispatcher\n      batch-accept-limit = 10\n\n      # The number of bytes per direct buffer in the pool used to read or write\n      # network data from the kernel.\n      direct-buffer-size = 128 KiB\n\n      # The maximal number of direct buffers kept in the direct buffer pool for\n      # reuse.\n      direct-buffer-pool-limit = 1000\n\n      # The duration a connection actor waits for a `Register` message from\n      # its commander before aborting the connection.\n      register-timeout = 5s\n\n      # The maximum number of bytes delivered by a `Received` message. Before\n      # more data is read from the network the connection actor will try to\n      # do other work.\n      # The purpose of this setting is to impose a smaller limit than the\n      # configured receive buffer size. When using value 'unlimited' it will\n      # try to read all from the receive buffer.\n      max-received-message-size = unlimited\n\n      # Enable fine grained logging of what goes on inside the implementation.\n      # Be aware that this may log more than once per message sent to the actors\n      # of the tcp implementation.\n      trace-logging = off\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # to be used for running the select() calls in the selectors\n      selector-dispatcher = \"pekko.io.pinned-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # for the read/write worker actors\n      worker-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # for the selector management actors\n      management-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # on which file IO tasks are scheduled\n      file-io-dispatcher = \"pekko.actor.default-blocking-io-dispatcher\"\n\n      # The maximum number of bytes (or \"unlimited\") to transfer in one batch\n      # when using `WriteFile` command which uses `FileChannel.transferTo` to\n      # pipe files to a TCP socket. On some OS like Linux `FileChannel.transferTo`\n      # may block for a long time when network IO is faster than file IO.\n      # Decreasing the value may improve fairness while increasing may improve\n      # throughput.\n      file-io-transferTo-limit = 512 KiB\n\n      # The number of times to retry the `finishConnect` call after being notified about\n      # OP_CONNECT. Retries are needed if the OP_CONNECT notification doesn't imply that\n      # `finishConnect` will succeed, which is the case on Android.\n      finish-connect-retries = 5\n\n      # On Windows connection aborts are not reliably detected unless an OP_READ is\n      # registered on the selector _after_ the connection has been reset. This\n      # workaround enables an OP_CONNECT which forces the abort to be visible on Windows.\n      # Enabling this setting on other platforms than Windows will cause various failures\n      # and undefined behavior.\n      # Possible values of this key are on, off and auto where auto will enable the\n      # workaround if Windows is detected automatically.\n      windows-connection-abort-workaround-enabled = off\n    }\n\n    udp {\n\n      # The number of selectors to stripe the served channels over; each of\n      # these will use one select loop on the selector-dispatcher.\n      nr-of-selectors = 1\n\n      # Maximum number of open channels supported by this UDP module Generally\n      # UDP does not require a large number of channels, therefore it is\n      # recommended to keep this setting low.\n      max-channels = 4096\n\n      # The select loop can be used in two modes:\n      # - setting \"infinite\" will select without a timeout, hogging a thread\n      # - setting a positive timeout will do a bounded select call,\n      #   enabling sharing of a single thread between multiple selectors\n      #   (in this case you will have to use a different configuration for the\n      #   selector-dispatcher, e.g. using \"type=Dispatcher\" with size 1)\n      # - setting it to zero means polling, i.e. calling selectNow()\n      select-timeout = infinite\n\n      # When trying to assign a new connection to a selector and the chosen\n      # selector is at full capacity, retry selector choosing and assignment\n      # this many times before giving up\n      selector-association-retries = 10\n\n      # The maximum number of datagrams that are read in one go,\n      # higher numbers decrease latency, lower numbers increase fairness on\n      # the worker-dispatcher\n      receive-throughput = 3\n\n      # The number of bytes per direct buffer in the pool used to read or write\n      # network data from the kernel.\n      direct-buffer-size = 128 KiB\n\n      # The maximal number of direct buffers kept in the direct buffer pool for\n      # reuse.\n      direct-buffer-pool-limit = 1000\n\n      # Enable fine grained logging of what goes on inside the implementation.\n      # Be aware that this may log more than once per message sent to the actors\n      # of the tcp implementation.\n      trace-logging = off\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # to be used for running the select() calls in the selectors\n      selector-dispatcher = \"pekko.io.pinned-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # for the read/write worker actors\n      worker-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # for the selector management actors\n      management-dispatcher = \"pekko.actor.internal-dispatcher\"\n    }\n\n    udp-connected {\n\n      # The number of selectors to stripe the served channels over; each of\n      # these will use one select loop on the selector-dispatcher.\n      nr-of-selectors = 1\n\n      # Maximum number of open channels supported by this UDP module Generally\n      # UDP does not require a large number of channels, therefore it is\n      # recommended to keep this setting low.\n      max-channels = 4096\n\n      # The select loop can be used in two modes:\n      # - setting \"infinite\" will select without a timeout, hogging a thread\n      # - setting a positive timeout will do a bounded select call,\n      #   enabling sharing of a single thread between multiple selectors\n      #   (in this case you will have to use a different configuration for the\n      #   selector-dispatcher, e.g. using \"type=Dispatcher\" with size 1)\n      # - setting it to zero means polling, i.e. calling selectNow()\n      select-timeout = infinite\n\n      # When trying to assign a new connection to a selector and the chosen\n      # selector is at full capacity, retry selector choosing and assignment\n      # this many times before giving up\n      selector-association-retries = 10\n\n      # The maximum number of datagrams that are read in one go,\n      # higher numbers decrease latency, lower numbers increase fairness on\n      # the worker-dispatcher\n      receive-throughput = 3\n\n      # The number of bytes per direct buffer in the pool used to read or write\n      # network data from the kernel.\n      direct-buffer-size = 128 KiB\n\n      # The maximal number of direct buffers kept in the direct buffer pool for\n      # reuse.\n      direct-buffer-pool-limit = 1000\n\n      # Enable fine grained logging of what goes on inside the implementation.\n      # Be aware that this may log more than once per message sent to the actors\n      # of the tcp implementation.\n      trace-logging = off\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # to be used for running the select() calls in the selectors\n      selector-dispatcher = \"pekko.io.pinned-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # for the read/write worker actors\n      worker-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # for the selector management actors\n      management-dispatcher = \"pekko.actor.internal-dispatcher\"\n    }\n\n    dns {\n      # Fully qualified config path which holds the dispatcher configuration\n      # for the manager and resolver router actors.\n      # For actual router configuration see pekko.actor.deployment./IO-DNS/*\n      dispatcher = \"pekko.actor.internal-dispatcher\"\n\n      # Name of the subconfig at path pekko.io.dns, see inet-address below\n      #\n      # Change to `async-dns` to use the new \"native\" DNS resolver,\n      # which is also capable of resolving SRV records.\n      resolver = \"inet-address\"\n\n      # To-be-deprecated DNS resolver implementation which uses the Java InetAddress to resolve DNS records.\n      # To be replaced by `pekko.io.dns.async` which implements the DNS protocol natively and without blocking (which InetAddress does)\n      inet-address {\n        # Must implement org.apache.pekko.io.DnsProvider\n        provider-object = \"org.apache.pekko.io.InetAddressDnsProvider\"\n\n        # To set the time to cache name resolutions\n        # Possible values:\n        # default: sun.net.InetAddressCachePolicy.get() and getNegative()\n        # forever: cache forever\n        # never: no caching\n        # n [time unit]: positive timeout with unit, for example 30s\n        positive-ttl = default\n        negative-ttl = default\n\n        # How often to sweep out expired cache entries.\n        # Note that this interval has nothing to do with TTLs\n        cache-cleanup-interval = 120s\n      }\n\n      async-dns {\n        provider-object = \"org.apache.pekko.io.dns.internal.AsyncDnsProvider\"\n\n        # Set upper bound for caching successfully resolved dns entries\n        # if the DNS record has a smaller TTL value than the setting that\n        # will be used. Default is to use the record TTL with no cap.\n        # Possible values:\n        # forever: always use the minimum TTL from the found records\n        # never: never cache\n        # n [time unit] = cap the caching to this value\n        positive-ttl = forever\n\n        # Set how long the fact that a DNS record could not be found is\n        # cached. If a new resolution is done while the fact is cached it will\n        # be failed and not result in an actual DNS resolution. Default is\n        # to never cache.\n        # Possible values:\n        # never: never cache\n        # forever: cache a missing DNS record forever (you probably will not want to do this)\n        # n [time unit] = cache for this long\n        negative-ttl = never\n\n        # Configures nameservers to query during DNS resolution.\n        # Defaults to the nameservers that would be used by the JVM by default.\n        # Set to a list of IPs to override the servers, e.g. [ \"8.8.8.8\", \"8.8.4.4\" ] for Google's servers\n        # If multiple are defined then they are tried in order until one responds\n        nameservers = default\n\n        # The time that a request is allowed to live before being discarded\n        # given no reply. The lower bound of this should always be the amount\n        # of time to reasonably expect a DNS server to reply within.\n        # If multiple name servers are provided then each gets this long to response before trying\n        # the next one\n        resolve-timeout = 5s\n\n        # How often to sweep out expired cache entries.\n        # Note that this interval has nothing to do with TTLs\n        cache-cleanup-interval = 120s\n\n        # Configures the list of search domains.\n        # Defaults to a system dependent lookup (on Unix like OSes, will attempt to parse /etc/resolv.conf, on\n        # other platforms, will not make any attempt to lookup the search domains). Set to a single domain, or\n        # a list of domains, eg, [ \"example.com\", \"example.net\" ].\n        search-domains = default\n\n        # Any hosts that have a number of dots less than this will not be looked up directly, instead, a search on\n        # the search domains will be tried first. This corresponds to the ndots option in /etc/resolv.conf, see\n        # https://linux.die.net/man/5/resolver for more info.\n        # Defaults to a system dependent lookup (on Unix like OSes, will attempt to parse /etc/resolv.conf, on\n        # other platforms, will default to 1).\n        ndots = default\n      }\n    }\n  }\n\n\n  # CoordinatedShutdown is an extension that will perform registered\n  # tasks in the order that is defined by the phases. It is started\n  # by calling CoordinatedShutdown(system).run(). This can be triggered\n  # by different things, for example:\n  # - JVM shutdown hook will by default run CoordinatedShutdown\n  # - Cluster node will automatically run CoordinatedShutdown when it\n  #   sees itself as Exiting\n  # - A management console or other application specific command can\n  #   run CoordinatedShutdown\n  coordinated-shutdown {\n    # The timeout that will be used for a phase if not specified with\n    # 'timeout' in the phase\n    default-phase-timeout = 5 s\n\n    # Terminate the ActorSystem in the last phase actor-system-terminate.\n    terminate-actor-system = on\n\n    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate\n    # if this is set to 'on'. It is done after termination of the\n    # ActorSystem if terminate-actor-system=on, otherwise it is done\n    # immediately when the last phase is reached.\n    exit-jvm = off\n\n    # Exit status to use on System.exit(int) when 'exit-jvm' is 'on'.\n    exit-code = 0\n\n    # Run the coordinated shutdown when the JVM process exits, e.g.\n    # via kill SIGTERM signal (SIGINT ctrl-c doesn't work).\n    # This property is related to `pekko.jvm-shutdown-hooks` above.\n    run-by-jvm-shutdown-hook = on\n\n    # Run the coordinated shutdown when ActorSystem.terminate is called.\n    # Enabling this and disabling terminate-actor-system is not a supported\n    # combination (will throw ConfigurationException at startup).\n    run-by-actor-system-terminate = on\n\n    # When Coordinated Shutdown is triggered an instance of `Reason` is\n    # required. That value can be used to override the default settings.\n    # Only 'exit-jvm', 'exit-code' and 'terminate-actor-system' may be\n    # overridden depending on the reason.\n    reason-overrides {\n      # Overrides are applied using the `reason.getClass.getName`.\n      # Overrides the `exit-code` when the `Reason` is a cluster\n      # Downing or a Cluster Join Unsuccessful event\n      \"org.apache.pekko.actor.CoordinatedShutdown$ClusterDowningReason$\" {\n        exit-code = -1\n      }\n      \"org.apache.pekko.actor.CoordinatedShutdown$ClusterJoinUnsuccessfulReason$\" {\n        exit-code = -1\n      }\n    }\n\n    #//#coordinated-shutdown-phases\n    # CoordinatedShutdown is enabled by default and will run the tasks that\n    # are added to these phases by individual Pekko modules and user logic.\n    #\n    # The phases are ordered as a DAG by defining the dependencies between the phases\n    # to make sure shutdown tasks are run in the right order.\n    #\n    # In general user tasks belong in the first few phases, but there may be use\n    # cases where you would want to hook in new phases or register tasks later in\n    # the DAG.\n    #\n    # Each phase is defined as a named config section with the\n    # following optional properties:\n    # - timeout=15s: Override the default-phase-timeout for this phase.\n    # - recover=off: If the phase fails the shutdown is aborted\n    #                and depending phases will not be executed.\n    # - enabled=off: Skip all tasks registered in this phase. DO NOT use\n    #                this to disable phases unless you are absolutely sure what the\n    #                consequences are. Many of the built in tasks depend on other tasks\n    #                having been executed in earlier phases and may break if those are disabled.\n    # depends-on=[]: Run the phase after the given phases\n    phases {\n\n      # The first pre-defined phase that applications can add tasks to.\n      # Note that more phases can be added in the application's\n      # configuration by overriding this phase with an additional\n      # depends-on.\n      before-service-unbind {\n      }\n\n      # Stop accepting new incoming connections.\n      # This is where you can register tasks that makes a server stop accepting new connections. Already\n      # established connections should be allowed to continue and complete if possible.\n      service-unbind {\n        depends-on = [before-service-unbind]\n      }\n\n      # Wait for requests that are in progress to be completed.\n      # This is where you register tasks that will wait for already established connections to complete, potentially\n      # also first telling them that it is time to close down.\n      service-requests-done {\n        depends-on = [service-unbind]\n      }\n\n      # Final shutdown of service endpoints.\n      # This is where you would add tasks that forcefully kill connections that are still around.\n      service-stop {\n        depends-on = [service-requests-done]\n      }\n\n      # Phase for custom application tasks that are to be run\n      # after service shutdown and before cluster shutdown.\n      before-cluster-shutdown {\n        depends-on = [service-stop]\n      }\n\n      # Graceful shutdown of the Cluster Sharding regions.\n      # This phase is not meant for users to add tasks to.\n      cluster-sharding-shutdown-region {\n        timeout = 10 s\n        depends-on = [before-cluster-shutdown]\n      }\n\n      # Emit the leave command for the node that is shutting down.\n      # This phase is not meant for users to add tasks to.\n      cluster-leave {\n        depends-on = [cluster-sharding-shutdown-region]\n      }\n\n      # Shutdown cluster singletons\n      # This is done as late as possible to allow the shard region shutdown triggered in\n      # the \"cluster-sharding-shutdown-region\" phase to complete before the shard coordinator is shut down.\n      # This phase is not meant for users to add tasks to.\n      cluster-exiting {\n        timeout = 10 s\n        depends-on = [cluster-leave]\n      }\n\n      # Wait until exiting has been completed\n      # This phase is not meant for users to add tasks to.\n      cluster-exiting-done {\n        depends-on = [cluster-exiting]\n      }\n\n      # Shutdown the cluster extension\n      # This phase is not meant for users to add tasks to.\n      cluster-shutdown {\n        depends-on = [cluster-exiting-done]\n      }\n\n      # Phase for custom application tasks that are to be run\n      # after cluster shutdown and before ActorSystem termination.\n      before-actor-system-terminate {\n        depends-on = [cluster-shutdown]\n      }\n\n      # Last phase. See terminate-actor-system and exit-jvm above.\n      # Don't add phases that depends on this phase because the\n      # dispatcher and scheduler of the ActorSystem have been shutdown.\n      # This phase is not meant for users to add tasks to.\n      actor-system-terminate {\n        timeout = 10 s\n        depends-on = [before-actor-system-terminate]\n      }\n    }\n    #//#coordinated-shutdown-phases\n  }\n\n  #//#circuit-breaker-default\n  # Configuration for circuit breakers created with the APIs accepting an id to\n  # identify or look up the circuit breaker.\n  # Note: Circuit breakers created without ids are not affected by this configuration.\n  # A child configuration section with the same name as the circuit breaker identifier\n  # will be used, with fallback to the `pekko.circuit-breaker.default` section.\n  circuit-breaker {\n\n    # Default configuration that is used if a configuration section\n    # with the circuit breaker identifier is not defined.\n    default {\n      # Number of failures before opening the circuit.\n      max-failures = 10\n\n      # Duration of time after which to consider a call a failure.\n      call-timeout = 10s\n\n      # Duration of time in open state after which to attempt to close\n      # the circuit, by first entering the half-open state.\n      reset-timeout = 15s\n\n      # The upper bound of reset-timeout\n      max-reset-timeout = 36500d\n\n      # Exponential backoff\n      # For details see https://en.wikipedia.org/wiki/Exponential_backoff\n      exponential-backoff = 1.0\n\n      # Additional random delay based on this factor is added to backoff\n      # For example 0.2 adds up to 20% delay\n      # In order to skip this additional delay set as 0\n      random-factor = 0.0\n\n      # A allowlist of fqcn of Exceptions that the CircuitBreaker\n      # should not consider failures. By default all exceptions are\n      # considered failures.\n      exception-allowlist = []\n    }\n  }\n  #//#circuit-breaker-default\n\n}","title":"pekko-actor"},{"location":"/general/configuration-reference.html#pekko-actor-typed","text":"copysourcepekko.actor.typed {\n\n  # List FQCN of `org.apache.pekko.actor.typed.ExtensionId`s which shall be loaded at actor system startup.\n  # Should be on the format: 'extensions = [\"com.example.MyExtId1\", \"com.example.MyExtId2\"]' etc.\n  # See the Pekko Documentation for more info about Extensions\n  extensions = []\n\n  # List FQCN of extensions which shall be loaded at actor system startup.\n  # Library extensions are regular extensions that are loaded at startup and are\n  # available for third party library authors to enable auto-loading of extensions when\n  # present on the classpath. This is done by appending entries:\n  # 'library-extensions += \"Extension\"' in the library `reference.conf`.\n  #\n  # Should not be set by end user applications in 'application.conf', use the extensions property for that\n  #\n  library-extensions = ${?pekko.actor.typed.library-extensions} []\n\n  # Receptionist is started eagerly to allow clustered receptionist to gather remote registrations early on.\n  library-extensions += \"org.apache.pekko.actor.typed.receptionist.Receptionist$\"\n\n  # While an actor is restarted (waiting for backoff to expire and children to stop)\n  # incoming messages and signals are stashed, and delivered later to the newly restarted\n  # behavior. This property defines the capacity in number of messages of the stash\n  # buffer. If the capacity is exceed then additional incoming messages are dropped.\n  restart-stash-capacity = 1000\n\n  # Typed mailbox defaults to the single consumber mailbox as balancing dispatcher is not supported\n  default-mailbox {\n    mailbox-type = \"org.apache.pekko.dispatch.SingleConsumerOnlyUnboundedMailbox\"\n  }\n}\n\n# Load typed extensions by a classic extension.\npekko.library-extensions += \"org.apache.pekko.actor.typed.internal.adapter.ActorSystemAdapter$LoadTypedExtensions\"\n\npekko.actor {\n  serializers {\n    typed-misc = \"org.apache.pekko.actor.typed.internal.MiscMessageSerializer\"\n    service-key = \"org.apache.pekko.actor.typed.internal.receptionist.ServiceKeySerializer\"\n  }\n\n  serialization-identifiers {\n    \"org.apache.pekko.actor.typed.internal.MiscMessageSerializer\" = 24\n    \"org.apache.pekko.actor.typed.internal.receptionist.ServiceKeySerializer\" = 26\n  }\n\n  serialization-bindings {\n    \"org.apache.pekko.actor.typed.ActorRef\" = typed-misc\n    \"org.apache.pekko.actor.typed.internal.adapter.ActorRefAdapter\" = typed-misc\n    \"org.apache.pekko.actor.typed.internal.receptionist.DefaultServiceKey\" = service-key\n  }\n}\n\n# When using Pekko Typed (having pekko-actor-typed in classpath) the\n# org.apache.pekko.event.slf4j.Slf4jLogger is enabled instead of the DefaultLogger\n# even though it has not been explicitly defined in `pekko.loggers`\n# configuration.\n#\n# Slf4jLogger will be used for all Pekko classic logging via eventStream,\n# including logging from Pekko internals. The Slf4jLogger is then using\n# an ordinary org.slf4j.Logger to emit the log events.\n#\n# The Slf4jLoggingFilter is also enabled automatically.\n#\n# This behavior can be disabled by setting this property to `off`.\npekko.use-slf4j = on\n\npekko.reliable-delivery {\n  producer-controller {\n\n    # To avoid head of line blocking from serialization and transfer\n    # of large messages this can be enabled.\n    # Large messages are chunked into pieces of the given size in bytes. The\n    # chunked messages are sent separatetely and assembled on the consumer side.\n    # Serialization and deserialization is performed by the ProducerController and\n    # ConsumerController respectively instead of in the remote transport layer.\n    chunk-large-messages = off\n\n    durable-queue {\n      # The ProducerController uses this timeout for the requests to\n      # the durable queue. If there is no reply within the timeout it\n      # will be retried.\n      request-timeout = 3s\n\n      # The ProducerController retries requests to the durable queue this\n      # number of times before failing.\n      retry-attempts = 10\n\n      # The ProducerController retries sending the first message with this interval\n      # until it has been confirmed.\n      resend-first-interval = 1s\n    }\n  }\n\n  consumer-controller {\n    # Number of messages in flight between ProducerController and\n    # ConsumerController. The ConsumerController requests for more messages\n    # when half of the window has been used.\n    flow-control-window = 50\n\n    # The ConsumerController resends flow control messages to the\n    # ProducerController with the resend-interval-min, and increasing\n    # it gradually to resend-interval-max when idle.\n    resend-interval-min = 2s\n    resend-interval-max = 30s\n\n    # If this is enabled lost messages will not be resent, but flow control is used.\n    # This can be more efficient since messages don't have to be\n    # kept in memory in the `ProducerController` until they have been\n    # confirmed, but the drawback is that lost messages will not be delivered.\n    only-flow-control = false\n  }\n\n  work-pulling {\n    producer-controller = ${pekko.reliable-delivery.producer-controller}\n    producer-controller {\n      # Limit of how many messages that can be buffered when there\n      # is no demand from the consumer side.\n      buffer-size = 1000\n\n      # Ask timeout for sending message to worker until receiving Ack from worker\n      internal-ask-timeout = 60s\n\n      # Chunked messages not implemented for work-pulling yet. Override to not\n      # propagate property from pekko.reliable-delivery.producer-controller.\n      chunk-large-messages = off\n    }\n  }\n}","title":"pekko-actor-typed"},{"location":"/general/configuration-reference.html#pekko-cluster-typed","text":"copysource############################################\n# Pekko Cluster Typed Reference Config File #\n############################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\npekko.cluster.typed.receptionist {\n  # Updates with Distributed Data are done with this consistency level.\n  # Possible values: local, majority, all, 2, 3, 4 (n)\n  write-consistency = local\n\n  # Period task to remove actor references that are hosted by removed nodes,\n  # in case of abrupt termination.\n  pruning-interval = 3 s\n\n  # The periodic task to remove actor references that are hosted by removed nodes\n  # will only remove entries older than this duration. The reason for this\n  # is to avoid removing entries of nodes that haven't been visible as joining.\n  prune-removed-older-than = 60 s\n\n  # Shard the services over this many Distributed Data keys, with large amounts of different\n  # service keys storing all of them in the same Distributed Data entry would lead to large updates\n  # etc. instead the keys are sharded across this number of keys. This must be the same on all nodes\n  # in a cluster, changing it requires a full cluster restart (stopping all nodes before starting them again)\n  distributed-key-count = 5\n\n  # Settings for the Distributed Data replicator used by Receptionist.\n  # Same layout as pekko.cluster.distributed-data.\n  distributed-data = ${pekko.cluster.distributed-data}\n  # make sure that by default it's for all roles (Play loads config in different way)\n  distributed-data.role = \"\"\n}\n\npekko.cluster.ddata.typed {\n  # The timeout to use for ask operations in ReplicatorMessageAdapter.\n  # This should be longer than the timeout given in Replicator.WriteConsistency and\n  # Replicator.ReadConsistency. The replicator will always send a reply within those\n  # timeouts so the unexpected ask timeout should not occur, but for cleanup in a\n  # failure situation it must still exist.\n  # If askUpdate, askGet or askDelete takes longer then this timeout a\n  # java.util.concurrent.TimeoutException will be thrown by the requesting actor and\n  # may be handled by supervision.\n  replicator-message-adapter-unexpected-ask-timeout = 20 s\n}\n\npekko {\n  actor {\n    serialization-identifiers {\n      \"org.apache.pekko.cluster.typed.internal.PekkoClusterTypedSerializer\" = 28\n      \"org.apache.pekko.cluster.typed.internal.delivery.ReliableDeliverySerializer\" = 36\n    }\n    serializers {\n      typed-cluster = \"org.apache.pekko.cluster.typed.internal.PekkoClusterTypedSerializer\"\n      reliable-delivery = \"org.apache.pekko.cluster.typed.internal.delivery.ReliableDeliverySerializer\"\n    }\n    serialization-bindings {\n      \"org.apache.pekko.cluster.typed.internal.receptionist.ClusterReceptionist$Entry\" = typed-cluster\n      \"org.apache.pekko.actor.typed.internal.pubsub.TopicImpl$MessagePublished\" = typed-cluster\n      \"org.apache.pekko.actor.typed.delivery.internal.DeliverySerializable\" = reliable-delivery\n    }\n  }\n  cluster.configuration-compatibility-check.checkers {\n    receptionist = \"org.apache.pekko.cluster.typed.internal.receptionist.ClusterReceptionistConfigCompatChecker\"\n  }\n}","title":"pekko-cluster-typed"},{"location":"/general/configuration-reference.html#pekko-cluster","text":"copysource######################################\n# Pekko Cluster Reference Config File #\n######################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\npekko {\n\n  cluster {\n    # Initial contact points of the cluster.\n    # The nodes to join automatically at startup.\n    # Comma separated full URIs defined by a string on the form of\n    # \"akka://system@hostname:port\"\n    # Leave as empty if the node is supposed to be joined manually.\n    seed-nodes = []\n\n    # How long to wait for one of the seed nodes to reply to initial join request.\n    # When this is the first seed node and there is no positive reply from the other\n    # seed nodes within this timeout it will join itself to bootstrap the cluster.\n    # When this is not the first seed node the join attempts will be performed with\n    # this interval.  \n    seed-node-timeout = 5s\n\n    # If a join request fails it will be retried after this period.\n    # Disable join retry by specifying \"off\".\n    retry-unsuccessful-join-after = 10s\n    \n    # The joining of given seed nodes will by default be retried indefinitely until\n    # a successful join. That process can be aborted if unsuccessful by defining this\n    # timeout. When aborted it will run CoordinatedShutdown, which by default will\n    # terminate the ActorSystem. CoordinatedShutdown can also be configured to exit\n    # the JVM. It is useful to define this timeout if the seed-nodes are assembled\n    # dynamically and a restart with new seed-nodes should be tried after unsuccessful\n    # attempts.   \n    shutdown-after-unsuccessful-join-seed-nodes = off\n\n    # Time margin after which shards or singletons that belonged to a downed/removed\n    # partition are created in surviving partition. The purpose of this margin is that\n    # in case of a network partition the persistent actors in the non-surviving partitions\n    # must be stopped before corresponding persistent actors are started somewhere else.\n    # This is useful if you implement downing strategies that handle network partitions,\n    # e.g. by keeping the larger side of the partition and shutting down the smaller side.\n    # Disable with \"off\" or specify a duration to enable.\n    #\n    # When using the `org.apache.pekko.cluster.sbr.SplitBrainResolver` as downing provider it will use\n    # the org.apache.pekko.cluster.split-brain-resolver.stable-after as the default down-removal-margin\n    # if this down-removal-margin is undefined.\n    down-removal-margin = off\n\n    # Pluggable support for downing of nodes in the cluster.\n    # If this setting is left empty the `NoDowning` provider is used and no automatic downing will be performed.\n    #\n    # If specified the value must be the fully qualified class name of a subclass of\n    # `org.apache.pekko.cluster.DowningProvider` having a public one argument constructor accepting an `ActorSystem`\n    downing-provider-class = \"\"\n\n    # Artery only setting\n    # When a node has been gracefully removed, let this time pass (to allow for example\n    # cluster singleton handover to complete) and then quarantine the removed node.\n    quarantine-removed-node-after = 5s\n\n    # If this is set to \"off\", the leader will not move 'Joining' members to 'Up' during a network\n    # split. This feature allows the leader to accept 'Joining' members to be 'WeaklyUp'\n    # so they become part of the cluster even during a network split. The leader will\n    # move `Joining` members to 'WeaklyUp' after this configured duration without convergence.\n    # The leader will move 'WeaklyUp' members to 'Up' status once convergence has been reached.\n    allow-weakly-up-members = 7s\n\n    # The roles of this member. List of strings, e.g. roles = [\"A\", \"B\"].\n    # The roles are part of the membership information and can be used by\n    # routers or other services to distribute work to certain member types,\n    # e.g. front-end and back-end nodes.\n    # Roles are not allowed to start with \"dc-\" as that is reserved for the\n    # special role assigned from the data-center a node belongs to (see the\n    # multi-data-center section below)\n    roles = []\n    \n    # Run the coordinated shutdown from phase 'cluster-shutdown' when the cluster\n    # is shutdown for other reasons than when leaving, e.g. when downing. This\n    # will terminate the ActorSystem when the cluster extension is shutdown.\n    run-coordinated-shutdown-when-down = on\n\n    role {\n      # Minimum required number of members of a certain role before the leader\n      # changes member status of 'Joining' members to 'Up'. Typically used together\n      # with 'Cluster.registerOnMemberUp' to defer some action, such as starting\n      # actors, until the cluster has reached a certain size.\n      # E.g. to require 2 nodes with role 'frontend' and 3 nodes with role 'backend':\n      #   frontend.min-nr-of-members = 2\n      #   backend.min-nr-of-members = 3\n      #<role-name>.min-nr-of-members = 1\n    }\n\n    # Application version of the deployment. Used by rolling update features\n    # to distinguish between old and new nodes. The typical convention is to use\n    # 3 digit version numbers `major.minor.patch`, but 1 or two digits are also\n    # supported.\n    #\n    # If no `.` is used it is interpreted as a single digit version number or as\n    # plain alphanumeric if it couldn't be parsed as a number.\n    #\n    # It may also have a qualifier at the end for 2 or 3 digit version numbers such\n    # as \"1.2-RC1\".\n    # For 1 digit with qualifier, 1-RC1, it is interpreted as plain alphanumeric.\n    #\n    # It has support for https://github.com/dwijnand/sbt-dynver format with `+` or\n    # `-` separator. The number of commits from the tag is handled as a numeric part.\n    # For example `1.0.0+3-73475dce26` is less than `1.0.10+10-ed316bd024` (3 < 10).\n    app-version = \"0.0.0\"\n\n    # Minimum required number of members before the leader changes member status\n    # of 'Joining' members to 'Up'. Typically used together with\n    # 'Cluster.registerOnMemberUp' to defer some action, such as starting actors,\n    # until the cluster has reached a certain size.\n    min-nr-of-members = 1\n\n    # Enable/disable info level logging of cluster events.\n    # These are logged with logger name `org.apache.pekko.cluster.Cluster`.\n    log-info = on\n\n    # Enable/disable verbose info-level logging of cluster events\n    # for temporary troubleshooting. Defaults to 'off'.\n    # These are logged with logger name `org.apache.pekko.cluster.Cluster`.\n    log-info-verbose = off\n\n    # Enable or disable JMX MBeans for management of the cluster\n    jmx.enabled = on\n\n    # Enable or disable multiple JMX MBeans in the same JVM\n    # If this is disabled, the MBean Object name is \"pekko:type=Cluster\"\n    # If this is enabled, them MBean Object names become \"pekko:type=Cluster,port=$clusterPortNumber\"\n    jmx.multi-mbeans-in-same-jvm = off\n\n    # how long should the node wait before starting the periodic tasks\n    # maintenance tasks?\n    periodic-tasks-initial-delay = 1s\n\n    # how often should the node send out gossip information?\n    gossip-interval = 1s\n    \n    # discard incoming gossip messages if not handled within this duration\n    gossip-time-to-live = 2s\n\n    # how often should the leader perform maintenance tasks?\n    leader-actions-interval = 1s\n\n    # how often should the node move nodes, marked as unreachable by the failure\n    # detector, out of the membership ring?\n    unreachable-nodes-reaper-interval = 1s\n\n    # How often the current internal stats should be published.\n    # A value of 0s can be used to always publish the stats, when it happens.\n    # Disable with \"off\".\n    publish-stats-interval = off\n\n    # The id of the dispatcher to use for cluster actors.\n    # If specified you need to define the settings of the actual dispatcher.\n    use-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n    # Gossip to random node with newer or older state information, if any with\n    # this probability. Otherwise Gossip to any random live node.\n    # Probability value is between 0.0 and 1.0. 0.0 means never, 1.0 means always.\n    gossip-different-view-probability = 0.8\n    \n    # Reduced the above probability when the number of nodes in the cluster\n    # greater than this value.\n    reduce-gossip-different-view-probability = 400\n\n    # When a node is removed the removal is marked with a tombstone\n    # which is kept at least this long, after which it is pruned, if there is a partition\n    # longer than this it could lead to removed nodes being re-added to the cluster\n    prune-gossip-tombstones-after = 24h\n\n    # Settings for the Phi accrual failure detector (http://www.jaist.ac.jp/~defago/files/pdf/IS_RR_2004_010.pdf\n    # [Hayashibara et al]) used by the cluster subsystem to detect unreachable\n    # members.\n    # The default PhiAccrualFailureDetector will trigger if there are no heartbeats within\n    # the duration heartbeat-interval + acceptable-heartbeat-pause + threshold_adjustment,\n    # i.e. around 5.5 seconds with default settings.\n    failure-detector {\n\n      # FQCN of the failure detector implementation.\n      # It must implement org.apache.pekko.remote.FailureDetector and have\n      # a public constructor with a com.typesafe.config.Config and\n      # org.apache.pekko.actor.EventStream parameter.\n      implementation-class = \"org.apache.pekko.remote.PhiAccrualFailureDetector\"\n\n      # How often keep-alive heartbeat messages should be sent to each connection.\n      heartbeat-interval = 1 s\n\n      # Defines the failure detector threshold.\n      # A low threshold is prone to generate many wrong suspicions but ensures\n      # a quick detection in the event of a real crash. Conversely, a high\n      # threshold generates fewer mistakes but needs more time to detect\n      # actual crashes.\n      threshold = 8.0\n\n      # Number of the samples of inter-heartbeat arrival times to adaptively\n      # calculate the failure timeout for connections.\n      max-sample-size = 1000\n\n      # Minimum standard deviation to use for the normal distribution in\n      # AccrualFailureDetector. Too low standard deviation might result in\n      # too much sensitivity for sudden, but normal, deviations in heartbeat\n      # inter arrival times.\n      min-std-deviation = 100 ms\n\n      # Number of potentially lost/delayed heartbeats that will be\n      # accepted before considering it to be an anomaly.\n      # This margin is important to be able to survive sudden, occasional,\n      # pauses in heartbeat arrivals, due to for example garbage collect or\n      # network drop.\n      acceptable-heartbeat-pause = 3 s\n\n      # Number of member nodes that each member will send heartbeat messages to,\n      # i.e. each node will be monitored by this number of other nodes.\n      monitored-by-nr-of-members = 9\n      \n      # After the heartbeat request has been sent the first failure detection\n      # will start after this period, even though no heartbeat message has\n      # been received.\n      expected-response-after = 1 s\n\n    }\n\n    # Configures multi-dc specific heartbeating and other mechanisms,\n    # many of them have a direct counter-part in \"one datacenter mode\",\n    # in which case these settings would not be used at all - they only apply,\n    # if your cluster nodes are configured with at-least 2 different `pekko.cluster.data-center` values.\n    multi-data-center {\n\n      # Defines which data center this node belongs to. It is typically used to make islands of the\n      # cluster that are colocated. This can be used to make the cluster aware that it is running\n      # across multiple availability zones or regions. It can also be used for other logical\n      # grouping of nodes.\n      self-data-center = \"default\"\n\n\n      # Try to limit the number of connections between data centers. Used for gossip and heartbeating.\n      # This will not limit connections created for the messaging of the application.\n      # If the cluster does not span multiple data centers, this value has no effect.\n      cross-data-center-connections = 5\n\n      # The n oldest nodes in a data center will choose to gossip to another data center with\n      # this probability. Must be a value between 0.0 and 1.0 where 0.0 means never, 1.0 means always.\n      # When a data center is first started (nodes < 5) a higher probability is used so other data\n      # centers find out about the new nodes more quickly\n      cross-data-center-gossip-probability = 0.2\n\n      failure-detector {\n        # FQCN of the failure detector implementation.\n        # It must implement org.apache.pekko.remote.FailureDetector and have\n        # a public constructor with a com.typesafe.config.Config and\n        # org.apache.pekko.actor.EventStream parameter.\n        implementation-class = \"org.apache.pekko.remote.DeadlineFailureDetector\"\n  \n        # Number of potentially lost/delayed heartbeats that will be\n        # accepted before considering it to be an anomaly.\n        # This margin is important to be able to survive sudden, occasional,\n        # pauses in heartbeat arrivals, due to for example garbage collect or\n        # network drop.\n        acceptable-heartbeat-pause = 10 s\n        \n        # How often keep-alive heartbeat messages should be sent to each connection.\n        heartbeat-interval = 3 s\n  \n        # After the heartbeat request has been sent the first failure detection\n        # will start after this period, even though no heartbeat message has\n        # been received.\n        expected-response-after = 1 s\n      }\n    }\n\n    # If the tick-duration of the default scheduler is longer than the\n    # tick-duration configured here a dedicated scheduler will be used for\n    # periodic tasks of the cluster, otherwise the default scheduler is used.\n    # See pekko.scheduler settings for more details.\n    scheduler {\n      tick-duration = 33ms\n      ticks-per-wheel = 512\n    }\n\n    debug {\n      # Log heartbeat events (very verbose, useful mostly when debugging heartbeating issues).\n      # These are logged with logger name `org.apache.pekko.cluster.ClusterHeartbeat`.\n      verbose-heartbeat-logging = off\n\n      # log verbose details about gossip\n      verbose-gossip-logging = off\n    }\n\n    configuration-compatibility-check {\n\n      # Enforce configuration compatibility checks when joining a cluster.\n      # Set to off to allow joining nodes to join a cluster even when configuration incompatibilities are detected or\n      # when the cluster does not support this feature. Compatibility checks are always performed and warning and\n      # error messages are logged.\n      #\n      # This is particularly useful for rolling updates on clusters that do not support that feature. Since the old\n      # cluster won't be able to send the compatibility confirmation to the joining node, the joining node won't be able\n      # to 'know' if its allowed to join.\n      enforce-on-join = on\n\n      # Add named entry to this section with fully qualified class name of the JoinConfigCompatChecker\n      # to enable.\n      # Checkers defined in reference.conf can be disabled by application by using empty string value\n      # for the named entry.\n      checkers {\n        pekko-cluster = \"org.apache.pekko.cluster.JoinConfigCompatCheckCluster\"\n      }\n\n      # Some configuration properties might not be appropriate to transfer between nodes\n      # and such properties can be excluded from the configuration compatibility check by adding\n      # the paths of the properties to this list. Sensitive paths are grouped by key. Modules and third-party libraries\n      # can define their own set of sensitive paths without clashing with each other (as long they use unique keys).\n      #\n      # All properties starting with the paths defined here are excluded, i.e. you can add the path of a whole\n      # section here to skip everything inside that section.\n      sensitive-config-paths {\n        pekko = [\n          \"user.home\", \"user.name\", \"user.dir\",\n          \"socksNonProxyHosts\", \"http.nonProxyHosts\", \"ftp.nonProxyHosts\",\n          \"pekko.remote.secure-cookie\",\n          \"pekko.remote.classic.netty.ssl.security\",\n          # Pre 2.6 path, keep around to avoid sending things misconfigured with old paths\n          \"pekko.remote.netty.ssl.security\",\n          \"pekko.remote.artery.ssl\"\n        ]\n      }\n\n    }\n  }\n\n  actor.deployment.default.cluster {\n    # enable cluster aware router that deploys to nodes in the cluster\n    enabled = off\n\n    # Maximum number of routees that will be deployed on each cluster\n    # member node.\n    # Note that max-total-nr-of-instances defines total number of routees, but\n    # number of routees per node will not be exceeded, i.e. if you\n    # define max-total-nr-of-instances = 50 and max-nr-of-instances-per-node = 2\n    # it will deploy 2 routees per new member in the cluster, up to\n    # 25 members.\n    max-nr-of-instances-per-node = 1\n    \n    # Maximum number of routees that will be deployed, in total\n    # on all nodes. See also description of max-nr-of-instances-per-node.\n    # For backwards compatibility reasons, nr-of-instances\n    # has the same purpose as max-total-nr-of-instances for cluster\n    # aware routers and nr-of-instances (if defined by user) takes\n    # precedence over max-total-nr-of-instances. \n    max-total-nr-of-instances = 10000\n\n    # Defines if routees are allowed to be located on the same node as\n    # the head router actor, or only on remote nodes.\n    # Useful for master-worker scenario where all routees are remote.\n    allow-local-routees = on\n\n    # Use members with all specified roles, or all members if undefined or empty.\n    use-roles = []\n\n    # Deprecated, since Pekko 2.5.4, replaced by use-roles\n    # Use members with specified role, or all members if undefined or empty.\n    use-role = \"\"\n  }\n\n  # Protobuf serializer for cluster messages\n  actor {\n    serializers {\n      pekko-cluster = \"org.apache.pekko.cluster.protobuf.ClusterMessageSerializer\"\n    }\n\n    serialization-bindings {\n      \"org.apache.pekko.cluster.ClusterMessage\" = pekko-cluster\n      \"org.apache.pekko.cluster.routing.ClusterRouterPool\" = pekko-cluster\n    }\n    \n    serialization-identifiers {\n      \"org.apache.pekko.cluster.protobuf.ClusterMessageSerializer\" = 5\n    }\n    \n  }\n\n}\n\n#//#split-brain-resolver\n\n# To enable the split brain resolver you first need to enable the provider in your application.conf:\n# pekko.cluster.downing-provider-class = \"org.apache.pekko.cluster.sbr.SplitBrainResolverProvider\"\n\npekko.cluster.split-brain-resolver {\n  # Select one of the available strategies (see descriptions below):\n  # static-quorum, keep-majority, keep-oldest, down-all, lease-majority\n  active-strategy = keep-majority\n\n  #//#stable-after\n  # Time margin after which shards or singletons that belonged to a downed/removed\n  # partition are created in surviving partition. The purpose of this margin is that\n  # in case of a network partition the persistent actors in the non-surviving partitions\n  # must be stopped before corresponding persistent actors are started somewhere else.\n  # This is useful if you implement downing strategies that handle network partitions,\n  # e.g. by keeping the larger side of the partition and shutting down the smaller side.\n  # Decision is taken by the strategy when there has been no membership or\n  # reachability changes for this duration, i.e. the cluster state is stable.\n  stable-after = 20s\n  #//#stable-after\n\n  # When reachability observations by the failure detector are changed the SBR decisions\n  # are deferred until there are no changes within the 'stable-after' duration.\n  # If this continues for too long it might be an indication of an unstable system/network\n  # and it could result in delayed or conflicting decisions on separate sides of a network\n  # partition.\n  # As a precaution for that scenario all nodes are downed if no decision is made within\n  # `stable-after + down-all-when-unstable` from the first unreachability event.\n  # The measurement is reset if all unreachable have been healed, downed or removed, or\n  # if there are no changes within `stable-after * 2`.\n  # The value can be on, off, or a duration.\n  # By default it is 'on' and then it is derived to be 3/4 of stable-after, but not less than\n  # 4 seconds.\n  down-all-when-unstable = on\n\n}\n#//#split-brain-resolver\n\n# Down the unreachable nodes if the number of remaining nodes are greater than or equal to\n# the given 'quorum-size'. Otherwise down the reachable nodes, i.e. it will shut down that\n# side of the partition. In other words, the 'size' defines the minimum number of nodes\n# that the cluster must have to be operational. If there are unreachable nodes when starting\n# up the cluster, before reaching this limit, the cluster may shutdown itself immediately.\n# This is not an issue if you start all nodes at approximately the same time.\n#\n# Note that you must not add more members to the cluster than 'quorum-size * 2 - 1', because\n# then both sides may down each other and thereby form two separate clusters. For example,\n# quorum-size configured to 3 in a 6 node cluster may result in a split where each side\n# consists of 3 nodes each, i.e. each side thinks it has enough nodes to continue by\n# itself. A warning is logged if this recommendation is violated.\n#//#static-quorum\npekko.cluster.split-brain-resolver.static-quorum {\n  # minimum number of nodes that the cluster must have\n  quorum-size = undefined\n\n  # if the 'role' is defined the decision is based only on members with that 'role'\n  role = \"\"\n}\n#//#static-quorum\n\n# Down the unreachable nodes if the current node is in the majority part based the last known\n# membership information. Otherwise down the reachable nodes, i.e. the own part. If the\n# the parts are of equal size the part containing the node with the lowest address is kept.\n# Note that if there are more than two partitions and none is in majority each part\n# will shutdown itself, terminating the whole cluster.\n#//#keep-majority\npekko.cluster.split-brain-resolver.keep-majority {\n  # if the 'role' is defined the decision is based only on members with that 'role'\n  role = \"\"\n}\n#//#keep-majority\n\n# Down the part that does not contain the oldest member (current singleton).\n#\n# There is one exception to this rule if 'down-if-alone' is defined to 'on'.\n# Then, if the oldest node has partitioned from all other nodes the oldest\n# will down itself and keep all other nodes running. The strategy will not\n# down the single oldest node when it is the only remaining node in the cluster.\n#\n# Note that if the oldest node crashes the others will remove it from the cluster\n# when 'down-if-alone' is 'on', otherwise they will down themselves if the\n# oldest node crashes, i.e. shutdown the whole cluster together with the oldest node.\n#//#keep-oldest\npekko.cluster.split-brain-resolver.keep-oldest {\n  # Enable downing of the oldest node when it is partitioned from all other nodes\n  down-if-alone = on\n\n  # if the 'role' is defined the decision is based only on members with that 'role',\n  # i.e. using the oldest member (singleton) within the nodes with that role\n  role = \"\"\n}\n#//#keep-oldest\n\n# Keep the part that can acquire the lease, and down the other part.\n# Best effort is to keep the side that has most nodes, i.e. the majority side.\n# This is achieved by adding a delay before trying to acquire the lease on the\n# minority side.\n#//#lease-majority\npekko.cluster.split-brain-resolver.lease-majority {\n  lease-implementation = \"\"\n\n  # The recommended format for the lease name is \"<service-name>-pekko-sbr\".\n  # When lease-name is not defined, the name will be set to \"<actor-system-name>-pekko-sbr\"\n  lease-name = \"\"\n\n  # This delay is used on the minority side before trying to acquire the lease,\n  # as an best effort to try to keep the majority side.\n  acquire-lease-delay-for-minority = 2s\n\n  # Release the lease after this duration.\n  release-after = 40s\n\n  # If the 'role' is defined the majority/minority is based only on members with that 'role'.\n  role = \"\"\n}\n#//#lease-majority","title":"pekko-cluster"},{"location":"/general/configuration-reference.html#pekko-discovery","text":"copysource######################################################\n# Pekko Discovery Config                              #\n######################################################\n\npekko.actor.deployment {\n  \"/SD-DNS/async-dns\" {\n    mailbox = \"unbounded\"\n    router = \"round-robin-pool\"\n    nr-of-instances = 1\n  }\n}\n\npekko.discovery {\n\n  # Users MUST configure this value to set the default discovery method.\n  #\n  # The value can be an implementation config path name, such as \"pekko-dns\",\n  # which would attempt to resolve as `pekko.discovery.pekko-dns` which is expected\n  # to contain a `class` setting. As fallback, the root `pekko-dns` setting scope\n  # would be used. If none of those contained a `class` setting, then the value is\n  # assumed to be a class name, and an attempt is made to instantiate it.\n  method = \"<method>\"\n\n  # Config based service discovery\n  config {\n    class = org.apache.pekko.discovery.config.ConfigServiceDiscovery\n\n    # Location of the services in configuration\n    services-path = \"pekko.discovery.config.services\"\n\n    # A map of services to resolve from configuration.\n    # See docs for more examples.\n    # A list of endpoints with host/port where port is optional e.g.\n    # services {\n    #  service1 {\n    #    endpoints = [\n    #      {\n    #        host = \"cat.com\"\n    #        port = 1233\n    #      },\n    #      {\n    #        host = \"dog.com\"\n    #      }\n    #    ]\n    #  },\n    #  service2 {\n    #    endpoints = [\n    #    {\n    #        host = \"fish.com\"\n    #        port = 1233\n    #      }\n    #    ]\n    #  }\n    # }\n    services = {\n\n    }\n  }\n\n  # Aggregate multiple service discovery mechanisms\n  aggregate {\n    class = org.apache.pekko.discovery.aggregate.AggregateServiceDiscovery\n\n    # List of service discovery methods to try in order. E.g config then fall back to DNS\n    # [\"config\", \"pekko-dns\"]\n    discovery-methods = []\n\n  }\n\n  # DNS based service discovery\n  pekko-dns {\n    class = org.apache.pekko.discovery.dns.DnsServiceDiscovery\n  }\n}","title":"pekko-discovery"},{"location":"/general/configuration-reference.html#pekko-coordination","text":"copysourcepekko.coordination {\n\n  # Defaults for any lease implementation that doesn't include these properties\n  lease {\n\n    # FQCN of the implementation of the Lease\n    lease-class = \"\"\n\n    #defaults\n    # if the node that acquired the leases crashes, how long should the lease be held before another owner can get it\n    heartbeat-timeout = 120s\n\n    # interval for communicating with the third party to confirm the lease is still held\n    heartbeat-interval = 12s\n\n    # lease implementations are expected to time out acquire and release calls or document\n    # that they do not implement an operation timeout\n    lease-operation-timeout = 5s\n\n    #defaults\n  }\n}","title":"pekko-coordination"},{"location":"/general/configuration-reference.html#pekko-multi-node-testkit","text":"copysource#############################################\n# Pekko Remote Testing Reference Config File #\n#############################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\npekko {\n  testconductor {\n\n    # Timeout for joining a barrier: this is the maximum time any participants\n    # waits for everybody else to join a named barrier.\n    barrier-timeout = 30s\n    \n    # Timeout for interrogation of TestConductor’s Controller actor\n    query-timeout = 10s\n    \n    # Threshold for packet size in time unit above which the failure injector will\n    # split the packet and deliver in smaller portions; do not give value smaller\n    # than HashedWheelTimer resolution (would not make sense)\n    packet-split-threshold = 100ms\n    \n    # amount of time for the ClientFSM to wait for the connection to the conductor\n    # to be successful\n    connect-timeout = 20s\n    \n    # Number of connect attempts to be made to the conductor controller\n    client-reconnects = 30\n    \n    # minimum time interval which is to be inserted between reconnect attempts\n    reconnect-backoff = 1s\n\n    netty {\n      # (I&O) Used to configure the number of I/O worker threads on server sockets\n      server-socket-worker-pool {\n        # Min number of threads to cap factor-based number to\n        pool-size-min = 1\n\n        # The pool size factor is used to determine thread pool size\n        # using the following formula: ceil(available processors * factor).\n        # Resulting size is then bounded by the pool-size-min and\n        # pool-size-max values.\n        pool-size-factor = 1.0\n\n        # Max number of threads to cap factor-based number to\n        pool-size-max = 2\n      }\n\n      # (I&O) Used to configure the number of I/O worker threads on client sockets\n      client-socket-worker-pool {\n        # Min number of threads to cap factor-based number to\n        pool-size-min = 1\n\n        # The pool size factor is used to determine thread pool size\n        # using the following formula: ceil(available processors * factor).\n        # Resulting size is then bounded by the pool-size-min and\n        # pool-size-max values.\n        pool-size-factor = 1.0\n\n        # Max number of threads to cap factor-based number to\n        pool-size-max = 2\n      }\n    }\n  }\n}","title":"pekko-multi-node-testkit"},{"location":"/general/configuration-reference.html#pekko-persistence-typed","text":"copysourcepekko.actor {\n\n  serialization-identifiers.\"org.apache.pekko.persistence.typed.serialization.ReplicatedEventSourcingSerializer\" = 40\n\n  serializers.replicated-event-sourcing = \"org.apache.pekko.persistence.typed.serialization.ReplicatedEventSourcingSerializer\"\n\n  serialization-bindings {\n    \"org.apache.pekko.persistence.typed.internal.VersionVector\" = replicated-event-sourcing\n    \"org.apache.pekko.persistence.typed.crdt.Counter\" = replicated-event-sourcing\n    \"org.apache.pekko.persistence.typed.crdt.Counter$Updated\" = replicated-event-sourcing\n    \"org.apache.pekko.persistence.typed.crdt.ORSet\" = replicated-event-sourcing\n    \"org.apache.pekko.persistence.typed.crdt.ORSet$DeltaOp\" = replicated-event-sourcing\n    \"org.apache.pekko.persistence.typed.internal.ReplicatedEventMetadata\" = replicated-event-sourcing\n    \"org.apache.pekko.persistence.typed.internal.ReplicatedSnapshotMetadata\" = replicated-event-sourcing\n    \"org.apache.pekko.persistence.typed.internal.PublishedEventImpl\" = replicated-event-sourcing\n  }\n}\n\npekko.persistence.typed {\n\n  # Persistent actors stash while recovering or persisting events,\n  # this setting configures the default capacity of this stash.\n  #\n  # Stashing is always bounded to the size that is defined in this setting.\n  # You can set it to large values, however \"unbounded\" buffering is not supported.\n  # Negative or 0 values are not allowed.\n  stash-capacity = 4096\n\n  # Configure how to react when the event sourced stash overflows. This can happen in two scenarios:\n  # when a event sourced actor is doing recovery, persisting or snapshotting and it gets more than\n  # 'stash-capacity' commands, or if more than 'stash-capacity' commands are manually stashed with the\n  # 'stash' effect.\n  #\n  # Possible options\n  # - drop - the message is published as a org.apache.pekko.actor.typed.Dropped message on the event bus\n  # - fail - an exception is thrown so that the actor is failed\n  stash-overflow-strategy = \"drop\"\n\n  # enables automatic DEBUG level logging of messages stashed automatically by an EventSourcedBehavior,\n  # this may happen while it receives commands while it is recovering events or while it is persisting events\n  log-stashing = off\n\n  # By default, internal event sourced behavior logging are sent to\n  # org.apache.pekko.persistence.typed.internal.EventSourcedBehaviorImpl\n  # this can be changed by setting this to 'true' in which case the internal logging is sent to\n  # the actor context logger.\n  use-context-logger-for-internal-logging = false\n}\n\npekko.reliable-delivery {\n  producer-controller {\n    event-sourced-durable-queue {\n      # Max duration for the exponential backoff for persist failures.\n      restart-max-backoff = 10s\n\n      # Snapshot after this number of events. See RetentionCriteria.\n      snapshot-every = 1000\n\n      # Number of snapshots to keep. See RetentionCriteria.\n      keep-n-snapshots = 2\n\n      # Delete events after snapshotting. See RetentionCriteria.\n      delete-events = on\n\n      # Cleanup entries that haven't be used for this duration.\n      cleanup-unused-after = 3600s\n\n      # The journal plugin to use, by default it will use the plugin configured by\n      # `pekko.persistence.journal.plugin`.\n      journal-plugin-id = \"\"\n\n      # The journal plugin to use, by default it will use the plugin configured by\n      # `pekko.persistence.snapshot-store.plugin`.\n      snapshot-plugin-id = \"\"\n    }\n  }\n}","title":"pekko-persistence-typed"},{"location":"/general/configuration-reference.html#pekko-persistence","text":"copysource###########################################################\n# Pekko Persistence Extension Reference Configuration File #\n###########################################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits in your application.conf in order to override these settings.\n\n# Directory of persistence journal and snapshot store plugins is available at the\n# Pekko Community Projects page https://akka.io/community/\n\n# Default persistence extension settings.\npekko.persistence {\n\n    # When starting many persistent actors at the same time the journal\n    # and its data store is protected from being overloaded by limiting number\n    # of recoveries that can be in progress at the same time. When\n    # exceeding the limit the actors will wait until other recoveries have\n    # been completed.   \n    max-concurrent-recoveries = 50\n\n    # Fully qualified class name providing a default internal stash overflow strategy.\n    # It needs to be a subclass of org.apache.pekko.persistence.StashOverflowStrategyConfigurator.\n    # The default strategy throws StashOverflowException.\n    internal-stash-overflow-strategy = \"org.apache.pekko.persistence.ThrowExceptionConfigurator\"\n    journal {\n        # Absolute path to the journal plugin configuration entry used by \n        # persistent actor by default.\n        # Persistent actor can override `journalPluginId` method \n        # in order to rely on a different journal plugin.\n        plugin = \"\"\n        # List of journal plugins to start automatically. Use \"\" for the default journal plugin.\n        auto-start-journals = []\n    }\n    snapshot-store {\n        # Absolute path to the snapshot plugin configuration entry used by\n        # persistent actor by default.\n        # Persistent actor can override `snapshotPluginId` method\n        # in order to rely on a different snapshot plugin.\n        # It is not mandatory to specify a snapshot store plugin.\n        # If you don't use snapshots you don't have to configure it.\n        # Note that Cluster Sharding is using snapshots, so if you\n        # use Cluster Sharding you need to define a snapshot store plugin. \n        plugin = \"\"\n        # List of snapshot stores to start automatically. Use \"\" for the default snapshot store.\n        auto-start-snapshot-stores = []\n    }\n    # used as default-snapshot store if no plugin configured \n    # (see `pekko.persistence.snapshot-store`)\n    no-snapshot-store {\n      class = \"org.apache.pekko.persistence.snapshot.NoSnapshotStore\"\n    }\n    # Default reliable delivery settings.\n    at-least-once-delivery {\n        # Interval between re-delivery attempts.\n        redeliver-interval = 5s\n        # Maximum number of unconfirmed messages that will be sent in one \n        # re-delivery burst.\n        redelivery-burst-limit = 10000\n        # After this number of delivery attempts a \n        # `ReliableRedelivery.UnconfirmedWarning`, message will be sent to the actor.\n        warn-after-number-of-unconfirmed-attempts = 5\n        # Maximum number of unconfirmed messages that an actor with \n        # AtLeastOnceDelivery is allowed to hold in memory.\n        max-unconfirmed-messages = 100000\n    }\n    # Default persistent extension thread pools.\n    dispatchers {\n        # Dispatcher used by every plugin which does not declare explicit\n        # `plugin-dispatcher` field.\n        default-plugin-dispatcher {\n            type = PinnedDispatcher\n            executor = \"thread-pool-executor\"\n        }\n        # Default dispatcher for message replay.\n        default-replay-dispatcher {\n            type = Dispatcher\n            executor = \"fork-join-executor\"\n            fork-join-executor {\n                parallelism-min = 2\n                parallelism-max = 8\n            }\n        }\n        # Default dispatcher for streaming snapshot IO\n        default-stream-dispatcher {\n            type = Dispatcher\n            executor = \"fork-join-executor\"\n            fork-join-executor {\n                parallelism-min = 2\n                parallelism-max = 8\n            }\n        }\n    }\n\n    # Fallback settings for journal plugin configurations.\n    # These settings are used if they are not defined in plugin config section.\n    journal-plugin-fallback {\n\n      # Fully qualified class name providing journal plugin api implementation.\n      # It is mandatory to specify this property.\n      # The class must have a constructor without parameters or constructor with\n      # one `com.typesafe.config.Config` parameter.\n      class = \"\"\n\n      # Dispatcher for the plugin actor.\n      plugin-dispatcher = \"pekko.persistence.dispatchers.default-plugin-dispatcher\"\n\n      # Dispatcher for message replay.\n      replay-dispatcher = \"pekko.persistence.dispatchers.default-replay-dispatcher\"\n\n      # Removed: used to be the Maximum size of a persistent message batch written to the journal.\n      # Now this setting is without function, PersistentActor will write as many messages\n      # as it has accumulated since the last write.\n      max-message-batch-size = 200\n\n      # If there is more time in between individual events gotten from the journal\n      # recovery than this the recovery will fail.\n      # Note that it also affects reading the snapshot before replaying events on\n      # top of it, even though it is configured for the journal.\n      recovery-event-timeout = 30s\n\n      circuit-breaker {\n        max-failures = 10\n        call-timeout = 10s\n        reset-timeout = 30s\n      }\n\n      # The replay filter can detect a corrupt event stream by inspecting\n      # sequence numbers and writerUuid when replaying events.\n      replay-filter {\n        # What the filter should do when detecting invalid events.\n        # Supported values:\n        # `repair-by-discard-old` : discard events from old writers,\n        #                           warning is logged\n        # `fail` : fail the replay, error is logged\n        # `warn` : log warning but emit events untouched\n        # `off` : disable this feature completely\n        mode = repair-by-discard-old\n\n        # It uses a look ahead buffer for analyzing the events.\n        # This defines the size (in number of events) of the buffer.\n        window-size = 100\n\n        # How many old writerUuid to remember\n        max-old-writers = 10\n\n        # Set this to `on` to enable detailed debug logging of each\n        # replayed event.\n        debug = off\n      }\n    }\n\n    # Fallback settings for snapshot store plugin configurations\n    # These settings are used if they are not defined in plugin config section.\n    snapshot-store-plugin-fallback {\n\n      # Fully qualified class name providing snapshot store plugin api\n      # implementation. It is mandatory to specify this property if\n      # snapshot store is enabled.\n      # The class must have a constructor without parameters or constructor with\n      # one `com.typesafe.config.Config` parameter.\n      class = \"\"\n\n      # Dispatcher for the plugin actor.\n      plugin-dispatcher = \"pekko.persistence.dispatchers.default-plugin-dispatcher\"\n\n      circuit-breaker {\n        max-failures = 5\n        call-timeout = 20s\n        reset-timeout = 60s\n      }\n\n      # Set this to true if successful loading of snapshot is not necessary.\n      # This can be useful when it is alright to ignore snapshot in case of\n      # for example deserialization errors. When snapshot loading fails it will instead\n      # recover by replaying all events.\n      # Don't set to true if events are deleted because that would\n      # result in wrong recovered state if snapshot load fails.\n      snapshot-is-optional = false\n\n    }\n\n  fsm {\n    # PersistentFSM saves snapshots after this number of persistent\n    # events. Snapshots are used to reduce recovery times.\n    # When you disable this feature, specify snapshot-after = off.\n    # To enable the feature, specify a number like snapshot-after = 1000\n    # which means a snapshot is taken after persisting every 1000 events.\n    snapshot-after = off\n  }\n\n  # DurableStateStore settings\n  state {\n    # Absolute path to the KeyValueStore plugin configuration entry used by\n    # DurableStateBehavior actors by default.\n    # DurableStateBehavior can override `durableStateStorePluginId` method (`withDurableStateStorePluginId`)\n    # in order to rely on a different plugin.\n    plugin = \"\"\n  }\n\n  # Fallback settings for DurableStateStore plugin configurations\n  # These settings are used if they are not defined in plugin config section.\n  state-plugin-fallback {\n    recovery-timeout = 30s\n  }\n}\n\n# Protobuf serialization for the persistent extension messages.\npekko.actor {\n    serializers {\n        pekko-persistence-message = \"org.apache.pekko.persistence.serialization.MessageSerializer\"\n        pekko-persistence-snapshot = \"org.apache.pekko.persistence.serialization.SnapshotSerializer\"\n    }\n    serialization-bindings {\n        \"org.apache.pekko.persistence.serialization.Message\" = pekko-persistence-message\n        \"org.apache.pekko.persistence.serialization.Snapshot\" = pekko-persistence-snapshot\n    }\n    serialization-identifiers {\n        \"org.apache.pekko.persistence.serialization.MessageSerializer\" = 7\n        \"org.apache.pekko.persistence.serialization.SnapshotSerializer\" = 8\n    }\n}\n\n\n###################################################\n# Persistence plugins included with the extension #\n###################################################\n\n# In-memory journal plugin.\npekko.persistence.journal.inmem {\n    # Class name of the plugin.\n    class = \"org.apache.pekko.persistence.journal.inmem.InmemJournal\"\n    # Dispatcher for the plugin actor.\n    plugin-dispatcher = \"pekko.actor.default-dispatcher\"\n\n    # Turn this on to test serialization of the events\n    test-serialization = off\n}\n\n# Local file system snapshot store plugin.\npekko.persistence.snapshot-store.local {\n    # Class name of the plugin.\n    class = \"org.apache.pekko.persistence.snapshot.local.LocalSnapshotStore\"\n    # Dispatcher for the plugin actor.\n    plugin-dispatcher = \"pekko.persistence.dispatchers.default-plugin-dispatcher\"\n    # Dispatcher for streaming snapshot IO.\n    stream-dispatcher = \"pekko.persistence.dispatchers.default-stream-dispatcher\"\n    # Storage location of snapshot files.\n    dir = \"snapshots\"\n    # Number load attempts when recovering from the latest snapshot fails\n    # yet older snapshot files are available. Each recovery attempt will try\n    # to recover using an older than previously failed-on snapshot file \n    # (if any are present). If all attempts fail the recovery will fail and\n    # the persistent actor will be stopped.\n    max-load-attempts = 3\n}\n\n# LevelDB journal plugin.\n# Note: this plugin requires explicit LevelDB dependency, see below. \npekko.persistence.journal.leveldb {\n    # Class name of the plugin.\n    class = \"org.apache.pekko.persistence.journal.leveldb.LeveldbJournal\"\n    # Dispatcher for the plugin actor.\n    plugin-dispatcher = \"pekko.persistence.dispatchers.default-plugin-dispatcher\"\n    # Dispatcher for message replay.\n    replay-dispatcher = \"pekko.persistence.dispatchers.default-replay-dispatcher\"\n    # Storage location of LevelDB files.\n    dir = \"journal\"\n    # Use fsync on write.\n    fsync = on\n    # Verify checksum on read.\n    checksum = off\n    # Native LevelDB (via JNI) or LevelDB Java port.\n    native = on\n    # Number of deleted messages per persistence id that will trigger journal compaction\n    compaction-intervals {\n    }\n}\n\n# Shared LevelDB journal plugin (for testing only).\n# Note: this plugin requires explicit LevelDB dependency, see below. \npekko.persistence.journal.leveldb-shared {\n    # Class name of the plugin.\n    class = \"org.apache.pekko.persistence.journal.leveldb.SharedLeveldbJournal\"\n    # Dispatcher for the plugin actor.\n    plugin-dispatcher = \"pekko.actor.default-dispatcher\"\n    # Timeout for async journal operations.\n    timeout = 10s\n    store {\n        # Dispatcher for shared store actor.\n        store-dispatcher = \"pekko.persistence.dispatchers.default-plugin-dispatcher\"\n        # Dispatcher for message replay.\n        replay-dispatcher = \"pekko.persistence.dispatchers.default-replay-dispatcher\"\n        # Storage location of LevelDB files.\n        dir = \"journal\"\n        # Use fsync on write.\n        fsync = on\n        # Verify checksum on read.\n        checksum = off\n        # Native LevelDB (via JNI) or LevelDB Java port.\n        native = on\n        # Number of deleted messages per persistence id that will trigger journal compaction\n        compaction-intervals {\n        }\n    }\n}\n\npekko.persistence.journal.proxy {\n  # Class name of the plugin.\n  class = \"org.apache.pekko.persistence.journal.PersistencePluginProxy\"\n  # Dispatcher for the plugin actor.\n  plugin-dispatcher = \"pekko.actor.default-dispatcher\"\n  # Set this to on in the configuration of the ActorSystem\n  # that will host the target journal\n  start-target-journal = off\n  # The journal plugin config path to use for the target journal\n  target-journal-plugin = \"\"\n  # The address of the proxy to connect to from other nodes. Optional setting.\n  target-journal-address = \"\"\n  # Initialization timeout of target lookup\n  init-timeout = 10s\n}\n\npekko.persistence.snapshot-store.proxy {\n  # Class name of the plugin.\n  class = \"org.apache.pekko.persistence.journal.PersistencePluginProxy\"\n  # Dispatcher for the plugin actor.\n  plugin-dispatcher = \"pekko.actor.default-dispatcher\"\n  # Set this to on in the configuration of the ActorSystem\n  # that will host the target snapshot-store\n  start-target-snapshot-store = off\n  # The journal plugin config path to use for the target snapshot-store\n  target-snapshot-store-plugin = \"\"\n  # The address of the proxy to connect to from other nodes. Optional setting.\n  target-snapshot-store-address = \"\"\n  # Initialization timeout of target lookup\n  init-timeout = 10s\n}\n\n# LevelDB persistence requires the following dependency declarations:\n#\n# SBT:\n#       \"org.iq80.leveldb\"            % \"leveldb\"          % \"0.7\"\n#       \"org.fusesource.leveldbjni\"   % \"leveldbjni-all\"   % \"1.8\"\n#\n# Maven:\n#        <dependency>\n#            <groupId>org.iq80.leveldb</groupId>\n#            <artifactId>leveldb</artifactId>\n#            <version>0.7</version>\n#        </dependency>\n#        <dependency>\n#            <groupId>org.fusesource.leveldbjni</groupId>\n#            <artifactId>leveldbjni-all</artifactId>\n#            <version>1.8</version>\n#        </dependency>","title":"pekko-persistence"},{"location":"/general/configuration-reference.html#pekko-persistence-query","text":"copysource#######################################################\n# Pekko Persistence Query Reference Configuration File #\n#######################################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits in your application.conf in order to override these settings.\n\n#//#query-leveldb\n# Configuration for the LeveldbReadJournal\npekko.persistence.query.journal.leveldb {\n  # Implementation class of the LevelDB ReadJournalProvider\n  class = \"org.apache.pekko.persistence.query.journal.leveldb.LeveldbReadJournalProvider\"\n  \n  # Absolute path to the write journal plugin configuration entry that this \n  # query journal will connect to. That must be a LeveldbJournal or SharedLeveldbJournal.\n  # If undefined (or \"\") it will connect to the default journal as specified by the\n  # pekko.persistence.journal.plugin property.\n  write-plugin = \"\"\n  \n  # The LevelDB write journal is notifying the query side as soon as things\n  # are persisted, but for efficiency reasons the query side retrieves the events \n  # in batches that sometimes can be delayed up to the configured `refresh-interval`.\n  refresh-interval = 3s\n  \n  # How many events to fetch in one query (replay) and keep buffered until they\n  # are delivered downstreams.\n  max-buffer-size = 100\n}\n#//#query-leveldb\n\npekko.actor {\n  serializers {\n    pekko-persistence-query = \"org.apache.pekko.persistence.query.internal.QuerySerializer\"\n  }\n  serialization-bindings {\n    \"org.apache.pekko.persistence.query.typed.EventEnvelope\" = pekko-persistence-query\n    \"org.apache.pekko.persistence.query.Offset\" = pekko-persistence-query\n  }\n  serialization-identifiers {\n    \"org.apache.pekko.persistence.query.internal.QuerySerializer\" = 39\n  }\n}","title":"pekko-persistence-query"},{"location":"/general/configuration-reference.html#pekko-persistence-testkit","text":"copysource##################################################\n# Pekko Persistence Testkit Reference Config File #\n##################################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\npekko.persistence.testkit {\n\n  # configuration for persistence testkit for events\n  events {\n    # enable serialization of the persisted events\n    serialize = true\n    # timeout for assertions\n    assert-timeout = 3s\n    # poll interval for assertions with timeout\n    assert-poll-interval = 100millis\n  }\n\n  # configuration for persistence testkit for snapshots\n  snapshots {\n    # enable serialization of the persisted snapshots\n    serialize = true\n    # timeout for assertions\n    assert-timeout = 3s\n    # poll interval for assertions with timeout\n    assert-poll-interval = 100millis\n  }\n\n}\n\npekko.persistence.testkit.query {\n  class = \"org.apache.pekko.persistence.testkit.query.PersistenceTestKitReadJournalProvider\"\n}\n\npekko.persistence.testkit.state {\n  class = \"org.apache.pekko.persistence.testkit.state.PersistenceTestKitDurableStateStoreProvider\"\n}","title":"pekko-persistence-testkit"},{"location":"/general/configuration-reference.html#pekko-remote-artery","text":"copysource#####################################\n# Pekko Remote Reference Config File #\n#####################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\n# comments about pekko.actor settings left out where they are already in pekko-\n# actor.jar, because otherwise they would be repeated in config rendering.\n#\n# For the configuration of the new remoting implementation (Artery) please look\n# at the bottom section of this file as it is listed separately.\n\npekko {\n\n  actor {\n\n    serializers {\n      pekko-containers = \"org.apache.pekko.remote.serialization.MessageContainerSerializer\"\n      pekko-misc = \"org.apache.pekko.remote.serialization.MiscMessageSerializer\"\n      artery = \"org.apache.pekko.remote.serialization.ArteryMessageSerializer\"\n      proto = \"org.apache.pekko.remote.serialization.ProtobufSerializer\"\n      daemon-create = \"org.apache.pekko.remote.serialization.DaemonMsgCreateSerializer\"\n      pekko-system-msg = \"org.apache.pekko.remote.serialization.SystemMessageSerializer\"\n    }\n\n    serialization-bindings {\n      \"org.apache.pekko.actor.ActorSelectionMessage\" = pekko-containers\n\n      \"org.apache.pekko.remote.DaemonMsgCreate\" = daemon-create\n\n      \"org.apache.pekko.remote.artery.ArteryMessage\" = artery\n\n      # Since org.apache.pekko.protobuf.Message does not extend Serializable but\n      # GeneratedMessage does, need to use the more specific one here in order\n      # to avoid ambiguity.\n      # This is only loaded if pekko-protobuf is on the classpath\n      # It should not be used and users should migrate to using the protobuf classes\n      # directly\n      # Remove in 2.7\n      \"org.apache.pekko.protobuf.GeneratedMessage\" = proto\n\n      \"org.apache.pekko.protobufv3.internal.GeneratedMessageV3\" = proto\n\n      # Since com.google.protobuf.Message does not extend Serializable but\n      # GeneratedMessage does, need to use the more specific one here in order\n      # to avoid ambiguity.\n      # This com.google.protobuf serialization binding is only used if the class can be loaded,\n      # i.e. com.google.protobuf dependency has been added in the application project.\n      \"com.google.protobuf.GeneratedMessage\" = proto\n      \"com.google.protobuf.GeneratedMessageV3\" = proto\n\n      \"org.apache.pekko.actor.Identify\" = pekko-misc\n      \"org.apache.pekko.actor.ActorIdentity\" = pekko-misc\n      \"scala.Some\" = pekko-misc\n      \"scala.None$\" = pekko-misc\n      \"java.util.Optional\" = pekko-misc\n      \"org.apache.pekko.actor.Status$Success\" = pekko-misc\n      \"org.apache.pekko.actor.Status$Failure\" = pekko-misc\n      \"org.apache.pekko.actor.ActorRef\" = pekko-misc\n      \"org.apache.pekko.actor.PoisonPill$\" = pekko-misc\n      \"org.apache.pekko.actor.Kill$\" = pekko-misc\n      \"org.apache.pekko.remote.RemoteWatcher$Heartbeat$\" = pekko-misc\n      \"org.apache.pekko.remote.RemoteWatcher$HeartbeatRsp\" = pekko-misc\n      \"org.apache.pekko.Done\" = pekko-misc\n      \"org.apache.pekko.NotUsed\" = pekko-misc\n      \"org.apache.pekko.actor.Address\" = pekko-misc\n      \"org.apache.pekko.remote.UniqueAddress\" = pekko-misc\n\n      \"org.apache.pekko.actor.ActorInitializationException\" = pekko-misc\n      \"org.apache.pekko.actor.IllegalActorStateException\" = pekko-misc\n      \"org.apache.pekko.actor.ActorKilledException\" = pekko-misc\n      \"org.apache.pekko.actor.InvalidActorNameException\" = pekko-misc\n      \"org.apache.pekko.actor.InvalidMessageException\" = pekko-misc\n      \"java.util.concurrent.TimeoutException\" = pekko-misc\n      \"org.apache.pekko.remote.serialization.ThrowableNotSerializableException\" = pekko-misc\n\n      \"org.apache.pekko.actor.LocalScope$\" = pekko-misc\n      \"org.apache.pekko.remote.RemoteScope\" = pekko-misc\n\n      \"com.typesafe.config.impl.SimpleConfig\" = pekko-misc\n      \"com.typesafe.config.Config\" = pekko-misc\n\n      \"org.apache.pekko.routing.FromConfig\" = pekko-misc\n      \"org.apache.pekko.routing.DefaultResizer\" = pekko-misc\n      \"org.apache.pekko.routing.BalancingPool\" = pekko-misc\n      \"org.apache.pekko.routing.BroadcastGroup\" = pekko-misc\n      \"org.apache.pekko.routing.BroadcastPool\" = pekko-misc\n      \"org.apache.pekko.routing.RandomGroup\" = pekko-misc\n      \"org.apache.pekko.routing.RandomPool\" = pekko-misc\n      \"org.apache.pekko.routing.RoundRobinGroup\" = pekko-misc\n      \"org.apache.pekko.routing.RoundRobinPool\" = pekko-misc\n      \"org.apache.pekko.routing.ScatterGatherFirstCompletedGroup\" = pekko-misc\n      \"org.apache.pekko.routing.ScatterGatherFirstCompletedPool\" = pekko-misc\n      \"org.apache.pekko.routing.SmallestMailboxPool\" = pekko-misc\n      \"org.apache.pekko.routing.TailChoppingGroup\" = pekko-misc\n      \"org.apache.pekko.routing.TailChoppingPool\" = pekko-misc\n      \"org.apache.pekko.remote.routing.RemoteRouterConfig\" = pekko-misc\n\n      \"org.apache.pekko.pattern.StatusReply\" = pekko-misc\n\n      \"org.apache.pekko.dispatch.sysmsg.SystemMessage\" = pekko-system-msg\n\n      # Java Serializer is by default used for exceptions and will by default\n      # not be allowed to be serialized, but in certain cases they are replaced\n      # by `org.apache.pekko.remote.serialization.ThrowableNotSerializableException` if\n      # no specific serializer has been defined:\n      # - when wrapped in `org.apache.pekko.actor.Status.Failure` for ask replies\n      # - when wrapped in system messages for exceptions from remote deployed child actors\n      #\n      # It's recommended that you implement custom serializer for exceptions that are\n      # sent remotely, You can add binding to pekko-misc (MiscMessageSerializer) for the\n      # exceptions that have a constructor with single message String or constructor with\n      # message String as first parameter and cause Throwable as second parameter. Note that it's not\n      # safe to add this binding for general exceptions such as IllegalArgumentException\n      # because it may have a subclass without required constructor.\n      \"java.lang.Throwable\" = java\n    }\n\n    serialization-identifiers {\n      \"org.apache.pekko.remote.serialization.ProtobufSerializer\" = 2\n      \"org.apache.pekko.remote.serialization.DaemonMsgCreateSerializer\" = 3\n      \"org.apache.pekko.remote.serialization.MessageContainerSerializer\" = 6\n      \"org.apache.pekko.remote.serialization.MiscMessageSerializer\" = 16\n      \"org.apache.pekko.remote.serialization.ArteryMessageSerializer\" = 17\n\n      \"org.apache.pekko.remote.serialization.SystemMessageSerializer\" = 22\n\n      # deprecated in 2.6.0, moved to pekko-actor\n      \"org.apache.pekko.remote.serialization.LongSerializer\" = 18\n      # deprecated in 2.6.0, moved to pekko-actor\n      \"org.apache.pekko.remote.serialization.IntSerializer\" = 19\n      # deprecated in 2.6.0, moved to pekko-actor\n      \"org.apache.pekko.remote.serialization.StringSerializer\" = 20\n      # deprecated in 2.6.0, moved to pekko-actor\n      \"org.apache.pekko.remote.serialization.ByteStringSerializer\" = 21\n    }\n\n    deployment {\n\n      default {\n\n        # if this is set to a valid remote address, the named actor will be\n        # deployed at that node e.g. \"akka://sys@host:port\"\n        remote = \"\"\n\n        target {\n\n          # A list of hostnames and ports for instantiating the children of a\n          # router\n          #   The format should be on \"akka://sys@host:port\", where:\n          #    - sys is the remote actor system name\n          #    - hostname can be either hostname or IP address the remote actor\n          #      should connect to\n          #    - port should be the port for the remote server on the other node\n          # The number of actor instances to be spawned is still taken from the\n          # nr-of-instances setting as for local routers; the instances will be\n          # distributed round-robin among the given nodes.\n          nodes = []\n\n        }\n      }\n    }\n  }\n\n  remote {\n    ### Settings shared by classic remoting and Artery (the new implementation of remoting)\n\n    # Using remoting directly is typically not desirable, so a warning will\n    # be shown to make this clear. Set this setting to 'off' to suppress that\n    # warning.\n    warn-about-direct-use = on\n\n\n    # If Cluster is not used, remote watch and deployment are disabled.\n    # To optionally use them while not using Cluster, set to 'on'.\n    use-unsafe-remote-features-outside-cluster = off\n\n    # A warning will be logged on remote watch attempts if Cluster\n    # is not in use and 'use-unsafe-remote-features-outside-cluster'\n    # is 'off'. Set this to 'off' to suppress these.\n    warn-unsafe-watch-outside-cluster = on\n\n    # Settings for the Phi accrual failure detector (http://www.jaist.ac.jp/~defago/files/pdf/IS_RR_2004_010.pdf\n    # [Hayashibara et al]) used for remote death watch.\n    # The default PhiAccrualFailureDetector will trigger if there are no heartbeats within\n    # the duration heartbeat-interval + acceptable-heartbeat-pause + threshold_adjustment,\n    # i.e. around 12.5 seconds with default settings.\n    watch-failure-detector {\n\n      # FQCN of the failure detector implementation.\n      # It must implement org.apache.pekko.remote.FailureDetector and have\n      # a public constructor with a com.typesafe.config.Config and\n      # org.apache.pekko.actor.EventStream parameter.\n      implementation-class = \"org.apache.pekko.remote.PhiAccrualFailureDetector\"\n\n      # How often keep-alive heartbeat messages should be sent to each connection.\n      heartbeat-interval = 1 s\n\n      # Defines the failure detector threshold.\n      # A low threshold is prone to generate many wrong suspicions but ensures\n      # a quick detection in the event of a real crash. Conversely, a high\n      # threshold generates fewer mistakes but needs more time to detect\n      # actual crashes.\n      threshold = 10.0\n\n      # Number of the samples of inter-heartbeat arrival times to adaptively\n      # calculate the failure timeout for connections.\n      max-sample-size = 200\n\n      # Minimum standard deviation to use for the normal distribution in\n      # AccrualFailureDetector. Too low standard deviation might result in\n      # too much sensitivity for sudden, but normal, deviations in heartbeat\n      # inter arrival times.\n      min-std-deviation = 100 ms\n\n      # Number of potentially lost/delayed heartbeats that will be\n      # accepted before considering it to be an anomaly.\n      # This margin is important to be able to survive sudden, occasional,\n      # pauses in heartbeat arrivals, due to for example garbage collect or\n      # network drop.\n      acceptable-heartbeat-pause = 10 s\n\n\n      # How often to check for nodes marked as unreachable by the failure\n      # detector\n      unreachable-nodes-reaper-interval = 1s\n\n      # After the heartbeat request has been sent the first failure detection\n      # will start after this period, even though no heartbeat mesage has\n      # been received.\n      expected-response-after = 1 s\n\n    }\n\n    # remote deployment configuration section\n    deployment {\n      # deprecated, use `enable-allow-list`\n      enable-whitelist = off\n\n      # If true, will only allow specific classes listed in `allowed-actor-classes` to be instanciated on this\n      # system via remote deployment\n      enable-allow-list = ${pekko.remote.deployment.enable-whitelist}\n\n\n      # deprecated, use `allowed-actor-classes`\n      whitelist = []\n\n      allowed-actor-classes = ${pekko.remote.deployment.whitelist}\n    }\n\n    ### Default dispatcher for the remoting subsystem\n    default-remote-dispatcher {\n      type = Dispatcher\n      executor = \"fork-join-executor\"\n      fork-join-executor {\n        parallelism-min = 2\n        parallelism-factor = 0.5\n        parallelism-max = 16\n      }\n      throughput = 10\n    }\npekko {\n\n  remote {\n\n    ### Configuration for Artery, the new implementation of remoting\n    artery {\n\n      # Disable artery with this flag\n      enabled = on\n\n      # Select the underlying transport implementation.\n      #\n      # Possible values: aeron-udp, tcp, tls-tcp\n      # See https://pekko.apache.org/docs/pekko/current/remoting-artery.html#selecting-a-transport for the tradeoffs\n      # for each transport\n      transport = tcp\n\n      # Canonical address is the address other clients should connect to.\n      # Artery transport will expect messages to this address.\n      canonical {\n\n        # The default remote server port clients should connect to.\n        # Default is 25520, use 0 if you want a random available port\n        # This port needs to be unique for each actor system on the same machine.\n        port = 25520\n\n        # Hostname clients should connect to. Can be set to an ip, hostname\n        # or one of the following special values:\n        #   \"<getHostAddress>\"   InetAddress.getLocalHost.getHostAddress\n        #   \"<getHostName>\"      InetAddress.getLocalHost.getHostName\n        #\n        hostname = \"<getHostAddress>\"\n      }\n\n      # Use these settings to bind a network interface to a different address\n      # than artery expects messages at. This may be used when running Pekko\n      # nodes in a separated networks (under NATs or in containers). If canonical\n      # and bind addresses are different, then network configuration that relays\n      # communications from canonical to bind addresses is expected.\n      bind {\n\n        # Port to bind a network interface to. Can be set to a port number\n        # of one of the following special values:\n        #   0    random available port\n        #   \"\"   pekko.remote.artery.canonical.port\n        #\n        port = \"\"\n\n        # Hostname to bind a network interface to. Can be set to an ip, hostname\n        # or one of the following special values:\n        #   \"0.0.0.0\"            all interfaces\n        #   \"\"                   pekko.remote.artery.canonical.hostname\n        #   \"<getHostAddress>\"   InetAddress.getLocalHost.getHostAddress\n        #   \"<getHostName>\"      InetAddress.getLocalHost.getHostName\n        #\n        hostname = \"\"\n\n        # Time to wait for Aeron/TCP to bind\n        bind-timeout = 3s\n      }\n\n\n      # Actor paths to use the large message stream for when a message\n      # is sent to them over remoting. The large message stream dedicated\n      # is separate from \"normal\" and system messages so that sending a\n      # large message does not interfere with them.\n      # Entries should be the full path to the actor. Wildcards in the form of \"*\"\n      # can be supplied at any place and matches any name at that segment -\n      # \"/user/supervisor/actor/*\" will match any direct child to actor,\n      # while \"/supervisor/*/child\" will match any grandchild to \"supervisor\" that\n      # has the name \"child\"\n      # Entries have to be specified on both the sending and receiving side.\n      # Messages sent to ActorSelections will not be passed through the large message\n      # stream, to pass such messages through the large message stream the selections\n      # but must be resolved to ActorRefs first.\n      large-message-destinations = []\n\n      # Enable untrusted mode, which discards inbound system messages, PossiblyHarmful and\n      # ActorSelection messages. E.g. remote watch and remote deployment will not work.\n      # ActorSelection messages can be enabled for specific paths with the trusted-selection-paths\n      untrusted-mode = off\n\n      # When 'untrusted-mode=on' inbound actor selections are by default discarded.\n      # Actors with paths defined in this list are granted permission to receive actor\n      # selections messages.\n      # E.g. trusted-selection-paths = [\"/user/receptionist\", \"/user/namingService\"]\n      trusted-selection-paths = []\n\n      # If this is \"on\", all inbound remote messages will be logged at DEBUG level,\n      # if off then they are not logged\n      log-received-messages = off\n\n      # If this is \"on\", all outbound remote messages will be logged at DEBUG level,\n      # if off then they are not logged\n      log-sent-messages = off\n\n      # Logging of message types with payload size in bytes larger than\n      # this value. Maximum detected size per message type is logged once,\n      # with an increase threshold of 10%.\n      # By default this feature is turned off. Activate it by setting the property to\n      # a value in bytes, such as 1000b. Note that for all messages larger than this\n      # limit there will be extra performance and scalability cost.\n      log-frame-size-exceeding = off\n\n      advanced {\n\n        # Maximum serialized message size, including header data.\n        maximum-frame-size = 256 KiB\n\n        # Direct byte buffers are reused in a pool with this maximum size.\n        # Each buffer has the size of 'maximum-frame-size'.\n        # This is not a hard upper limit on number of created buffers. Additional\n        # buffers will be created if needed, e.g. when using many outbound\n        # associations at the same time. Such additional buffers will be garbage\n        # collected, which is not as efficient as reusing buffers in the pool.\n        buffer-pool-size = 128\n\n        # Maximum serialized message size for the large messages, including header data.\n        # If the value of pekko.remote.artery.transport is set to aeron-udp, it is currently\n        # restricted to 1/8th the size of a term buffer that can be configured by setting the\n        # 'aeron.term.buffer.length' system property.\n        # See 'large-message-destinations'.\n        maximum-large-frame-size = 2 MiB\n\n        # Direct byte buffers for the large messages are reused in a pool with this maximum size.\n        # Each buffer has the size of 'maximum-large-frame-size'.\n        # See 'large-message-destinations'.\n        # This is not a hard upper limit on number of created buffers. Additional\n        # buffers will be created if needed, e.g. when using many outbound\n        # associations at the same time. Such additional buffers will be garbage\n        # collected, which is not as efficient as reusing buffers in the pool.\n        large-buffer-pool-size = 32\n\n        # For enabling testing features, such as blackhole in pekko-remote-testkit.\n        test-mode = off\n\n        # Settings for the materializer that is used for the remote streams.\n        materializer = ${pekko.stream.materializer}\n\n        # Remoting will use the given dispatcher for the ordinary and large message\n        # streams.\n        use-dispatcher = \"pekko.remote.default-remote-dispatcher\"\n\n        # Remoting will use the given dispatcher for the control stream.\n        # It can be good to not use the same dispatcher for the control stream as\n        # the dispatcher for the ordinary message stream so that heartbeat messages\n        # are not disturbed.\n        use-control-stream-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n\n        # Total number of inbound lanes, shared among all inbound associations. A value\n        # greater than 1 means that deserialization can be performed in parallel for\n        # different destination actors. The selection of lane is based on consistent\n        # hashing of the recipient ActorRef to preserve message ordering per receiver.\n        # Lowest latency can be achieved with inbound-lanes=1 because of one less\n        # asynchronous boundary.\n        inbound-lanes = 4\n\n        # Number of outbound lanes for each outbound association. A value greater than 1\n        # means that serialization and other work can be performed in parallel for different\n        # destination actors. The selection of lane is based on consistent hashing of the\n        # recipient ActorRef to preserve message ordering per receiver. Note that messages\n        # for different destination systems (hosts) are handled by different streams also\n        # when outbound-lanes=1. Lowest latency can be achieved with outbound-lanes=1\n        # because of one less asynchronous boundary.\n        outbound-lanes = 1\n\n        # Size of the send queue for outgoing messages. Messages will be dropped if\n        # the queue becomes full. This may happen if you send a burst of many messages\n        # without end-to-end flow control. Note that there is one such queue per\n        # outbound association. The trade-off of using a larger queue size is that\n        # it consumes more memory, since the queue is based on preallocated array with\n        # fixed size.\n        outbound-message-queue-size = 3072\n\n        # Size of the send queue for outgoing control messages, such as system messages.\n        # If this limit is reached the remote system is declared to be dead and its UID\n        # marked as quarantined. Note that there is one such queue per outbound association.\n        # It is a linked queue so it will not use more memory than needed but by increasing\n        # too much you may risk OutOfMemoryError in the worst case.\n        outbound-control-queue-size = 20000\n\n        # Size of the send queue for outgoing large messages. Messages will be dropped if\n        # the queue becomes full. This may happen if you send a burst of many messages\n        # without end-to-end flow control. Note that there is one such queue per\n        # outbound association.\n        # It is a linked queue so it will not use more memory than needed but by increasing\n        # too much you may risk OutOfMemoryError, especially since the message payload\n        # of these messages may be large.\n        outbound-large-message-queue-size = 256\n\n        # This setting defines the maximum number of unacknowledged system messages\n        # allowed for a remote system. If this limit is reached the remote system is\n        # declared to be dead and its UID marked as quarantined.\n        system-message-buffer-size = 20000\n\n        # unacknowledged system messages are re-delivered with this interval\n        system-message-resend-interval = 1 second\n\n\n\n        # The timeout for outbound associations to perform the initial handshake.\n        # This timeout must be greater than the 'image-liveness-timeout' when\n        # transport is aeron-udp.\n        handshake-timeout = 20 seconds\n\n        # incomplete initial handshake attempt is retried with this interval\n        handshake-retry-interval = 1 second\n\n        # Handshake requests are performed periodically with this interval,\n        # also after the handshake has been completed to be able to establish\n        # a new session with a restarted destination system.\n        inject-handshake-interval = 1 second\n\n\n        # System messages that are not acknowledged after re-sending for this period are\n        # dropped and will trigger quarantine. The value should be longer than the length\n        # of a network partition that you need to survive.\n        give-up-system-message-after = 6 hours\n\n        # Outbound streams are stopped when they haven't been used for this duration.\n        # They are started again when new messages are sent.\n        stop-idle-outbound-after = 5 minutes\n\n        # Outbound streams are quarantined when they haven't been used for this duration\n        # to cleanup resources used by the association, such as compression tables.\n        # This will cleanup association to crashed systems that didn't announce their\n        # termination.\n        # The value should be longer than the length of a network partition that you\n        # need to survive.\n        # The value must also be greater than stop-idle-outbound-after.\n        # Once every 1/10 of this duration an extra handshake message will be sent.\n        # Therfore it's also recommended to use a value that is greater than 10 times\n        # the stop-idle-outbound-after, since otherwise the idle streams will not be\n        # stopped.\n        quarantine-idle-outbound-after = 6 hours\n\n        # Stop outbound stream of a quarantined association after this idle timeout, i.e.\n        # when not used any more.\n        stop-quarantined-after-idle = 3 seconds\n\n        # After catastrophic communication failures that could result in the loss of system\n        # messages or after the remote DeathWatch triggers the remote system gets\n        # quarantined to prevent inconsistent behavior.\n        # This setting controls how long the quarantined association will be kept around\n        # before being removed to avoid long-term memory leaks. It must be quarantined\n        # and also unused for this duration before it's removed. When removed the historical\n        # information about which UIDs that were quarantined for that hostname:port is\n        # gone which could result in communication with a previously quarantined node\n        # if it wakes up again. Therfore this shouldn't be set too low.\n        remove-quarantined-association-after = 1 h\n\n        # during ActorSystem termination the remoting will wait this long for\n        # an acknowledgment by the destination system that flushing of outstanding\n        # remote messages has been completed\n        shutdown-flush-timeout = 1 second\n\n        # Before sending notificaiton of terminated actor (DeathWatchNotification) other messages\n        # will be flushed to make sure that the Terminated message arrives after other messages.\n        # It will wait this long for the flush acknowledgement before continuing.\n        # The flushing can be disabled by setting this to `off`.\n        death-watch-notification-flush-timeout = 3 seconds\n\n        # See 'inbound-max-restarts'\n        inbound-restart-timeout = 5 seconds\n\n        # Max number of restarts within 'inbound-restart-timeout' for the inbound streams.\n        # If more restarts occurs the ActorSystem will be terminated.\n        inbound-max-restarts = 5\n\n        # Retry outbound connection after this backoff.\n        # Only used when transport is tcp or tls-tcp.\n        outbound-restart-backoff = 1 second\n\n        # See 'outbound-max-restarts'\n        outbound-restart-timeout = 5 seconds\n\n        # Max number of restarts within 'outbound-restart-timeout' for the outbound streams.\n        # If more restarts occurs the ActorSystem will be terminated.\n        outbound-max-restarts = 5\n\n        # compression of common strings in remoting messages, like actor destinations, serializers etc\n        compression {\n\n          actor-refs {\n            # Max number of compressed actor-refs\n            # Note that compression tables are \"rolling\" (i.e. a new table replaces the old\n            # compression table once in a while), and this setting is only about the total number\n            # of compressions within a single such table.\n            # Must be a positive natural number. Can be disabled with \"off\".\n            max = 256\n\n            # interval between new table compression advertisements.\n            # this means the time during which we collect heavy-hitter data and then turn it into a compression table.\n            advertisement-interval = 1 minute\n          }\n          manifests {\n            # Max number of compressed manifests\n            # Note that compression tables are \"rolling\" (i.e. a new table replaces the old\n            # compression table once in a while), and this setting is only about the total number\n            # of compressions within a single such table.\n            # Must be a positive natural number. Can be disabled with \"off\".\n            max = 256\n\n            # interval between new table compression advertisements.\n            # this means the time during which we collect heavy-hitter data and then turn it into a compression table.\n            advertisement-interval = 1 minute\n          }\n        }\n\n        # List of fully qualified class names of remote instruments which should\n        # be initialized and used for monitoring of remote messages.\n        # The class must extend org.apache.pekko.remote.artery.RemoteInstrument and\n        # have a public constructor with empty parameters or one ExtendedActorSystem\n        # parameter.\n        # A new instance of RemoteInstrument will be created for each encoder and decoder.\n        # It's only called from the stage, so if it dosn't delegate to any shared instance\n        # it doesn't have to be thread-safe.\n        # Refer to `org.apache.pekko.remote.artery.RemoteInstrument` for more information.\n        instruments = ${?pekko.remote.artery.advanced.instruments} []\n\n        # Only used when transport is aeron-udp\n        aeron {\n          # Only used when transport is aeron-udp.\n          log-aeron-counters = false\n\n          # Controls whether to start the Aeron media driver in the same JVM or use external\n          # process. Set to 'off' when using external media driver, and then also set the\n          # 'aeron-dir'.\n          # Only used when transport is aeron-udp.\n          embedded-media-driver = on\n\n          # Directory used by the Aeron media driver. It's mandatory to define the 'aeron-dir'\n          # if using external media driver, i.e. when 'embedded-media-driver = off'.\n          # Embedded media driver will use a this directory, or a temporary directory if this\n          # property is not defined (empty).\n          # Only used when transport is aeron-udp.\n          aeron-dir = \"\"\n\n          # Whether to delete aeron embedded driver directory upon driver stop.\n          # Only used when transport is aeron-udp.\n          delete-aeron-dir = yes\n\n          # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.\n          # The tradeoff is that to have low latency more CPU time must be used to be\n          # able to react quickly on incoming messages or send as fast as possible after\n          # backoff backpressure.\n          # Level 1 strongly prefer low CPU consumption over low latency.\n          # Level 10 strongly prefer low latency over low CPU consumption.\n          # Only used when transport is aeron-udp.\n          idle-cpu-level = 5\n\n          # messages that are not accepted by Aeron are dropped after retrying for this period\n          # Only used when transport is aeron-udp.\n          give-up-message-after = 60 seconds\n\n          # Timeout after which aeron driver has not had keepalive messages\n          # from a client before it considers the client dead.\n          # Only used when transport is aeron-udp.\n          client-liveness-timeout = 20 seconds\n\n          # Timout after after which an uncommitted publication will be unblocked\n          # Only used when transport is aeron-udp.\n          publication-unblock-timeout = 40 seconds\n\n          # Timeout for each the INACTIVE and LINGER stages an aeron image\n          # will be retained for when it is no longer referenced.\n          # This timeout must be less than the 'handshake-timeout'.\n          # Only used when transport is aeron-udp.\n          image-liveness-timeout = 10 seconds\n\n          # Timeout after which the aeron driver is considered dead\n          # if it does not update its C'n'C timestamp.\n          # Only used when transport is aeron-udp.\n          driver-timeout = 20 seconds\n        }\n\n        # Only used when transport is tcp or tls-tcp.\n        tcp {\n          # Timeout of establishing outbound connections.\n          connection-timeout = 5 seconds\n\n          # The local address that is used for the client side of the TCP connection.\n          outbound-client-hostname = \"\"\n        }\n\n      }\n\n      # SSL configuration that is used when transport=tls-tcp.\n      ssl {\n        # Factory of SSLEngine.\n        # Must implement org.apache.pekko.remote.artery.tcp.SSLEngineProvider and have a public\n        # constructor with an ActorSystem parameter.\n        # The default ConfigSSLEngineProvider is configured by properties in section\n        # pekko.remote.artery.ssl.config-ssl-engine\n        ssl-engine-provider = org.apache.pekko.remote.artery.tcp.ConfigSSLEngineProvider\n\n        # Config of org.apache.pekko.remote.artery.tcp.ConfigSSLEngineProvider\n        config-ssl-engine {\n\n          # This is the Java Key Store used by the server connection\n          key-store = \"keystore\"\n\n          # This password is used for decrypting the key store\n          # Use substitution from environment variables for passwords. Don't define\n          # real passwords in config files. key-store-password=${SSL_KEY_STORE_PASSWORD}\n          key-store-password = \"changeme\"\n\n          # This password is used for decrypting the key\n          # Use substitution from environment variables for passwords. Don't define\n          # real passwords in config files. key-password=${SSL_KEY_PASSWORD}\n          key-password = \"changeme\"\n\n          # This is the Java Key Store used by the client connection\n          trust-store = \"truststore\"\n\n          # This password is used for decrypting the trust store\n          # Use substitution from environment variables for passwords. Don't define\n          # real passwords in config files. trust-store-password=${SSL_TRUST_STORE_PASSWORD}\n          trust-store-password = \"changeme\"\n\n          # Protocol to use for SSL encryption.\n          protocol = \"TLSv1.2\"\n\n          # Example: [\"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \n          #   \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\n          #   \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\",\n          #   \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"]\n          # When doing rolling upgrades, make sure to include both the algorithm used \n          # by old nodes and the preferred algorithm.\n          # If you use a JDK 8 prior to 8u161 you need to install\n          # the JCE Unlimited Strength Jurisdiction Policy Files to use AES 256.\n          # More info here:\n          # https://www.oracle.com/java/technologies/javase-jce-all-downloads.html\n          enabled-algorithms = [\"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\",\n            \"TLS_RSA_WITH_AES_128_CBC_SHA\"]\n\n          # There are two options, and the default SecureRandom is recommended:\n          # \"\" or \"SecureRandom\" => (default)\n          # \"SHA1PRNG\" => Can be slow because of blocking issues on Linux\n          #\n          # Setting a value here may require you to supply the appropriate cipher\n          # suite (see enabled-algorithms section above)\n          random-number-generator = \"\"\n\n          # Require mutual authentication between TLS peers\n          #\n          # Without mutual authentication only the peer that actively establishes a connection (TLS client side)\n          # checks if the passive side (TLS server side) sends over a trusted certificate. With the flag turned on,\n          # the passive side will also request and verify a certificate from the connecting peer.\n          #\n          # To prevent man-in-the-middle attacks this setting is enabled by default.\n          require-mutual-authentication = on\n\n          # Set this to `on` to verify hostnames with sun.security.util.HostnameChecker\n          # If possible it is recommended to have this enabled. Hostname verification is designed for\n          # situations where things locate each other by hostname, in scenarios where host names are dynamic\n          # and not known up front it can make sense to have this disabled.\n          hostname-verification = off\n        }\n\n        # Config of org.apache.pekko.remote.artery.tcp.ssl.RotatingKeysSSLEngineProvider\n        # This engine provider reads PEM files from a mount point shared with the secret\n        # manager. The constructed SSLContext is cached some time (configurable) so when\n        # the credentials rotate the new credentials are eventually picked up.\n        # By default mTLS is enabled.\n        # This provider also includes a verification phase that runs after the TLS handshake\n        # phase. In this verification, both peers run an authorization and verify they are\n        # part of the same pekko cluster. The verification happens via comparing the subject\n        # names in the peer's certificate with the name on the own certificate so if you\n        # use this SSLEngineProvider you should make sure all nodes on the cluster include\n        # at least one common subject name (CN or SAN).\n        # The Key setup this implementation supports has some limitations:\n        #   1. the private key must be provided on a PKCS#1 or a non-encrypted PKCS#8 PEM-formatted file\n        #   2. the private key must be be of an algorythm supported by `pekko-pki` tools (e.g. \"RSA\", not \"EC\")\n        #   3. the node certificate must be issued by a root CA (not an intermediate CA)\n        #   4. both the node and the CA certificates must be provided in PEM-formatted files\n        rotating-keys-engine {\n\n          # This is a convention that people may follow if they wish to save themselves some configuration\n          secret-mount-point = /var/run/secrets/pekko-tls/rotating-keys-engine\n\n          # The absolute path the PEM file with the private key.\n          key-file = ${pekko.remote.artery.ssl.rotating-keys-engine.secret-mount-point}/tls.key\n          # The absolute path to the PEM file of the certificate for the private key above.\n          cert-file = ${pekko.remote.artery.ssl.rotating-keys-engine.secret-mount-point}/tls.crt\n          # The absolute path to the PEM file of the certificate of the CA that emited\n          # the node certificate above.\n          ca-cert-file = ${pekko.remote.artery.ssl.rotating-keys-engine.secret-mount-point}/ca.crt\n\n          # There are two options, and the default SecureRandom is recommended:\n          # \"\" or \"SecureRandom\" => (default)\n          # \"SHA1PRNG\" => Can be slow because of blocking issues on Linux\n          #\n          # Setting a value here may require you to supply the appropriate cipher\n          # suite (see enabled-algorithms section)\n          random-number-generator = \"\"\n\n          # Example: [\"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\",\n          #   \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\n          #   \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\",\n          #   \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"]\n          # If you use a JDK 8 prior to 8u161 you need to install\n          # the JCE Unlimited Strength Jurisdiction Policy Files to use AES 256.\n          # More info here:\n          # https://www.oracle.com/java/technologies/javase-jce-all-downloads.html\n          enabled-algorithms = [\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"]\n\n          # Protocol to use for SSL encryption.\n          protocol = \"TLSv1.2\"\n\n          # How long should an SSLContext instance be cached. When rotating keys and certificates,\n          # there must a time overlap between the old certificate/key and the new ones. The\n          # value of this setting should be lower than duration of that overlap.\n          ssl-context-cache-ttl = 5m\n        }\n      }\n    }\n  }\n\n}","title":"pekko-remote artery"},{"location":"/general/configuration-reference.html#pekko-remote-classic-deprecated-","text":"copysource#####################################\n# Pekko Remote Reference Config File #\n#####################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\n# comments about pekko.actor settings left out where they are already in pekko-\n# actor.jar, because otherwise they would be repeated in config rendering.\n#\n# For the configuration of the new remoting implementation (Artery) please look\n# at the bottom section of this file as it is listed separately.\n\npekko {\n\n  actor {\n\n    serializers {\n      pekko-containers = \"org.apache.pekko.remote.serialization.MessageContainerSerializer\"\n      pekko-misc = \"org.apache.pekko.remote.serialization.MiscMessageSerializer\"\n      artery = \"org.apache.pekko.remote.serialization.ArteryMessageSerializer\"\n      proto = \"org.apache.pekko.remote.serialization.ProtobufSerializer\"\n      daemon-create = \"org.apache.pekko.remote.serialization.DaemonMsgCreateSerializer\"\n      pekko-system-msg = \"org.apache.pekko.remote.serialization.SystemMessageSerializer\"\n    }\n\n    serialization-bindings {\n      \"org.apache.pekko.actor.ActorSelectionMessage\" = pekko-containers\n\n      \"org.apache.pekko.remote.DaemonMsgCreate\" = daemon-create\n\n      \"org.apache.pekko.remote.artery.ArteryMessage\" = artery\n\n      # Since org.apache.pekko.protobuf.Message does not extend Serializable but\n      # GeneratedMessage does, need to use the more specific one here in order\n      # to avoid ambiguity.\n      # This is only loaded if pekko-protobuf is on the classpath\n      # It should not be used and users should migrate to using the protobuf classes\n      # directly\n      # Remove in 2.7\n      \"org.apache.pekko.protobuf.GeneratedMessage\" = proto\n\n      \"org.apache.pekko.protobufv3.internal.GeneratedMessageV3\" = proto\n\n      # Since com.google.protobuf.Message does not extend Serializable but\n      # GeneratedMessage does, need to use the more specific one here in order\n      # to avoid ambiguity.\n      # This com.google.protobuf serialization binding is only used if the class can be loaded,\n      # i.e. com.google.protobuf dependency has been added in the application project.\n      \"com.google.protobuf.GeneratedMessage\" = proto\n      \"com.google.protobuf.GeneratedMessageV3\" = proto\n\n      \"org.apache.pekko.actor.Identify\" = pekko-misc\n      \"org.apache.pekko.actor.ActorIdentity\" = pekko-misc\n      \"scala.Some\" = pekko-misc\n      \"scala.None$\" = pekko-misc\n      \"java.util.Optional\" = pekko-misc\n      \"org.apache.pekko.actor.Status$Success\" = pekko-misc\n      \"org.apache.pekko.actor.Status$Failure\" = pekko-misc\n      \"org.apache.pekko.actor.ActorRef\" = pekko-misc\n      \"org.apache.pekko.actor.PoisonPill$\" = pekko-misc\n      \"org.apache.pekko.actor.Kill$\" = pekko-misc\n      \"org.apache.pekko.remote.RemoteWatcher$Heartbeat$\" = pekko-misc\n      \"org.apache.pekko.remote.RemoteWatcher$HeartbeatRsp\" = pekko-misc\n      \"org.apache.pekko.Done\" = pekko-misc\n      \"org.apache.pekko.NotUsed\" = pekko-misc\n      \"org.apache.pekko.actor.Address\" = pekko-misc\n      \"org.apache.pekko.remote.UniqueAddress\" = pekko-misc\n\n      \"org.apache.pekko.actor.ActorInitializationException\" = pekko-misc\n      \"org.apache.pekko.actor.IllegalActorStateException\" = pekko-misc\n      \"org.apache.pekko.actor.ActorKilledException\" = pekko-misc\n      \"org.apache.pekko.actor.InvalidActorNameException\" = pekko-misc\n      \"org.apache.pekko.actor.InvalidMessageException\" = pekko-misc\n      \"java.util.concurrent.TimeoutException\" = pekko-misc\n      \"org.apache.pekko.remote.serialization.ThrowableNotSerializableException\" = pekko-misc\n\n      \"org.apache.pekko.actor.LocalScope$\" = pekko-misc\n      \"org.apache.pekko.remote.RemoteScope\" = pekko-misc\n\n      \"com.typesafe.config.impl.SimpleConfig\" = pekko-misc\n      \"com.typesafe.config.Config\" = pekko-misc\n\n      \"org.apache.pekko.routing.FromConfig\" = pekko-misc\n      \"org.apache.pekko.routing.DefaultResizer\" = pekko-misc\n      \"org.apache.pekko.routing.BalancingPool\" = pekko-misc\n      \"org.apache.pekko.routing.BroadcastGroup\" = pekko-misc\n      \"org.apache.pekko.routing.BroadcastPool\" = pekko-misc\n      \"org.apache.pekko.routing.RandomGroup\" = pekko-misc\n      \"org.apache.pekko.routing.RandomPool\" = pekko-misc\n      \"org.apache.pekko.routing.RoundRobinGroup\" = pekko-misc\n      \"org.apache.pekko.routing.RoundRobinPool\" = pekko-misc\n      \"org.apache.pekko.routing.ScatterGatherFirstCompletedGroup\" = pekko-misc\n      \"org.apache.pekko.routing.ScatterGatherFirstCompletedPool\" = pekko-misc\n      \"org.apache.pekko.routing.SmallestMailboxPool\" = pekko-misc\n      \"org.apache.pekko.routing.TailChoppingGroup\" = pekko-misc\n      \"org.apache.pekko.routing.TailChoppingPool\" = pekko-misc\n      \"org.apache.pekko.remote.routing.RemoteRouterConfig\" = pekko-misc\n\n      \"org.apache.pekko.pattern.StatusReply\" = pekko-misc\n\n      \"org.apache.pekko.dispatch.sysmsg.SystemMessage\" = pekko-system-msg\n\n      # Java Serializer is by default used for exceptions and will by default\n      # not be allowed to be serialized, but in certain cases they are replaced\n      # by `org.apache.pekko.remote.serialization.ThrowableNotSerializableException` if\n      # no specific serializer has been defined:\n      # - when wrapped in `org.apache.pekko.actor.Status.Failure` for ask replies\n      # - when wrapped in system messages for exceptions from remote deployed child actors\n      #\n      # It's recommended that you implement custom serializer for exceptions that are\n      # sent remotely, You can add binding to pekko-misc (MiscMessageSerializer) for the\n      # exceptions that have a constructor with single message String or constructor with\n      # message String as first parameter and cause Throwable as second parameter. Note that it's not\n      # safe to add this binding for general exceptions such as IllegalArgumentException\n      # because it may have a subclass without required constructor.\n      \"java.lang.Throwable\" = java\n    }\n\n    serialization-identifiers {\n      \"org.apache.pekko.remote.serialization.ProtobufSerializer\" = 2\n      \"org.apache.pekko.remote.serialization.DaemonMsgCreateSerializer\" = 3\n      \"org.apache.pekko.remote.serialization.MessageContainerSerializer\" = 6\n      \"org.apache.pekko.remote.serialization.MiscMessageSerializer\" = 16\n      \"org.apache.pekko.remote.serialization.ArteryMessageSerializer\" = 17\n\n      \"org.apache.pekko.remote.serialization.SystemMessageSerializer\" = 22\n\n      # deprecated in 2.6.0, moved to pekko-actor\n      \"org.apache.pekko.remote.serialization.LongSerializer\" = 18\n      # deprecated in 2.6.0, moved to pekko-actor\n      \"org.apache.pekko.remote.serialization.IntSerializer\" = 19\n      # deprecated in 2.6.0, moved to pekko-actor\n      \"org.apache.pekko.remote.serialization.StringSerializer\" = 20\n      # deprecated in 2.6.0, moved to pekko-actor\n      \"org.apache.pekko.remote.serialization.ByteStringSerializer\" = 21\n    }\n\n    deployment {\n\n      default {\n\n        # if this is set to a valid remote address, the named actor will be\n        # deployed at that node e.g. \"akka://sys@host:port\"\n        remote = \"\"\n\n        target {\n\n          # A list of hostnames and ports for instantiating the children of a\n          # router\n          #   The format should be on \"akka://sys@host:port\", where:\n          #    - sys is the remote actor system name\n          #    - hostname can be either hostname or IP address the remote actor\n          #      should connect to\n          #    - port should be the port for the remote server on the other node\n          # The number of actor instances to be spawned is still taken from the\n          # nr-of-instances setting as for local routers; the instances will be\n          # distributed round-robin among the given nodes.\n          nodes = []\n\n        }\n      }\n    }\n  }\n\n  remote {\n    ### Settings shared by classic remoting and Artery (the new implementation of remoting)\n\n    # Using remoting directly is typically not desirable, so a warning will\n    # be shown to make this clear. Set this setting to 'off' to suppress that\n    # warning.\n    warn-about-direct-use = on\n\n\n    # If Cluster is not used, remote watch and deployment are disabled.\n    # To optionally use them while not using Cluster, set to 'on'.\n    use-unsafe-remote-features-outside-cluster = off\n\n    # A warning will be logged on remote watch attempts if Cluster\n    # is not in use and 'use-unsafe-remote-features-outside-cluster'\n    # is 'off'. Set this to 'off' to suppress these.\n    warn-unsafe-watch-outside-cluster = on\n\n    # Settings for the Phi accrual failure detector (http://www.jaist.ac.jp/~defago/files/pdf/IS_RR_2004_010.pdf\n    # [Hayashibara et al]) used for remote death watch.\n    # The default PhiAccrualFailureDetector will trigger if there are no heartbeats within\n    # the duration heartbeat-interval + acceptable-heartbeat-pause + threshold_adjustment,\n    # i.e. around 12.5 seconds with default settings.\n    watch-failure-detector {\n\n      # FQCN of the failure detector implementation.\n      # It must implement org.apache.pekko.remote.FailureDetector and have\n      # a public constructor with a com.typesafe.config.Config and\n      # org.apache.pekko.actor.EventStream parameter.\n      implementation-class = \"org.apache.pekko.remote.PhiAccrualFailureDetector\"\n\n      # How often keep-alive heartbeat messages should be sent to each connection.\n      heartbeat-interval = 1 s\n\n      # Defines the failure detector threshold.\n      # A low threshold is prone to generate many wrong suspicions but ensures\n      # a quick detection in the event of a real crash. Conversely, a high\n      # threshold generates fewer mistakes but needs more time to detect\n      # actual crashes.\n      threshold = 10.0\n\n      # Number of the samples of inter-heartbeat arrival times to adaptively\n      # calculate the failure timeout for connections.\n      max-sample-size = 200\n\n      # Minimum standard deviation to use for the normal distribution in\n      # AccrualFailureDetector. Too low standard deviation might result in\n      # too much sensitivity for sudden, but normal, deviations in heartbeat\n      # inter arrival times.\n      min-std-deviation = 100 ms\n\n      # Number of potentially lost/delayed heartbeats that will be\n      # accepted before considering it to be an anomaly.\n      # This margin is important to be able to survive sudden, occasional,\n      # pauses in heartbeat arrivals, due to for example garbage collect or\n      # network drop.\n      acceptable-heartbeat-pause = 10 s\n\n\n      # How often to check for nodes marked as unreachable by the failure\n      # detector\n      unreachable-nodes-reaper-interval = 1s\n\n      # After the heartbeat request has been sent the first failure detection\n      # will start after this period, even though no heartbeat mesage has\n      # been received.\n      expected-response-after = 1 s\n\n    }\n\n    # remote deployment configuration section\n    deployment {\n      # deprecated, use `enable-allow-list`\n      enable-whitelist = off\n\n      # If true, will only allow specific classes listed in `allowed-actor-classes` to be instanciated on this\n      # system via remote deployment\n      enable-allow-list = ${pekko.remote.deployment.enable-whitelist}\n\n\n      # deprecated, use `allowed-actor-classes`\n      whitelist = []\n\n      allowed-actor-classes = ${pekko.remote.deployment.whitelist}\n    }\n\n    ### Default dispatcher for the remoting subsystem\n    default-remote-dispatcher {\n      type = Dispatcher\n      executor = \"fork-join-executor\"\n      fork-join-executor {\n        parallelism-min = 2\n        parallelism-factor = 0.5\n        parallelism-max = 16\n      }\n      throughput = 10\n    }\n    classic {\n\n      ### Configuration for classic remoting. Classic remoting is deprecated, use artery.\n\n\n      # If set to a nonempty string remoting will use the given dispatcher for\n      # its internal actors otherwise the default dispatcher is used. Please note\n      # that since remoting can load arbitrary 3rd party drivers (see\n      # \"enabled-transport\" and \"adapters\" entries) it is not guaranteed that\n      # every module will respect this setting.\n      use-dispatcher = \"pekko.remote.default-remote-dispatcher\"\n\n      # Settings for the failure detector to monitor connections.\n      # For TCP it is not important to have fast failure detection, since\n      # most connection failures are captured by TCP itself.\n      # The default DeadlineFailureDetector will trigger if there are no heartbeats within\n      # the duration heartbeat-interval + acceptable-heartbeat-pause, i.e. 124 seconds\n      # with the default settings.\n      transport-failure-detector {\n\n        # FQCN of the failure detector implementation.\n        # It must implement org.apache.pekko.remote.FailureDetector and have\n        # a public constructor with a com.typesafe.config.Config and\n        # org.apache.pekko.actor.EventStream parameter.\n        implementation-class = \"org.apache.pekko.remote.DeadlineFailureDetector\"\n\n        # How often keep-alive heartbeat messages should be sent to each connection.\n        heartbeat-interval = 4 s\n\n        # Number of potentially lost/delayed heartbeats that will be\n        # accepted before considering it to be an anomaly.\n        # A margin to the `heartbeat-interval` is important to be able to survive sudden,\n        # occasional, pauses in heartbeat arrivals, due to for example garbage collect or\n        # network drop.\n        acceptable-heartbeat-pause = 120 s\n      }\n\n\n      # Timeout after which the startup of the remoting subsystem is considered\n      # to be failed. Increase this value if your transport drivers (see the\n      # enabled-transports section) need longer time to be loaded.\n      startup-timeout = 10 s\n\n      # Timout after which the graceful shutdown of the remoting subsystem is\n      # considered to be failed. After the timeout the remoting system is\n      # forcefully shut down. Increase this value if your transport drivers\n      # (see the enabled-transports section) need longer time to stop properly.\n      shutdown-timeout = 10 s\n\n      # Before shutting down the drivers, the remoting subsystem attempts to flush\n      # all pending writes. This setting controls the maximum time the remoting is\n      # willing to wait before moving on to shut down the drivers.\n      flush-wait-on-shutdown = 2 s\n\n      # Reuse inbound connections for outbound messages\n      use-passive-connections = on\n\n      # Controls the backoff interval after a refused write is reattempted.\n      # (Transports may refuse writes if their internal buffer is full)\n      backoff-interval = 5 ms\n\n      # Acknowledgment timeout of management commands sent to the transport stack.\n      command-ack-timeout = 30 s\n\n      # The timeout for outbound associations to perform the handshake.\n      # If the transport is pekko.remote.classic.netty.tcp or pekko.remote.classic.netty.ssl\n      # the configured connection-timeout for the transport will be used instead.\n      handshake-timeout = 15 s\n\n      ### Security settings\n\n      # Enable untrusted mode for full security of server managed actors, prevents\n      # system messages to be send by clients, e.g. messages like 'Create',\n      # 'Suspend', 'Resume', 'Terminate', 'Supervise', 'Link' etc.\n      untrusted-mode = off\n\n      # When 'untrusted-mode=on' inbound actor selections are by default discarded.\n      # Actors with paths defined in this list are granted permission to receive actor\n      # selections messages.\n      # E.g. trusted-selection-paths = [\"/user/receptionist\", \"/user/namingService\"]\n      trusted-selection-paths = []\n\n      ### Logging\n\n      # If this is \"on\", Pekko will log all inbound messages at DEBUG level,\n      # if off then they are not logged\n      log-received-messages = off\n\n      # If this is \"on\", Pekko will log all outbound messages at DEBUG level,\n      # if off then they are not logged\n      log-sent-messages = off\n\n      # Sets the log granularity level at which Pekko logs remoting events. This setting\n      # can take the values OFF, ERROR, WARNING, INFO, DEBUG, or ON. For compatibility\n      # reasons the setting \"on\" will default to \"debug\" level. Please note that the effective\n      # logging level is still determined by the global logging level of the actor system:\n      # for example debug level remoting events will be only logged if the system\n      # is running with debug level logging.\n      # Failures to deserialize received messages also fall under this flag.\n      log-remote-lifecycle-events = on\n\n      # Logging of message types with payload size in bytes larger than\n      # this value. Maximum detected size per message type is logged once,\n      # with an increase threshold of 10%.\n      # By default this feature is turned off. Activate it by setting the property to\n      # a value in bytes, such as 1000b. Note that for all messages larger than this\n      # limit there will be extra performance and scalability cost.\n      log-frame-size-exceeding = off\n\n      # Log warning if the number of messages in the backoff buffer in the endpoint\n      # writer exceeds this limit. It can be disabled by setting the value to off.\n      log-buffer-size-exceeding = 50000\n\n      # After failed to establish an outbound connection, the remoting will mark the\n      # address as failed. This configuration option controls how much time should\n      # be elapsed before reattempting a new connection. While the address is\n      # gated, all messages sent to the address are delivered to dead-letters.\n      # Since this setting limits the rate of reconnects setting it to a\n      # very short interval (i.e. less than a second) may result in a storm of\n      # reconnect attempts.\n      retry-gate-closed-for = 5 s\n\n      # After catastrophic communication failures that result in the loss of system\n      # messages or after the remote DeathWatch triggers the remote system gets\n      # quarantined to prevent inconsistent behavior.\n      # This setting controls how long the Quarantine marker will be kept around\n      # before being removed to avoid long-term memory leaks.\n      # WARNING: DO NOT change this to a small value to re-enable communication with\n      # quarantined nodes. Such feature is not supported and any behavior between\n      # the affected systems after lifting the quarantine is undefined.\n      prune-quarantine-marker-after = 5 d\n\n      # If system messages have been exchanged between two systems (i.e. remote death\n      # watch or remote deployment has been used) a remote system will be marked as\n      # quarantined after the two system has no active association, and no\n      # communication happens during the time configured here.\n      # The only purpose of this setting is to avoid storing system message redelivery\n      # data (sequence number state, etc.) for an undefined amount of time leading to long\n      # term memory leak. Instead, if a system has been gone for this period,\n      # or more exactly\n      # - there is no association between the two systems (TCP connection, if TCP transport is used)\n      # - neither side has been attempting to communicate with the other\n      # - there are no pending system messages to deliver\n      # for the amount of time configured here, the remote system will be quarantined and all state\n      # associated with it will be dropped.\n      #\n      # Maximum value depends on the scheduler's max limit (default 248 days) and if configured\n      # to a longer duration this feature will effectively be disabled. Setting the value to\n      # 'off' will also disable the feature. Note that if disabled there is a risk of a long\n      # term memory leak.\n      quarantine-after-silence = 2 d\n\n      # This setting defines the maximum number of unacknowledged system messages\n      # allowed for a remote system. If this limit is reached the remote system is\n      # declared to be dead and its UID marked as tainted.\n      system-message-buffer-size = 20000\n\n      # This setting defines the maximum idle time after an individual\n      # acknowledgement for system messages is sent. System message delivery\n      # is guaranteed by explicit acknowledgement messages. These acks are\n      # piggybacked on ordinary traffic messages. If no traffic is detected\n      # during the time period configured here, the remoting will send out\n      # an individual ack.\n      system-message-ack-piggyback-timeout = 0.3 s\n\n      # This setting defines the time after internal management signals\n      # between actors (used for DeathWatch and supervision) that have not been\n      # explicitly acknowledged or negatively acknowledged are resent.\n      # Messages that were negatively acknowledged are always immediately\n      # resent.\n      resend-interval = 2 s\n\n      # Maximum number of unacknowledged system messages that will be resent\n      # each 'resend-interval'. If you watch many (> 1000) remote actors you can\n      # increase this value to for example 600, but a too large limit (e.g. 10000)\n      # may flood the connection and might cause false failure detection to trigger.\n      # Test such a configuration by watching all actors at the same time and stop\n      # all watched actors at the same time.\n      resend-limit = 200\n\n      # WARNING: this setting should not be not changed unless all of its consequences\n      # are properly understood which assumes experience with remoting internals\n      # or expert advice.\n      # This setting defines the time after redelivery attempts of internal management\n      # signals are stopped to a remote system that has been not confirmed to be alive by\n      # this system before.\n      initial-system-message-delivery-timeout = 3 m\n\n      ### Transports and adapters\n\n      # List of the transport drivers that will be loaded by the remoting.\n      # A list of fully qualified config paths must be provided where\n      # the given configuration path contains a transport-class key\n      # pointing to an implementation class of the Transport interface.\n      # If multiple transports are provided, the address of the first\n      # one will be used as a default address.\n      enabled-transports = [\"pekko.remote.classic.netty.tcp\"]\n\n      # Transport drivers can be augmented with adapters by adding their\n      # name to the applied-adapters setting in the configuration of a\n      # transport. The available adapters should be configured in this\n      # section by providing a name, and the fully qualified name of\n      # their corresponding implementation. The class given here\n      # must implement org.apache.pekko.remote.transport.TransportAdapterProvider\n      # and have public constructor without parameters.\n      adapters {\n        gremlin = \"org.apache.pekko.remote.transport.FailureInjectorProvider\"\n        trttl = \"org.apache.pekko.remote.transport.ThrottlerProvider\"\n      }\n\n      ### Default configuration for the Netty based transport drivers\n\n      netty.tcp {\n        # The class given here must implement the org.apache.pekko.remote.transport.Transport\n        # interface and offer a public constructor which takes two arguments:\n        #  1) org.apache.pekko.actor.ExtendedActorSystem\n        #  2) com.typesafe.config.Config\n        transport-class = \"org.apache.pekko.remote.transport.netty.NettyTransport\"\n\n        # Transport drivers can be augmented with adapters by adding their\n        # name to the applied-adapters list. The last adapter in the\n        # list is the adapter immediately above the driver, while\n        # the first one is the top of the stack below the standard\n        # Pekko protocol\n        applied-adapters = []\n\n        # The default remote server port clients should connect to.\n        # Default is 2552 (AKKA), use 0 if you want a random available port\n        # This port needs to be unique for each actor system on the same machine.\n        port = 2552\n\n        # The hostname or ip clients should connect to.\n        # InetAddress.getLocalHost.getHostAddress is used if empty\n        hostname = \"\"\n\n        # Use this setting to bind a network interface to a different port\n        # than remoting protocol expects messages at. This may be used\n        # when running pekko nodes in a separated networks (under NATs or docker containers).\n        # Use 0 if you want a random available port. Examples:\n        #\n        # pekko.remote.classic.netty.tcp.port = 2552\n        # pekko.remote.classic.netty.tcp.bind-port = 2553\n        # Network interface will be bound to the 2553 port, but remoting protocol will\n        # expect messages sent to port 2552.\n        #\n        # pekko.remote.classic.netty.tcp.port = 0\n        # pekko.remote.classic.netty.tcp.bind-port = 0\n        # Network interface will be bound to a random port, and remoting protocol will\n        # expect messages sent to the bound port.\n        #\n        # pekko.remote.classic.netty.tcp.port = 2552\n        # pekko.remote.classic.netty.tcp.bind-port = 0\n        # Network interface will be bound to a random port, but remoting protocol will\n        # expect messages sent to port 2552.\n        #\n        # pekko.remote.classic.netty.tcp.port = 0\n        # pekko.remote.classic.netty.tcp.bind-port = 2553\n        # Network interface will be bound to the 2553 port, and remoting protocol will\n        # expect messages sent to the bound port.\n        #\n        # pekko.remote.classic.netty.tcp.port = 2552\n        # pekko.remote.classic.netty.tcp.bind-port = \"\"\n        # Network interface will be bound to the 2552 port, and remoting protocol will\n        # expect messages sent to the bound port.\n        #\n        # pekko.remote.classic.netty.tcp.port if empty\n        bind-port = \"\"\n\n        # Use this setting to bind a network interface to a different hostname or ip\n        # than remoting protocol expects messages at.\n        # Use \"0.0.0.0\" to bind to all interfaces.\n        # pekko.remote.classic.netty.tcp.hostname if empty\n        bind-hostname = \"\"\n\n        # Enables SSL support on this transport\n        enable-ssl = false\n\n        # Sets the connectTimeoutMillis of all outbound connections,\n        # i.e. how long a connect may take until it is timed out\n        connection-timeout = 15 s\n\n        # If set to \"<id.of.dispatcher>\" then the specified dispatcher\n        # will be used to accept inbound connections, and perform IO. If \"\" then\n        # dedicated threads will be used.\n        # Please note that the Netty driver only uses this configuration and does\n        # not read the \"pekko.remote.use-dispatcher\" entry. Instead it has to be\n        # configured manually to point to the same dispatcher if needed.\n        use-dispatcher-for-io = \"\"\n\n        # Sets the high water mark for the in and outbound sockets,\n        # set to 0b for platform default\n        write-buffer-high-water-mark = 0b\n\n        # Sets the low water mark for the in and outbound sockets,\n        # set to 0b for platform default\n        write-buffer-low-water-mark = 0b\n\n        # Sets the send buffer size of the Sockets,\n        # set to 0b for platform default\n        send-buffer-size = 256000b\n\n        # Sets the receive buffer size of the Sockets,\n        # set to 0b for platform default\n        receive-buffer-size = 256000b\n\n        # Maximum message size the transport will accept, but at least\n        # 32000 bytes.\n        # Please note that UDP does not support arbitrary large datagrams,\n        # so this setting has to be chosen carefully when using UDP.\n        # Both send-buffer-size and receive-buffer-size settings has to\n        # be adjusted to be able to buffer messages of maximum size.\n        maximum-frame-size = 128000b\n\n        # Sets the size of the connection backlog\n        backlog = 4096\n\n        # Enables the TCP_NODELAY flag, i.e. disables Nagle’s algorithm\n        tcp-nodelay = on\n\n        # Enables TCP Keepalive, subject to the O/S kernel’s configuration\n        tcp-keepalive = on\n\n        # Enables SO_REUSEADDR, which determines when an ActorSystem can open\n        # the specified listen port (the meaning differs between *nix and Windows)\n        # Valid values are \"on\", \"off\" and \"off-for-windows\"\n        # due to the following Windows bug: https://bugs.java.com/bugdatabase/view_bug.do?bug_id=4476378\n        # \"off-for-windows\" of course means that it's \"on\" for all other platforms\n        tcp-reuse-addr = off-for-windows\n\n        # Used to configure the number of I/O worker threads on server sockets\n        server-socket-worker-pool {\n          # Min number of threads to cap factor-based number to\n          pool-size-min = 2\n\n          # The pool size factor is used to determine thread pool size\n          # using the following formula: ceil(available processors * factor).\n          # Resulting size is then bounded by the pool-size-min and\n          # pool-size-max values.\n          pool-size-factor = 1.0\n\n          # Max number of threads to cap factor-based number to\n          pool-size-max = 2\n        }\n\n        # Used to configure the number of I/O worker threads on client sockets\n        client-socket-worker-pool {\n          # Min number of threads to cap factor-based number to\n          pool-size-min = 2\n\n          # The pool size factor is used to determine thread pool size\n          # using the following formula: ceil(available processors * factor).\n          # Resulting size is then bounded by the pool-size-min and\n          # pool-size-max values.\n          pool-size-factor = 1.0\n\n          # Max number of threads to cap factor-based number to\n          pool-size-max = 2\n        }\n\n\n      }\n\n      netty.ssl = ${pekko.remote.classic.netty.tcp}\n      netty.ssl = {\n        # Enable SSL/TLS encryption.\n        # This must be enabled on both the client and server to work.\n        enable-ssl = true\n\n        # Factory of SSLEngine.\n        # Must implement org.apache.pekko.remote.transport.netty.SSLEngineProvider and have a public\n        # constructor with an ActorSystem parameter.\n        # The default ConfigSSLEngineProvider is configured by properties in section\n        # pekko.remote.classic.netty.ssl.security\n        #\n        # The SSLEngineProvider can also be defined via ActorSystemSetup with\n        # SSLEngineProviderSetup  when starting the ActorSystem. That is useful when\n        # the SSLEngineProvider implementation requires other external constructor\n        # parameters or is created before the ActorSystem is created.\n        # If such SSLEngineProviderSetup is defined this config property is not used.\n        ssl-engine-provider = org.apache.pekko.remote.transport.netty.ConfigSSLEngineProvider\n\n        security {\n          # This is the Java Key Store used by the server connection\n          key-store = \"keystore\"\n\n          # This password is used for decrypting the key store\n          key-store-password = \"changeme\"\n\n          # This password is used for decrypting the key\n          key-password = \"changeme\"\n\n          # This is the Java Key Store used by the client connection\n          trust-store = \"truststore\"\n\n          # This password is used for decrypting the trust store\n          trust-store-password = \"changeme\"\n\n          # Protocol to use for SSL encryption.\n          protocol = \"TLSv1.2\"\n\n          # Example: [\"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \n          #   \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\n          #   \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\",\n          #   \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"]\n          # When doing rolling upgrades, make sure to include both the algorithm used \n          # by old nodes and the preferred algorithm.\n          # If you use a JDK 8 prior to 8u161 you need to install\n          # the JCE Unlimited Strength Jurisdiction Policy Files to use AES 256.\n          # More info here:\n          # https://www.oracle.com/java/technologies/javase-jce-all-downloads.html\n          enabled-algorithms = [\"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\",\n            \"TLS_RSA_WITH_AES_128_CBC_SHA\"]\n\n          # There are two options, and the default SecureRandom is recommended:\n          # \"\" or \"SecureRandom\" => (default)\n          # \"SHA1PRNG\" => Can be slow because of blocking issues on Linux\n          #\n          # Setting a value here may require you to supply the appropriate cipher\n          # suite (see enabled-algorithms section above)\n          random-number-generator = \"\"\n\n          # Require mutual authentication between TLS peers\n          #\n          # Without mutual authentication only the peer that actively establishes a connection (TLS client side)\n          # checks if the passive side (TLS server side) sends over a trusted certificate. With the flag turned on,\n          # the passive side will also request and verify a certificate from the connecting peer.\n          #\n          # To prevent man-in-the-middle attacks this setting is enabled by default.\n          #\n          # Note: Nodes that are configured with this setting to 'on' might not be able to receive messages from nodes that\n          # run on older versions of pekko-remote. This is because in versions of Pekko < 2.4.12 the active side of the remoting\n          # connection will not send over certificates even if asked.\n          #\n          # However, starting with Pekko 2.4.12, even with this setting \"off\", the active side (TLS client side)\n          # will use the given key-store to send over a certificate if asked. A rolling upgrade from versions of\n          # Pekko < 2.4.12 can therefore work like this:\n          #   - upgrade all nodes to a Pekko version >= 2.4.12, in the best case the latest version, but keep this setting at \"off\"\n          #   - then switch this flag to \"on\" and do again a rolling upgrade of all nodes\n          # The first step ensures that all nodes will send over a certificate when asked to. The second\n          # step will ensure that all nodes finally enforce the secure checking of client certificates.\n          require-mutual-authentication = on\n        }\n      }\n\n      ### Default configuration for the failure injector transport adapter\n\n      gremlin {\n        # Enable debug logging of the failure injector transport adapter\n        debug = off\n      }\n\n      backoff-remote-dispatcher {\n        type = Dispatcher\n        executor = \"fork-join-executor\"\n        fork-join-executor {\n          # Min number of threads to cap factor-based parallelism number to\n          parallelism-min = 2\n          parallelism-max = 2\n        }\n      }\n    }\n  }\n}","title":"pekko-remote classic (deprecated)"},{"location":"/general/configuration-reference.html#pekko-testkit","text":"copysource######################################\n# Pekko Testkit Reference Config File #\n######################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\npekko {\n  test {\n    # factor by which to scale timeouts during tests, e.g. to account for shared\n    # build system load\n    timefactor =  1.0\n\n    # duration of EventFilter.intercept waits after the block is finished until\n    # all required messages are received\n    filter-leeway = 3s\n\n    # duration to wait in expectMsg and friends outside of within() block\n    # by default, will be dilated by the timefactor.\n    single-expect-default = 3s\n\n    # duration to wait in expectNoMessage by default,\n    # will be dilated by the timefactor.\n    expect-no-message-default = 100ms\n\n    # The timeout that is added as an implicit by DefaultTimeout trait\n    default-timeout = 5s\n\n    calling-thread-dispatcher {\n      type = org.apache.pekko.testkit.CallingThreadDispatcherConfigurator\n    }\n  }\n\n  actor {\n\n    serializers {\n      java-test = \"org.apache.pekko.testkit.TestJavaSerializer\"\n    }\n\n    serialization-identifiers {\n      \"org.apache.pekko.testkit.TestJavaSerializer\" = 23\n    }\n\n    serialization-bindings {\n      \"org.apache.pekko.testkit.JavaSerializable\" = java-test\n    }\n  }\n}","title":"pekko-testkit"},{"location":"/general/configuration-reference.html#pekko-cluster-metrics","text":"copysource##############################################\n# Pekko Cluster Metrics Reference Config File #\n##############################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits in your application.conf in order to override these settings.\n\n# Sigar provisioning:\n#\n#  User can provision sigar classes and native library in one of the following ways:\n# \n#  1) Use https://github.com/kamon-io/sigar-loader Kamon sigar-loader as a project dependency for the user project.\n#  Metrics extension will extract and load sigar library on demand with help of Kamon sigar provisioner.\n# \n#  2) Use https://github.com/kamon-io/sigar-loader Kamon sigar-loader as java agent: `java -javaagent:/path/to/sigar-loader.jar`\n#  Kamon sigar loader agent will extract and load sigar library during JVM start.\n# \n#  3) Place `sigar.jar` on the `classpath` and sigar native library for the o/s on the `java.library.path`\n#  User is required to manage both project dependency and library deployment manually.\n\n# Cluster metrics extension.\n# Provides periodic statistics collection and publication throughout the cluster.\npekko.cluster.metrics {\n  # Full path of dispatcher configuration key.\n  dispatcher = \"pekko.actor.default-dispatcher\"\n  # How long should any actor wait before starting the periodic tasks.\n  periodic-tasks-initial-delay = 1s\n  # Sigar native library extract location.\n  # Use per-application-instance scoped location, such as program working directory.\n  native-library-extract-folder = ${user.dir}\"/native\"\n  # Metrics supervisor actor.\n  supervisor {\n    # Actor name. Example name space: /system/cluster-metrics\n    name = \"cluster-metrics\"\n    # Supervision strategy.\n    strategy {\n      #\n      # FQCN of class providing `org.apache.pekko.actor.SupervisorStrategy`.\n      # Must have a constructor with signature `<init>(com.typesafe.config.Config)`.\n      # Default metrics strategy provider is a configurable extension of `OneForOneStrategy`.\n      provider = \"org.apache.pekko.cluster.metrics.ClusterMetricsStrategy\"\n      #\n      # Configuration of the default strategy provider.\n      # Replace with custom settings when overriding the provider.\n      configuration = {\n        # Log restart attempts.\n        loggingEnabled = true\n        # Child actor restart-on-failure window.\n        withinTimeRange = 3s\n        # Maximum number of restart attempts before child actor is stopped.\n        maxNrOfRetries = 3\n      }\n    }\n  }\n  # Metrics collector actor.\n  collector {\n    # Enable or disable metrics collector for load-balancing nodes.\n    # Metrics collection can also be controlled at runtime by sending control messages\n    # to /system/cluster-metrics actor: `org.apache.pekko.cluster.metrics.{CollectionStartMessage,CollectionStopMessage}`\n    enabled = on\n    # FQCN of the metrics collector implementation.\n    # It must implement `org.apache.pekko.cluster.metrics.MetricsCollector` and\n    # have public constructor with org.apache.pekko.actor.ActorSystem parameter.\n    # Will try to load in the following order of priority:\n    # 1) configured custom collector 2) internal `SigarMetricsCollector` 3) internal `JmxMetricsCollector`\n    provider = \"\"\n    # Try all 3 available collector providers, or else fail on the configured custom collector provider.\n    fallback = true\n    # How often metrics are sampled on a node.\n    # Shorter interval will collect the metrics more often.\n    # Also controls frequency of the metrics publication to the node system event bus.\n    sample-interval = 3s\n    # How often a node publishes metrics information to the other nodes in the cluster.\n    # Shorter interval will publish the metrics gossip more often.\n    gossip-interval = 3s\n    # How quickly the exponential weighting of past data is decayed compared to\n    # new data. Set lower to increase the bias toward newer values.\n    # The relevance of each data sample is halved for every passing half-life\n    # duration, i.e. after 4 times the half-life, a data sample’s relevance is\n    # reduced to 6% of its original relevance. The initial relevance of a data\n    # sample is given by 1 – 0.5 ^ (collect-interval / half-life).\n    # See https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\n    moving-average-half-life = 12s\n  }\n}\n\n# Cluster metrics extension serializers and routers.\npekko.actor {\n  # Protobuf serializer for remote cluster metrics messages.\n  serializers {\n    pekko-cluster-metrics = \"org.apache.pekko.cluster.metrics.protobuf.MessageSerializer\"\n  }\n  # Interface binding for remote cluster metrics messages.\n  serialization-bindings {\n    \"org.apache.pekko.cluster.metrics.ClusterMetricsMessage\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.AdaptiveLoadBalancingPool\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.MixMetricsSelector\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.CpuMetricsSelector$\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.HeapMetricsSelector$\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.SystemLoadAverageMetricsSelector$\" = pekko-cluster-metrics\n  }\n  # Globally unique metrics extension serializer identifier.\n  serialization-identifiers {\n    \"org.apache.pekko.cluster.metrics.protobuf.MessageSerializer\" = 10\n  }\n  #  Provide routing of messages based on cluster metrics.\n  router.type-mapping {\n    cluster-metrics-adaptive-pool  = \"org.apache.pekko.cluster.metrics.AdaptiveLoadBalancingPool\"\n    cluster-metrics-adaptive-group = \"org.apache.pekko.cluster.metrics.AdaptiveLoadBalancingGroup\"\n  }\n}","title":"pekko-cluster-metrics"},{"location":"/general/configuration-reference.html#pekko-cluster-tools","text":"copysource############################################\n# Pekko Cluster Tools Reference Config File #\n############################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\n# //#pub-sub-ext-config\n# Settings for the DistributedPubSub extension\npekko.cluster.pub-sub {\n  # Actor name of the mediator actor, /system/distributedPubSubMediator\n  name = distributedPubSubMediator\n\n  # Start the mediator on members tagged with this role.\n  # All members are used if undefined or empty.\n  role = \"\"\n\n  # The routing logic to use for 'Send'\n  # Possible values: random, round-robin, broadcast\n  routing-logic = random\n\n  # How often the DistributedPubSubMediator should send out gossip information\n  gossip-interval = 1s\n\n  # Removed entries are pruned after this duration\n  removed-time-to-live = 120s\n\n  # Maximum number of elements to transfer in one message when synchronizing the registries.\n  # Next chunk will be transferred in next round of gossip.\n  max-delta-elements = 3000\n\n  # When a message is published to a topic with no subscribers send it to the dead letters.\n  send-to-dead-letters-when-no-subscribers = on\n  \n  # The id of the dispatcher to use for DistributedPubSubMediator actors. \n  # If specified you need to define the settings of the actual dispatcher.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n}\n# //#pub-sub-ext-config\n\n# Protobuf serializer for cluster DistributedPubSubMeditor messages\npekko.actor {\n  serializers {\n    pekko-pubsub = \"org.apache.pekko.cluster.pubsub.protobuf.DistributedPubSubMessageSerializer\"\n  }\n  serialization-bindings {\n    \"org.apache.pekko.cluster.pubsub.DistributedPubSubMessage\" = pekko-pubsub\n    \"org.apache.pekko.cluster.pubsub.DistributedPubSubMediator$Internal$SendToOneSubscriber\" = pekko-pubsub\n  }\n  serialization-identifiers {\n    \"org.apache.pekko.cluster.pubsub.protobuf.DistributedPubSubMessageSerializer\" = 9\n  }\n}\n\n\n# //#receptionist-ext-config\n# Settings for the ClusterClientReceptionist extension\npekko.cluster.client.receptionist {\n  # Actor name of the ClusterReceptionist actor, /system/receptionist\n  name = receptionist\n\n  # Start the receptionist on members tagged with this role.\n  # All members are used if undefined or empty.\n  role = \"\"\n\n  # The receptionist will send this number of contact points to the client\n  number-of-contacts = 3\n\n  # The actor that tunnel response messages to the client will be stopped\n  # after this time of inactivity.\n  response-tunnel-receive-timeout = 30s\n  \n  # The id of the dispatcher to use for ClusterReceptionist actors.\n  # If specified you need to define the settings of the actual dispatcher.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n  # How often failure detection heartbeat messages should be received for\n  # each ClusterClient\n  heartbeat-interval = 2s\n\n  # Number of potentially lost/delayed heartbeats that will be\n  # accepted before considering it to be an anomaly.\n  # The ClusterReceptionist is using the org.apache.pekko.remote.DeadlineFailureDetector, which\n  # will trigger if there are no heartbeats within the duration\n  # heartbeat-interval + acceptable-heartbeat-pause, i.e. 15 seconds with\n  # the default settings.\n  acceptable-heartbeat-pause = 13s\n\n  # Failure detection checking interval for checking all ClusterClients\n  failure-detection-interval = 2s\n}\n# //#receptionist-ext-config\n\n# //#cluster-client-config\n# Settings for the ClusterClient\npekko.cluster.client {\n  # Actor paths of the ClusterReceptionist actors on the servers (cluster nodes)\n  # that the client will try to contact initially. It is mandatory to specify\n  # at least one initial contact. \n  # Comma separated full actor paths defined by a string on the form of\n  # \"akka://system@hostname:port/system/receptionist\"\n  initial-contacts = []\n  \n  # Interval at which the client retries to establish contact with one of \n  # ClusterReceptionist on the servers (cluster nodes)\n  establishing-get-contacts-interval = 3s\n  \n  # Interval at which the client will ask the ClusterReceptionist for\n  # new contact points to be used for next reconnect.\n  refresh-contacts-interval = 60s\n  \n  # How often failure detection heartbeat messages should be sent\n  heartbeat-interval = 2s\n  \n  # Number of potentially lost/delayed heartbeats that will be\n  # accepted before considering it to be an anomaly.\n  # The ClusterClient is using the org.apache.pekko.remote.DeadlineFailureDetector, which\n  # will trigger if there are no heartbeats within the duration \n  # heartbeat-interval + acceptable-heartbeat-pause, i.e. 15 seconds with\n  # the default settings.\n  acceptable-heartbeat-pause = 13s\n  \n  # If connection to the receptionist is not established the client will buffer\n  # this number of messages and deliver them the connection is established.\n  # When the buffer is full old messages will be dropped when new messages are sent\n  # via the client. Use 0 to disable buffering, i.e. messages will be dropped\n  # immediately if the location of the singleton is unknown.\n  # Maximum allowed buffer size is 10000.\n  buffer-size = 1000\n\n  # If connection to the receiptionist is lost and the client has not been\n  # able to acquire a new connection for this long the client will stop itself.\n  # This duration makes it possible to watch the cluster client and react on a more permanent\n  # loss of connection with the cluster, for example by accessing some kind of\n  # service registry for an updated set of initial contacts to start a new cluster client with.\n  # If this is not wanted it can be set to \"off\" to disable the timeout and retry\n  # forever.\n  reconnect-timeout = off\n}\n# //#cluster-client-config\n\n# Protobuf serializer for ClusterClient messages\npekko.actor {\n  serializers {\n    pekko-cluster-client = \"org.apache.pekko.cluster.client.protobuf.ClusterClientMessageSerializer\"\n  }\n  serialization-bindings {\n    \"org.apache.pekko.cluster.client.ClusterClientMessage\" = pekko-cluster-client\n  }\n  serialization-identifiers {\n    \"org.apache.pekko.cluster.client.protobuf.ClusterClientMessageSerializer\" = 15\n  }\n}\n\n# //#singleton-config\npekko.cluster.singleton {\n  # The actor name of the child singleton actor.\n  singleton-name = \"singleton\"\n  \n  # Singleton among the nodes tagged with specified role.\n  # If the role is not specified it's a singleton among all nodes in the cluster.\n  role = \"\"\n  \n  # When a node is becoming oldest it sends hand-over request to previous oldest, \n  # that might be leaving the cluster. This is retried with this interval until \n  # the previous oldest confirms that the hand over has started or the previous \n  # oldest member is removed from the cluster (+ pekko.cluster.down-removal-margin).\n  hand-over-retry-interval = 1s\n  \n  # The number of retries are derived from hand-over-retry-interval and\n  # pekko.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),\n  # but it will never be less than this property.\n  # After the hand over retries and it's still not able to exchange the hand over messages\n  # with the previous oldest it will restart itself by throwing ClusterSingletonManagerIsStuck,\n  # to start from a clean state. After that it will still not start the singleton instance\n  # until the previous oldest node has been removed from the cluster.\n  # On the other side, on the previous oldest node, the same number of retries - 3 are used\n  # and after that the singleton instance is stopped.\n  # For large clusters it might be necessary to increase this to avoid too early timeouts while\n  # gossip dissemination of the Leaving to Exiting phase occurs. For normal leaving scenarios\n  # it will not be a quicker hand over by reducing this value, but in extreme failure scenarios\n  # the recovery might be faster.\n  min-number-of-hand-over-retries = 15\n\n  # Config path of the lease to be taken before creating the singleton actor\n  # if the lease is lost then the actor is restarted and it will need to re-acquire the lease\n  # the default is no lease\n  use-lease = \"\"\n\n  # The interval between retries for acquiring the lease\n  lease-retry-interval = 5s\n}\n# //#singleton-config\n\n# //#singleton-proxy-config\npekko.cluster.singleton-proxy {\n  # The actor name of the singleton actor that is started by the ClusterSingletonManager\n  singleton-name = ${pekko.cluster.singleton.singleton-name}\n  \n  # The role of the cluster nodes where the singleton can be deployed.\n  # Corresponding to the role used by the `ClusterSingletonManager`. If the role is not\n  # specified it's a singleton among all nodes in the cluster, and the `ClusterSingletonManager`\n  # must then also be configured in same way.\n  role = \"\"\n  \n  # Interval at which the proxy will try to resolve the singleton instance.\n  singleton-identification-interval = 1s\n  \n  # If the location of the singleton is unknown the proxy will buffer this\n  # number of messages and deliver them when the singleton is identified. \n  # When the buffer is full old messages will be dropped when new messages are\n  # sent via the proxy.\n  # Use 0 to disable buffering, i.e. messages will be dropped immediately if\n  # the location of the singleton is unknown.\n  # Maximum allowed buffer size is 10000.\n  buffer-size = 1000 \n}\n# //#singleton-proxy-config\n\n# Serializer for cluster ClusterSingleton messages\npekko.actor {\n  serializers {\n    pekko-singleton = \"org.apache.pekko.cluster.singleton.protobuf.ClusterSingletonMessageSerializer\"\n  }\n  serialization-bindings {\n    \"org.apache.pekko.cluster.singleton.ClusterSingletonMessage\" = pekko-singleton\n  }\n  serialization-identifiers {\n    \"org.apache.pekko.cluster.singleton.protobuf.ClusterSingletonMessageSerializer\" = 14\n  }\n}","title":"pekko-cluster-tools"},{"location":"/general/configuration-reference.html#pekko-cluster-sharding-typed","text":"copysource # //#sharding-ext-config\n# //#number-of-shards\npekko.cluster.sharding {\n  # Number of shards used by the default HashCodeMessageExtractor\n  # when no other message extractor is defined. This value must be\n  # the same for all nodes in the cluster and that is verified by\n  # configuration check when joining. Changing the value requires\n  # stopping all nodes in the cluster.\n  number-of-shards = 1000\n}\n# //#number-of-shards\n# //#sharding-ext-config\n\n\npekko.cluster.sharded-daemon-process {\n  # Settings for the sharded dameon process internal usage of sharding are using the pekko.cluste.sharding defaults.\n  # Some of the settings can be overriden specifically for the sharded daemon process here. For example can the\n  # `role` setting limit what nodes the daemon processes and the keep alive pingers will run on.\n  # Some settings can not be changed (remember-entitites and related settings, passivation, number-of-shards),\n  # overriding those settings will be ignored.\n  sharding = ${pekko.cluster.sharding}\n\n  # Each entity is pinged at this interval from each node in the\n  # cluster to trigger a start if it has stopped, for example during\n  # rebalancing.\n  # Note: How the set of actors is kept alive may change in the future meaning this setting may go away.\n  keep-alive-interval = 10s\n}\n\npekko.cluster.configuration-compatibility-check.checkers {\n  pekko-cluster-sharding-hash-extractor = \"org.apache.pekko.cluster.sharding.typed.internal.JoinConfigCompatCheckerClusterSharding\"\n}\n\npekko.actor {\n  serializers {\n    typed-sharding = \"org.apache.pekko.cluster.sharding.typed.internal.ShardingSerializer\"\n  }\n  serialization-identifiers {\n    \"org.apache.pekko.cluster.sharding.typed.internal.ShardingSerializer\" = 25\n  }\n  serialization-bindings {\n    \"org.apache.pekko.cluster.sharding.typed.ShardingEnvelope\" = typed-sharding\n  }\n}\n\npekko.reliable-delivery {\n  sharding {\n    producer-controller = ${pekko.reliable-delivery.producer-controller}\n    producer-controller {\n      # Limit of how many messages that can be buffered when there\n      # is no demand from the consumer side.\n      buffer-size = 1000\n\n      # Ask timeout for sending message to worker until receiving Ack from worker\n      internal-ask-timeout = 60s\n\n      # If no messages are sent to an entity within this duration the\n      # ProducerController for that entity will be removed.\n      cleanup-unused-after = 120s\n\n      # In case ShardingConsumerController is stopped and there are pending\n      # unconfirmed messages the ShardingConsumerController has to \"wake up\"\n      # the consumer again by resending the first unconfirmed message.\n      resend-first-unconfirmed-idle-timeout = 10s\n\n      # Chunked messages not implemented for sharding yet. Override to not\n      # propagate property from pekko.reliable-delivery.producer-controller.\n      chunk-large-messages = off\n    }\n\n    consumer-controller = ${pekko.reliable-delivery.consumer-controller}\n    consumer-controller {\n      # Limit of how many messages that can be buffered before the\n      # ShardingConsumerController is initialized by the Start message.\n      buffer-size = 1000\n    }\n  }\n}","title":"pekko-cluster-sharding-typed"},{"location":"/general/configuration-reference.html#pekko-cluster-sharding","text":"copysource###############################################\n# Pekko Cluster Sharding Reference Config File #\n###############################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\n\n# //#sharding-ext-config\n# Settings for the ClusterShardingExtension\npekko.cluster.sharding {\n\n  # The extension creates a top level actor with this name in top level system scope,\n  # e.g. '/system/sharding'\n  guardian-name = sharding\n\n  # Specifies that entities run on cluster nodes with a specific role.\n  # If the role is not specified (or empty) all nodes in the cluster are used.\n  role = \"\"\n\n  # When this is set to 'on' the active entity actors will automatically be restarted\n  # upon Shard restart. i.e. if the Shard is started on a different ShardRegion\n  # due to rebalance or crash.\n  remember-entities = off\n\n  # When 'remember-entities' is enabled and the state store mode is ddata this controls\n  # how the remembered entities and shards are stored. Possible values are \"eventsourced\" and \"ddata\"\n  # Default is ddata for backwards compatibility.\n  remember-entities-store = \"ddata\"\n\n  # Deprecated: use the `passivation.default-idle-strategy.idle-entity.timeout` setting instead.\n  # Set this to a time duration to have sharding passivate entities when they have not\n  # received any message in this length of time. Set to 'off' to disable.\n  # It is always disabled if `remember-entities` is enabled.\n  passivate-idle-entity-after = null\n\n  # Automatic entity passivation settings.\n  passivation {\n\n    # Automatic passivation strategy to use.\n    # Set to \"none\" or \"off\" to disable automatic passivation.\n    # Set to \"default-strategy\" to switch to the recommended default strategy with an active entity limit.\n    # See the strategy-defaults section for possible passivation strategy settings and default values.\n    # Passivation strategies are always disabled if `remember-entities` is enabled.\n    #\n    # API MAY CHANGE: Configuration for passivation strategies, except default-idle-strategy,\n    # may change after additional testing and feedback.\n    strategy = \"default-idle-strategy\"\n\n    # Default passivation strategy without active entity limit; time out idle entities after 2 minutes.\n    default-idle-strategy {\n      idle-entity.timeout = 120s\n    }\n\n    # Recommended default strategy for automatic passivation with an active entity limit.\n    # Configured with an adaptive recency-based admission window, a frequency-based admission filter, and\n    # a segmented least recently used (SLRU) replacement policy for the main active entity tracking.\n    default-strategy {\n      # Default limit of 100k active entities in a shard region (in a cluster node).\n      active-entity-limit = 100000\n\n      # Admisson window with LRU policy and adaptive sizing, and a frequency sketch admission filter to the main area.\n      admission {\n        window {\n          policy = least-recently-used\n          optimizer = hill-climbing\n        }\n        filter = frequency-sketch\n      }\n\n      # Main area with segmented LRU replacement policy with an 80% \"protected\" level by default.\n      replacement {\n        policy = least-recently-used\n        least-recently-used {\n          segmented {\n            levels = 2\n            proportions = [0.2, 0.8]\n          }\n        }\n      }\n    }\n\n    strategy-defaults {\n      # Passivate entities when they have not received a message for a specified length of time.\n      idle-entity {\n        # Passivate idle entities after the timeout. Set to \"none\" or \"off\" to disable.\n        timeout = none\n\n        # Check idle entities every interval. Set to \"default\" to use half the timeout by default.\n        interval = default\n      }\n\n      # Limit of active entities in a shard region.\n      # Passivate entities when the number of active entities in a shard region reaches this limit.\n      # The per-region limit is divided evenly among the active shards in a region.\n      # Set to \"none\" or \"off\" to disable limit-based automatic passivation, to only use idle entity timeouts.\n      active-entity-limit = none\n\n      # Entity replacement settings, for when the active entity limit is reached.\n      replacement {\n        # Entity replacement policy to use when the active entity limit is reached. Possible values are:\n        #   - \"least-recently-used\"\n        #   - \"most-recently-used\"\n        #   - \"least-frequently-used\"\n        # Set to \"none\" or \"off\" to disable the replacement policy and ignore the active entity limit.\n        policy = none\n\n        # Least recently used entity replacement policy.\n        least-recently-used {\n          # Optionally use a \"segmented\" least recently used strategy.\n          # Disabled when segmented.levels are set to \"none\" or \"off\".\n          segmented {\n            # Number of segmented levels.\n            levels = none\n\n            # Fractional proportions for the segmented levels.\n            # If empty then segments are divided evenly by the number of levels.\n            proportions = []\n          }\n        }\n\n        # Most recently used entity replacement policy.\n        most-recently-used {}\n\n        # Least frequently used entity replacement policy.\n        least-frequently-used {\n          # New frequency counts will be \"dynamically aged\" when enabled.\n          dynamic-aging = off\n        }\n      }\n\n      # An optional admission area, with a window for newly and recently activated entities, and an admission filter\n      # to determine whether a candidate should be admitted to the main area of the passivation strategy.\n      admission {\n        # An optional window area, where newly created entities will be admitted initially, and when evicted\n        # from the window area have an opportunity to move to the main area based on the admission filter.\n        window {\n          # The initial sizing for the window area (if enabled), as a fraction of the total active entity limit.\n          proportion = 0.01\n\n          # The minimum adaptive sizing for the window area, as a fraction of the total active entity limit.\n          # Only applies when an adaptive window optimizer is enabled.\n          minimum-proportion = 0.01\n\n          # The maximum adaptive sizing for the window area, as a fraction of the total active entity limit.\n          # Only applies when an adaptive window optimizer is enabled.\n          maximum-proportion = 1.0\n\n          # Adaptive optimizer to use for dynamically resizing the window area. Possible values are:\n          #   - \"hill-climbing\"\n          # Set to \"none\" or \"off\" to disable adaptive sizing of the window area.\n          optimizer = off\n\n          # A window proportion optimizer using a simple hill-climbing algorithm.\n          hill-climbing {\n            # Multiplier of the active entity limit for how often (in accesses) to adjust the window proportion.\n            adjust-multiplier = 10.0\n\n            # The size of the initial step to take (also used when the climbing restarts).\n            initial-step = 0.0625\n\n            # A threshold for the change in active rate (hit rate) to restart climbing.\n            restart-threshold = 0.05\n\n            # The decay ratio applied on each climbing step.\n            step-decay = 0.98\n          }\n\n          # Replacement policy to use for the window area.\n          # Entities that are evicted from the window area may move to the main area, based on the admission filter.\n          # Possible values are the same as for the main replacement policy.\n          # Set to \"none\" or \"off\" to disable the window area.\n          policy = none\n\n          least-recently-used {\n            segmented {\n              levels = none\n              proportions = []\n            }\n          }\n\n          most-recently-used {}\n\n          least-frequently-used {\n            dynamic-aging = off\n          }\n        }\n\n        # The admission filter for the main area of the passivation strategy. Possible values are:\n        #   - \"frequency-sketch\"\n        # Set to \"none\" or \"off\" to disable the admission filter and always admit to the main area.\n        filter = none\n\n        # An admission filter based on a frequency sketch (a variation of a count-min sketch).\n        frequency-sketch {\n          # The depth of the frequency sketch (the number of hash functions).\n          depth = 4\n\n          # The size of the frequency counters in bits: 2, 4, 8, 16, 32, or 64 bits.\n          counter-bits = 4\n\n          # Multiplier of the active entity limit for the width of the frequency sketch.\n          width-multiplier = 4\n\n          # Multiplier of the active entity limit for how often the reset operation of the frequency sketch is applied.\n          reset-multiplier = 10.0\n        }\n      }\n    }\n  }\n\n  # If the coordinator can't store state changes it will be stopped\n  # and started again after this duration, with an exponential back-off\n  # of up to 5 times this duration.\n  coordinator-failure-backoff = 5 s\n\n  # The ShardRegion retries registration and shard location requests to the\n  # ShardCoordinator with this interval if it does not reply.\n  retry-interval = 2 s\n\n  # Maximum number of messages that are buffered by a ShardRegion actor.\n  buffer-size = 100000\n\n  # Timeout of the shard rebalancing process.\n  # Additionally, if an entity doesn't handle the stopMessage\n  # after (handoff-timeout - 5.seconds).max(1.second) it will be stopped forcefully\n  handoff-timeout = 60 s\n\n  # Time given to a region to acknowledge it's hosting a shard.\n  shard-start-timeout = 10 s\n\n  # If the shard is remembering entities and can't store state changes, it\n  # will be stopped and then started again after this duration. Any messages\n  # sent to an affected entity may be lost in this process.\n  shard-failure-backoff = 10 s\n\n  # If the shard is remembering entities and an entity stops itself without\n  # using passivate, the entity will be restarted after this duration or when\n  # the next message for it is received, whichever occurs first.\n  entity-restart-backoff = 10 s\n\n  # Rebalance check is performed periodically with this interval.\n  rebalance-interval = 10 s\n\n  # Absolute path to the journal plugin configuration entity that is to be\n  # used for the internal persistence of ClusterSharding. If not defined,\n  # the default journal plugin is used. Note that this is not related to\n  # persistence used by the entity actors.\n  # Only used when state-store-mode=persistence\n  journal-plugin-id = \"\"\n\n  # Absolute path to the snapshot plugin configuration entity that is to be\n  # used for the internal persistence of ClusterSharding. If not defined,\n  # the default snapshot plugin is used. Note that this is not related to\n  # persistence used by the entity actors.\n  # Only used when state-store-mode=persistence\n  snapshot-plugin-id = \"\"\n\n  # Defines how the coordinator stores its state. Same is also used by the\n  # shards for rememberEntities.\n  # Valid values are \"ddata\" or \"persistence\".\n  # \"persistence\" mode is deprecated\n  state-store-mode = \"ddata\"\n\n  # The shard saves persistent snapshots after this number of persistent\n  # events. Snapshots are used to reduce recovery times. A snapshot trigger might be delayed if a batch of updates is processed.\n  # Only used when state-store-mode=persistence\n  snapshot-after = 1000\n\n  # The shard deletes persistent events (messages and snapshots) after doing snapshot\n  # keeping this number of old persistent batches.\n  # Batch is of size `snapshot-after`.\n  # When set to 0, after snapshot is successfully done, all events with equal or lower sequence number will be deleted.\n  # Default value of 2 leaves last maximum 2*`snapshot-after` events and 3 snapshots (2 old ones + latest snapshot).\n  # If larger than 0, one additional batch of journal messages is kept when state-store-mode=persistence to include messages from delayed snapshots.\n  keep-nr-of-batches = 2\n\n  # Settings for LeastShardAllocationStrategy.\n  #\n  # A new rebalance algorithm was included in Pekko 2.6.10. It can reach optimal balance in\n  # less rebalance rounds (typically 1 or 2 rounds). The amount of shards to rebalance in each\n  # round can still be limited to make it progress slower. For backwards compatibility,\n  # the new algorithm is not enabled by default. Enable the new algorithm by setting\n  # `rebalance-absolute-limit` > 0, for example:\n  # pekko.cluster.sharding.least-shard-allocation-strategy.rebalance-absolute-limit=20\n  # The new algorithm is recommended and will become the default in future versions of Pekko.\n  least-shard-allocation-strategy {\n    # Maximum number of shards that will be rebalanced in one rebalance round.\n    # The lower of this and `rebalance-relative-limit` will be used.\n    rebalance-absolute-limit = 0\n\n    # Maximum number of shards that will be rebalanced in one rebalance round.\n    # Fraction of total number of (known) shards.\n    # The lower of this and `rebalance-absolute-limit` will be used.\n    rebalance-relative-limit = 0.1\n\n    # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used\n    # when rebalance-absolute-limit > 0.\n    #\n    # Threshold of how large the difference between most and least number of\n    # allocated shards must be to begin the rebalancing.\n    # The difference between number of shards in the region with most shards and\n    # the region with least shards must be greater than (>) the `rebalanceThreshold`\n    # for the rebalance to occur.\n    # It is also the maximum number of shards that will start rebalancing per rebalance-interval\n    # 1 gives the best distribution and therefore typically the best choice.\n    # Increasing the threshold can result in quicker rebalance but has the\n    # drawback of increased difference between number of shards (and therefore load)\n    # on different nodes before rebalance will occur.\n    rebalance-threshold = 1\n\n    # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used\n    # when rebalance-absolute-limit > 0.\n    #\n    # The number of ongoing rebalancing processes is limited to this number.\n    max-simultaneous-rebalance = 3\n  }\n\n  external-shard-allocation-strategy {\n    # How long to wait for the client to persist an allocation to ddata or get all shard locations\n    client-timeout = 5s\n  }\n\n  # Timeout of waiting the initial distributed state for the shard coordinator (an initial state will be queried again if the timeout happened)\n  # and for a shard to get its state when remembered entities is enabled\n  # The read from ddata is a ReadMajority, for small clusters (< majority-min-cap) every node needs to respond\n  # so is more likely to time out if there are nodes restarting e.g. when there is a rolling re-deploy happening\n  waiting-for-state-timeout = 2 s\n\n  # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)\n  # Also used as timeout for writes of remember entities when that is enabled\n  updating-state-timeout = 5 s\n\n  # Timeout to wait for querying all shards for a given `ShardRegion`.\n  shard-region-query-timeout = 3 s\n\n  # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used\n  # by the persistent shard when rebalancing or restarting and is applied per remembered shard starting up (not for\n  # entire shard region). The value can either be \"all\" or \"constant\". The \"all\"\n  # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying\n  # entity actors at a fix rate. The default strategy \"all\".\n  entity-recovery-strategy = \"all\"\n\n  # Default settings for the constant rate entity recovery strategy\n  entity-recovery-constant-rate-strategy {\n    # Sets the frequency at which a batch of entity actors is started.\n    frequency = 100 ms\n    # Sets the number of entity actors to be restart at a particular interval\n    number-of-entities = 5\n  }\n\n  event-sourced-remember-entities-store {\n    # When using remember entities and the event sourced remember entities store the batches\n    # written to the store are limited by this number to avoid getting a too large event for\n    # the journal to handle. If using long persistence ids you may have to increase this.\n    max-updates-per-write = 100\n  }\n\n  # Settings for the coordinator singleton. Same layout as pekko.cluster.singleton.\n  # The \"role\" of the singleton configuration is not used. The singleton role will\n  # be the same as \"pekko.cluster.sharding.role\" if\n  # \"pekko.cluster.sharding.coordinator-singleton-role-override\" is enabled. Disabling it will allow to\n  # use separate nodes for the shard coordinator and the shards themselves.\n  # A lease can be configured in these settings for the coordinator singleton\n  coordinator-singleton = ${pekko.cluster.singleton}\n\n\n  # Copies the role for the coordinator singleton from the shards role instead of using the one provided in the\n  # \"pekko.cluster.sharding.coordinator-singleton.role\"\n  coordinator-singleton-role-override = on\n\n  coordinator-state {\n    # State updates are required to be written to a majority of nodes plus this\n    # number of additional nodes. Can also be set to \"all\" to require\n    # writes to all nodes. The reason for write/read to more than majority\n    # is to have more tolerance for membership changes between write and read.\n    # The tradeoff of increasing this is that updates will be slower.\n    # It is more important to increase the `read-majority-plus`.\n    write-majority-plus = 3\n    # State retrieval when ShardCoordinator is started is required to be read\n    # from a majority of nodes plus this number of additional nodes. Can also\n    # be set to \"all\" to require reads from all nodes. The reason for write/read\n    # to more than majority is to have more tolerance for membership changes between\n    # write and read.\n    # The tradeoff of increasing this is that coordinator startup will be slower.\n    read-majority-plus = 5\n  }\n  \n  # Settings for the Distributed Data replicator. \n  # Same layout as pekko.cluster.distributed-data.\n  # The \"role\" of the distributed-data configuration is not used. The distributed-data\n  # role will be the same as \"pekko.cluster.sharding.role\".\n  # Note that there is one Replicator per role and it's not possible\n  # to have different distributed-data settings for different sharding entity types.\n  # Only used when state-store-mode=ddata\n  distributed-data = ${pekko.cluster.distributed-data}\n  distributed-data {\n    # minCap parameter to MajorityWrite and MajorityRead consistency level.\n    majority-min-cap = 5\n    durable.keys = [\"shard-*\"]\n    \n    # When using many entities with \"remember entities\" the Gossip message\n    # can become too large if including too many in same message. Limit to\n    # the same number as the number of ORSet per shard.\n    max-delta-elements = 5\n\n    # ShardCoordinator is singleton running on oldest\n    prefer-oldest = on\n  }\n\n  # The id of the dispatcher to use for ClusterSharding actors.\n  # If specified, you need to define the settings of the actual dispatcher.\n  # This dispatcher for the entity actors is defined by the user provided\n  # Props, i.e. this dispatcher is not used for the entity actors.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n  # Config path of the lease that each shard must acquire before starting entity actors\n  # default is no lease\n  # A lease can also be used for the singleton coordinator by settings it in the coordinator-singleton properties\n  use-lease = \"\"\n\n  # The interval between retries for acquiring the lease\n  lease-retry-interval = 5s\n\n  # Provide a higher level of details in the debug logs, often per routed message. Be careful about enabling\n  # in production systems.\n  verbose-debug-logging = off\n\n  # Throw an exception if the internal state machine in the Shard actor does an invalid state transition.\n  # Mostly for the Pekko test suite. If off, the invalid transition is logged as a warning instead of throwing and\n  # crashing the shard.\n  fail-on-invalid-entity-state-transition = off\n\n  # Healthcheck that can be used with Pekko management health checks: https://pekko.apache.org/docs/pekko-management/current/healthchecks.html\n  healthcheck {\n    # sharding names to check have registered with the coordinator for the health check to pass\n    # once initial registration has taken place the health check always returns true to prevent the coordinator\n    # moving making the health check of all nodes fail\n    # by default no sharding instances are monitored\n    names = []\n\n    # Timeout for the local shard region to respond. This should be lower than your monitoring system's\n    # timeout for health checks\n    timeout = 5s\n  }\n}\n# //#sharding-ext-config\n\n# Enable health check by default for when Pekko management is on the classpath\npekko.management.health-checks.readiness-checks {\n  sharding = \"org.apache.pekko.cluster.sharding.ClusterShardingHealthCheck\"\n}\n\npekko.cluster {\n  configuration-compatibility-check {\n    checkers {\n      pekko-cluster-sharding = \"org.apache.pekko.cluster.sharding.JoinConfigCompatCheckSharding\"\n    }\n  }\n}\n\n# Protobuf serializer for Cluster Sharding messages\npekko.actor {\n  serializers {\n    pekko-sharding = \"org.apache.pekko.cluster.sharding.protobuf.ClusterShardingMessageSerializer\"\n  }\n  serialization-bindings {\n    \"org.apache.pekko.cluster.sharding.ClusterShardingSerializable\" = pekko-sharding\n  }\n  serialization-identifiers {\n    \"org.apache.pekko.cluster.sharding.protobuf.ClusterShardingMessageSerializer\" = 13\n  }\n}","title":"pekko-cluster-sharding"},{"location":"/general/configuration-reference.html#pekko-distributed-data","text":"copysource##############################################\n# Pekko Distributed DataReference Config File #\n##############################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits/overrides in your application.conf.\n\n\n#//#distributed-data\n# Settings for the DistributedData extension\npekko.cluster.distributed-data {\n  # Actor name of the Replicator actor, /system/ddataReplicator\n  name = ddataReplicator\n\n  # Replicas are running on members tagged with this role.\n  # All members are used if undefined or empty.\n  role = \"\"\n\n  # How often the Replicator should send out gossip information\n  gossip-interval = 2 s\n  \n  # How often the subscribers will be notified of changes, if any\n  notify-subscribers-interval = 500 ms\n\n  # Logging of data with payload size in bytes larger than\n  # this value. Maximum detected size per key is logged once,\n  # with an increase threshold of 10%.\n  # It can be disabled by setting the property to off.\n  log-data-size-exceeding = 10 KiB\n\n  # Maximum number of entries to transfer in one round of gossip exchange when\n  # synchronizing the replicas. Next chunk will be transferred in next round of gossip.\n  # The actual number of data entries in each Gossip message is dynamically\n  # adjusted to not exceed the maximum remote message size (maximum-frame-size).\n  max-delta-elements = 500\n  \n  # The id of the dispatcher to use for Replicator actors.\n  # If specified you need to define the settings of the actual dispatcher.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n  # How often the Replicator checks for pruning of data associated with\n  # removed cluster nodes. If this is set to 'off' the pruning feature will\n  # be completely disabled.\n  pruning-interval = 120 s\n  \n  # How long time it takes to spread the data to all other replica nodes.\n  # This is used when initiating and completing the pruning process of data associated\n  # with removed cluster nodes. The time measurement is stopped when any replica is \n  # unreachable, but it's still recommended to configure this with certain margin.\n  # It should be in the magnitude of minutes even though typical dissemination time\n  # is shorter (grows logarithmic with number of nodes). There is no advantage of \n  # setting this too low. Setting it to large value will delay the pruning process.\n  max-pruning-dissemination = 300 s\n  \n  # The markers of that pruning has been performed for a removed node are kept for this\n  # time and thereafter removed. If and old data entry that was never pruned is somehow\n  # injected and merged with existing data after this time the value will not be correct.\n  # This would be possible (although unlikely) in the case of a long network partition.\n  # It should be in the magnitude of hours. For durable data it is configured by \n  # 'pekko.cluster.distributed-data.durable.pruning-marker-time-to-live'.\n pruning-marker-time-to-live = 6 h\n  \n  # Serialized Write and Read messages are cached when they are sent to \n  # several nodes. If no further activity they are removed from the cache\n  # after this duration.\n  serializer-cache-time-to-live = 10s\n\n  # Update and Get operations are sent to oldest nodes first.\n  # This is useful together with Cluster Singleton, which is running on oldest nodes.\n  prefer-oldest = off\n  \n  # Settings for delta-CRDT\n  delta-crdt {\n    # enable or disable delta-CRDT replication\n    enabled = on\n    \n    # Some complex deltas grow in size for each update and above this\n    # threshold such deltas are discarded and sent as full state instead.\n    # This is number of elements or similar size hint, not size in bytes.\n    max-delta-size = 50\n  }\n  \n  durable {\n    # List of keys that are durable. Prefix matching is supported by using * at the\n    # end of a key.  \n    keys = []\n    \n    # The markers of that pruning has been performed for a removed node are kept for this\n    # time and thereafter removed. If and old data entry that was never pruned is\n    # injected and merged with existing data after this time the value will not be correct.\n    # This would be possible if replica with durable data didn't participate in the pruning\n    # (e.g. it was shutdown) and later started after this time. A durable replica should not \n    # be stopped for longer time than this duration and if it is joining again after this\n    # duration its data should first be manually removed (from the lmdb directory).\n    # It should be in the magnitude of days. Note that there is a corresponding setting\n    # for non-durable data: 'pekko.cluster.distributed-data.pruning-marker-time-to-live'.\n    pruning-marker-time-to-live = 10 d\n    \n    # Fully qualified class name of the durable store actor. It must be a subclass\n    # of pekko.actor.Actor and handle the protocol defined in \n    # org.apache.pekko.cluster.ddata.DurableStore. The class must have a constructor with\n    # com.typesafe.config.Config parameter.\n    store-actor-class = org.apache.pekko.cluster.ddata.LmdbDurableStore\n    \n    use-dispatcher = pekko.cluster.distributed-data.durable.pinned-store\n    \n    pinned-store {\n      executor = thread-pool-executor\n      type = PinnedDispatcher\n    }\n    \n    # Config for the LmdbDurableStore\n    lmdb {\n      # Directory of LMDB file. There are two options:\n      # 1. A relative or absolute path to a directory that ends with 'ddata'\n      #    the full name of the directory will contain name of the ActorSystem\n      #    and its remote port.\n      # 2. Otherwise the path is used as is, as a relative or absolute path to\n      #    a directory.\n      #\n      # When running in production you may want to configure this to a specific\n      # path (alt 2), since the default directory contains the remote port of the\n      # actor system to make the name unique. If using a dynamically assigned \n      # port (0) it will be different each time and the previously stored data \n      # will not be loaded.\n      dir = \"ddata\"\n      \n      # Size in bytes of the memory mapped file.\n      map-size = 100 MiB\n      \n      # Accumulate changes before storing improves performance with the\n      # risk of losing the last writes if the JVM crashes.\n      # The interval is by default set to 'off' to write each update immediately.\n      # Enabling write behind by specifying a duration, e.g. 200ms, is especially \n      # efficient when performing many writes to the same key, because it is only \n      # the last value for each key that will be serialized and stored.  \n      # write-behind-interval = 200 ms\n      write-behind-interval = off\n    }\n  }\n  \n}\n#//#distributed-data\n\n# Protobuf serializer for cluster DistributedData messages\npekko.actor {\n  serializers {\n    pekko-data-replication = \"org.apache.pekko.cluster.ddata.protobuf.ReplicatorMessageSerializer\"\n    pekko-replicated-data = \"org.apache.pekko.cluster.ddata.protobuf.ReplicatedDataSerializer\"\n  }\n  serialization-bindings {\n    \"org.apache.pekko.cluster.ddata.Replicator$ReplicatorMessage\" = pekko-data-replication\n    \"org.apache.pekko.cluster.ddata.ReplicatedDataSerialization\" = pekko-replicated-data\n  }\n  serialization-identifiers {\n    \"org.apache.pekko.cluster.ddata.protobuf.ReplicatedDataSerializer\" = 11\n    \"org.apache.pekko.cluster.ddata.protobuf.ReplicatorMessageSerializer\" = 12\n  }\n}","title":"pekko-distributed-data"},{"location":"/general/configuration-reference.html#pekko-stream","text":"copysource#####################################\n# Pekko Stream Reference Config File #\n#####################################\n\n# eager creation of the system wide materializer\npekko.library-extensions += \"org.apache.pekko.stream.SystemMaterializer$\"\npekko {\n  stream {\n\n    # Default materializer settings\n    materializer {\n\n      # Initial size of buffers used in stream elements\n      initial-input-buffer-size = 4\n      # Maximum size of buffers used in stream elements\n      max-input-buffer-size = 16\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # or full dispatcher configuration to be used by ActorMaterializer when creating Actors.\n      dispatcher = \"pekko.actor.default-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # or full dispatcher configuration to be used by stream operators that\n      # perform blocking operations\n      blocking-io-dispatcher = \"pekko.actor.default-blocking-io-dispatcher\"\n\n      # Cleanup leaked publishers and subscribers when they are not used within a given\n      # deadline\n      subscription-timeout {\n        # when the subscription timeout is reached one of the following strategies on\n        # the \"stale\" publisher:\n        # cancel - cancel it (via `onError` or subscribing to the publisher and\n        #          `cancel()`ing the subscription right away\n        # warn   - log a warning statement about the stale element (then drop the\n        #          reference to it)\n        # noop   - do nothing (not recommended)\n        mode = cancel\n\n        # time after which a subscriber / publisher is considered stale and eligible\n        # for cancelation (see `pekko.stream.subscription-timeout.mode`)\n        timeout = 5s\n      }\n\n      # Enable additional troubleshooting logging at DEBUG log level\n      debug-logging = off\n\n      # Maximum number of elements emitted in batch if downstream signals large demand\n      output-burst-limit = 1000\n\n      # Enable automatic fusing of all graphs that are run. For short-lived streams\n      # this may cause an initial runtime overhead, but most of the time fusing is\n      # desirable since it reduces the number of Actors that are created.\n      # Deprecated, since Pekko 2.5.0, setting does not have any effect.\n      auto-fusing = on\n\n      # Those stream elements which have explicit buffers (like mapAsync, mapAsyncUnordered,\n      # buffer, flatMapMerge, Source.actorRef, Source.queue, etc.) will preallocate a fixed\n      # buffer upon stream materialization if the requested buffer size is less than this\n      # configuration parameter. The default is very high because failing early is better\n      # than failing under load.\n      #\n      # Buffers sized larger than this will dynamically grow/shrink and consume more memory\n      # per element than the fixed size buffers.\n      max-fixed-buffer-size = 1000000000\n\n      # Maximum number of sync messages that actor can process for stream to substream communication.\n      # Parameter allows to interrupt synchronous processing to get upstream/downstream messages.\n      # Allows to accelerate message processing that happening within same actor but keep system responsive.\n      sync-processing-limit = 1000\n\n      debug {\n        # Enables the fuzzing mode which increases the chance of race conditions\n        # by aggressively reordering events and making certain operations more\n        # concurrent than usual.\n        # This setting is for testing purposes, NEVER enable this in a production\n        # environment!\n        # To get the best results, try combining this setting with a throughput\n        # of 1 on the corresponding dispatchers.\n        fuzzing-mode = off\n      }\n\n      io.tcp {\n        # The outgoing bytes are accumulated in a buffer while waiting for acknowledgment\n        # of pending write. This improves throughput for small messages (frames) without\n        # sacrificing latency. While waiting for the ack the stage will eagerly pull\n        # from upstream until the buffer exceeds this size. That means that the buffer may hold\n        # slightly more bytes than this limit (at most one element more). It can be set to 0\n        # to disable the usage of the buffer.\n        write-buffer-size = 16 KiB\n\n        # In addition to the buffering described for property write-buffer-size, try to collect\n        # more consecutive writes from the upstream stream producers.\n        #\n        # The rationale is to increase write efficiency by avoiding separate small \n        # writes to the network which is expensive to do. Merging those writes together\n        # (up to `write-buffer-size`) improves throughput for small writes.\n        #\n        # The idea is that a running stream may produce multiple small writes consecutively\n        # in one go without waiting for any external input. To probe the stream for\n        # data, this features delays sending a write immediately by probing the stream\n        # for more writes. This works by rescheduling the TCP connection stage via the\n        # actor mailbox of the underlying actor. Thus, before the stage is reactivated\n        # the upstream gets another opportunity to emit writes.\n        #\n        # When the stage is reactivated and if new writes are detected another round-trip\n        # is scheduled. The loop repeats until either the number of round trips given in this\n        # setting is reached, the buffer reaches `write-buffer-size`, or no new writes\n        # were detected during the last round-trip.\n        #\n        # This mechanism ensures that a write is guaranteed to be sent when the remaining stream\n        # becomes idle waiting for external signals.\n        #\n        # In most cases, the extra latency this mechanism introduces should be negligible,\n        # but depending on the stream setup it may introduce a noticeable delay,\n        # if the upstream continuously produces small amounts of writes in a\n        # blocking (CPU-bound) way.\n        #\n        # In that case, the feature can either be disabled, or the producing CPU-bound\n        # work can be taken off-stream to avoid excessive delays (e.g. using `mapAsync` instead of `map`).\n        #\n        # A value of 0 disables this feature.\n        coalesce-writes = 10\n      }\n\n      # Time to wait for async materializer creation before throwing an exception\n      creation-timeout = 20 seconds\n\n      //#stream-ref\n      # configure defaults for SourceRef and SinkRef\n      stream-ref {\n        # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref\n        #\n        # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,\n        # because the delay of requesting over network boundaries is much higher.\n        buffer-capacity = 32\n\n        # Demand is signalled by sending a cumulative demand message (\"requesting messages until the n-th sequence number)\n        # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should\n        # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).\n        #\n        # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.\n        #\n        # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive\n        # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.\n        demand-redelivery-interval = 1 second\n\n        # Subscription timeout, during which the \"remote side\" MUST subscribe (materialize) the handed out stream ref.\n        # This timeout does not have to be very low in normal situations, since the remote side may also need to\n        # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking\n        # in-active streams which are never subscribed to.\n        subscription-timeout = 30 seconds\n\n        # In order to guard the receiving end of a stream ref from never terminating (since awaiting a Completion or Failed\n        # message) after / before a Terminated is seen, a special timeout is applied once Terminated is received by it.\n        # This allows us to terminate stream refs that have been targeted to other nodes which are Downed, and as such the\n        # other side of the stream ref would never send the \"final\" terminal message.\n        #\n        # The timeout specifically means the time between the Terminated signal being received and when the local SourceRef\n        # determines to fail itself, assuming there was message loss or a complete partition of the completion signal.\n        final-termination-signal-deadline = 2 seconds\n      }\n      //#stream-ref\n    }\n\n    # Deprecated, left here to not break Pekko HTTP which refers to it\n    blocking-io-dispatcher = \"pekko.actor.default-blocking-io-dispatcher\"\n\n    # Deprecated, will not be used unless user code refer to it, use 'pekko.stream.materializer.blocking-io-dispatcher'\n    # instead, or if from code, prefer the 'ActorAttributes.IODispatcher' attribute\n    default-blocking-io-dispatcher = \"pekko.actor.default-blocking-io-dispatcher\"\n  }\n\n  # configure overrides to ssl-configuration here (to be used by pekko-streams, and pekko-http – i.e. when serving https connections)\n  ssl-config {\n    protocol = \"TLSv1.2\"\n  }\n\n  actor {\n\n    serializers {\n      pekko-stream-ref = \"org.apache.pekko.stream.serialization.StreamRefSerializer\"\n    }\n\n    serialization-bindings {\n      \"org.apache.pekko.stream.SinkRef\"                           = pekko-stream-ref\n      \"org.apache.pekko.stream.SourceRef\"                         = pekko-stream-ref\n      \"org.apache.pekko.stream.impl.streamref.StreamRefsProtocol\" = pekko-stream-ref\n    }\n\n    serialization-identifiers {\n      \"org.apache.pekko.stream.serialization.StreamRefSerializer\" = 30\n    }\n  }\n}\n\n# ssl configuration\n# folded in from former ssl-config-pekko module\nssl-config {\n  logger = \"com.typesafe.sslconfig.pekko.util.PekkoLoggerBridge\"\n}","title":"pekko-stream"},{"location":"/general/configuration-reference.html#pekko-stream-testkit","text":"copysourcepekko.stream.testkit {\n  all-stages-stopped-timeout = 5 s\n}","title":"pekko-stream-testkit"},{"location":"/typed/index.html","text":"","title":"Actors"},{"location":"/typed/index.html#actors","text":"Introduction to Actors Module info Pekko Actors First example A More Complex Example Actor lifecycle Dependency Introduction Creating Actors Stopping Actors Watching Actors Interaction Patterns Dependency Introduction Fire and Forget Request-Response Adapted Response Request-Response with ask between two actors Request-Response with ask from outside an Actor Generic response wrapper Ignoring replies Send Future result to self Per session child Actor General purpose response aggregator Latency tail chopping Scheduling messages to self Responding to a sharded actor Fault Tolerance Supervision Child actors are stopped when parent is restarting The PreRestart signal Bubble failures up through the hierarchy Actor discovery Dependency Obtaining Actor references Receptionist Cluster Receptionist Receptionist Scalability Routers Dependency Introduction Pool Router Group Router Routing strategies Routers and performance Stash Dependency Introduction Behaviors as finite state machines Example project Coordinated Shutdown Dispatchers Dependency Introduction Default dispatcher Internal dispatcher Looking up a Dispatcher Selecting a dispatcher Types of dispatchers Dispatcher aliases Blocking Needs Careful Management More dispatcher configuration examples Mailboxes Dependency Introduction Selecting what mailbox is used Mailbox Implementations Custom Mailbox type Testing Module info Introduction Asynchronous testing Synchronous behavior testing Coexistence Dependency Introduction Classic to typed Typed to classic Supervision Style guide Functional versus object-oriented style Passing around too many parameters Behavior factory method Where to define messages Public versus private messages Lambdas versus method references Partial versus total Function How to compose Partial Functions ask versus ? ReceiveBuilder Nesting setup Additional naming conventions Learning Pekko Typed from Classic Dependencies Package names Actor definition actorOf and Props ActorRef ActorSystem become sender parent Supervision Lifecycle hooks watch Stopping ActorSelection ask pipeTo ActorContext.children Remote deployment Routers FSM Timers Stash PersistentActor Asynchronous Testing Synchronous Testing","title":"Actors"},{"location":"/typed/actors.html","text":"","title":"Introduction to Actors"},{"location":"/typed/actors.html#introduction-to-actors","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Actors.","title":"Introduction to Actors"},{"location":"/typed/actors.html#module-info","text":"To use Pekko Actors, add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-actor-testkit-typed\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-testkit-typed_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-actor-testkit-typed_${versions.ScalaBinary}\"\n}\nBoth the Java and Scala DSLs of Pekko modules are bundled in the same JAR. For a smooth development experience, when using an IDE such as Eclipse or IntelliJ, you can disable the auto-importer from suggesting javadsl imports when working in Scala, or viceversa. See IDE Tips.\nProject Info: Pekko Actors (typed) Artifact org.apache.pekko pekko-actor-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.actor.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/actors.html#pekko-actors","text":"The Actor Model provides a higher level of abstraction for writing concurrent and distributed systems. It alleviates the developer from having to deal with explicit locking and thread management, making it easier to write correct concurrent and parallel systems. Actors were defined in the 1973 paper by Carl Hewitt but have been popularized by the Erlang language, and used for example at Ericsson with great success to build highly concurrent and reliable telecom systems. The API of Pekko’s Actors has borrowed some of its syntax from Erlang.","title":"Pekko Actors"},{"location":"/typed/actors.html#first-example","text":"If you are new to Pekko you might want to start with reading the Getting Started Guide and then come back here to learn more.\nIt is helpful to become familiar with the foundational, external and internal ecosystem of your Actors, to see what you can leverage and customize as needed, see Actor Systems and Actor References, Paths and Addresses.\nAs discussed in Actor Systems Actors are about sending messages between independent units of computation, but what does that look like?\nIn all of the following these imports are assumed:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.scaladsl.LoggerOps\nimport pekko.actor.typed.{ ActorRef, ActorSystem, Behavior } Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\nWith these in place we can define our first Actor, and it will say hello!\nScala copysourceobject HelloWorld {\n  final case class Greet(whom: String, replyTo: ActorRef[Greeted])\n  final case class Greeted(whom: String, from: ActorRef[Greet])\n\n  def apply(): Behavior[Greet] = Behaviors.receive { (context, message) =>\n    context.log.info(\"Hello {}!\", message.whom)\n    message.replyTo ! Greeted(message.whom, context.self)\n    Behaviors.same\n  }\n} Java copysourcepublic class HelloWorld extends AbstractBehavior<HelloWorld.Greet> {\n\n  public static final class Greet {\n    public final String whom;\n    public final ActorRef<Greeted> replyTo;\n\n    public Greet(String whom, ActorRef<Greeted> replyTo) {\n      this.whom = whom;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class Greeted {\n    public final String whom;\n    public final ActorRef<Greet> from;\n\n    public Greeted(String whom, ActorRef<Greet> from) {\n      this.whom = whom;\n      this.from = from;\n    }\n  }\n\n  public static Behavior<Greet> create() {\n    return Behaviors.setup(HelloWorld::new);\n  }\n\n  private HelloWorld(ActorContext<Greet> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Greet> createReceive() {\n    return newReceiveBuilder().onMessage(Greet.class, this::onGreet).build();\n  }\n\n  private Behavior<Greet> onGreet(Greet command) {\n    getContext().getLog().info(\"Hello {}!\", command.whom);\n    command.replyTo.tell(new Greeted(command.whom, getContext().getSelf()));\n    return this;\n  }\n}\nThis small piece of code defines two message types, one for commanding the Actor to greet someone and one that the Actor will use to confirm that it has done so. The Greet type contains not only the information of whom to greet, it also holds an ActorRefActorRef that the sender of the message supplies so that the HelloWorld Actor can send back the confirmation message.\nThe behavior of the Actor is defined as the Greeter with the help of the receivereceive behavior factory. Processing the next message then results in a new behavior that can potentially be different from this one. State is updated by returning a new behavior that holds the new immutable state. In this case we don’t need to update any state, so we return samesame, which means the next behavior is “the same as the current one”.\nThe type of the messages handled by this behavior is declared to be of class Greet., meaning that message argument is also typed as such. This is why we can access the whom and replyTo members without needing to use a pattern match. Typically, an actor handles more than one specific message type where all of them directly or indirectly extendimplement a common traitinterface.\nOn the last line we see the HelloWorld Actor send a message to another Actor, which is done using the ! operator (pronounced “bang” or “tell”)tell method. It is an asynchronous operation that doesn’t block the caller’s thread.\nSince the replyTo address is declared to be of type ActorRef[Greeted]ActorRef<Greeted>, the compiler will only permit us to send messages of this type, other usage will be a compiler error.\nThe accepted message types of an Actor together with all reply types defines the protocol spoken by this Actor; in this case it is a simple request–reply protocol but Actors can model arbitrarily complex protocols when needed. The protocol is bundled together with the behavior that implements it in a nicely wrapped scope—the HelloWorld objectclass.\nAs Carl Hewitt said, one Actor is no Actor — it would be quite lonely with nobody to talk to. We need another Actor that interacts with the Greeter. Let’s make a HelloWorldBot that receives the reply from the Greeter and sends a number of additional greeting messages and collect the replies until a given max number of messages have been reached.\nScala copysourceobject HelloWorldBot {\n\n  def apply(max: Int): Behavior[HelloWorld.Greeted] = {\n    bot(0, max)\n  }\n\n  private def bot(greetingCounter: Int, max: Int): Behavior[HelloWorld.Greeted] =\n    Behaviors.receive { (context, message) =>\n      val n = greetingCounter + 1\n      context.log.info2(\"Greeting {} for {}\", n, message.whom)\n      if (n == max) {\n        Behaviors.stopped\n      } else {\n        message.from ! HelloWorld.Greet(message.whom, context.self)\n        bot(n, max)\n      }\n    }\n} Java copysourcepublic class HelloWorldBot extends AbstractBehavior<HelloWorld.Greeted> {\n\n  public static Behavior<HelloWorld.Greeted> create(int max) {\n    return Behaviors.setup(context -> new HelloWorldBot(context, max));\n  }\n\n  private final int max;\n  private int greetingCounter;\n\n  private HelloWorldBot(ActorContext<HelloWorld.Greeted> context, int max) {\n    super(context);\n    this.max = max;\n  }\n\n  @Override\n  public Receive<HelloWorld.Greeted> createReceive() {\n    return newReceiveBuilder().onMessage(HelloWorld.Greeted.class, this::onGreeted).build();\n  }\n\n  private Behavior<HelloWorld.Greeted> onGreeted(HelloWorld.Greeted message) {\n    greetingCounter++;\n    getContext().getLog().info(\"Greeting {} for {}\", greetingCounter, message.whom);\n    if (greetingCounter == max) {\n      return Behaviors.stopped();\n    } else {\n      message.from.tell(new HelloWorld.Greet(message.whom, getContext().getSelf()));\n      return this;\n    }\n  }\n}\nNote how this Actor manages the counter by changing the behavior for each Greeted reply rather than using any variables.Note how this Actor manages the counter with an instance variable. No concurrency guards such as synchronized or AtomicInteger are needed since an actor instance processes one message at a time.\nA third actor spawns the Greeter and the HelloWorldBot and starts the interaction between those.\nScala copysourceobject HelloWorldMain {\n\n  final case class SayHello(name: String)\n\n  def apply(): Behavior[SayHello] =\n    Behaviors.setup { context =>\n      val greeter = context.spawn(HelloWorld(), \"greeter\")\n\n      Behaviors.receiveMessage { message =>\n        val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)\n        greeter ! HelloWorld.Greet(message.name, replyTo)\n        Behaviors.same\n      }\n    }\n\n} Java copysourcepublic class HelloWorldMain extends AbstractBehavior<HelloWorldMain.SayHello> {\n\n  public static class SayHello {\n    public final String name;\n\n    public SayHello(String name) {\n      this.name = name;\n    }\n  }\n\n  public static Behavior<SayHello> create() {\n    return Behaviors.setup(HelloWorldMain::new);\n  }\n\n  private final ActorRef<HelloWorld.Greet> greeter;\n\n  private HelloWorldMain(ActorContext<SayHello> context) {\n    super(context);\n    greeter = context.spawn(HelloWorld.create(), \"greeter\");\n  }\n\n  @Override\n  public Receive<SayHello> createReceive() {\n    return newReceiveBuilder().onMessage(SayHello.class, this::onStart).build();\n  }\n\n  private Behavior<SayHello> onStart(SayHello command) {\n    ActorRef<HelloWorld.Greeted> replyTo =\n        getContext().spawn(HelloWorldBot.create(3), command.name);\n    greeter.tell(new HelloWorld.Greet(command.name, replyTo));\n    return this;\n  }\n}\nNow we want to try out this Actor, so we must start an ActorSystem to host it:\nScala copysource val system: ActorSystem[HelloWorldMain.SayHello] =\n  ActorSystem(HelloWorldMain(), \"hello\")\n\nsystem ! HelloWorldMain.SayHello(\"World\")\nsystem ! HelloWorldMain.SayHello(\"Akka\")\n Java copysourcefinal ActorSystem<HelloWorldMain.SayHello> system =\n    ActorSystem.create(HelloWorldMain.create(), \"hello\");\n\nsystem.tell(new HelloWorldMain.SayHello(\"World\"));\nsystem.tell(new HelloWorldMain.SayHello(\"Akka\"));\nWe start an Actor system from the defined HelloWorldMain behavior and send two SayHello messages that will kick-off the interaction between two separate HelloWorldBot actors and the single Greeter actor.\nAn application normally consists of a single ActorSystemActorSystem, running many actors, per JVM.\nThe console output may look like this:\n[INFO] [03/13/2018 15:50:05.814] [hello-pekko.actor.default-dispatcher-4] [pekko://hello/user/greeter] Hello World!\n[INFO] [03/13/2018 15:50:05.815] [hello-pekko.actor.default-dispatcher-4] [pekko://hello/user/greeter] Hello Pekko!\n[INFO] [03/13/2018 15:50:05.815] [hello-pekko.actor.default-dispatcher-2] [pekko://hello/user/World] Greeting 1 for World\n[INFO] [03/13/2018 15:50:05.815] [hello-pekko.actor.default-dispatcher-4] [pekko://hello/user/Pekko] Greeting 1 for Pekko\n[INFO] [03/13/2018 15:50:05.815] [hello-pekko.actor.default-dispatcher-5] [pekko://hello/user/greeter] Hello World!\n[INFO] [03/13/2018 15:50:05.815] [hello-pekko.actor.default-dispatcher-5] [pekko://hello/user/greeter] Hello Pekko!\n[INFO] [03/13/2018 15:50:05.815] [hello-pekko.actor.default-dispatcher-4] [pekko://hello/user/World] Greeting 2 for World\n[INFO] [03/13/2018 15:50:05.815] [hello-pekko.actor.default-dispatcher-5] [pekko://hello/user/greeter] Hello World!\n[INFO] [03/13/2018 15:50:05.815] [hello-pekko.actor.default-dispatcher-4] [pekko://hello/user/Pekko] Greeting 2 for Pekko\n[INFO] [03/13/2018 15:50:05.816] [hello-pekko.actor.default-dispatcher-5] [pekko://hello/user/greeter] Hello Pekko!\n[INFO] [03/13/2018 15:50:05.816] [hello-pekko.actor.default-dispatcher-4] [pekko://hello/user/World] Greeting 3 for World\n[INFO] [03/13/2018 15:50:05.816] [hello-pekko.actor.default-dispatcher-6] [pekko://hello/user/Pekko] Greeting 3 for Pekko\nYou will also need to add a logging dependency to see that output when running.\nHere is another example that you can edit and run in the browser: import org.apache.pekko\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.scaladsl.LoggerOps\nimport pekko.actor.typed.{ ActorRef, ActorSystem, Behavior }\n\nobject HelloWorld {\n  final case class Greet(whom: String, replyTo: ActorRef[Greeted])\n  final case class Greeted(whom: String, from: ActorRef[Greet])\n\n  def apply(): Behavior[Greet] = Behaviors.receive { (context, message) =>\n    println(s\"Hello ${message.whom}!\")\n    message.replyTo ! Greeted(message.whom, context.self)\n    Behaviors.same\n  }\n}\n\nobject HelloWorldBot {\n\n  def apply(max: Int): Behavior[HelloWorld.Greeted] = {\n    bot(0, max)\n  }\n\n  private def bot(greetingCounter: Int, max: Int): Behavior[HelloWorld.Greeted] =\n    Behaviors.receive { (context, message) =>\n      val n = greetingCounter + 1\n      println(s\"Greeting $n for ${message.whom}\")\n      if (n == max) {\n        Behaviors.stopped\n      } else {\n        message.from ! HelloWorld.Greet(message.whom, context.self)\n        bot(n, max)\n      }\n    }\n}\n\nobject HelloWorldMain {\n\n  final case class SayHello(name: String)\n\n  def apply(): Behavior[SayHello] =\n    Behaviors.setup { context =>\n      val greeter = context.spawn(HelloWorld(), \"greeter\")\n\n      Behaviors.receiveMessage { message =>\n        val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)\n        greeter ! HelloWorld.Greet(message.name, replyTo)\n        Behaviors.same\n      }\n    }\n\n  def main(args: Array[String]): Unit = {\n    val system: ActorSystem[HelloWorldMain.SayHello] =\n      ActorSystem(HelloWorldMain(), \"hello\")\n\n    system ! HelloWorldMain.SayHello(\"World\")\n    system ! HelloWorldMain.SayHello(\"Akka\")\n  }\n}\n\n// This is run by ScalaFiddle\nHelloWorldMain.main(Array.empty)","title":"First example"},{"location":"/typed/actors.html#a-more-complex-example","text":"The next example is more realistic and demonstrates some important patterns:\nUsing a sealed trait and case class/objectsan interface and classes implementing that interface to represent multiple messages an actor can receive Handle sessions by using child actors Handling state by changing behavior Using multiple actors to represent different parts of a protocol in a type safe way","title":"A More Complex Example"},{"location":"/typed/actors.html#functional-style","text":"First we will show this example in a functional style, and then the same example is shown with an Object-oriented style. Which style you choose to use is a matter of taste and both styles can be mixed depending on which is best for a specific actor. Considerations for the choice is provided in the Style Guide.\nConsider an Actor that runs a chat room: client Actors may connect by sending a message that contains their screen name and then they can post messages. The chat room Actor will disseminate all posted messages to all currently connected client Actors. The protocol definition could look like the following:\nScala copysourceobject ChatRoom {\n  sealed trait RoomCommand\n  final case class GetSession(screenName: String, replyTo: ActorRef[SessionEvent]) extends RoomCommand\n\n  sealed trait SessionEvent\n  final case class SessionGranted(handle: ActorRef[PostMessage]) extends SessionEvent\n  final case class SessionDenied(reason: String) extends SessionEvent\n  final case class MessagePosted(screenName: String, message: String) extends SessionEvent\n\n  sealed trait SessionCommand\n  final case class PostMessage(message: String) extends SessionCommand\n  private final case class NotifyClient(message: MessagePosted) extends SessionCommand\n} Java copysourcestatic interface RoomCommand {}\n\npublic static final class GetSession implements RoomCommand {\n  public final String screenName;\n  public final ActorRef<SessionEvent> replyTo;\n\n  public GetSession(String screenName, ActorRef<SessionEvent> replyTo) {\n    this.screenName = screenName;\n    this.replyTo = replyTo;\n  }\n}\n\ninterface SessionEvent {}\n\npublic static final class SessionGranted implements SessionEvent {\n  public final ActorRef<PostMessage> handle;\n\n  public SessionGranted(ActorRef<PostMessage> handle) {\n    this.handle = handle;\n  }\n}\n\npublic static final class SessionDenied implements SessionEvent {\n  public final String reason;\n\n  public SessionDenied(String reason) {\n    this.reason = reason;\n  }\n}\n\npublic static final class MessagePosted implements SessionEvent {\n  public final String screenName;\n  public final String message;\n\n  public MessagePosted(String screenName, String message) {\n    this.screenName = screenName;\n    this.message = message;\n  }\n}\n\ninterface SessionCommand {}\n\npublic static final class PostMessage implements SessionCommand {\n  public final String message;\n\n  public PostMessage(String message) {\n    this.message = message;\n  }\n}\n\nprivate static final class NotifyClient implements SessionCommand {\n  final MessagePosted message;\n\n  NotifyClient(MessagePosted message) {\n    this.message = message;\n  }\n}\nInitially the client Actors only get access to an ActorRef[GetSession]ActorRef<GetSession> which allows them to make the first step. Once a client’s session has been established it gets a SessionGranted message that contains a handle to unlock the next protocol step, posting messages. The PostMessage command will need to be sent to this particular address that represents the session that has been added to the chat room. The other aspect of a session is that the client has revealed its own address, via the replyTo argument, so that subsequent MessagePosted events can be sent to it.\nThis illustrates how Actors can express more than just the equivalent of method calls on Java objects. The declared message types and their contents describe a full protocol that can involve multiple Actors and that can evolve over multiple steps. Here’s the implementation of the chat room protocol:\nScala copysourceobject ChatRoom {\n  private final case class PublishSessionMessage(screenName: String, message: String) extends RoomCommand\n\n  def apply(): Behavior[RoomCommand] =\n    chatRoom(List.empty)\n\n  private def chatRoom(sessions: List[ActorRef[SessionCommand]]): Behavior[RoomCommand] =\n    Behaviors.receive { (context, message) =>\n      message match {\n        case GetSession(screenName, client) =>\n          // create a child actor for further interaction with the client\n          val ses = context.spawn(\n            session(context.self, screenName, client),\n            name = URLEncoder.encode(screenName, StandardCharsets.UTF_8.name))\n          client ! SessionGranted(ses)\n          chatRoom(ses :: sessions)\n        case PublishSessionMessage(screenName, message) =>\n          val notification = NotifyClient(MessagePosted(screenName, message))\n          sessions.foreach(_ ! notification)\n          Behaviors.same\n      }\n    }\n\n  private def session(\n      room: ActorRef[PublishSessionMessage],\n      screenName: String,\n      client: ActorRef[SessionEvent]): Behavior[SessionCommand] =\n    Behaviors.receiveMessage {\n      case PostMessage(message) =>\n        // from client, publish to others via the room\n        room ! PublishSessionMessage(screenName, message)\n        Behaviors.same\n      case NotifyClient(message) =>\n        // published from the room\n        client ! message\n        Behaviors.same\n    }\n} Java copysourcepublic class ChatRoom {\n  private static final class PublishSessionMessage implements RoomCommand {\n    public final String screenName;\n    public final String message;\n\n    public PublishSessionMessage(String screenName, String message) {\n      this.screenName = screenName;\n      this.message = message;\n    }\n  }\n\n  public static Behavior<RoomCommand> create() {\n    return Behaviors.setup(\n        ctx -> new ChatRoom(ctx).chatRoom(new ArrayList<ActorRef<SessionCommand>>()));\n  }\n\n  private final ActorContext<RoomCommand> context;\n\n  private ChatRoom(ActorContext<RoomCommand> context) {\n    this.context = context;\n  }\n\n  private Behavior<RoomCommand> chatRoom(List<ActorRef<SessionCommand>> sessions) {\n    return Behaviors.receive(RoomCommand.class)\n        .onMessage(GetSession.class, getSession -> onGetSession(sessions, getSession))\n        .onMessage(PublishSessionMessage.class, pub -> onPublishSessionMessage(sessions, pub))\n        .build();\n  }\n\n  private Behavior<RoomCommand> onGetSession(\n      List<ActorRef<SessionCommand>> sessions, GetSession getSession)\n      throws UnsupportedEncodingException {\n    ActorRef<SessionEvent> client = getSession.replyTo;\n    ActorRef<SessionCommand> ses =\n        context.spawn(\n            Session.create(context.getSelf(), getSession.screenName, client),\n            URLEncoder.encode(getSession.screenName, StandardCharsets.UTF_8.name()));\n    // narrow to only expose PostMessage\n    client.tell(new SessionGranted(ses.narrow()));\n    List<ActorRef<SessionCommand>> newSessions = new ArrayList<>(sessions);\n    newSessions.add(ses);\n    return chatRoom(newSessions);\n  }\n\n  private Behavior<RoomCommand> onPublishSessionMessage(\n      List<ActorRef<SessionCommand>> sessions, PublishSessionMessage pub) {\n    NotifyClient notification =\n        new NotifyClient((new MessagePosted(pub.screenName, pub.message)));\n    sessions.forEach(s -> s.tell(notification));\n    return Behaviors.same();\n  }\n\n  static class Session {\n    static Behavior<ChatRoom.SessionCommand> create(\n        ActorRef<RoomCommand> room, String screenName, ActorRef<SessionEvent> client) {\n      return Behaviors.receive(ChatRoom.SessionCommand.class)\n          .onMessage(PostMessage.class, post -> onPostMessage(room, screenName, post))\n          .onMessage(NotifyClient.class, notification -> onNotifyClient(client, notification))\n          .build();\n    }\n\n    private static Behavior<SessionCommand> onPostMessage(\n        ActorRef<RoomCommand> room, String screenName, PostMessage post) {\n      // from client, publish to others via the room\n      room.tell(new PublishSessionMessage(screenName, post.message));\n      return Behaviors.same();\n    }\n\n    private static Behavior<SessionCommand> onNotifyClient(\n        ActorRef<SessionEvent> client, NotifyClient notification) {\n      // published from the room\n      client.tell(notification.message);\n      return Behaviors.same();\n    }\n  }\n}\nThe state is managed by changing behavior rather than using any variables.\nWhen a new GetSession command comes in we add that client to the list that is in the returned behavior. Then we also need to create the session’s ActorRefActorRef that will be used to post messages. In this case we want to create a very simple Actor that repackages the PostMessage command into a PublishSessionMessage command which also includes the screen name.\nThe behavior that we declare here can handle both subtypes of RoomCommand. GetSession has been explained already and the PublishSessionMessage commands coming from the session Actors will trigger the dissemination of the contained chat room message to all connected clients. But we do not want to give the ability to send PublishSessionMessage commands to arbitrary clients, we reserve that right to the internal session actors we create—otherwise clients could pose as completely different screen names (imagine the GetSession protocol to include authentication information to further secure this). Therefore PublishSessionMessage has private visibility and can’t be created outside the ChatRoom objectclass.\nIf we did not care about securing the correspondence between a session and a screen name then we could change the protocol such that PostMessage is removed and all clients just get an ActorRef[PublishSessionMessage]ActorRef<PublishSessionMessage> to send to. In this case no session actor would be needed and we could use context.selfcontext.getSelf(). The type-checks work out in that case because ActorRef[-T]ActorRef<-T>ActorRef<T>ActorRef<T> is contravariant in its type parameter, meaning that we can use a ActorRef[RoomCommand]ActorRef<RoomCommand> wherever an ActorRef[PublishSessionMessage]ActorRef<PublishSessionMessage> is needed—this makes sense because the former simply speaks more languages than the latter. The opposite would be problematic, so passing an ActorRef[PublishSessionMessage]ActorRef<PublishSessionMessage> where ActorRef[RoomCommand]ActorRef<RoomCommand> is required will lead to a type error.","title":"Functional Style"},{"location":"/typed/actors.html#trying-it-out","text":"In order to see this chat room in action we need to write a client Actor that can use it:\nScala copysourceobject Gabbler {\n  import ChatRoom._\n\n  def apply(): Behavior[SessionEvent] =\n    Behaviors.setup { context =>\n      Behaviors.receiveMessage {\n        case SessionGranted(handle) =>\n          handle ! PostMessage(\"Hello World!\")\n          Behaviors.same\n        case MessagePosted(screenName, message) =>\n          context.log.info2(\"message has been posted by '{}': {}\", screenName, message)\n          Behaviors.stopped\n      }\n    }\n} Java copysourcepublic class Gabbler {\n  public static Behavior<ChatRoom.SessionEvent> create() {\n    return Behaviors.setup(ctx -> new Gabbler(ctx).behavior());\n  }\n\n  private final ActorContext<ChatRoom.SessionEvent> context;\n\n  private Gabbler(ActorContext<ChatRoom.SessionEvent> context) {\n    this.context = context;\n  }\n\n  private Behavior<ChatRoom.SessionEvent> behavior() {\n    return Behaviors.receive(ChatRoom.SessionEvent.class)\n        .onMessage(ChatRoom.SessionDenied.class, this::onSessionDenied)\n        .onMessage(ChatRoom.SessionGranted.class, this::onSessionGranted)\n        .onMessage(ChatRoom.MessagePosted.class, this::onMessagePosted)\n        .build();\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onSessionDenied(ChatRoom.SessionDenied message) {\n    context.getLog().info(\"cannot start chat room session: {}\", message.reason);\n    return Behaviors.stopped();\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onSessionGranted(ChatRoom.SessionGranted message) {\n    message.handle.tell(new ChatRoom.PostMessage(\"Hello World!\"));\n    return Behaviors.same();\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onMessagePosted(ChatRoom.MessagePosted message) {\n    context\n        .getLog()\n        .info(\"message has been posted by '{}': {}\", message.screenName, message.message);\n    return Behaviors.stopped();\n  }\n}\nFrom this behavior we can create an Actor that will accept a chat room session, post a message, wait to see it published, and then terminate. The last step requires the ability to change behavior, we need to transition from the normal running behavior into the terminated state. This is why here we do not return samesame, as above, but another special value stoppedstopped.\nSince SessionEvent is a sealed trait the Scala compiler will warn us if we forget to handle one of the subtypes; in this case it reminded us that alternatively to SessionGranted we may also receive a SessionDenied event.\nNow to try things out we must start both a chat room and a gabbler and of course we do this inside an Actor system. Since there can be only one user guardian we could either start the chat room from the gabbler (which we don’t want—it complicates its logic) or the gabbler from the chat room (which is nonsensical) or we start both of them from a third Actor—our only sensible choice:\nScala copysourceobject Main {\n  def apply(): Behavior[NotUsed] =\n    Behaviors.setup { context =>\n      val chatRoom = context.spawn(ChatRoom(), \"chatroom\")\n      val gabblerRef = context.spawn(Gabbler(), \"gabbler\")\n      context.watch(gabblerRef)\n      chatRoom ! ChatRoom.GetSession(\"ol’ Gabbler\", gabblerRef)\n\n      Behaviors.receiveSignal {\n        case (_, Terminated(_)) =>\n          Behaviors.stopped\n      }\n    }\n\n  def main(args: Array[String]): Unit = {\n    ActorSystem(Main(), \"ChatRoomDemo\")\n  }\n\n} Java copysourcepublic class Main {\n  public static Behavior<Void> create() {\n    return Behaviors.setup(\n        context -> {\n          ActorRef<ChatRoom.RoomCommand> chatRoom = context.spawn(ChatRoom.create(), \"chatRoom\");\n          ActorRef<ChatRoom.SessionEvent> gabbler = context.spawn(Gabbler.create(), \"gabbler\");\n          context.watch(gabbler);\n          chatRoom.tell(new ChatRoom.GetSession(\"ol’ Gabbler\", gabbler));\n\n          return Behaviors.receive(Void.class)\n              .onSignal(Terminated.class, sig -> Behaviors.stopped())\n              .build();\n        });\n  }\n\n  public static void main(String[] args) {\n    ActorSystem.create(Main.create(), \"ChatRoomDemo\");\n  }\n}\nIn good tradition we call the Main Actor what it is, it directly corresponds to the main method in a traditional Java application. This Actor will perform its job on its own accord, we do not need to send messages from the outside, so we declare it to be of type NotUsedVoid. Actors receive not only external messages, they also are notified of certain system events, so-called Signals. In order to get access to those we choose to implement this particular one using the receivereceive behavior decorator. The provided onSignal function will be invoked for signals (subclasses of SignalSignal) or the onMessage function for user messages.\nThis particular Main Actor is created using Behaviors.setupBehaviors.setup, which is like a factory for a behavior. Creation of the behavior instance is deferred until the actor is started, as opposed to Behaviors.receiveBehaviors.receive that creates the behavior instance immediately before the actor is running. The factory function in setup is passed the ActorContextActorContext as parameter and that can for example be used for spawning child actors. This Main Actor creates the chat room and the gabbler and the session between them is initiated, and when the gabbler is finished we will receive the TerminatedTerminated event due to having called context.watchcontext.watch for it. This allows us to shut down the Actor system: when the Main Actor terminates there is nothing more to do.\nTherefore after creating the Actor system with the Main Actor’s BehaviorBehavior we can let the main method return, the ActorSystemActorSystem will continue running and the JVM alive until the root actor stops.","title":"Trying it out"},{"location":"/typed/actors.html#object-oriented-style","text":"The above sample used the functional programming style where you pass a function to a factory which then constructs a behavior, for stateful actors this means passing immutable state around as parameters and switching to a new behavior whenever you need to act on a changed state. An alternative way to express the same is a more object oriented style where a concrete class for the actor behavior is defined and mutable state is kept inside of it as fields.\nWhich style you choose to use is a matter of taste and both styles can be mixed depending on which is best for a specific actor. Considerations for the choice is provided in the Style Guide.","title":"Object-oriented style"},{"location":"/typed/actors.html#abstractbehavior-api","text":"Defining a class based actor behavior starts with extending AbstractBehaviorAbstractBehavior<T>[T] where T is the type of messages the behavior will accept.\nLet’s repeat the chat room sample from A more complex example above but implemented using AbstractBehavior. The protocol for interacting with the actor looks the same:\nScala copysourceobject ChatRoom {\n  sealed trait RoomCommand\n  final case class GetSession(screenName: String, replyTo: ActorRef[SessionEvent]) extends RoomCommand\n\n  sealed trait SessionEvent\n  final case class SessionGranted(handle: ActorRef[PostMessage]) extends SessionEvent\n  final case class SessionDenied(reason: String) extends SessionEvent\n  final case class MessagePosted(screenName: String, message: String) extends SessionEvent\n\n  sealed trait SessionCommand\n  final case class PostMessage(message: String) extends SessionCommand\n  private final case class NotifyClient(message: MessagePosted) extends SessionCommand\n} Java copysourcestatic interface RoomCommand {}\n\npublic static final class GetSession implements RoomCommand {\n  public final String screenName;\n  public final ActorRef<SessionEvent> replyTo;\n\n  public GetSession(String screenName, ActorRef<SessionEvent> replyTo) {\n    this.screenName = screenName;\n    this.replyTo = replyTo;\n  }\n}\n\nstatic interface SessionEvent {}\n\npublic static final class SessionGranted implements SessionEvent {\n  public final ActorRef<PostMessage> handle;\n\n  public SessionGranted(ActorRef<PostMessage> handle) {\n    this.handle = handle;\n  }\n}\n\npublic static final class SessionDenied implements SessionEvent {\n  public final String reason;\n\n  public SessionDenied(String reason) {\n    this.reason = reason;\n  }\n}\n\npublic static final class MessagePosted implements SessionEvent {\n  public final String screenName;\n  public final String message;\n\n  public MessagePosted(String screenName, String message) {\n    this.screenName = screenName;\n    this.message = message;\n  }\n}\n\nstatic interface SessionCommand {}\n\npublic static final class PostMessage implements SessionCommand {\n  public final String message;\n\n  public PostMessage(String message) {\n    this.message = message;\n  }\n}\n\nprivate static final class NotifyClient implements SessionCommand {\n  final MessagePosted message;\n\n  NotifyClient(MessagePosted message) {\n    this.message = message;\n  }\n}\nInitially the client Actors only get access to an ActorRef[GetSession]ActorRef<GetSession> which allows them to make the first step. Once a client’s session has been established it gets a SessionGranted message that contains a handle to unlock the next protocol step, posting messages. The PostMessage command will need to be sent to this particular address that represents the session that has been added to the chat room. The other aspect of a session is that the client has revealed its own address, via the replyTo argument, so that subsequent MessagePosted events can be sent to it.\nThis illustrates how Actors can express more than just the equivalent of method calls on Java objects. The declared message types and their contents describe a full protocol that can involve multiple Actors and that can evolve over multiple steps. Here’s the AbstractBehavior implementation of the chat room protocol:\nScala copysourceobject ChatRoom {\n  private final case class PublishSessionMessage(screenName: String, message: String) extends RoomCommand\n\n  def apply(): Behavior[RoomCommand] =\n    Behaviors.setup(context => new ChatRoomBehavior(context))\n\n  class ChatRoomBehavior(context: ActorContext[RoomCommand]) extends AbstractBehavior[RoomCommand](context) {\n    private var sessions: List[ActorRef[SessionCommand]] = List.empty\n\n    override def onMessage(message: RoomCommand): Behavior[RoomCommand] = {\n      message match {\n        case GetSession(screenName, client) =>\n          // create a child actor for further interaction with the client\n          val ses = context.spawn(\n            SessionBehavior(context.self, screenName, client),\n            name = URLEncoder.encode(screenName, StandardCharsets.UTF_8.name))\n          client ! SessionGranted(ses)\n          sessions = ses :: sessions\n          this\n        case PublishSessionMessage(screenName, message) =>\n          val notification = NotifyClient(MessagePosted(screenName, message))\n          sessions.foreach(_ ! notification)\n          this\n      }\n    }\n  }\n\n  private object SessionBehavior {\n    def apply(\n        room: ActorRef[PublishSessionMessage],\n        screenName: String,\n        client: ActorRef[SessionEvent]): Behavior[SessionCommand] =\n      Behaviors.setup(ctx => new SessionBehavior(ctx, room, screenName, client))\n  }\n\n  private class SessionBehavior(\n      context: ActorContext[SessionCommand],\n      room: ActorRef[PublishSessionMessage],\n      screenName: String,\n      client: ActorRef[SessionEvent])\n      extends AbstractBehavior[SessionCommand](context) {\n\n    override def onMessage(msg: SessionCommand): Behavior[SessionCommand] =\n      msg match {\n        case PostMessage(message) =>\n          // from client, publish to others via the room\n          room ! PublishSessionMessage(screenName, message)\n          Behaviors.same\n        case NotifyClient(message) =>\n          // published from the room\n          client ! message\n          Behaviors.same\n      }\n  }\n} Java copysourcepublic class ChatRoom {\n  private static final class PublishSessionMessage implements RoomCommand {\n    public final String screenName;\n    public final String message;\n\n    public PublishSessionMessage(String screenName, String message) {\n      this.screenName = screenName;\n      this.message = message;\n    }\n  }\n\n  public static Behavior<RoomCommand> create() {\n    return Behaviors.setup(ChatRoomBehavior::new);\n  }\n\n  public static class ChatRoomBehavior extends AbstractBehavior<RoomCommand> {\n    final List<ActorRef<SessionCommand>> sessions = new ArrayList<>();\n\n    private ChatRoomBehavior(ActorContext<RoomCommand> context) {\n      super(context);\n    }\n\n    @Override\n    public Receive<RoomCommand> createReceive() {\n      ReceiveBuilder<RoomCommand> builder = newReceiveBuilder();\n\n      builder.onMessage(GetSession.class, this::onGetSession);\n      builder.onMessage(PublishSessionMessage.class, this::onPublishSessionMessage);\n\n      return builder.build();\n    }\n\n    private Behavior<RoomCommand> onGetSession(GetSession getSession)\n        throws UnsupportedEncodingException {\n      ActorRef<SessionEvent> client = getSession.replyTo;\n      ActorRef<SessionCommand> ses =\n          getContext()\n              .spawn(\n                  SessionBehavior.create(getContext().getSelf(), getSession.screenName, client),\n                  URLEncoder.encode(getSession.screenName, StandardCharsets.UTF_8.name()));\n      // narrow to only expose PostMessage\n      client.tell(new SessionGranted(ses.narrow()));\n      sessions.add(ses);\n      return this;\n    }\n\n    private Behavior<RoomCommand> onPublishSessionMessage(PublishSessionMessage pub) {\n      NotifyClient notification =\n          new NotifyClient((new MessagePosted(pub.screenName, pub.message)));\n      sessions.forEach(s -> s.tell(notification));\n      return this;\n    }\n  }\n\n  static class SessionBehavior extends AbstractBehavior<ChatRoom.SessionCommand> {\n    private final ActorRef<RoomCommand> room;\n    private final String screenName;\n    private final ActorRef<SessionEvent> client;\n\n    public static Behavior<ChatRoom.SessionCommand> create(\n        ActorRef<RoomCommand> room, String screenName, ActorRef<SessionEvent> client) {\n      return Behaviors.setup(context -> new SessionBehavior(context, room, screenName, client));\n    }\n\n    private SessionBehavior(\n        ActorContext<ChatRoom.SessionCommand> context,\n        ActorRef<RoomCommand> room,\n        String screenName,\n        ActorRef<SessionEvent> client) {\n      super(context);\n      this.room = room;\n      this.screenName = screenName;\n      this.client = client;\n    }\n\n    @Override\n    public Receive<SessionCommand> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(PostMessage.class, this::onPostMessage)\n          .onMessage(NotifyClient.class, this::onNotifyClient)\n          .build();\n    }\n\n    private Behavior<SessionCommand> onPostMessage(PostMessage post) {\n      // from client, publish to others via the room\n      room.tell(new PublishSessionMessage(screenName, post.message));\n      return Behaviors.same();\n    }\n\n    private Behavior<SessionCommand> onNotifyClient(NotifyClient notification) {\n      // published from the room\n      client.tell(notification.message);\n      return Behaviors.same();\n    }\n  }\n}\nThe state is managed through fields in the class, just like with a regular object oriented class. As the state is mutable, we never return a different behavior from the message logic, but can return the AbstractBehavior instance itself (this) as a behavior to use for processing the next message coming in. We could also return Behaviors.sameBehaviors.same to achieve the same.\nIn this sample we make separate statements for creating the behavior builder, but it also returns the builder itself from each step so a more fluent behavior definition style is also possible. What you should prefer depends on how big the set of messages the actor accepts is.\nIt is also possible to return a new different AbstractBehavior, for example to represent a different state in a finite state machine (FSM), or use one of the functional behavior factories to combine the object oriented with the functional style for different parts of the lifecycle of the same Actor behavior.\nWhen a new GetSession command comes in we add that client to the list of current sessions. Then we also need to create the session’s ActorRefActorRef that will be used to post messages. In this case we want to create a very simple Actor that repackages the PostMessage command into a PublishSessionMessage command which also includes the screen name.\nTo implement the logic where we spawn a child for the session we need access to the ActorContextActorContext. This is injected as a constructor parameter upon creation of the behavior, note how we combine the AbstractBehaviorAbstractBehavior with Behaviors.setupBehaviors.setup to do this in the applycreate factory method.\nThe behavior that we declare here can handle both subtypes of RoomCommand. GetSession has been explained already and the PublishSessionMessage commands coming from the session Actors will trigger the dissemination of the contained chat room message to all connected clients. But we do not want to give the ability to send PublishSessionMessage commands to arbitrary clients, we reserve that right to the internal session actors we create—otherwise clients could pose as completely different screen names (imagine the GetSession protocol to include authentication information to further secure this). Therefore PublishSessionMessage has private visibility and can’t be created outside the ChatRoom objectclass.\nIf we did not care about securing the correspondence between a session and a screen name then we could change the protocol such that PostMessage is removed and all clients just get an ActorRef[PublishSessionMessage]ActorRef<PublishSessionMessage> to send to. In this case no session actor would be needed and we could use context.selfcontext.getSelf(). The type-checks work out in that case because ActorRef[-T]ActorRef<T> is contravariant in its type parameter, meaning that we can use a ActorRef[RoomCommand]ActorRef<RoomCommand> wherever an ActorRef[PublishSessionMessage]ActorRef<PublishSessionMessage> is needed—this makes sense because the former simply speaks more languages than the latter. The opposite would be problematic, so passing an ActorRef[PublishSessionMessage]ActorRef<PublishSessionMessage> where ActorRef[RoomCommand]ActorRef<RoomCommand> is required will lead to a type error.","title":"AbstractBehavior API"},{"location":"/typed/actors.html#try-it-out","text":"In order to see this chat room in action we need to write a client Actor that can use it , for this stateless actor it doesn’t make much sense to use the AbstractBehavior so let’s just reuse the functional style gabbler from the sample above:\nScala copysourceobject Gabbler {\n  import ChatRoom._\n\n  def apply(): Behavior[SessionEvent] =\n    Behaviors.setup { context =>\n      Behaviors.receiveMessage {\n        case SessionDenied(reason) =>\n          context.log.info(\"cannot start chat room session: {}\", reason)\n          Behaviors.stopped\n        case SessionGranted(handle) =>\n          handle ! PostMessage(\"Hello World!\")\n          Behaviors.same\n        case MessagePosted(screenName, message) =>\n          context.log.info2(\"message has been posted by '{}': {}\", screenName, message)\n          Behaviors.stopped\n      }\n    } Java copysourcepublic class Gabbler extends AbstractBehavior<ChatRoom.SessionEvent> {\n  public static Behavior<ChatRoom.SessionEvent> create() {\n    return Behaviors.setup(Gabbler::new);\n  }\n\n  private Gabbler(ActorContext<ChatRoom.SessionEvent> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<ChatRoom.SessionEvent> createReceive() {\n    ReceiveBuilder<ChatRoom.SessionEvent> builder = newReceiveBuilder();\n    return builder\n        .onMessage(ChatRoom.SessionDenied.class, this::onSessionDenied)\n        .onMessage(ChatRoom.SessionGranted.class, this::onSessionGranted)\n        .onMessage(ChatRoom.MessagePosted.class, this::onMessagePosted)\n        .build();\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onSessionDenied(ChatRoom.SessionDenied message) {\n    getContext().getLog().info(\"cannot start chat room session: {}\", message.reason);\n    return Behaviors.stopped();\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onSessionGranted(ChatRoom.SessionGranted message) {\n    message.handle.tell(new ChatRoom.PostMessage(\"Hello World!\"));\n    return Behaviors.same();\n  }\n\n  private Behavior<ChatRoom.SessionEvent> onMessagePosted(ChatRoom.MessagePosted message) {\n    getContext()\n        .getLog()\n        .info(\"message has been posted by '{}': {}\", message.screenName, message.message);\n    return Behaviors.stopped();\n  }\n}\nNow to try things out we must start both a chat room and a gabbler and of course we do this inside an Actor system. Since there can be only one user guardian we could either start the chat room from the gabbler (which we don’t want—it complicates its logic) or the gabbler from the chat room (which is nonsensical) or we start both of them from a third Actor—our only sensible choice:\nScala copysourceobject Main {\n  def apply(): Behavior[NotUsed] =\n    Behaviors.setup { context =>\n      val chatRoom = context.spawn(ChatRoom(), \"chatroom\")\n      val gabblerRef = context.spawn(Gabbler(), \"gabbler\")\n      context.watch(gabblerRef)\n      chatRoom ! ChatRoom.GetSession(\"ol’ Gabbler\", gabblerRef)\n\n      Behaviors.receiveSignal {\n        case (_, Terminated(_)) =>\n          Behaviors.stopped\n      }\n    }\n\n  def main(args: Array[String]): Unit = {\n    ActorSystem(Main(), \"ChatRoomDemo\")\n  }\n\n} Java copysourcepublic class Main {\n  public static Behavior<Void> create() {\n    return Behaviors.setup(\n        context -> {\n          ActorRef<ChatRoom.RoomCommand> chatRoom = context.spawn(ChatRoom.create(), \"chatRoom\");\n          ActorRef<ChatRoom.SessionEvent> gabbler = context.spawn(Gabbler.create(), \"gabbler\");\n          context.watch(gabbler);\n          chatRoom.tell(new ChatRoom.GetSession(\"ol’ Gabbler\", gabbler));\n\n          return Behaviors.receive(Void.class)\n              .onSignal(Terminated.class, sig -> Behaviors.stopped())\n              .build();\n        });\n  }\n\n  public static void main(String[] args) {\n    ActorSystem.create(Main.create(), \"ChatRoomDemo\");\n  }\n}\nIn good tradition we call the Main Actor what it is, it directly corresponds to the main method in a traditional Java application. This Actor will perform its job on its own accord, we do not need to send messages from the outside, so we declare it to be of type NotUsedVoid. Actors receive not only external messages, they also are notified of certain system events, so-called Signals. In order to get access to those we choose to implement this particular one using the receivereceive behavior decorator. The provided onSignal function will be invoked for signals (subclasses of SignalSignal) or the onMessage function for user messages.\nThis particular Main Actor is created using Behaviors.setupBehaviors.setup, which is like a factory for a behavior. Creation of the behavior instance is deferred until the actor is started, as opposed to Behaviors.receiveBehaviors.receive that creates the behavior instance immediately before the actor is running. The factory function in setup is passed the ActorContextActorContext as parameter and that can for example be used for spawning child actors. This Main Actor creates the chat room and the gabbler and the session between them is initiated, and when the gabbler is finished we will receive the TerminatedTerminated event due to having called context.watchcontext.watch for it. This allows us to shut down the Actor system: when the Main Actor terminates there is nothing more to do.\nTherefore after creating the Actor system with the Main Actor’s BehaviorBehavior we can let the main method return, the ActorSystemActorSystem will continue running and the JVM alive until the root actor stops.","title":"Try it out"},{"location":"/typed/actor-lifecycle.html","text":"","title":"Actor lifecycle"},{"location":"/typed/actor-lifecycle.html#actor-lifecycle","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Actors.","title":"Actor lifecycle"},{"location":"/typed/actor-lifecycle.html#dependency","text":"To use Pekko Actor Typed, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/actor-lifecycle.html#introduction","text":"An actor is a stateful resource that has to be explicitly started and stopped.\nIt is important to note that actors do not stop automatically when no longer referenced, every Actor that is created must also explicitly be destroyed. The only simplification is that stopping a parent Actor will also recursively stop all the child Actors that this parent has created. All actors are also stopped automatically when the ActorSystemActorSystem is shut down.\nNote An ActorSystem is a heavyweight structure that will allocate threads, so create one per logical application. Typically one ActorSystem per JVM process.","title":"Introduction"},{"location":"/typed/actor-lifecycle.html#creating-actors","text":"An actor can create, or spawn, an arbitrary number of child actors, which in turn can spawn children of their own, thus forming an actor hierarchy. ActorSystemActorSystem hosts the hierarchy and there can be only one root actor, an actor at the top of the hierarchy of the ActorSystem. The lifecycle of a child actor is tied to the parent – a child can stop itself or be stopped at any time but it can never outlive its parent.","title":"Creating Actors"},{"location":"/typed/actor-lifecycle.html#the-actorcontext","text":"The ActorContextActorContext can be accessed for many purposes such as:\nSpawning child actors and supervision Watching other actors to receive a Terminated(otherActor)Terminated(otherActor) event should the watched actor stop permanently Logging Creating message adapters Request-response interactions (ask) with another actor Access to the selfgetSelf() ActorRef\nIf a behavior needs to use the ActorContext, for example to spawn child actors, or use context.selfcontext.getSelf(), it can be obtained by wrapping construction with Behaviors.setupBehaviors.setup:\nScala copysourceobject HelloWorldMain {\n\n  final case class SayHello(name: String)\n\n  def apply(): Behavior[SayHello] =\n    Behaviors.setup { context =>\n      val greeter = context.spawn(HelloWorld(), \"greeter\")\n\n      Behaviors.receiveMessage { message =>\n        val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)\n        greeter ! HelloWorld.Greet(message.name, replyTo)\n        Behaviors.same\n      }\n    }\n\n} Java copysourcepublic class HelloWorldMain extends AbstractBehavior<HelloWorldMain.SayHello> {\n  public static Behavior<SayHello> create() {\n    return Behaviors.setup(HelloWorldMain::new);\n  }\n\n  private final ActorRef<HelloWorld.Greet> greeter;\n\n  private HelloWorldMain(ActorContext<SayHello> context) {\n    super(context);\n    greeter = context.spawn(HelloWorld.create(), \"greeter\");\n  }\n}","title":"The ActorContext"},{"location":"/typed/actor-lifecycle.html#actorcontext-thread-safety","text":"Many of the methods in ActorContextActorContext are not thread-safe and\nMust not be accessed by threads from scala.concurrent.Futurejava.util.concurrent.CompletionStage callbacks Must not be shared between several actor instances Must only be used in the ordinary actor message processing thread","title":"ActorContext Thread Safety"},{"location":"/typed/actor-lifecycle.html#the-guardian-actor","text":"The top level actor, also called the user guardian actor, is created along with the ActorSystemActorSystem. Messages sent to the actor system are directed to the root actor. The root actor is defined by the behavior used to create the ActorSystem, named HelloWorldMain in the example below:\nScala copysource val system: ActorSystem[HelloWorldMain.SayHello] =\n  ActorSystem(HelloWorldMain(), \"hello\")\n\nsystem ! HelloWorldMain.SayHello(\"World\")\nsystem ! HelloWorldMain.SayHello(\"Akka\")\n Java copysourcefinal ActorSystem<HelloWorldMain.SayHello> system =\n    ActorSystem.create(HelloWorldMain.create(), \"hello\");\n\nsystem.tell(new HelloWorldMain.SayHello(\"World\"));\nsystem.tell(new HelloWorldMain.SayHello(\"Akka\"));\nFor very simple applications the guardian may contain the actual application logic and handle messages. As soon as the application handles more than one concern the guardian should instead just bootstrap the application, spawn the various subsystems as children and monitor their lifecycles.\nWhen the guardian actor stops this will stop the ActorSystem.\nWhen ActorSystem.terminateActorSystem.terminate is invoked the Coordinated Shutdown process will stop actors and services in a specific order.","title":"The Guardian Actor"},{"location":"/typed/actor-lifecycle.html#spawning-children","text":"Child actors are created and started with ActorContext’s spawnspawn. In the example below, when the root actor is started, it spawns a child actor described by the HelloWorld behavior. Additionally, when the root actor receives a SayHello message, it creates a child actor defined by the behavior HelloWorldBot:\nScala copysourceobject HelloWorldMain {\n\n  final case class SayHello(name: String)\n\n  def apply(): Behavior[SayHello] =\n    Behaviors.setup { context =>\n      val greeter = context.spawn(HelloWorld(), \"greeter\")\n\n      Behaviors.receiveMessage { message =>\n        val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)\n        greeter ! HelloWorld.Greet(message.name, replyTo)\n        Behaviors.same\n      }\n    }\n\n} Java copysourcepublic class HelloWorldMain extends AbstractBehavior<HelloWorldMain.SayHello> {\n\n  public static class SayHello {\n    public final String name;\n\n    public SayHello(String name) {\n      this.name = name;\n    }\n  }\n\n  public static Behavior<SayHello> create() {\n    return Behaviors.setup(HelloWorldMain::new);\n  }\n\n  private final ActorRef<HelloWorld.Greet> greeter;\n\n  private HelloWorldMain(ActorContext<SayHello> context) {\n    super(context);\n    greeter = context.spawn(HelloWorld.create(), \"greeter\");\n  }\n\n  @Override\n  public Receive<SayHello> createReceive() {\n    return newReceiveBuilder().onMessage(SayHello.class, this::onStart).build();\n  }\n\n  private Behavior<SayHello> onStart(SayHello command) {\n    ActorRef<HelloWorld.Greeted> replyTo =\n        getContext().spawn(HelloWorldBot.create(3), command.name);\n    greeter.tell(new HelloWorld.Greet(command.name, replyTo));\n    return this;\n  }\n}\nTo specify a dispatcher when spawning an actor use DispatcherSelectorDispatcherSelector. If not specified, the actor will use the default dispatcher, see Default dispatcher for details.\nScala copysourcedef apply(): Behavior[SayHello] =\n  Behaviors.setup { context =>\n    val dispatcherPath = \"pekko.actor.default-blocking-io-dispatcher\"\n\n    val props = DispatcherSelector.fromConfig(dispatcherPath)\n    val greeter = context.spawn(HelloWorld(), \"greeter\", props)\n\n    Behaviors.receiveMessage { message =>\n      val replyTo = context.spawn(HelloWorldBot(max = 3), message.name)\n\n      greeter ! HelloWorld.Greet(message.name, replyTo)\n      Behaviors.same\n    }\n  } Java copysourcepublic class HelloWorldMain extends AbstractBehavior<HelloWorldMain.SayHello> {\n\n  // Start message...\n\n  public static Behavior<SayHello> create() {\n    return Behaviors.setup(HelloWorldMain::new);\n  }\n\n  private final ActorRef<HelloWorld.Greet> greeter;\n\n  private HelloWorldMain(ActorContext<SayHello> context) {\n    super(context);\n\n    final String dispatcherPath = \"pekko.actor.default-blocking-io-dispatcher\";\n    Props greeterProps = DispatcherSelector.fromConfig(dispatcherPath);\n    greeter = getContext().spawn(HelloWorld.create(), \"greeter\", greeterProps);\n  }\n\n  // createReceive ...\n}\nRefer to Actors for a walk-through of the above examples.","title":"Spawning Children"},{"location":"/typed/actor-lifecycle.html#spawnprotocol","text":"The guardian actor should be responsible for initialization of tasks and create the initial actors of the application, but sometimes you might want to spawn new actors from the outside of the guardian actor. For example creating one actor per HTTP request.\nThat is not difficult to implement in your behavior, but since this is a common pattern there is a predefined message protocol and implementation of a behavior for this. It can be used as the guardian actor of the ActorSystemActorSystem, possibly combined with Behaviors.setupBehaviors.setup to start some initial tasks or actors. Child actors can then be started from the outside by telltelling or askasking SpawnProtocol.SpawnSpawnProtocol.Spawn to the actor reference of the system. Using ask is similar to how ActorSystem.actorOf can be used in classic actors with the difference that a FutureCompletionStage of the ActorRefActorRef is returned.\nThe guardian behavior can be defined as:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.SpawnProtocol\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.scaladsl.LoggerOps\n\nobject HelloWorldMain {\n  def apply(): Behavior[SpawnProtocol.Command] =\n    Behaviors.setup { context =>\n      // Start initial tasks\n      // context.spawn(...)\n\n      SpawnProtocol()\n    }\n} Java copysourceimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.SpawnProtocol;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\n\npublic abstract class HelloWorldMain {\n  private HelloWorldMain() {}\n\n  public static Behavior<SpawnProtocol.Command> create() {\n    return Behaviors.setup(\n        context -> {\n          // Start initial tasks\n          // context.spawn(...)\n\n          return SpawnProtocol.create();\n        });\n  }\n}\nand the ActorSystemActorSystem can be created with that main behavior and asked to spawn other actors:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.ActorSystem\nimport pekko.actor.typed.Props\nimport pekko.util.Timeout\n\n\nimplicit val system: ActorSystem[SpawnProtocol.Command] =\n  ActorSystem(HelloWorldMain(), \"hello\")\n\n// needed in implicit scope for ask (?)\nimport pekko.actor.typed.scaladsl.AskPattern._\nimplicit val ec: ExecutionContext = system.executionContext\nimplicit val timeout: Timeout = Timeout(3.seconds)\n\nval greeter: Future[ActorRef[HelloWorld.Greet]] =\n  system.ask(SpawnProtocol.Spawn(behavior = HelloWorld(), name = \"greeter\", props = Props.empty, _))\n\nval greetedBehavior = Behaviors.receive[HelloWorld.Greeted] { (context, message) =>\n  context.log.info2(\"Greeting for {} from {}\", message.whom, message.from)\n  Behaviors.stopped\n}\n\nval greetedReplyTo: Future[ActorRef[HelloWorld.Greeted]] =\n  system.ask(SpawnProtocol.Spawn(greetedBehavior, name = \"\", props = Props.empty, _))\n\nfor (greeterRef <- greeter; replyToRef <- greetedReplyTo) {\n  greeterRef ! HelloWorld.Greet(\"Akka\", replyToRef)\n}\n Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.actor.typed.Props;\nimport org.apache.pekko.actor.typed.javadsl.AskPattern;\n\nfinal ActorSystem<SpawnProtocol.Command> system =\n    ActorSystem.create(HelloWorldMain.create(), \"hello\");\nfinal Duration timeout = Duration.ofSeconds(3);\n\nCompletionStage<ActorRef<HelloWorld.Greet>> greeter =\n    AskPattern.ask(\n        system,\n        replyTo ->\n            new SpawnProtocol.Spawn<>(HelloWorld.create(), \"greeter\", Props.empty(), replyTo),\n        timeout,\n        system.scheduler());\n\nBehavior<HelloWorld.Greeted> greetedBehavior =\n    Behaviors.receive(\n        (context, message) -> {\n          context.getLog().info(\"Greeting for {} from {}\", message.whom, message.from);\n          return Behaviors.stopped();\n        });\n\nCompletionStage<ActorRef<HelloWorld.Greeted>> greetedReplyTo =\n    AskPattern.ask(\n        system,\n        replyTo -> new SpawnProtocol.Spawn<>(greetedBehavior, \"\", Props.empty(), replyTo),\n        timeout,\n        system.scheduler());\n\ngreeter.whenComplete(\n    (greeterRef, exc) -> {\n      if (exc == null) {\n        greetedReplyTo.whenComplete(\n            (greetedReplyToRef, exc2) -> {\n              if (exc2 == null) {\n                greeterRef.tell(new HelloWorld.Greet(\"Akka\", greetedReplyToRef));\n              }\n            });\n      }\n    });\nThe SpawnProtocolSpawnProtocol can also be used at other places in the actor hierarchy. It doesn’t have to be the root guardian actor.\nA way to find running actors is described in Actor discovery.","title":"SpawnProtocol"},{"location":"/typed/actor-lifecycle.html#stopping-actors","text":"An actor can stop itself by returning Behaviors.stoppedBehaviors.stopped as the next behavior.\nA child actor can be forced to stop after it finishes processing its current message by using the stopstop method of the ActorContext from the parent actor. Only child actors can be stopped in that way.\nAll child actors will be stopped when their parent is stopped.\nWhen an actor is stopped, it receives the PostStopPostStop signal that can be used for cleaning up resources.\nHere is an illustrating example:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.{ ActorSystem, PostStop }\n\n\nobject MasterControlProgram {\n  sealed trait Command\n  final case class SpawnJob(name: String) extends Command\n  case object GracefulShutdown extends Command\n\n  def apply(): Behavior[Command] = {\n    Behaviors\n      .receive[Command] { (context, message) =>\n        message match {\n          case SpawnJob(jobName) =>\n            context.log.info(\"Spawning job {}!\", jobName)\n            context.spawn(Job(jobName), name = jobName)\n            Behaviors.same\n          case GracefulShutdown =>\n            context.log.info(\"Initiating graceful shutdown...\")\n            // Here it can perform graceful stop (possibly asynchronous) and when completed\n            // return `Behaviors.stopped` here or after receiving another message.\n            Behaviors.stopped\n        }\n      }\n      .receiveSignal {\n        case (context, PostStop) =>\n          context.log.info(\"Master Control Program stopped\")\n          Behaviors.same\n      }\n  }\n}\n\nobject Job {\n  sealed trait Command\n\n  def apply(name: String): Behavior[Command] = {\n    Behaviors.receiveSignal[Command] {\n      case (context, PostStop) =>\n        context.log.info(\"Worker {} stopped\", name)\n        Behaviors.same\n    }\n  }\n} Java copysource import java.util.concurrent.TimeUnit;\n\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.PostStop;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\n\npublic class MasterControlProgram extends AbstractBehavior<MasterControlProgram.Command> {\n\n  interface Command {}\n\n  public static final class SpawnJob implements Command {\n    public final String name;\n\n    public SpawnJob(String name) {\n      this.name = name;\n    }\n  }\n\n  public enum GracefulShutdown implements Command {\n    INSTANCE\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(MasterControlProgram::new);\n  }\n\n  public MasterControlProgram(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(SpawnJob.class, this::onSpawnJob)\n        .onMessage(GracefulShutdown.class, message -> onGracefulShutdown())\n        .onSignal(PostStop.class, signal -> onPostStop())\n        .build();\n  }\n\n  private Behavior<Command> onSpawnJob(SpawnJob message) {\n    getContext().getSystem().log().info(\"Spawning job {}!\", message.name);\n    getContext().spawn(Job.create(message.name), message.name);\n    return this;\n  }\n\n  private Behavior<Command> onGracefulShutdown() {\n    getContext().getSystem().log().info(\"Initiating graceful shutdown...\");\n\n    // Here it can perform graceful stop (possibly asynchronous) and when completed\n    // return `Behaviors.stopped()` here or after receiving another message.\n    return Behaviors.stopped();\n  }\n\n  private Behavior<Command> onPostStop() {\n    getContext().getSystem().log().info(\"Master Control Program stopped\");\n    return this;\n  }\n}\n\npublic class Job extends AbstractBehavior<Job.Command> {\n\n  interface Command {}\n\n  public static Behavior<Command> create(String name) {\n    return Behaviors.setup(context -> new Job(context, name));\n  }\n\n  private final String name;\n\n  public Job(ActorContext<Command> context, String name) {\n    super(context);\n    this.name = name;\n  }\n\n  @Override\n  public Receive<Job.Command> createReceive() {\n    return newReceiveBuilder().onSignal(PostStop.class, postStop -> onPostStop()).build();\n  }\n\n  private Behavior<Command> onPostStop() {\n    getContext().getSystem().log().info(\"Worker {} stopped\", name);\n    return this;\n  }\n}\nWhen cleaning up resources from PostStop you should also consider doing the same for the PreRestartPreRestart signal, which is emitted when the actor is restarted. Note that PostStop is not emitted for a restart.","title":"Stopping Actors"},{"location":"/typed/actor-lifecycle.html#watching-actors","text":"In order to be notified when another actor terminates (i.e. stops permanently, not temporary failure and restart), an actor can watchwatch another actor. It will receive the TerminatedTerminated signal upon termination (see Stopping Actors) of the watched actor.\nScala copysource object MasterControlProgram {\n  sealed trait Command\n  final case class SpawnJob(name: String) extends Command\n\n  def apply(): Behavior[Command] = {\n    Behaviors\n      .receive[Command] { (context, message) =>\n        message match {\n          case SpawnJob(jobName) =>\n            context.log.info(\"Spawning job {}!\", jobName)\n            val job = context.spawn(Job(jobName), name = jobName)\n            context.watch(job)\n            Behaviors.same\n        }\n      }\n      .receiveSignal {\n        case (context, Terminated(ref)) =>\n          context.log.info(\"Job stopped: {}\", ref.path.name)\n          Behaviors.same\n      }\n  }\n} Java copysourcepublic class MasterControlProgram extends AbstractBehavior<MasterControlProgram.Command> {\n\n  interface Command {}\n\n  public static final class SpawnJob implements Command {\n    public final String name;\n\n    public SpawnJob(String name) {\n      this.name = name;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(MasterControlProgram::new);\n  }\n\n  public MasterControlProgram(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(SpawnJob.class, this::onSpawnJob)\n        .onSignal(Terminated.class, this::onTerminated)\n        .build();\n  }\n\n  private Behavior<Command> onSpawnJob(SpawnJob message) {\n    getContext().getSystem().log().info(\"Spawning job {}!\", message.name);\n    ActorRef<Job.Command> job = getContext().spawn(Job.create(message.name), message.name);\n    getContext().watch(job);\n    return this;\n  }\n\n  private Behavior<Command> onTerminated(Terminated terminated) {\n    getContext().getSystem().log().info(\"Job stopped: {}\", terminated.getRef().path().name());\n    return this;\n  }\n}\nAn alternative to watchwatch is watchWithwatchWith, which allows specifying a custom message instead of the Terminated. This is often preferred over using watch and the Terminated signal because additional information can be included in the message that can be used later when receiving it.\nSimilar example as above, but using watchWith and replies to the original requestor when the job has finished.\nScala copysource object MasterControlProgram {\n  sealed trait Command\n  final case class SpawnJob(name: String, replyToWhenDone: ActorRef[JobDone]) extends Command\n  final case class JobDone(name: String)\n  private final case class JobTerminated(name: String, replyToWhenDone: ActorRef[JobDone]) extends Command\n\n  def apply(): Behavior[Command] = {\n    Behaviors.receive { (context, message) =>\n      message match {\n        case SpawnJob(jobName, replyToWhenDone) =>\n          context.log.info(\"Spawning job {}!\", jobName)\n          val job = context.spawn(Job(jobName), name = jobName)\n          context.watchWith(job, JobTerminated(jobName, replyToWhenDone))\n          Behaviors.same\n        case JobTerminated(jobName, replyToWhenDone) =>\n          context.log.info(\"Job stopped: {}\", jobName)\n          replyToWhenDone ! JobDone(jobName)\n          Behaviors.same\n      }\n    }\n  }\n} Java copysourcepublic class MasterControlProgram extends AbstractBehavior<MasterControlProgram.Command> {\n\n  interface Command {}\n\n  public static final class SpawnJob implements Command {\n    public final String name;\n    public final ActorRef<JobDone> replyToWhenDone;\n\n    public SpawnJob(String name, ActorRef<JobDone> replyToWhenDone) {\n      this.name = name;\n      this.replyToWhenDone = replyToWhenDone;\n    }\n  }\n\n  public static final class JobDone {\n    public final String name;\n\n    public JobDone(String name) {\n      this.name = name;\n    }\n  }\n\n  private static final class JobTerminated implements Command {\n    final String name;\n    final ActorRef<JobDone> replyToWhenDone;\n\n    JobTerminated(String name, ActorRef<JobDone> replyToWhenDone) {\n      this.name = name;\n      this.replyToWhenDone = replyToWhenDone;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(MasterControlProgram::new);\n  }\n\n  public MasterControlProgram(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(SpawnJob.class, this::onSpawnJob)\n        .onMessage(JobTerminated.class, this::onJobTerminated)\n        .build();\n  }\n\n  private Behavior<Command> onSpawnJob(SpawnJob message) {\n    getContext().getSystem().log().info(\"Spawning job {}!\", message.name);\n    ActorRef<Job.Command> job = getContext().spawn(Job.create(message.name), message.name);\n    getContext().watchWith(job, new JobTerminated(message.name, message.replyToWhenDone));\n    return this;\n  }\n\n  private Behavior<Command> onJobTerminated(JobTerminated terminated) {\n    getContext().getSystem().log().info(\"Job stopped: {}\", terminated.name);\n    terminated.replyToWhenDone.tell(new JobDone(terminated.name));\n    return this;\n  }\n}\nNote how the replyToWhenDone is included in the watchWith message and then used later when receiving the JobTerminated message.\nThe watched actor can be any ActorRefActorRef, it doesn’t have to be a child actor as in the above example.\nIt should be noted that the terminated message is generated independent of the order in which registration and termination occur. In particular, the watching actor will receive a terminated message even if the watched actor has already been terminated at the time of registration.\nRegistering multiple times does not necessarily lead to multiple messages being generated, but there is no guarantee that only exactly one such message is received: if termination of the watched actor has generated and queued the message, and another registration is done before this message has been processed, then a second message will be queued, because registering for monitoring of an already terminated actor leads to the immediate generation of the terminated message.\nIt is also possible to deregister from watching another actor’s liveliness using context.unwatch(target)context.unwatch(target). This works even if the terminated message has already been enqueued in the mailbox; after calling unwatch no terminated message for that actor will be processed anymore.\nThe terminated message is also sent when the watched actor is on a node that has been removed from the Cluster.","title":"Watching Actors"},{"location":"/typed/interaction-patterns.html","text":"","title":"Interaction Patterns"},{"location":"/typed/interaction-patterns.html#interaction-patterns","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Actors.","title":"Interaction Patterns"},{"location":"/typed/interaction-patterns.html#dependency","text":"To use Pekko Actor Typed, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/interaction-patterns.html#introduction","text":"Interacting with an Actor in Pekko is done through an ActorRef[T]ActorRef<T> where T is the type of messages the actor accepts, also known as the “protocol”. This ensures that only the right kind of messages can be sent to an actor and also that no one else but the Actor itself can access the Actor instance internals.\nMessage exchange with Actors follow a few common patterns, let’s go through each one of them.","title":"Introduction"},{"location":"/typed/interaction-patterns.html#fire-and-forget","text":"The fundamental way to interact with an actor is through “tell”, which is so common that it has a special symbolic method name: actorRef ! messageactorRef.tell(message). Sending a message with tell can safely be done from any thread.\nTell is asynchronous which means that the method returns right away. After the statement is executed there is no guarantee that the message has been processed by the recipient yet. It also means there is no way to know if the message was received, the processing succeeded or failed.\nExample:\nWith the given protocol and actor behavior:\nScala copysourceobject Printer {\n\n  case class PrintMe(message: String)\n\n  def apply(): Behavior[PrintMe] =\n    Behaviors.receive {\n      case (context, PrintMe(message)) =>\n        context.log.info(message)\n        Behaviors.same\n    }\n} Java copysourcepublic class Printer {\n  public static class PrintMe {\n    public final String message;\n\n    public PrintMe(String message) {\n      this.message = message;\n    }\n  }\n\n  public static Behavior<PrintMe> create() {\n    return Behaviors.setup(\n        context ->\n            Behaviors.receive(PrintMe.class)\n                .onMessage(\n                    PrintMe.class,\n                    printMe -> {\n                      context.getLog().info(printMe.message);\n                      return Behaviors.same();\n                    })\n                .build());\n  }\n}\nFire and forget looks like this:\nScala copysourceval system = ActorSystem(Printer(), \"fire-and-forget-sample\")\n\n// note how the system is also the top level actor ref\nval printer: ActorRef[Printer.PrintMe] = system\n\n// these are all fire and forget\nprinter ! Printer.PrintMe(\"message 1\")\nprinter ! Printer.PrintMe(\"not message 2\") Java copysourcefinal ActorSystem<Printer.PrintMe> system =\n    ActorSystem.create(Printer.create(), \"printer-sample-system\");\n\n// note that system is also the ActorRef to the guardian actor\nfinal ActorRef<Printer.PrintMe> ref = system;\n\n// these are all fire and forget\nref.tell(new Printer.PrintMe(\"message 1\"));\nref.tell(new Printer.PrintMe(\"message 2\"));\nUseful when:\nIt is not critical to be sure that the message was processed There is no way to act on non successful delivery or processing We want to minimize the number of messages created to get higher throughput (sending a response would require creating twice the number of messages)\nProblems:\nIf the inflow of messages is higher than the actor can process the inbox will fill up and can in the worst case cause the JVM crash with an OutOfMemoryError If the message gets lost, the sender will not know","title":"Fire and Forget"},{"location":"/typed/interaction-patterns.html#request-response","text":"Many interactions between actors require one or more response message being sent back from the receiving actor. A response message can be a result of a query, some form of acknowledgment that the message was received and processed or events that the request subscribed to.\nIn Pekko, the recipient of responses has to be encoded as a field in the message itself, which the recipient can then use to send (tell) a response back.\nExample:\nWith the following protocol:\nScala copysourcecase class Request(query: String, replyTo: ActorRef[Response])\ncase class Response(result: String) Java copysourcepublic static class Request {\n  public final String query;\n  public final ActorRef<Response> replyTo;\n\n  public Request(String query, ActorRef<Response> replyTo) {\n    this.query = query;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class Response {\n  public final String result;\n\n  public Response(String result) {\n    this.result = result;\n  }\n}\nThe sender would use its own ActorRef[Response]ActorRef<Response>, which it can access through ActorContext.selfActorContext.getSelf(), for the replyTo.\nScala copysourcecookieFabric ! CookieFabric.Request(\"give me cookies\", context.self) Java copysourcecookieFabric.tell(new CookieFabric.Request(\"give me cookies\", context.getSelf()));\nOn the receiving side the ActorRef[Response]ActorRef<Response> can then be used to send one or more responses back:\nScala copysourcedef apply(): Behaviors.Receive[Request] =\n  Behaviors.receiveMessage[Request] {\n    case Request(query, replyTo) =>\n      // ... process query ...\n      replyTo ! Response(s\"Here are the cookies for [$query]!\")\n      Behaviors.same\n  } Java copysource// actor behavior\npublic static Behavior<Request> create() {\n  return Behaviors.receive(Request.class)\n      .onMessage(Request.class, CookieFabric::onRequest)\n      .build();\n}\n\nprivate static Behavior<Request> onRequest(Request request) {\n  // ... process request ...\n  request.replyTo.tell(new Response(\"Here are the cookies for \" + request.query));\n  return Behaviors.same();\n}\nUseful when:\nSubscribing to an actor that will send many response messages back\nProblems:\nActors seldom have a response message from another actor as a part of their protocol (see adapted response) It is hard to detect that a message request was not delivered or processed (see ask) Unless the protocol already includes a way to provide context, for example a request id that is also sent in the response, it is not possible to tie an interaction to some specific context without introducing a new, separate, actor (see ask or per session child actor)","title":"Request-Response"},{"location":"/typed/interaction-patterns.html#adapted-response","text":"Most often the sending actor does not, and should not, support receiving the response messages of another actor. In such cases we need to provide an ActorRefActorRef of the right type and adapt the response message to a type that the sending actor can handle.\nExample:\nScala copysource object Backend {\n  sealed trait Request\n  final case class StartTranslationJob(taskId: Int, site: URI, replyTo: ActorRef[Response]) extends Request\n\n  sealed trait Response\n  final case class JobStarted(taskId: Int) extends Response\n  final case class JobProgress(taskId: Int, progress: Double) extends Response\n  final case class JobCompleted(taskId: Int, result: URI) extends Response\n}\n\nobject Frontend {\n\n  sealed trait Command\n  final case class Translate(site: URI, replyTo: ActorRef[URI]) extends Command\n  private final case class WrappedBackendResponse(response: Backend.Response) extends Command\n\n  def apply(backend: ActorRef[Backend.Request]): Behavior[Command] =\n    Behaviors.setup[Command] { context =>\n      val backendResponseMapper: ActorRef[Backend.Response] =\n        context.messageAdapter(rsp => WrappedBackendResponse(rsp))\n\n      def active(inProgress: Map[Int, ActorRef[URI]], count: Int): Behavior[Command] = {\n        Behaviors.receiveMessage[Command] {\n          case Translate(site, replyTo) =>\n            val taskId = count + 1\n            backend ! Backend.StartTranslationJob(taskId, site, backendResponseMapper)\n            active(inProgress.updated(taskId, replyTo), taskId)\n\n          case wrapped: WrappedBackendResponse =>\n            wrapped.response match {\n              case Backend.JobStarted(taskId) =>\n                context.log.info(\"Started {}\", taskId)\n                Behaviors.same\n              case Backend.JobProgress(taskId, progress) =>\n                context.log.info2(\"Progress {}: {}\", taskId, progress)\n                Behaviors.same\n              case Backend.JobCompleted(taskId, result) =>\n                context.log.info2(\"Completed {}: {}\", taskId, result)\n                inProgress(taskId) ! result\n                active(inProgress - taskId, count)\n            }\n        }\n      }\n\n      active(inProgress = Map.empty, count = 0)\n    }\n} Java copysource public class Backend {\n  public interface Request {}\n\n  public static class StartTranslationJob implements Request {\n    public final int taskId;\n    public final URI site;\n    public final ActorRef<Response> replyTo;\n\n    public StartTranslationJob(int taskId, URI site, ActorRef<Response> replyTo) {\n      this.taskId = taskId;\n      this.site = site;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public interface Response {}\n\n  public static class JobStarted implements Response {\n    public final int taskId;\n\n    public JobStarted(int taskId) {\n      this.taskId = taskId;\n    }\n  }\n\n  public static class JobProgress implements Response {\n    public final int taskId;\n    public final double progress;\n\n    public JobProgress(int taskId, double progress) {\n      this.taskId = taskId;\n      this.progress = progress;\n    }\n  }\n\n  public static class JobCompleted implements Response {\n    public final int taskId;\n    public final URI result;\n\n    public JobCompleted(int taskId, URI result) {\n      this.taskId = taskId;\n      this.result = result;\n    }\n  }\n}\n\npublic class Frontend {\n\n  public interface Command {}\n\n  public static class Translate implements Command {\n    public final URI site;\n    public final ActorRef<URI> replyTo;\n\n    public Translate(URI site, ActorRef<URI> replyTo) {\n      this.site = site;\n      this.replyTo = replyTo;\n    }\n  }\n\n  private static class WrappedBackendResponse implements Command {\n    final Backend.Response response;\n\n    public WrappedBackendResponse(Backend.Response response) {\n      this.response = response;\n    }\n  }\n\n  public static class Translator extends AbstractBehavior<Command> {\n    private final ActorRef<Backend.Request> backend;\n    private final ActorRef<Backend.Response> backendResponseAdapter;\n\n    private int taskIdCounter = 0;\n    private Map<Integer, ActorRef<URI>> inProgress = new HashMap<>();\n\n    public Translator(ActorContext<Command> context, ActorRef<Backend.Request> backend) {\n      super(context);\n      this.backend = backend;\n      this.backendResponseAdapter =\n          context.messageAdapter(Backend.Response.class, WrappedBackendResponse::new);\n    }\n\n    @Override\n    public Receive<Command> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(Translate.class, this::onTranslate)\n          .onMessage(WrappedBackendResponse.class, this::onWrappedBackendResponse)\n          .build();\n    }\n\n    private Behavior<Command> onTranslate(Translate cmd) {\n      taskIdCounter += 1;\n      inProgress.put(taskIdCounter, cmd.replyTo);\n      backend.tell(\n          new Backend.StartTranslationJob(taskIdCounter, cmd.site, backendResponseAdapter));\n      return this;\n    }\n\n    private Behavior<Command> onWrappedBackendResponse(WrappedBackendResponse wrapped) {\n      Backend.Response response = wrapped.response;\n      if (response instanceof Backend.JobStarted) {\n        Backend.JobStarted rsp = (Backend.JobStarted) response;\n        getContext().getLog().info(\"Started {}\", rsp.taskId);\n      } else if (response instanceof Backend.JobProgress) {\n        Backend.JobProgress rsp = (Backend.JobProgress) response;\n        getContext().getLog().info(\"Progress {}\", rsp.taskId);\n      } else if (response instanceof Backend.JobCompleted) {\n        Backend.JobCompleted rsp = (Backend.JobCompleted) response;\n        getContext().getLog().info(\"Completed {}\", rsp.taskId);\n        inProgress.get(rsp.taskId).tell(rsp.result);\n        inProgress.remove(rsp.taskId);\n      } else {\n        return Behaviors.unhandled();\n      }\n\n      return this;\n    }\n  }\n}\nYou can register several message adapters for different message classes. It’s only possible to have one message adapter per message class to make sure that the number of adapters are not growing unbounded if registered repeatedly. That also means that a registered adapter will replace an existing adapter for the same message class.\nA message adapter will be used if the message class matches the given class or is a subclass thereof. The registered adapters are tried in reverse order of their registration order, i.e. the last registered first.\nA message adapter (and the returned ActorRefActorRef) has the same lifecycle as the receiving actor. It’s recommended to register the adapters in a top level Behaviors.setupBehaviors.setup or constructor of AbstractBehaviorAbstractBehavior but it’s possible to register them later if needed.\nThe adapter function is running in the receiving actor and can safely access its state, but if it throws an exception the actor is stopped.\nUseful when:\nTranslating between different actor message protocols Subscribing to an actor that will send many response messages back\nProblems:\nIt is hard to detect that a message request was not delivered or processed (see ask) Only one adaption can be made per response message type, if a new one is registered the old one is replaced, for example different target actors can’t have different adaption if they use the same response types, unless some correlation is encoded in the messages Unless the protocol already includes a way to provide context, for example a request id that is also sent in the response, it is not possible to tie an interaction to some specific context without introducing a new, separate, actor","title":"Adapted Response"},{"location":"/typed/interaction-patterns.html#request-response-with-ask-between-two-actors","text":"In an interaction where there is a 1:1 mapping between a request and a response we can use ask on the ActorContextActorContext to interact with another actor.\nThe interaction has two steps, first we need to construct the outgoing message, to do that we need an ActorRef[Response]ActorRef<Response> to put as recipient in the outgoing message. The second step is to transform the successful Response or failure into a message that is part of the protocol of the sending actor. See also the Generic response wrapper for replies that are either a success or an error.\nExample:\nScala copysourceobject Hal {\n  sealed trait Command\n  case class OpenThePodBayDoorsPlease(replyTo: ActorRef[Response]) extends Command\n  case class Response(message: String)\n\n  def apply(): Behaviors.Receive[Hal.Command] =\n    Behaviors.receiveMessage[Command] {\n      case OpenThePodBayDoorsPlease(replyTo) =>\n        replyTo ! Response(\"I'm sorry, Dave. I'm afraid I can't do that.\")\n        Behaviors.same\n    }\n}\n\nobject Dave {\n\n  sealed trait Command\n  // this is a part of the protocol that is internal to the actor itself\n  private case class AdaptedResponse(message: String) extends Command\n\n  def apply(hal: ActorRef[Hal.Command]): Behavior[Dave.Command] =\n    Behaviors.setup[Command] { context =>\n      // asking someone requires a timeout, if the timeout hits without response\n      // the ask is failed with a TimeoutException\n      implicit val timeout: Timeout = 3.seconds\n\n      // Note: The second parameter list takes a function `ActorRef[T] => Message`,\n      // as OpenThePodBayDoorsPlease is a case class it has a factory apply method\n      // that is what we are passing as the second parameter here it could also be written\n      // as `ref => OpenThePodBayDoorsPlease(ref)`\n      context.ask(hal, Hal.OpenThePodBayDoorsPlease.apply) {\n        case Success(Hal.Response(message)) => AdaptedResponse(message)\n        case Failure(_)                     => AdaptedResponse(\"Request failed\")\n      }\n\n      // we can also tie in request context into an interaction, it is safe to look at\n      // actor internal state from the transformation function, but remember that it may have\n      // changed at the time the response arrives and the transformation is done, best is to\n      // use immutable state we have closed over like here.\n      val requestId = 1\n      context.ask(hal, Hal.OpenThePodBayDoorsPlease.apply) {\n        case Success(Hal.Response(message)) => AdaptedResponse(s\"$requestId: $message\")\n        case Failure(_)                     => AdaptedResponse(s\"$requestId: Request failed\")\n      }\n\n      Behaviors.receiveMessage {\n        // the adapted message ends up being processed like any other\n        // message sent to the actor\n        case AdaptedResponse(message) =>\n          context.log.info(\"Got response from hal: {}\", message)\n          Behaviors.same\n      }\n    }\n} Java copysourcepublic class Hal extends AbstractBehavior<Hal.Command> {\n\n  public static Behavior<Hal.Command> create() {\n    return Behaviors.setup(Hal::new);\n  }\n\n  private Hal(ActorContext<Command> context) {\n    super(context);\n  }\n\n  public interface Command {}\n\n  public static final class OpenThePodBayDoorsPlease implements Command {\n    public final ActorRef<HalResponse> respondTo;\n\n    public OpenThePodBayDoorsPlease(ActorRef<HalResponse> respondTo) {\n      this.respondTo = respondTo;\n    }\n  }\n\n  public static final class HalResponse {\n    public final String message;\n\n    public HalResponse(String message) {\n      this.message = message;\n    }\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(OpenThePodBayDoorsPlease.class, this::onOpenThePodBayDoorsPlease)\n        .build();\n  }\n\n  private Behavior<Command> onOpenThePodBayDoorsPlease(OpenThePodBayDoorsPlease message) {\n    message.respondTo.tell(new HalResponse(\"I'm sorry, Dave. I'm afraid I can't do that.\"));\n    return this;\n  }\n}\n\npublic class Dave extends AbstractBehavior<Dave.Command> {\n\n  public interface Command {}\n\n  // this is a part of the protocol that is internal to the actor itself\n  private static final class AdaptedResponse implements Command {\n    public final String message;\n\n    public AdaptedResponse(String message) {\n      this.message = message;\n    }\n  }\n\n  public static Behavior<Command> create(ActorRef<Hal.Command> hal) {\n    return Behaviors.setup(context -> new Dave(context, hal));\n  }\n\n  private Dave(ActorContext<Command> context, ActorRef<Hal.Command> hal) {\n    super(context);\n\n    // asking someone requires a timeout, if the timeout hits without response\n    // the ask is failed with a TimeoutException\n    final Duration timeout = Duration.ofSeconds(3);\n\n    context.ask(\n        Hal.HalResponse.class,\n        hal,\n        timeout,\n        // construct the outgoing message\n        (ActorRef<Hal.HalResponse> ref) -> new Hal.OpenThePodBayDoorsPlease(ref),\n        // adapt the response (or failure to respond)\n        (response, throwable) -> {\n          if (response != null) {\n            return new AdaptedResponse(response.message);\n          } else {\n            return new AdaptedResponse(\"Request failed\");\n          }\n        });\n\n    // we can also tie in request context into an interaction, it is safe to look at\n    // actor internal state from the transformation function, but remember that it may have\n    // changed at the time the response arrives and the transformation is done, best is to\n    // use immutable state we have closed over like here.\n    final int requestId = 1;\n    context.ask(\n        Hal.HalResponse.class,\n        hal,\n        timeout,\n        // construct the outgoing message\n        (ActorRef<Hal.HalResponse> ref) -> new Hal.OpenThePodBayDoorsPlease(ref),\n        // adapt the response (or failure to respond)\n        (response, throwable) -> {\n          if (response != null) {\n            return new AdaptedResponse(requestId + \": \" + response.message);\n          } else {\n            return new AdaptedResponse(requestId + \": Request failed\");\n          }\n        });\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        // the adapted message ends up being processed like any other\n        // message sent to the actor\n        .onMessage(AdaptedResponse.class, this::onAdaptedResponse)\n        .build();\n  }\n\n  private Behavior<Command> onAdaptedResponse(AdaptedResponse response) {\n    getContext().getLog().info(\"Got response from HAL: {}\", response.message);\n    return this;\n  }\n}\nThe response adapting function is running in the receiving actor and can safely access its state, but if it throws an exception the actor is stopped.\nUseful when:\nSingle response queries An actor needs to know that the message was processed before continuing To allow an actor to resend if a timely response is not produced To keep track of outstanding requests and not overwhelm a recipient with messages (“backpressure”) Context should be attached to the interaction but the protocol does not support that (request id, what query the response was for)\nProblems:\nThere can only be a single response to one ask (see per session child Actor) When ask times out, the receiving actor does not know and may still process it to completion, or even start processing it after the fact Finding a good value for the timeout, especially when ask triggers chained asks in the receiving actor. You want a short timeout to be responsive and answer back to the requester, but at the same time you do not want to have many false positives","title":"Request-Response with ask between two actors"},{"location":"/typed/interaction-patterns.html#request-response-with-ask-from-outside-an-actor","text":"Sometimes, you need to interact with actors from the outside of the actor system, this can be done with fire-and-forget as described above or through another version of ask that returns a Future[Response]CompletionStage<Response> that is either completed with a successful response or failed with a TimeoutException if there was no response within the specified timeout.\nTo do this we use ask (or the symbolic ?) implicitly added to ActorRef by org.apache.pekko.actor.typed.scaladsl.AskPattern._ to send a message to an actor and get a Future[Response] back. ask takes implicit Timeout and ActorSystem parameters. To do this we use org.apache.pekko.actor.typed.javadsl.AskPattern.ask to send a message to an actor and get a CompletionState[Response] back.\nExample:\nScala copysourceobject CookieFabric {\n  sealed trait Command\n  case class GiveMeCookies(count: Int, replyTo: ActorRef[Reply]) extends Command\n\n  sealed trait Reply\n  case class Cookies(count: Int) extends Reply\n  case class InvalidRequest(reason: String) extends Reply\n\n  def apply(): Behaviors.Receive[CookieFabric.GiveMeCookies] =\n    Behaviors.receiveMessage { message =>\n      if (message.count >= 5)\n        message.replyTo ! InvalidRequest(\"Too many cookies.\")\n      else\n        message.replyTo ! Cookies(message.count)\n      Behaviors.same\n    }\n}\n\nimport org.apache.pekko\nimport pekko.actor.typed.scaladsl.AskPattern._\nimport pekko.util.Timeout\n\n// asking someone requires a timeout if the timeout hits without response\n// the ask is failed with a TimeoutException\nimplicit val timeout: Timeout = 3.seconds\n// implicit ActorSystem in scope\nimplicit val system: ActorSystem[_] = theSystem\n\nval result: Future[CookieFabric.Reply] = cookieFabric.ask(ref => CookieFabric.GiveMeCookies(3, ref))\n\n// the response callback will be executed on this execution context\nimplicit val ec = system.executionContext\n\nresult.onComplete {\n  case Success(CookieFabric.Cookies(count))         => println(s\"Yay, $count cookies!\")\n  case Success(CookieFabric.InvalidRequest(reason)) => println(s\"No cookies for me. $reason\")\n  case Failure(ex)                                  => println(s\"Boo! didn't get cookies: ${ex.getMessage}\")\n} Java copysourcepublic class CookieFabric extends AbstractBehavior<CookieFabric.Command> {\n\n  interface Command {}\n\n  public static class GiveMeCookies implements Command {\n    public final int count;\n    public final ActorRef<Reply> replyTo;\n\n    public GiveMeCookies(int count, ActorRef<Reply> replyTo) {\n      this.count = count;\n      this.replyTo = replyTo;\n    }\n  }\n\n  interface Reply {}\n\n  public static class Cookies implements Reply {\n    public final int count;\n\n    public Cookies(int count) {\n      this.count = count;\n    }\n  }\n\n  public static class InvalidRequest implements Reply {\n    public final String reason;\n\n    public InvalidRequest(String reason) {\n      this.reason = reason;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(CookieFabric::new);\n  }\n\n  private CookieFabric(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder().onMessage(GiveMeCookies.class, this::onGiveMeCookies).build();\n  }\n\n  private Behavior<Command> onGiveMeCookies(GiveMeCookies request) {\n    if (request.count >= 5) request.replyTo.tell(new InvalidRequest(\"Too many cookies.\"));\n    else request.replyTo.tell(new Cookies(request.count));\n\n    return this;\n  }\n}\n\n  public void askAndPrint(\n      ActorSystem<Void> system, ActorRef<CookieFabric.Command> cookieFabric) {\n    CompletionStage<CookieFabric.Reply> result =\n        AskPattern.ask(\n            cookieFabric,\n            replyTo -> new CookieFabric.GiveMeCookies(3, replyTo),\n            // asking someone requires a timeout and a scheduler, if the timeout hits without\n            // response the ask is failed with a TimeoutException\n            Duration.ofSeconds(3),\n            system.scheduler());\n\n    result.whenComplete(\n        (reply, failure) -> {\n          if (reply instanceof CookieFabric.Cookies)\n            System.out.println(\"Yay, \" + ((CookieFabric.Cookies) reply).count + \" cookies!\");\n          else if (reply instanceof CookieFabric.InvalidRequest)\n            System.out.println(\n                \"No cookies for me. \" + ((CookieFabric.InvalidRequest) reply).reason);\n          else System.out.println(\"Boo! didn't get cookies in time. \" + failure);\n        });\n  }\nNote that validation errors are also explicit in the message protocol. The GiveMeCookies request can reply with Cookies or InvalidRequest. The requestor has to decide how to handle an InvalidRequest reply. Sometimes it should be treated as a failed FutureCompletionStage and for that the reply can be mapped on the requestor side. See also the Generic response wrapper for replies that are either a success or an error.\nScala copysourceval cookies: Future[CookieFabric.Cookies] =\n  cookieFabric.ask[CookieFabric.Reply](ref => CookieFabric.GiveMeCookies(3, ref)).flatMap {\n    case c: CookieFabric.Cookies             => Future.successful(c)\n    case CookieFabric.InvalidRequest(reason) => Future.failed(new IllegalArgumentException(reason))\n  }\n\ncookies.onComplete {\n  case Success(CookieFabric.Cookies(count)) => println(s\"Yay, $count cookies!\")\n  case Failure(ex)                          => println(s\"Boo! didn't get cookies: ${ex.getMessage}\")\n} Java copysourceCompletionStage<CookieFabric.Reply> result =\n    AskPattern.ask(\n        cookieFabric,\n        replyTo -> new CookieFabric.GiveMeCookies(3, replyTo),\n        Duration.ofSeconds(3),\n        system.scheduler());\n\nCompletionStage<CookieFabric.Cookies> cookies =\n    result.thenCompose(\n        (CookieFabric.Reply reply) -> {\n          if (reply instanceof CookieFabric.Cookies) {\n            return CompletableFuture.completedFuture((CookieFabric.Cookies) reply);\n          } else if (reply instanceof CookieFabric.InvalidRequest) {\n            CompletableFuture<CookieFabric.Cookies> failed = new CompletableFuture<>();\n            failed.completeExceptionally(\n                new IllegalArgumentException(((CookieFabric.InvalidRequest) reply).reason));\n            return failed;\n          } else {\n            throw new IllegalStateException(\"Unexpected reply: \" + reply.getClass());\n          }\n        });\n\ncookies.whenComplete(\n    (cookiesReply, failure) -> {\n      if (cookiesReply != null)\n        System.out.println(\"Yay, \" + cookiesReply.count + \" cookies!\");\n      else System.out.println(\"Boo! didn't get cookies in time. \" + failure);\n    });\nUseful when:\nQuerying an actor from outside of the actor system\nProblems:\nIt is easy to accidentally close over and unsafely mutable state with the callbacks on the returned FutureCompletionStage as those will be executed on a different thread There can only be a single response to one ask (see per session child Actor) When ask times out, the receiving actor does not know and may still process it to completion, or even start processing it after the fact","title":"Request-Response with ask from outside an Actor"},{"location":"/typed/interaction-patterns.html#generic-response-wrapper","text":"In many cases the response can either be a successful result or an error (a validation error that the command was invalid for example). Having to define two response classes and a shared supertype for every request type can be repetitive, especially in a cluster context where you also have to make sure the messages can be serialized to be sent over the network.\nTo help with this a generic status-response type is included in Pekko: StatusReplyStatusReply, everywhere where ask can be used there is also a second method askWithStatusaskWithStatus which, given that the response is a StatusReply will unwrap successful responses and help with handling validation errors. Pekko includes pre-built serializers for the type, so in the normal use case a clustered application only needs to provide a serializer for the successful result.\nFor the case where the successful reply does not contain an actual value but is more of an acknowledgment there is a pre defined StatusReply.AckStatusReply.ack() of type StatusReply[Done]StatusReply<Done>.\nErrors are preferably sent as a text describing what is wrong, but using exceptions to attach a type is also possible.\nExample actor to actor ask:\nScala copysourceobject Hal {\n  sealed trait Command\n  case class OpenThePodBayDoorsPlease(replyTo: ActorRef[StatusReply[String]]) extends Command\n\n  def apply(): Behaviors.Receive[Hal.Command] =\n    Behaviors.receiveMessage[Command] {\n      case OpenThePodBayDoorsPlease(replyTo) =>\n        // reply with a validation error description\n        replyTo ! StatusReply.Error(\"I'm sorry, Dave. I'm afraid I can't do that.\")\n        Behaviors.same\n    }\n}\n\nobject Dave {\n\n  sealed trait Command\n  // this is a part of the protocol that is internal to the actor itself\n  private case class AdaptedResponse(message: String) extends Command\n\n  def apply(hal: ActorRef[Hal.Command]): Behavior[Dave.Command] =\n    Behaviors.setup[Command] { context =>\n      // asking someone requires a timeout, if the timeout hits without response\n      // the ask is failed with a TimeoutException\n      implicit val timeout: Timeout = 3.seconds\n\n      // A StatusReply.Success(m) ends up as a Success(m) here, while a\n      // StatusReply.Error(text) becomes a Failure(ErrorMessage(text))\n      context.askWithStatus(hal, Hal.OpenThePodBayDoorsPlease.apply) {\n        case Success(message)                        => AdaptedResponse(message)\n        case Failure(StatusReply.ErrorMessage(text)) => AdaptedResponse(s\"Request denied: $text\")\n        case Failure(_)                              => AdaptedResponse(\"Request failed\")\n      }\n\n      Behaviors.receiveMessage {\n        // the adapted message ends up being processed like any other\n        // message sent to the actor\n        case AdaptedResponse(message) =>\n          context.log.info(\"Got response from hal: {}\", message)\n          Behaviors.same\n      }\n    }\n} Java copysourceimport org.apache.pekko.pattern.StatusReply;\n\npublic class Hal extends AbstractBehavior<Hal.Command> {\n\n  public static Behavior<Hal.Command> create() {\n    return Behaviors.setup(Hal::new);\n  }\n\n  private Hal(ActorContext<Hal.Command> context) {\n    super(context);\n  }\n\n  public interface Command {}\n\n  public static final class OpenThePodBayDoorsPlease implements Hal.Command {\n    public final ActorRef<StatusReply<String>> respondTo;\n\n    public OpenThePodBayDoorsPlease(ActorRef<StatusReply<String>> respondTo) {\n      this.respondTo = respondTo;\n    }\n  }\n\n  @Override\n  public Receive<Hal.Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Hal.OpenThePodBayDoorsPlease.class, this::onOpenThePodBayDoorsPlease)\n        .build();\n  }\n\n  private Behavior<Hal.Command> onOpenThePodBayDoorsPlease(\n      Hal.OpenThePodBayDoorsPlease message) {\n    message.respondTo.tell(StatusReply.error(\"I'm sorry, Dave. I'm afraid I can't do that.\"));\n    return this;\n  }\n}\n\npublic class Dave extends AbstractBehavior<Dave.Command> {\n\n  public interface Command {}\n\n  // this is a part of the protocol that is internal to the actor itself\n  private static final class AdaptedResponse implements Dave.Command {\n    public final String message;\n\n    public AdaptedResponse(String message) {\n      this.message = message;\n    }\n  }\n\n  public static Behavior<Dave.Command> create(ActorRef<Hal.Command> hal) {\n    return Behaviors.setup(context -> new Dave(context, hal));\n  }\n\n  private Dave(ActorContext<Dave.Command> context, ActorRef<Hal.Command> hal) {\n    super(context);\n\n    // asking someone requires a timeout, if the timeout hits without response\n    // the ask is failed with a TimeoutException\n    final Duration timeout = Duration.ofSeconds(3);\n\n    context.askWithStatus(\n        String.class,\n        hal,\n        timeout,\n        // construct the outgoing message\n        (ActorRef<StatusReply<String>> ref) -> new Hal.OpenThePodBayDoorsPlease(ref),\n        // adapt the response (or failure to respond)\n        (response, throwable) -> {\n          if (response != null) {\n            // a ReponseWithStatus.success(m) is unwrapped and passed as response\n            return new Dave.AdaptedResponse(response);\n          } else {\n            // a ResponseWithStatus.error will end up as a StatusReply.ErrorMessage()\n            // exception here\n            return new Dave.AdaptedResponse(\"Request failed: \" + throwable.getMessage());\n          }\n        });\n  }\n\n  @Override\n  public Receive<Dave.Command> createReceive() {\n    return newReceiveBuilder()\n        // the adapted message ends up being processed like any other\n        // message sent to the actor\n        .onMessage(Dave.AdaptedResponse.class, this::onAdaptedResponse)\n        .build();\n  }\n\n  private Behavior<Dave.Command> onAdaptedResponse(Dave.AdaptedResponse response) {\n    getContext().getLog().info(\"Got response from HAL: {}\", response.message);\n    return this;\n  }\n}\nA validation error is turned into a Failure for the message adapter. In this case we are explicitly handling the validation error separately from other ask failures.\nExample ask from the outside:\nScala copysourceobject CookieFabric {\n  sealed trait Command\n  case class GiveMeCookies(count: Int, replyTo: ActorRef[StatusReply[Cookies]]) extends Command\n  case class Cookies(count: Int)\n\n  def apply(): Behaviors.Receive[CookieFabric.GiveMeCookies] =\n    Behaviors.receiveMessage { message =>\n      if (message.count >= 5)\n        message.replyTo ! StatusReply.Error(\"Too many cookies.\")\n      else\n        message.replyTo ! StatusReply.Success(Cookies(message.count))\n      Behaviors.same\n    }\n}\n\nimport org.apache.pekko\nimport pekko.actor.typed.scaladsl.AskPattern._\nimport pekko.util.Timeout\n\n// asking someone requires a timeout if the timeout hits without response\n// the ask is failed with a TimeoutException\nimplicit val timeout: Timeout = 3.seconds\n// implicit ActorSystem in scope\nimplicit val system: ActorSystem[_] = theSystem\n\nval result: Future[CookieFabric.Cookies] = cookieFabric.askWithStatus(ref => CookieFabric.GiveMeCookies(3, ref))\n\n// the response callback will be executed on this execution context\nimplicit val ec = system.executionContext\n\nresult.onComplete {\n  case Success(CookieFabric.Cookies(count))      => println(s\"Yay, $count cookies!\")\n  case Failure(StatusReply.ErrorMessage(reason)) => println(s\"No cookies for me. $reason\")\n  case Failure(ex)                               => println(s\"Boo! didn't get cookies: ${ex.getMessage}\")\n} Java copysourcepublic class CookieFabric extends AbstractBehavior<CookieFabric.Command> {\n\n  interface Command {}\n\n  public static class GiveMeCookies implements CookieFabric.Command {\n    public final int count;\n    public final ActorRef<StatusReply<CookieFabric.Cookies>> replyTo;\n\n    public GiveMeCookies(int count, ActorRef<StatusReply<CookieFabric.Cookies>> replyTo) {\n      this.count = count;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Cookies {\n    public final int count;\n\n    public Cookies(int count) {\n      this.count = count;\n    }\n  }\n\n  public static Behavior<CookieFabric.Command> create() {\n    return Behaviors.setup(CookieFabric::new);\n  }\n\n  private CookieFabric(ActorContext<CookieFabric.Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<CookieFabric.Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(CookieFabric.GiveMeCookies.class, this::onGiveMeCookies)\n        .build();\n  }\n\n  private Behavior<CookieFabric.Command> onGiveMeCookies(CookieFabric.GiveMeCookies request) {\n    if (request.count >= 5) request.replyTo.tell(StatusReply.error(\"Too many cookies.\"));\n    else request.replyTo.tell(StatusReply.success(new CookieFabric.Cookies(request.count)));\n\n    return this;\n  }\n}\n\n  public void askAndPrint(\n      ActorSystem<Void> system, ActorRef<CookieFabric.Command> cookieFabric) {\n    CompletionStage<CookieFabric.Cookies> result =\n        AskPattern.askWithStatus(\n            cookieFabric,\n            replyTo -> new CookieFabric.GiveMeCookies(3, replyTo),\n            // asking someone requires a timeout and a scheduler, if the timeout hits without\n            // response the ask is failed with a TimeoutException\n            Duration.ofSeconds(3),\n            system.scheduler());\n\n    result.whenComplete(\n        (reply, failure) -> {\n          if (reply != null) System.out.println(\"Yay, \" + reply.count + \" cookies!\");\n          else if (failure instanceof StatusReply.ErrorMessage)\n            System.out.println(\"No cookies for me. \" + failure.getMessage());\n          else System.out.println(\"Boo! didn't get cookies in time. \" + failure);\n        });\n  }\nNote that validation errors are also explicit in the message protocol, but encoded as the wrapper type, constructed using StatusReply.Error(text)StatusReply.error(text):\nScala copysourceval cookies: Future[CookieFabric.Cookies] =\n  cookieFabric.askWithStatus[CookieFabric.Cookies](ref => CookieFabric.GiveMeCookies(3, ref)).flatMap {\n    case c: CookieFabric.Cookies => Future.successful(c)\n  }\n\ncookies.onComplete {\n  case Success(CookieFabric.Cookies(count)) => println(s\"Yay, $count cookies!\")\n  case Failure(ex)                          => println(s\"Boo! didn't get cookies: ${ex.getMessage}\")\n} Java copysourceCompletionStage<CookieFabric.Cookies> cookies =\n    AskPattern.askWithStatus(\n        cookieFabric,\n        replyTo -> new CookieFabric.GiveMeCookies(3, replyTo),\n        Duration.ofSeconds(3),\n        system.scheduler());\n\ncookies.whenComplete(\n    (cookiesReply, failure) -> {\n      if (cookiesReply != null)\n        System.out.println(\"Yay, \" + cookiesReply.count + \" cookies!\");\n      else System.out.println(\"Boo! didn't get cookies in time. \" + failure);\n    });","title":"Generic response wrapper"},{"location":"/typed/interaction-patterns.html#ignoring-replies","text":"In some situations an actor has a response for a particular request message but you are not interested in the response. In this case you can pass system.ignoreRefsystem.ignoreRef() turning the request-response into a fire-and-forget.\nsystem.ignoreRefsystem.ignoreRef(), as the name indicates, returns an ActorRefActorRef that ignores any message sent to it.\nWith the same protocol as the request response above, if the sender would prefer to ignore the reply it could pass system.ignoreRefsystem.ignoreRef() for the replyTo, which it can access through ActorContext.system.ignoreRefActorContext.getSystem().ignoreRef().\nScala copysourcecookieFabric ! CookieFabric.Request(\"don't send cookies back\", context.system.ignoreRef) Java copysourcecookieFabric.tell(\n    new CookieFabric.Request(\"don't send cookies back\", context.getSystem().ignoreRef()));\nUseful when:\nSending a message for which the protocol defines a reply, but you are not interested in getting the reply\nProblems:\nThe returned ActorRefActorRef ignores all messages sent to it, therefore it should be used carefully.\nPassing it around inadvertently as if it was a normal ActorRef may result in broken actor-to-actor interactions. Using it when performing an ask from outside the Actor System will cause the FutureCompletionStage returned by the ask to timeout since it will never complete. Finally, it’s legal to watchwatch it, but since it’s of a special kind, it never terminates and therefore you will never receive a TerminatedTerminated signal from it.","title":"Ignoring replies"},{"location":"/typed/interaction-patterns.html#send-future-result-to-self","text":"When using an API that returns a FutureCompletionStage from an actor it’s common that you would like to use the value of the response in the actor when the FutureCompletionStage is completed. For this purpose the ActorContext provides a pipeToSelfpipeToSelf method.\nExample:\nAn actor, CustomerRepository, is invoking a method on CustomerDataAccess that returns a FutureCompletionStage.\nScala copysource trait CustomerDataAccess {\n  def update(value: Customer): Future[Done]\n}\n\nfinal case class Customer(id: String, version: Long, name: String, address: String)\n\nobject CustomerRepository {\n  sealed trait Command\n\n  final case class Update(value: Customer, replyTo: ActorRef[UpdateResult]) extends Command\n  sealed trait UpdateResult\n  final case class UpdateSuccess(id: String) extends UpdateResult\n  final case class UpdateFailure(id: String, reason: String) extends UpdateResult\n  private final case class WrappedUpdateResult(result: UpdateResult, replyTo: ActorRef[UpdateResult])\n      extends Command\n\n  private val MaxOperationsInProgress = 10\n\n  def apply(dataAccess: CustomerDataAccess): Behavior[Command] = {\n    next(dataAccess, operationsInProgress = 0)\n  }\n\n  private def next(dataAccess: CustomerDataAccess, operationsInProgress: Int): Behavior[Command] = {\n    Behaviors.receive { (context, command) =>\n      command match {\n        case Update(value, replyTo) =>\n          if (operationsInProgress == MaxOperationsInProgress) {\n            replyTo ! UpdateFailure(value.id, s\"Max $MaxOperationsInProgress concurrent operations supported\")\n            Behaviors.same\n          } else {\n            val futureResult = dataAccess.update(value)\n            context.pipeToSelf(futureResult) {\n              // map the Future value to a message, handled by this actor\n              case Success(_) => WrappedUpdateResult(UpdateSuccess(value.id), replyTo)\n              case Failure(e) => WrappedUpdateResult(UpdateFailure(value.id, e.getMessage), replyTo)\n            }\n            // increase operationsInProgress counter\n            next(dataAccess, operationsInProgress + 1)\n          }\n\n        case WrappedUpdateResult(result, replyTo) =>\n          // send result to original requestor\n          replyTo ! result\n          // decrease operationsInProgress counter\n          next(dataAccess, operationsInProgress - 1)\n      }\n    }\n  }\n} Java copysourcepublic interface CustomerDataAccess {\n  CompletionStage<Done> update(Customer customer);\n}\n\npublic class Customer {\n  public final String id;\n  public final long version;\n  public final String name;\n  public final String address;\n\n  public Customer(String id, long version, String name, String address) {\n    this.id = id;\n    this.version = version;\n    this.name = name;\n    this.address = address;\n  }\n}\n\npublic class CustomerRepository extends AbstractBehavior<CustomerRepository.Command> {\n\n  private static final int MAX_OPERATIONS_IN_PROGRESS = 10;\n\n  interface Command {}\n\n  public static class Update implements Command {\n    public final Customer customer;\n    public final ActorRef<OperationResult> replyTo;\n\n    public Update(Customer customer, ActorRef<OperationResult> replyTo) {\n      this.customer = customer;\n      this.replyTo = replyTo;\n    }\n  }\n\n  interface OperationResult {}\n\n  public static class UpdateSuccess implements OperationResult {\n    public final String id;\n\n    public UpdateSuccess(String id) {\n      this.id = id;\n    }\n  }\n\n  public static class UpdateFailure implements OperationResult {\n    public final String id;\n    public final String reason;\n\n    public UpdateFailure(String id, String reason) {\n      this.id = id;\n      this.reason = reason;\n    }\n  }\n\n  private static class WrappedUpdateResult implements Command {\n    public final OperationResult result;\n    public final ActorRef<OperationResult> replyTo;\n\n    private WrappedUpdateResult(OperationResult result, ActorRef<OperationResult> replyTo) {\n      this.result = result;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static Behavior<Command> create(CustomerDataAccess dataAccess) {\n    return Behaviors.setup(context -> new CustomerRepository(context, dataAccess));\n  }\n\n  private final CustomerDataAccess dataAccess;\n  private int operationsInProgress = 0;\n\n  private CustomerRepository(ActorContext<Command> context, CustomerDataAccess dataAccess) {\n    super(context);\n    this.dataAccess = dataAccess;\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Update.class, this::onUpdate)\n        .onMessage(WrappedUpdateResult.class, this::onUpdateResult)\n        .build();\n  }\n\n  private Behavior<Command> onUpdate(Update command) {\n    if (operationsInProgress == MAX_OPERATIONS_IN_PROGRESS) {\n      command.replyTo.tell(\n          new UpdateFailure(\n              command.customer.id,\n              \"Max \" + MAX_OPERATIONS_IN_PROGRESS + \" concurrent operations supported\"));\n    } else {\n      // increase operationsInProgress counter\n      operationsInProgress++;\n      CompletionStage<Done> futureResult = dataAccess.update(command.customer);\n      getContext()\n          .pipeToSelf(\n              futureResult,\n              (ok, exc) -> {\n                if (exc == null)\n                  return new WrappedUpdateResult(\n                      new UpdateSuccess(command.customer.id), command.replyTo);\n                else\n                  return new WrappedUpdateResult(\n                      new UpdateFailure(command.customer.id, exc.getMessage()),\n                      command.replyTo);\n              });\n    }\n    return this;\n  }\n\n  private Behavior<Command> onUpdateResult(WrappedUpdateResult wrapped) {\n    // decrease operationsInProgress counter\n    operationsInProgress--;\n    // send result to original requestor\n    wrapped.replyTo.tell(wrapped.result);\n    return this;\n  }\n}\nIt could be tempting to just use onComplete on the Futurea callback on the CompletionStage, but that introduces the risk of accessing internal state of the actor that is not thread-safe from an external thread. For example, the numberOfPendingOperations counter in above example can’t be accessed from such callback. Therefore it is better to map the result to a message and perform further processing when receiving that message.\nUseful when:\nAccessing APIs that are returning FutureCompletionStage from an actor, such as a database or an external service The actor needs to continue processing when the FutureCompletionStage has completed Keep context from the original request and use that when the FutureCompletionStage has completed, for example an replyTo actor reference\nProblems:\nBoilerplate of adding wrapper messages for the results","title":"Send Future result to self"},{"location":"/typed/interaction-patterns.html#per-session-child-actor","text":"In some cases a complete response to a request can only be created and sent back after collecting multiple answers from other actors. For these kinds of interaction it can be good to delegate the work to a per “session” child actor. The child could also contain arbitrary logic to implement retrying, failing on timeout, tail chopping, progress inspection etc.\nNote that this is essentially how ask is implemented, if all you need is a single response with a timeout it is better to use ask.\nThe child is created with the context it needs to do the work, including an ActorRefActorRef that it can respond to. When the complete result is there the child responds with the result and stops itself.\nAs the protocol of the session actor is not a public API but rather an implementation detail of the parent actor, it may not always make sense to have an explicit protocol and adapt the messages of the actors that the session actor interacts with. For this use case it is possible to express that the actor can receive any message (AnyObject).\nExample:\nScala copysource// dummy data types just for this sample\ncase class Keys()\ncase class Wallet()\n\n\nobject Home {\n  sealed trait Command\n  case class LeaveHome(who: String, replyTo: ActorRef[ReadyToLeaveHome]) extends Command\n  case class ReadyToLeaveHome(who: String, keys: Keys, wallet: Wallet)\n\n  def apply(): Behavior[Command] = {\n    Behaviors.setup[Command] { context =>\n      val keyCabinet: ActorRef[KeyCabinet.GetKeys] = context.spawn(KeyCabinet(), \"key-cabinet\")\n      val drawer: ActorRef[Drawer.GetWallet] = context.spawn(Drawer(), \"drawer\")\n\n      Behaviors.receiveMessage[Command] {\n        case LeaveHome(who, replyTo) =>\n          context.spawn(prepareToLeaveHome(who, replyTo, keyCabinet, drawer), s\"leaving-$who\")\n          Behaviors.same\n      }\n    }\n  }\n\n  // per session actor behavior\n  def prepareToLeaveHome(\n      whoIsLeaving: String,\n      replyTo: ActorRef[ReadyToLeaveHome],\n      keyCabinet: ActorRef[KeyCabinet.GetKeys],\n      drawer: ActorRef[Drawer.GetWallet]): Behavior[NotUsed] = {\n    // we don't _really_ care about the actor protocol here as nobody will send us\n    // messages except for responses to our queries, so we just accept any kind of message\n    // but narrow that to more limited types when we interact\n    Behaviors\n      .setup[AnyRef] { context =>\n        var wallet: Option[Wallet] = None\n        var keys: Option[Keys] = None\n\n        // we narrow the ActorRef type to any subtype of the actual type we accept\n        keyCabinet ! KeyCabinet.GetKeys(whoIsLeaving, context.self.narrow[Keys])\n        drawer ! Drawer.GetWallet(whoIsLeaving, context.self.narrow[Wallet])\n\n        def nextBehavior(): Behavior[AnyRef] =\n          (keys, wallet) match {\n            case (Some(w), Some(k)) =>\n              // we got both, \"session\" is completed!\n              replyTo ! ReadyToLeaveHome(whoIsLeaving, w, k)\n              Behaviors.stopped\n\n            case _ =>\n              Behaviors.same\n          }\n\n        Behaviors.receiveMessage {\n          case w: Wallet =>\n            wallet = Some(w)\n            nextBehavior()\n          case k: Keys =>\n            keys = Some(k)\n            nextBehavior()\n          case _ =>\n            Behaviors.unhandled\n        }\n      }\n      .narrow[NotUsed] // we don't let anyone else know we accept anything\n  }\n} Java copysource// dummy data types just for this sample\npublic class Keys {}\n\npublic class Wallet {}\n\npublic class KeyCabinet {\n  public static class GetKeys {\n    public final String whoseKeys;\n    public final ActorRef<Keys> replyTo;\n\n    public GetKeys(String whoseKeys, ActorRef<Keys> respondTo) {\n      this.whoseKeys = whoseKeys;\n      this.replyTo = respondTo;\n    }\n  }\n\n  public static Behavior<GetKeys> create() {\n    return Behaviors.receiveMessage(KeyCabinet::onGetKeys);\n  }\n\n  private static Behavior<GetKeys> onGetKeys(GetKeys message) {\n    message.replyTo.tell(new Keys());\n    return Behaviors.same();\n  }\n}\n\npublic class Drawer {\n\n  public static class GetWallet {\n    public final String whoseWallet;\n    public final ActorRef<Wallet> replyTo;\n\n    public GetWallet(String whoseWallet, ActorRef<Wallet> replyTo) {\n      this.whoseWallet = whoseWallet;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static Behavior<GetWallet> create() {\n    return Behaviors.receiveMessage(Drawer::onGetWallet);\n  }\n\n  private static Behavior<GetWallet> onGetWallet(GetWallet message) {\n    message.replyTo.tell(new Wallet());\n    return Behaviors.same();\n  }\n}\n\npublic class Home {\n\n  public interface Command {}\n\n  public static class LeaveHome implements Command {\n    public final String who;\n    public final ActorRef<ReadyToLeaveHome> respondTo;\n\n    public LeaveHome(String who, ActorRef<ReadyToLeaveHome> respondTo) {\n      this.who = who;\n      this.respondTo = respondTo;\n    }\n  }\n\n  public static class ReadyToLeaveHome {\n    public final String who;\n    public final Keys keys;\n    public final Wallet wallet;\n\n    public ReadyToLeaveHome(String who, Keys keys, Wallet wallet) {\n      this.who = who;\n      this.keys = keys;\n      this.wallet = wallet;\n    }\n  }\n\n  private final ActorContext<Command> context;\n\n  private final ActorRef<KeyCabinet.GetKeys> keyCabinet;\n  private final ActorRef<Drawer.GetWallet> drawer;\n\n  private Home(ActorContext<Command> context) {\n    this.context = context;\n    this.keyCabinet = context.spawn(KeyCabinet.create(), \"key-cabinet\");\n    this.drawer = context.spawn(Drawer.create(), \"drawer\");\n  }\n\n  private Behavior<Command> behavior() {\n    return Behaviors.receive(Command.class)\n        .onMessage(LeaveHome.class, this::onLeaveHome)\n        .build();\n  }\n\n  private Behavior<Command> onLeaveHome(LeaveHome message) {\n    context.spawn(\n        PrepareToLeaveHome.create(message.who, message.respondTo, keyCabinet, drawer),\n        \"leaving\" + message.who);\n    return Behaviors.same();\n  }\n\n  // actor behavior\n  public static Behavior<Command> create() {\n    return Behaviors.setup(context -> new Home(context).behavior());\n  }\n}\n\n// per session actor behavior\nclass PrepareToLeaveHome extends AbstractBehavior<Object> {\n  static Behavior<Object> create(\n      String whoIsLeaving,\n      ActorRef<Home.ReadyToLeaveHome> replyTo,\n      ActorRef<KeyCabinet.GetKeys> keyCabinet,\n      ActorRef<Drawer.GetWallet> drawer) {\n    return Behaviors.setup(\n        context -> new PrepareToLeaveHome(context, whoIsLeaving, replyTo, keyCabinet, drawer));\n  }\n\n  private final String whoIsLeaving;\n  private final ActorRef<Home.ReadyToLeaveHome> replyTo;\n  private final ActorRef<KeyCabinet.GetKeys> keyCabinet;\n  private final ActorRef<Drawer.GetWallet> drawer;\n  private Optional<Wallet> wallet = Optional.empty();\n  private Optional<Keys> keys = Optional.empty();\n\n  private PrepareToLeaveHome(\n      ActorContext<Object> context,\n      String whoIsLeaving,\n      ActorRef<Home.ReadyToLeaveHome> replyTo,\n      ActorRef<KeyCabinet.GetKeys> keyCabinet,\n      ActorRef<Drawer.GetWallet> drawer) {\n    super(context);\n    this.whoIsLeaving = whoIsLeaving;\n    this.replyTo = replyTo;\n    this.keyCabinet = keyCabinet;\n    this.drawer = drawer;\n  }\n\n  @Override\n  public Receive<Object> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Wallet.class, this::onWallet)\n        .onMessage(Keys.class, this::onKeys)\n        .build();\n  }\n\n  private Behavior<Object> onWallet(Wallet wallet) {\n    this.wallet = Optional.of(wallet);\n    return completeOrContinue();\n  }\n\n  private Behavior<Object> onKeys(Keys keys) {\n    this.keys = Optional.of(keys);\n    return completeOrContinue();\n  }\n\n  private Behavior<Object> completeOrContinue() {\n    if (wallet.isPresent() && keys.isPresent()) {\n      replyTo.tell(new Home.ReadyToLeaveHome(whoIsLeaving, keys.get(), wallet.get()));\n      return Behaviors.stopped();\n    } else {\n      return this;\n    }\n  }\n}\nIn an actual session child you would likely want to include some form of timeout as well (see scheduling messages to self).\nUseful when:\nA single incoming request should result in multiple interactions with other actors before a result can be built, for example aggregation of several results You need to handle acknowledgement and retry messages for at-least-once delivery\nProblems:\nChildren have life cycles that must be managed to not create a resource leak, it can be easy to miss a scenario where the session actor is not stopped It increases complexity, since each such child can execute concurrently with other children and the parent","title":"Per session child Actor"},{"location":"/typed/interaction-patterns.html#general-purpose-response-aggregator","text":"This is similar to above Per session child Actor pattern. Sometimes, you might end up repeating the same way of aggregating replies and want to extract that to a reusable actor.\nThere are many variations of this pattern and that is the reason this is provided as a documentation example rather than a built in BehaviorBehavior in Pekko. It is intended to be adjusted to your specific needs.\nExample:\nThis example is an aggregator of expected number of replies. Requests for quotes are sent with the given sendRequests function to the two hotel actors, which both speak different protocols. When both expected replies have been collected they are aggregated with the given aggregateReplies function and sent back to the replyTo. If replies don’t arrive within the timeout the replies so far are aggregated and sent back to the replyTo.\nScala copysourceobject Hotel1 {\n  final case class RequestQuote(replyTo: ActorRef[Quote])\n  final case class Quote(hotel: String, price: BigDecimal)\n}\nobject Hotel2 {\n  final case class RequestPrice(replyTo: ActorRef[Price])\n  final case class Price(hotel: String, price: BigDecimal)\n}\n\n// Any since no common type between Hotel1 and Hotel2\ntype Reply = Any\n\nobject HotelCustomer {\n  sealed trait Command\n  final case class Quote(hotel: String, price: BigDecimal)\n  final case class AggregatedQuotes(quotes: List[Quote]) extends Command\n\n  def apply(hotel1: ActorRef[Hotel1.RequestQuote], hotel2: ActorRef[Hotel2.RequestPrice]): Behavior[Command] = {\n\n    Behaviors.setup[Command] { context =>\n      context.spawnAnonymous(\n        Aggregator[Reply, AggregatedQuotes](\n          sendRequests = { replyTo =>\n            hotel1 ! Hotel1.RequestQuote(replyTo)\n            hotel2 ! Hotel2.RequestPrice(replyTo)\n          },\n          expectedReplies = 2,\n          context.self,\n          aggregateReplies = replies =>\n            // The hotels have different protocols with different replies,\n            // convert them to `HotelCustomer.Quote` that this actor understands.\n            AggregatedQuotes(\n              replies\n                .map {\n                  case Hotel1.Quote(hotel, price) => Quote(hotel, price)\n                  case Hotel2.Price(hotel, price) => Quote(hotel, price)\n                  case unknown                    => throw new RuntimeException(s\"Unknown reply $unknown\")\n                }\n                .sortBy(_.price)\n                .toList),\n          timeout = 5.seconds))\n\n      Behaviors.receiveMessage {\n        case AggregatedQuotes(quotes) =>\n          context.log.info(\"Best {}\", quotes.headOption.getOrElse(\"Quote N/A\"))\n          Behaviors.same\n      }\n    }\n  }\n} Java copysourcepublic class Hotel1 {\n  public static class RequestQuote {\n    public final ActorRef<Quote> replyTo;\n\n    public RequestQuote(ActorRef<Quote> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Quote {\n    public final String hotel;\n    public final BigDecimal price;\n\n    public Quote(String hotel, BigDecimal price) {\n      this.hotel = hotel;\n      this.price = price;\n    }\n  }\n}\n\npublic class Hotel2 {\n  public static class RequestPrice {\n    public final ActorRef<Price> replyTo;\n\n    public RequestPrice(ActorRef<Price> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Price {\n    public final String hotel;\n    public final BigDecimal price;\n\n    public Price(String hotel, BigDecimal price) {\n      this.hotel = hotel;\n      this.price = price;\n    }\n  }\n}\n\npublic class HotelCustomer extends AbstractBehavior<HotelCustomer.Command> {\n\n  interface Command {}\n\n  public static class Quote {\n    public final String hotel;\n    public final BigDecimal price;\n\n    public Quote(String hotel, BigDecimal price) {\n      this.hotel = hotel;\n      this.price = price;\n    }\n  }\n\n  public static class AggregatedQuotes implements Command {\n    public final List<Quote> quotes;\n\n    public AggregatedQuotes(List<Quote> quotes) {\n      this.quotes = quotes;\n    }\n  }\n\n  public static Behavior<Command> create(\n      ActorRef<Hotel1.RequestQuote> hotel1, ActorRef<Hotel2.RequestPrice> hotel2) {\n    return Behaviors.setup(context -> new HotelCustomer(context, hotel1, hotel2));\n  }\n\n  public HotelCustomer(\n      ActorContext<Command> context,\n      ActorRef<Hotel1.RequestQuote> hotel1,\n      ActorRef<Hotel2.RequestPrice> hotel2) {\n    super(context);\n\n    Consumer<ActorRef<Object>> sendRequests =\n        replyTo -> {\n          hotel1.tell(new Hotel1.RequestQuote(replyTo.narrow()));\n          hotel2.tell(new Hotel2.RequestPrice(replyTo.narrow()));\n        };\n\n    int expectedReplies = 2;\n    // Object since no common type between Hotel1 and Hotel2\n    context.spawnAnonymous(\n        Aggregator.create(\n            Object.class,\n            sendRequests,\n            expectedReplies,\n            context.getSelf(),\n            this::aggregateReplies,\n            Duration.ofSeconds(5)));\n  }\n\n  private AggregatedQuotes aggregateReplies(List<Object> replies) {\n    List<Quote> quotes =\n        replies.stream()\n            .map(\n                r -> {\n                  // The hotels have different protocols with different replies,\n                  // convert them to `HotelCustomer.Quote` that this actor understands.\n                  if (r instanceof Hotel1.Quote) {\n                    Hotel1.Quote q = (Hotel1.Quote) r;\n                    return new Quote(q.hotel, q.price);\n                  } else if (r instanceof Hotel2.Price) {\n                    Hotel2.Price p = (Hotel2.Price) r;\n                    return new Quote(p.hotel, p.price);\n                  } else {\n                    throw new IllegalArgumentException(\"Unknown reply \" + r);\n                  }\n                })\n            .sorted((a, b) -> a.price.compareTo(b.price))\n            .collect(Collectors.toList());\n\n    return new AggregatedQuotes(quotes);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(AggregatedQuotes.class, this::onAggregatedQuotes)\n        .build();\n  }\n\n  private Behavior<Command> onAggregatedQuotes(AggregatedQuotes aggregated) {\n    if (aggregated.quotes.isEmpty()) getContext().getLog().info(\"Best Quote N/A\");\n    else getContext().getLog().info(\"Best {}\", aggregated.quotes.get(0));\n    return this;\n  }\n}\nThe implementation of the Aggregator:\nScala copysourceimport scala.collection.immutable\nimport scala.concurrent.duration.FiniteDuration\nimport scala.reflect.ClassTag\n\nimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject Aggregator {\n\n  sealed trait Command\n  private case object ReceiveTimeout extends Command\n  private case class WrappedReply[R](reply: R) extends Command\n\n  def apply[Reply: ClassTag, Aggregate](\n      sendRequests: ActorRef[Reply] => Unit,\n      expectedReplies: Int,\n      replyTo: ActorRef[Aggregate],\n      aggregateReplies: immutable.IndexedSeq[Reply] => Aggregate,\n      timeout: FiniteDuration): Behavior[Command] = {\n    Behaviors.setup { context =>\n      context.setReceiveTimeout(timeout, ReceiveTimeout)\n      val replyAdapter = context.messageAdapter[Reply](WrappedReply(_))\n      sendRequests(replyAdapter)\n\n      def collecting(replies: immutable.IndexedSeq[Reply]): Behavior[Command] = {\n        Behaviors.receiveMessage {\n          case WrappedReply(reply) =>\n            val newReplies = replies :+ reply.asInstanceOf[Reply]\n            if (newReplies.size == expectedReplies) {\n              val result = aggregateReplies(newReplies)\n              replyTo ! result\n              Behaviors.stopped\n            } else\n              collecting(newReplies)\n\n          case ReceiveTimeout =>\n            val aggregate = aggregateReplies(replies)\n            replyTo ! aggregate\n            Behaviors.stopped\n        }\n      }\n\n      collecting(Vector.empty)\n    }\n  }\n\n} Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\nimport java.time.Duration;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\n\npublic class Aggregator<Reply, Aggregate> extends AbstractBehavior<Aggregator.Command> {\n\n  interface Command {}\n\n  private enum ReceiveTimeout implements Command {\n    INSTANCE\n  }\n\n  private class WrappedReply implements Command {\n    final Reply reply;\n\n    private WrappedReply(Reply reply) {\n      this.reply = reply;\n    }\n  }\n\n  public static <R, A> Behavior<Command> create(\n      Class<R> replyClass,\n      Consumer<ActorRef<R>> sendRequests,\n      int expectedReplies,\n      ActorRef<A> replyTo,\n      Function<List<R>, A> aggregateReplies,\n      Duration timeout) {\n    return Behaviors.setup(\n        context ->\n            new Aggregator<R, A>(\n                replyClass,\n                context,\n                sendRequests,\n                expectedReplies,\n                replyTo,\n                aggregateReplies,\n                timeout));\n  }\n\n  private final int expectedReplies;\n  private final ActorRef<Aggregate> replyTo;\n  private final Function<List<Reply>, Aggregate> aggregateReplies;\n  private final List<Reply> replies = new ArrayList<>();\n\n  private Aggregator(\n      Class<Reply> replyClass,\n      ActorContext<Command> context,\n      Consumer<ActorRef<Reply>> sendRequests,\n      int expectedReplies,\n      ActorRef<Aggregate> replyTo,\n      Function<List<Reply>, Aggregate> aggregateReplies,\n      Duration timeout) {\n    super(context);\n    this.expectedReplies = expectedReplies;\n    this.replyTo = replyTo;\n    this.aggregateReplies = aggregateReplies;\n\n    context.setReceiveTimeout(timeout, ReceiveTimeout.INSTANCE);\n\n    ActorRef<Reply> replyAdapter = context.messageAdapter(replyClass, WrappedReply::new);\n    sendRequests.accept(replyAdapter);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(WrappedReply.class, this::onReply)\n        .onMessage(ReceiveTimeout.class, notUsed -> onReceiveTimeout())\n        .build();\n  }\n\n  private Behavior<Command> onReply(WrappedReply wrappedReply) {\n    Reply reply = wrappedReply.reply;\n    replies.add(reply);\n    if (replies.size() == expectedReplies) {\n      Aggregate result = aggregateReplies.apply(replies);\n      replyTo.tell(result);\n      return Behaviors.stopped();\n    } else {\n      return this;\n    }\n  }\n\n  private Behavior<Command> onReceiveTimeout() {\n    Aggregate result = aggregateReplies.apply(replies);\n    replyTo.tell(result);\n    return Behaviors.stopped();\n  }\n}\nUseful when:\nAggregating replies are performed in the same way at multiple places and should be extracted to a more general purpose actor. A single incoming request should result in multiple interactions with other actors before a result can be built, for example aggregation of several results You need to handle acknowledgement and retry messages for at-least-once delivery\nProblems:\nMessage protocols with generic types are difficult since the generic types are erased in runtime Children have life cycles that must be managed to not create a resource leak, it can be easy to miss a scenario where the session actor is not stopped It increases complexity, since each such child can execute concurrently with other children and the parent","title":"General purpose response aggregator"},{"location":"/typed/interaction-patterns.html#latency-tail-chopping","text":"This is a variation of above General purpose response aggregator pattern.\nThe goal of this algorithm is to decrease tail latencies (“chop off the tail latency”) in situations where multiple destination actors can perform the same piece of work, and where an actor may occasionally respond more slowly than expected. In this case, sending the same work request (also known as a “backup request”) to another actor results in decreased response time - because it’s less probable that multiple actors are under heavy load simultaneously. This technique is explained in depth in Jeff Dean’s presentation on Achieving Rapid Response Times in Large Online Services.\nThere are many variations of this pattern and that is the reason this is provided as a documentation example rather than a built in BehaviorBehavior in Pekko. It is intended to be adjusted to your specific needs.\nExample:\nScala copysourceimport scala.concurrent.duration.FiniteDuration\nimport scala.reflect.ClassTag\n\nimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject TailChopping {\n\n  sealed trait Command\n  private case object RequestTimeout extends Command\n  private case object FinalTimeout extends Command\n  private case class WrappedReply[R](reply: R) extends Command\n\n  def apply[Reply: ClassTag](\n      sendRequest: (Int, ActorRef[Reply]) => Boolean,\n      nextRequestAfter: FiniteDuration,\n      replyTo: ActorRef[Reply],\n      finalTimeout: FiniteDuration,\n      timeoutReply: Reply): Behavior[Command] = {\n    Behaviors.setup { context =>\n      Behaviors.withTimers { timers =>\n        val replyAdapter = context.messageAdapter[Reply](WrappedReply(_))\n\n        def waiting(requestCount: Int): Behavior[Command] = {\n          Behaviors.receiveMessage {\n            case WrappedReply(reply) =>\n              replyTo ! reply.asInstanceOf[Reply]\n              Behaviors.stopped\n\n            case RequestTimeout =>\n              sendNextRequest(requestCount + 1)\n\n            case FinalTimeout =>\n              replyTo ! timeoutReply\n              Behaviors.stopped\n          }\n        }\n\n        def sendNextRequest(requestCount: Int): Behavior[Command] = {\n          if (sendRequest(requestCount, replyAdapter)) {\n            timers.startSingleTimer(RequestTimeout, nextRequestAfter)\n          } else {\n            timers.startSingleTimer(FinalTimeout, finalTimeout)\n          }\n          waiting(requestCount)\n        }\n\n        sendNextRequest(1)\n      }\n    }\n  }\n\n} Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\nimport org.apache.pekko.actor.typed.javadsl.TimerScheduler;\n\nimport java.time.Duration;\nimport java.util.function.BiFunction;\n\npublic class TailChopping<Reply> extends AbstractBehavior<TailChopping.Command> {\n\n  interface Command {}\n\n  private enum RequestTimeout implements Command {\n    INSTANCE\n  }\n\n  private enum FinalTimeout implements Command {\n    INSTANCE\n  }\n\n  private class WrappedReply implements Command {\n    final Reply reply;\n\n    private WrappedReply(Reply reply) {\n      this.reply = reply;\n    }\n  }\n\n  public static <R> Behavior<Command> create(\n      Class<R> replyClass,\n      BiFunction<Integer, ActorRef<R>, Boolean> sendRequest,\n      Duration nextRequestAfter,\n      ActorRef<R> replyTo,\n      Duration finalTimeout,\n      R timeoutReply) {\n    return Behaviors.setup(\n        context ->\n            Behaviors.withTimers(\n                timers ->\n                    new TailChopping<R>(\n                        replyClass,\n                        context,\n                        timers,\n                        sendRequest,\n                        nextRequestAfter,\n                        replyTo,\n                        finalTimeout,\n                        timeoutReply)));\n  }\n\n  private final TimerScheduler<Command> timers;\n  private final BiFunction<Integer, ActorRef<Reply>, Boolean> sendRequest;\n  private final Duration nextRequestAfter;\n  private final ActorRef<Reply> replyTo;\n  private final Duration finalTimeout;\n  private final Reply timeoutReply;\n  private final ActorRef<Reply> replyAdapter;\n\n  private int requestCount = 0;\n\n  private TailChopping(\n      Class<Reply> replyClass,\n      ActorContext<Command> context,\n      TimerScheduler<Command> timers,\n      BiFunction<Integer, ActorRef<Reply>, Boolean> sendRequest,\n      Duration nextRequestAfter,\n      ActorRef<Reply> replyTo,\n      Duration finalTimeout,\n      Reply timeoutReply) {\n    super(context);\n    this.timers = timers;\n    this.sendRequest = sendRequest;\n    this.nextRequestAfter = nextRequestAfter;\n    this.replyTo = replyTo;\n    this.finalTimeout = finalTimeout;\n    this.timeoutReply = timeoutReply;\n\n    replyAdapter = context.messageAdapter(replyClass, WrappedReply::new);\n\n    sendNextRequest();\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(WrappedReply.class, this::onReply)\n        .onMessage(RequestTimeout.class, notUsed -> onRequestTimeout())\n        .onMessage(FinalTimeout.class, notUsed -> onFinalTimeout())\n        .build();\n  }\n\n  private Behavior<Command> onReply(WrappedReply wrappedReply) {\n    Reply reply = wrappedReply.reply;\n    replyTo.tell(reply);\n    return Behaviors.stopped();\n  }\n\n  private Behavior<Command> onRequestTimeout() {\n    sendNextRequest();\n    return this;\n  }\n\n  private Behavior<Command> onFinalTimeout() {\n    replyTo.tell(timeoutReply);\n    return Behaviors.stopped();\n  }\n\n  private void sendNextRequest() {\n    requestCount++;\n    if (sendRequest.apply(requestCount, replyAdapter)) {\n      timers.startSingleTimer(RequestTimeout.INSTANCE, RequestTimeout.INSTANCE, nextRequestAfter);\n    } else {\n      timers.startSingleTimer(FinalTimeout.INSTANCE, FinalTimeout.INSTANCE, finalTimeout);\n    }\n  }\n}\nUseful when:\nReducing higher latency percentiles and variations of latency are important The “work” can be done more than once with the same result, e.g. a request to retrieve information\nProblems:\nIncreased load since more messages are sent and “work” is performed more than once Can’t be used when the “work” is not idempotent and must only be performed once Message protocols with generic types are difficult since the generic types are erased in runtime Children have life cycles that must be managed to not create a resource leak, it can be easy to miss a scenario where the session actor is not stopped","title":"Latency tail chopping"},{"location":"/typed/interaction-patterns.html#scheduling-messages-to-self","text":"The following example demonstrates how to use timers to schedule messages to an actor.\nExample:\nThe Buncher actor buffers a burst of incoming messages and delivers them as a batch after a timeout or when the number of batched messages exceeds a maximum size.\nScala copysourceobject Buncher {\n\n  sealed trait Command\n  final case class ExcitingMessage(message: String) extends Command\n  final case class Batch(messages: Vector[Command])\n  private case object Timeout extends Command\n  private case object TimerKey\n\n  def apply(target: ActorRef[Batch], after: FiniteDuration, maxSize: Int): Behavior[Command] = {\n    Behaviors.withTimers(timers => new Buncher(timers, target, after, maxSize).idle())\n  }\n}\n\nclass Buncher(\n    timers: TimerScheduler[Buncher.Command],\n    target: ActorRef[Buncher.Batch],\n    after: FiniteDuration,\n    maxSize: Int) {\n  import Buncher._\n\n  private def idle(): Behavior[Command] = {\n    Behaviors.receiveMessage[Command] { message =>\n      timers.startSingleTimer(TimerKey, Timeout, after)\n      active(Vector(message))\n    }\n  }\n\n  def active(buffer: Vector[Command]): Behavior[Command] = {\n    Behaviors.receiveMessage[Command] {\n      case Timeout =>\n        target ! Batch(buffer)\n        idle()\n      case m =>\n        val newBuffer = buffer :+ m\n        if (newBuffer.size == maxSize) {\n          timers.cancel(TimerKey)\n          target ! Batch(newBuffer)\n          idle()\n        } else\n          active(newBuffer)\n    }\n  }\n} Java copysourcepublic class Buncher {\n\n  public interface Command {}\n\n  public static final class Batch {\n    private final List<Command> messages;\n\n    public Batch(List<Command> messages) {\n      this.messages = Collections.unmodifiableList(messages);\n    }\n\n    public List<Command> getMessages() {\n      return messages;\n    }\n  }\n\n  public static final class ExcitingMessage implements Command {\n    public final String message;\n\n    public ExcitingMessage(String message) {\n      this.message = message;\n    }\n  }\n\n  private static final Object TIMER_KEY = new Object();\n\n  private enum Timeout implements Command {\n    INSTANCE\n  }\n\n  public static Behavior<Command> create(ActorRef<Batch> target, Duration after, int maxSize) {\n    return Behaviors.withTimers(timers -> new Buncher(timers, target, after, maxSize).idle());\n  }\n\n  private final TimerScheduler<Command> timers;\n  private final ActorRef<Batch> target;\n  private final Duration after;\n  private final int maxSize;\n\n  private Buncher(\n      TimerScheduler<Command> timers, ActorRef<Batch> target, Duration after, int maxSize) {\n    this.timers = timers;\n    this.target = target;\n    this.after = after;\n    this.maxSize = maxSize;\n  }\n\n  private Behavior<Command> idle() {\n    return Behaviors.receive(Command.class)\n        .onMessage(Command.class, this::onIdleCommand)\n        .build();\n  }\n\n  private Behavior<Command> onIdleCommand(Command message) {\n    timers.startSingleTimer(TIMER_KEY, Timeout.INSTANCE, after);\n    return Behaviors.setup(context -> new Active(context, message));\n  }\n\n  private class Active extends AbstractBehavior<Command> {\n\n    private final List<Command> buffer = new ArrayList<>();\n\n    Active(ActorContext<Command> context, Command firstCommand) {\n      super(context);\n      buffer.add(firstCommand);\n    }\n\n    @Override\n    public Receive<Command> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(Timeout.class, message -> onTimeout())\n          .onMessage(Command.class, this::onCommand)\n          .build();\n    }\n\n    private Behavior<Command> onTimeout() {\n      target.tell(new Batch(buffer));\n      return idle(); // switch to idle\n    }\n\n    private Behavior<Command> onCommand(Command message) {\n      buffer.add(message);\n      if (buffer.size() == maxSize) {\n        timers.cancel(TIMER_KEY);\n        target.tell(new Batch(buffer));\n        return idle(); // switch to idle\n      } else {\n        return this; // stay Active\n      }\n    }\n  }\n}\nThere are a few things worth noting here:\nTo get access to the timers you start with Behaviors.withTimersBehaviors.withTimers that will pass a TimerSchedulerTimerScheduler instance to the function. This can be used with any type of BehaviorBehavior, including receivereceive, receiveMessagereceiveMessage, but also setupsetup or any other behavior. Each timer has a key and if a new timer with the same key is started, the previous is cancelled. It is guaranteed that a message from the previous timer is not received, even if it was already enqueued in the mailbox when the new timer was started. Both periodic and single message timers are supported. The TimerScheduler is mutable in itself, because it performs and manages the side effects of registering the scheduled tasks. The TimerScheduler is bound to the lifecycle of the actor that owns it and is cancelled automatically when the actor is stopped. Behaviors.withTimers can also be used inside Behaviors.superviseBehaviors.supervise and it will automatically cancel the started timers correctly when the actor is restarted, so that the new incarnation will not receive scheduled messages from a previous incarnation.","title":"Scheduling messages to self"},{"location":"/typed/interaction-patterns.html#schedule-periodically","text":"Scheduling of recurring messages can have two different characteristics:\nfixed-delay - The delay between sending subsequent messages will always be (at least) the given delay. Use startTimerWithFixedDelaystartTimerWithFixedDelay. fixed-rate - The frequency of execution over time will meet the given interval. Use startTimerAtFixedRatestartTimerAtFixedRate.\nIf you are uncertain of which one to use you should pick startTimerWithFixedDelay.\nWhen using fixed-delay it will not compensate the delay between messages if the scheduling is delayed longer than specified for some reason. The delay between sending subsequent messages will always be (at least) the given delay. In the long run, the frequency of messages will generally be slightly lower than the reciprocal of the specified delay.\nFixed-delay execution is appropriate for recurring activities that require “smoothness.” In other words, it is appropriate for activities where it is more important to keep the frequency accurate in the short run than in the long run.\nWhen using fixed-rate it will compensate the delay for a subsequent task if the previous messages were delayed too long. In such cases, the actual sending interval will differ from the interval passed to the scheduleAtFixedRate method.\nIf the tasks are delayed longer than the interval, the subsequent message will be sent immediately after the prior one. This also has the consequence that after long garbage collection pauses or other reasons when the JVM was suspended all “missed” tasks will execute when the process wakes up again. For example, scheduleAtFixedRate with an interval of 1 second and the process is suspended for 30 seconds will result in 30 messages being sent in rapid succession to catch up. In the long run, the frequency of execution will be exactly the reciprocal of the specified interval.\nFixed-rate execution is appropriate for recurring activities that are sensitive to absolute time or where the total time to perform a fixed number of executions is important, such as a countdown timer that ticks once every second for ten seconds.\nWarning scheduleAtFixedRate can result in bursts of scheduled messages after long garbage collection pauses, which may in worst case cause undesired load on the system. scheduleWithFixedDelay is often preferred.","title":"Schedule periodically"},{"location":"/typed/interaction-patterns.html#responding-to-a-sharded-actor","text":"When Pekko Cluster is used to shard actors you need to take into account that an actor may move or get passivated.\nThe normal pattern for expecting a reply is to include an ActorRefActorRef in the message, typically a message adapter. This can be used for a sharded actor but if ctx.selfctx.getSelf() is sent and the sharded actor is moved or passivated then the reply will sent to dead letters.\nAn alternative is to send the entityId in the message and have the reply sent via sharding.\nExample:\nScala copysource// a sharded actor that needs counter updates\nobject CounterConsumer {\n  sealed trait Command\n  final case class NewCount(count: Long) extends Command\n  val TypeKey: EntityTypeKey[Command] = EntityTypeKey[Command](\"example-sharded-response\")\n}\n\n// a sharded counter that sends responses to another sharded actor\nobject Counter {\n  trait Command\n  case object Increment extends Command\n  final case class GetValue(replyToEntityId: String) extends Command\n  val TypeKey: EntityTypeKey[Command] = EntityTypeKey[Command](\"example-sharded-counter\")\n\n  private def apply(): Behavior[Command] =\n    Behaviors.setup { context =>\n      counter(ClusterSharding(context.system), 0)\n    }\n\n  private def counter(sharding: ClusterSharding, value: Long): Behavior[Command] =\n    Behaviors.receiveMessage {\n      case Increment =>\n        counter(sharding, value + 1)\n      case GetValue(replyToEntityId) =>\n        val replyToEntityRef = sharding.entityRefFor(CounterConsumer.TypeKey, replyToEntityId)\n        replyToEntityRef ! CounterConsumer.NewCount(value)\n        Behaviors.same\n    }\n\n} Java copysource// a sharded actor that needs counter updates\npublic class CounterConsumer {\n  public static EntityTypeKey<Command> typeKey =\n      EntityTypeKey.create(Command.class, \"example-sharded-response\");\n\n  public interface Command {}\n\n  public static class NewCount implements Command {\n    public final long value;\n\n    public NewCount(long value) {\n      this.value = value;\n    }\n  }\n}\n\n// a sharded counter that sends responses to another sharded actor\npublic class Counter extends AbstractBehavior<Counter.Command> {\n  public static EntityTypeKey<Command> typeKey =\n      EntityTypeKey.create(Command.class, \"example-sharded-counter\");\n\n  public interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    public final String replyToEntityId;\n\n    public GetValue(String replyToEntityId) {\n      this.replyToEntityId = replyToEntityId;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(Counter::new);\n  }\n\n  private final ClusterSharding sharding;\n  private int value = 0;\n\n  private Counter(ActorContext<Command> context) {\n    super(context);\n    this.sharding = ClusterSharding.get(context.getSystem());\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Increment.class, msg -> onIncrement())\n        .onMessage(GetValue.class, this::onGetValue)\n        .build();\n  }\n\n  private Behavior<Command> onIncrement() {\n    value++;\n    return this;\n  }\n\n  private Behavior<Command> onGetValue(GetValue msg) {\n    EntityRef<CounterConsumer.Command> entityRef =\n        sharding.entityRefFor(CounterConsumer.typeKey, msg.replyToEntityId);\n    entityRef.tell(new CounterConsumer.NewCount(value));\n    return this;\n  }\n}\nA disadvantage is that a message adapter can’t be used so the response has to be in the protocol of the actor being responded to. Additionally the EntityTypeKeyEntityTypeKey could be included in the message if it is not known statically.\nAs an “alternative to the alternative”, an EntityRefEntityRef can be included in the messages. The EntityRef transparently wraps messages in a ShardingEnvelopeShardingEnvelope and sends them via sharding. If the target sharded entity has been passivated, it will be delivered to a new incarnation of that entity; if the target sharded entity has been moved to a different cluster node, it will be routed to that new node. If using this approach, be aware that at this time, a custom serializer is required.\nAs with directly including the entityId and EntityTypeKey in the message, EntityRefs do not support message adaptation: the response has to be in the protocol of the entity being responded to.\nIn some cases, it may be useful to define messages with a RecipientRefRecipientRef which is a common supertype of ActorRefActorRef and EntityRef. At this time, serializing a RecipientRef requires a custom serializer.","title":"Responding to a sharded actor"},{"location":"/typed/fault-tolerance.html","text":"","title":"Fault Tolerance"},{"location":"/typed/fault-tolerance.html#fault-tolerance","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Fault Tolerance.\nWhen an actor throws an unexpected exception, a failure, while processing a message or during initialization, the actor will by default be stopped.\nNote An important difference between Typed actors and Classic actors is that by default: the former are stopped if an exception is thrown and no supervision strategy is defined while in Classic they are restarted.\nNote that there is an important distinction between failures and validation errors:\nA validation error means that the data of a command sent to an actor is not valid, this should rather be modelled as a part of the actor protocol than make the actor throw exceptions.\nA failure is instead something unexpected or outside the control of the actor itself, for example a database connection that broke. Opposite to validation errors, it is seldom useful to model failures as part of the protocol as a sending actor can very seldomly do anything useful about it.\nFor failures it is useful to apply the “let it crash” philosophy: instead of mixing fine grained recovery and correction of internal state that may have become partially invalid because of the failure with the business logic we move that responsibility somewhere else. For many cases the resolution can then be to “crash” the actor, and start a new one, with a fresh state that we know is valid.","title":"Fault Tolerance"},{"location":"/typed/fault-tolerance.html#supervision","text":"In Pekko, this “somewhere else” is called supervision. Supervision allows you to declaratively describe what should happen when certain types of exceptions are thrown inside an actor.\nThe default supervision strategy is to stop the actor if an exception is thrown. In many cases you will want to further customize this behavior. To use supervision the actual Actor behavior is wrapped using Behaviors.superviseBehaviors.supervise. Typically you would wrap the actor with supervision in the parent when spawning it as a child.\nThis example restarts the actor when it fails with an IllegalStateException:\nScala copysourceBehaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.restart) Java copysourceBehaviors.supervise(behavior)\n    .onFailure(IllegalStateException.class, SupervisorStrategy.restart());\nOr to resume, ignore the failure and process the next message, instead:\nScala copysourceBehaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.resume) Java copysourceBehaviors.supervise(behavior)\n    .onFailure(IllegalStateException.class, SupervisorStrategy.resume());\nMore complicated restart strategies can be used e.g. to restart no more than 10 times in a 10 second period:\nScala copysourceBehaviors\n  .supervise(behavior)\n  .onFailure[IllegalStateException](\n    SupervisorStrategy.restart.withLimit(maxNrOfRetries = 10, withinTimeRange = 10.seconds)) Java copysourceBehaviors.supervise(behavior)\n    .onFailure(\n        IllegalStateException.class,\n        SupervisorStrategy.restart().withLimit(10, Duration.ofSeconds(10)));\nTo handle different exceptions with different strategies calls to supervisesupervise can be nested:\nScala copysourceBehaviors\n  .supervise(Behaviors.supervise(behavior).onFailure[IllegalStateException](SupervisorStrategy.restart))\n  .onFailure[IllegalArgumentException](SupervisorStrategy.stop) Java copysourceBehaviors.supervise(\n        Behaviors.supervise(behavior)\n            .onFailure(IllegalStateException.class, SupervisorStrategy.restart()))\n    .onFailure(IllegalArgumentException.class, SupervisorStrategy.stop());\nFor a full list of strategies see the public methods on SupervisorStrategySupervisorStrategy.\nNote When the behavior is restarted the original BehaviorBehavior that was given to Behaviors.superviseBehaviors.supervise is re-installed, which means that if it contains mutable state it must be a factory via Behaviors.setupBehaviors.setup. When using the object-oriented style with a class extending AbstractBehaviorAbstractBehavior it’s always recommended to create it via Behaviors.setupBehaviors.setup as described in Behavior factory method. For the function style there is typically no need for the factory if the state is captured in immutable parameters.","title":"Supervision"},{"location":"/typed/fault-tolerance.html#wrapping-behaviors","text":"With the functional style it is very common to store state by changing behavior e.g.\nScala copysourceobject Counter {\n  sealed trait Command\n  case class Increment(nr: Int) extends Command\n  case class GetCount(replyTo: ActorRef[Int]) extends Command\n\n  def apply(): Behavior[Command] =\n    Behaviors.supervise(counter(1)).onFailure(SupervisorStrategy.restart)\n\n  private def counter(count: Int): Behavior[Command] =\n    Behaviors.receiveMessage[Command] {\n      case Increment(nr: Int) =>\n        counter(count + nr)\n      case GetCount(replyTo) =>\n        replyTo ! count\n        Behaviors.same\n    }\n} Java copysourcepublic static class Counter {\n  public interface Command {}\n\n  public static final class Increase implements Command {}\n\n  public static final class Get implements Command {\n    public final ActorRef<Got> replyTo;\n\n    public Get(ActorRef<Got> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class Got {\n    public final int n;\n\n    public Got(int n) {\n      this.n = n;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.supervise(counter(1)).onFailure(SupervisorStrategy.restart());\n  }\n\n  private static Behavior<Command> counter(int currentValue) {\n    return Behaviors.receive(Command.class)\n        .onMessage(Increase.class, o -> onIncrease(currentValue))\n        .onMessage(Get.class, command -> onGet(currentValue, command))\n        .build();\n  }\n\n  private static Behavior<Command> onIncrease(int currentValue) {\n    return counter(currentValue + 1);\n  }\n\n  private static Behavior<Command> onGet(int currentValue, Get command) {\n    command.replyTo.tell(new Got(currentValue));\n    return Behaviors.same();\n  }\n}\nWhen doing this supervision only needs to be added to the top level:\nScala copysourcedef apply(): Behavior[Command] =\n  Behaviors.supervise(counter(1)).onFailure(SupervisorStrategy.restart) Java copysourcepublic static Behavior<Command> create() {\n  return Behaviors.supervise(counter(1)).onFailure(SupervisorStrategy.restart());\n}\nEach returned behavior will be re-wrapped automatically with the supervisor.","title":"Wrapping behaviors"},{"location":"/typed/fault-tolerance.html#child-actors-are-stopped-when-parent-is-restarting","text":"Child actors are often started in a setupsetup block that is run again when the parent actor is restarted. The child actors are stopped to avoid resource leaks of creating new child actors each time the parent is restarted.\nScala copysourcedef child(size: Long): Behavior[String] =\n  Behaviors.receiveMessage(msg => child(size + msg.length))\n\ndef parent: Behavior[String] = {\n  Behaviors\n    .supervise[String] {\n      Behaviors.setup { ctx =>\n        val child1 = ctx.spawn(child(0), \"child1\")\n        val child2 = ctx.spawn(child(0), \"child2\")\n\n        Behaviors.receiveMessage[String] { msg =>\n          // message handling that might throw an exception\n          val parts = msg.split(\" \")\n          child1 ! parts(0)\n          child2 ! parts(1)\n          Behaviors.same\n        }\n      }\n    }\n    .onFailure(SupervisorStrategy.restart)\n} Java copysourcestatic Behavior<String> child(long size) {\n  return Behaviors.receiveMessage(msg -> child(size + msg.length()));\n}\n\nstatic Behavior<String> parent() {\n  return Behaviors.<String>supervise(\n          Behaviors.setup(\n              ctx -> {\n                final ActorRef<String> child1 = ctx.spawn(child(0), \"child1\");\n                final ActorRef<String> child2 = ctx.spawn(child(0), \"child2\");\n\n                return Behaviors.receiveMessage(\n                    msg -> {\n                      // message handling that might throw an exception\n                      String[] parts = msg.split(\" \");\n                      child1.tell(parts[0]);\n                      child2.tell(parts[1]);\n                      return Behaviors.same();\n                    });\n              }))\n      .onFailure(SupervisorStrategy.restart());\n}\nIt is possible to override this so that child actors are not influenced when the parent actor is restarted. The restarted parent instance will then have the same children as before the failure.\nIf child actors are created from setupsetup like in the previous example and they should remain intact (not stopped) when parent is restarted the supervisesupervise should be placed inside the setupsetup and using SupervisorStrategy.restart.withStopChildren(false)SupervisorStrategy.restart().withStopChildren(false) like this:\nScala copysourcedef parent2: Behavior[String] = {\n  Behaviors.setup { ctx =>\n    val child1 = ctx.spawn(child(0), \"child1\")\n    val child2 = ctx.spawn(child(0), \"child2\")\n\n    // supervision strategy inside the setup to not recreate children on restart\n    Behaviors\n      .supervise {\n        Behaviors.receiveMessage[String] { msg =>\n          // message handling that might throw an exception\n          val parts = msg.split(\" \")\n          child1 ! parts(0)\n          child2 ! parts(1)\n          Behaviors.same\n        }\n      }\n      .onFailure(SupervisorStrategy.restart.withStopChildren(false))\n  }\n} Java copysourcestatic Behavior<String> parent2() {\n  return Behaviors.setup(\n      ctx -> {\n        final ActorRef<String> child1 = ctx.spawn(child(0), \"child1\");\n        final ActorRef<String> child2 = ctx.spawn(child(0), \"child2\");\n\n        // supervision strategy inside the setup to not recreate children on restart\n        return Behaviors.<String>supervise(\n                Behaviors.receiveMessage(\n                    msg -> {\n                      // message handling that might throw an exception\n                      String[] parts = msg.split(\" \");\n                      child1.tell(parts[0]);\n                      child2.tell(parts[1]);\n                      return Behaviors.same();\n                    }))\n            .onFailure(SupervisorStrategy.restart().withStopChildren(false));\n      });\n}\nThat means that the setupsetup block will only be run when the parent actor is first started, and not when it is restarted.","title":"Child actors are stopped when parent is restarting"},{"location":"/typed/fault-tolerance.html#the-prerestart-signal","text":"Before a supervised actor is restarted it is sent the PreRestartPreRestart signal giving it a chance to clean up resources it has created, much like the PostStopPostStop signal when the actor stops. The returned behavior from the PreRestartPreRestart signal is ignored.\nScala copysourcedef withPreRestart: Behavior[String] = {\n  Behaviors\n    .supervise[String] {\n      Behaviors.setup { ctx =>\n        val resource = claimResource()\n\n        Behaviors\n          .receiveMessage[String] { msg =>\n            // message handling that might throw an exception\n\n            val parts = msg.split(\" \")\n            resource.process(parts)\n            Behaviors.same\n          }\n          .receiveSignal {\n            case (_, signal) if signal == PreRestart || signal == PostStop =>\n              resource.close()\n              Behaviors.same\n          }\n      }\n    }\n    .onFailure[Exception](SupervisorStrategy.restart)\n}\n Java copysourceBehaviors.supervise(\n        Behaviors.<String>setup(\n            ctx -> {\n              final Resource resource = claimResource();\n\n              return Behaviors.receive(String.class)\n                  .onMessage(\n                      String.class,\n                      msg -> {\n                        // message handling that might throw an exception\n                        String[] parts = msg.split(\" \");\n                        resource.process(parts);\n                        return Behaviors.same();\n                      })\n                  .onSignal(\n                      PreRestart.class,\n                      signal -> {\n                        resource.close();\n                        return Behaviors.same();\n                      })\n                  .onSignal(\n                      PostStop.class,\n                      signal -> {\n                        resource.close();\n                        return Behaviors.same();\n                      })\n                  .build();\n            }))\n    .onFailure(Exception.class, SupervisorStrategy.restart());\nNote that PostStopPostStop is not emitted for a restart, so typically you need to handle both PreRestartPreRestart and PostStopPostStop to cleanup resources.","title":"The PreRestart signal"},{"location":"/typed/fault-tolerance.html#bubble-failures-up-through-the-hierarchy","text":"In some scenarios it may be useful to push the decision about what to do on a failure upwards in the Actor hierarchy and let the parent actor handle what should happen on failures (in classic Pekko Actors this is how it works by default).\nFor a parent to be notified when a child is terminated it has to watch the child. If the child was stopped because of a failure the ChildFailedChildFailed signal will be received which will contain the cause. ChildFailedChildFailed extends TerminatedTerminated so if your use case does not need to distinguish between stopping and failing you can handle both cases with the TerminatedTerminated signal.\nIf the parent in turn does not handle the TerminatedTerminated message it will itself fail with an DeathPactExceptionDeathPactException.\nThis means that a hierarchy of actors can have a child failure bubble up making each actor on the way stop but informing the top-most parent that there was a failure and how to deal with it, however, the original exception that caused the failure will only be available to the immediate parent out of the box (this is most often a good thing, not leaking implementation details).\nThere might be cases when you want the original exception to bubble up the hierarchy, this can be done by handling the TerminatedTerminated signal, and rethrowing the exception in each actor.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.DeathPactException\nimport pekko.actor.typed.SupervisorStrategy\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject Protocol {\n  sealed trait Command\n  case class Fail(text: String) extends Command\n  case class Hello(text: String, replyTo: ActorRef[String]) extends Command\n}\nimport Protocol._\n\nobject Worker {\n  def apply(): Behavior[Command] =\n    Behaviors.receiveMessage {\n      case Fail(text) =>\n        throw new RuntimeException(text)\n      case Hello(text, replyTo) =>\n        replyTo ! text\n        Behaviors.same\n    }\n}\n\nobject MiddleManagement {\n  def apply(): Behavior[Command] =\n    Behaviors.setup[Command] { context =>\n      context.log.info(\"Middle management starting up\")\n      // default supervision of child, meaning that it will stop on failure\n      val child = context.spawn(Worker(), \"child\")\n      // we want to know when the child terminates, but since we do not handle\n      // the Terminated signal, we will in turn fail on child termination\n      context.watch(child)\n\n      // here we don't handle Terminated at all which means that\n      // when the child fails or stops gracefully this actor will\n      // fail with a DeathPactException\n      Behaviors.receiveMessage { message =>\n        child ! message\n        Behaviors.same\n      }\n    }\n}\n\nobject Boss {\n  def apply(): Behavior[Command] =\n    Behaviors\n      .supervise(Behaviors.setup[Command] { context =>\n        context.log.info(\"Boss starting up\")\n        // default supervision of child, meaning that it will stop on failure\n        val middleManagement = context.spawn(MiddleManagement(), \"middle-management\")\n        context.watch(middleManagement)\n\n        // here we don't handle Terminated at all which means that\n        // when middle management fails with a DeathPactException\n        // this actor will also fail\n        Behaviors.receiveMessage[Command] { message =>\n          middleManagement ! message\n          Behaviors.same\n        }\n      })\n      .onFailure[DeathPactException](SupervisorStrategy.restart)\n} Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.DeathPactException;\nimport org.apache.pekko.actor.typed.SupervisorStrategy;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\npublic interface Protocol {\n  public interface Command {}\n\n  public static class Fail implements Command {\n    public final String text;\n\n    public Fail(String text) {\n      this.text = text;\n    }\n  }\n\n  public static class Hello implements Command {\n    public final String text;\n    public final ActorRef<String> replyTo;\n\n    public Hello(String text, ActorRef<String> replyTo) {\n      this.text = text;\n      this.replyTo = replyTo;\n    }\n  }\n}\n\npublic static class Worker extends AbstractBehavior<Protocol.Command> {\n\n  public static Behavior<Protocol.Command> create() {\n    return Behaviors.setup(Worker::new);\n  }\n\n  private Worker(ActorContext<Protocol.Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Protocol.Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Protocol.Fail.class, this::onFail)\n        .onMessage(Protocol.Hello.class, this::onHello)\n        .build();\n  }\n\n  private Behavior<Protocol.Command> onFail(Protocol.Fail message) {\n    throw new RuntimeException(message.text);\n  }\n\n  private Behavior<Protocol.Command> onHello(Protocol.Hello message) {\n    message.replyTo.tell(message.text);\n    return this;\n  }\n}\n\npublic static class MiddleManagement extends AbstractBehavior<Protocol.Command> {\n\n  public static Behavior<Protocol.Command> create() {\n    return Behaviors.setup(MiddleManagement::new);\n  }\n\n  private final ActorRef<Protocol.Command> child;\n\n  private MiddleManagement(ActorContext<Protocol.Command> context) {\n    super(context);\n\n    context.getLog().info(\"Middle management starting up\");\n    // default supervision of child, meaning that it will stop on failure\n    child = context.spawn(Worker.create(), \"child\");\n\n    // we want to know when the child terminates, but since we do not handle\n    // the Terminated signal, we will in turn fail on child termination\n    context.watch(child);\n  }\n\n  @Override\n  public Receive<Protocol.Command> createReceive() {\n    // here we don't handle Terminated at all which means that\n    // when the child fails or stops gracefully this actor will\n    // fail with a DeathPactException\n    return newReceiveBuilder().onMessage(Protocol.Command.class, this::onCommand).build();\n  }\n\n  private Behavior<Protocol.Command> onCommand(Protocol.Command message) {\n    // just pass messages on to the child\n    child.tell(message);\n    return this;\n  }\n}\n\npublic static class Boss extends AbstractBehavior<Protocol.Command> {\n\n  public static Behavior<Protocol.Command> create() {\n    return Behaviors.supervise(Behaviors.setup(Boss::new))\n        .onFailure(DeathPactException.class, SupervisorStrategy.restart());\n  }\n\n  private final ActorRef<Protocol.Command> middleManagement;\n\n  private Boss(ActorContext<Protocol.Command> context) {\n    super(context);\n    context.getLog().info(\"Boss starting up\");\n    // default supervision of child, meaning that it will stop on failure\n    middleManagement = context.spawn(MiddleManagement.create(), \"middle-management\");\n    context.watch(middleManagement);\n  }\n\n  @Override\n  public Receive<Protocol.Command> createReceive() {\n    // here we don't handle Terminated at all which means that\n    // when middle management fails with a DeathPactException\n    // this actor will also fail\n    return newReceiveBuilder().onMessage(Protocol.Command.class, this::onCommand).build();\n  }\n\n  private Behavior<Protocol.Command> onCommand(Protocol.Command message) {\n    // just pass messages on to the child\n    middleManagement.tell(message);\n    return this;\n  }\n}","title":"Bubble failures up through the hierarchy"},{"location":"/typed/actor-discovery.html","text":"","title":"Actor discovery"},{"location":"/typed/actor-discovery.html#actor-discovery","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Actors.","title":"Actor discovery"},{"location":"/typed/actor-discovery.html#dependency","text":"To use Pekko Actor Typed, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/actor-discovery.html#obtaining-actor-references","text":"There are two general ways to obtain Actor references: by creating actors and by discovery using the Receptionist.\nYou can pass actor references between actors as constructor parameters or part of messages.\nSometimes, you need something to bootstrap the interaction, for example when actors are running on different nodes in the Cluster or when “dependency injection” with constructor parameters is not applicable.","title":"Obtaining Actor references"},{"location":"/typed/actor-discovery.html#receptionist","text":"When an actor needs to be discovered by another actor but you are unable to put a reference to it in an incoming message, you can use the ReceptionistReceptionist. It supports both local and cluster(see cluster). You register the specific actors that should be discoverable from each node in the local Receptionist instance. The API of the receptionist is also based on actor messages. This registry of actor references is then automatically distributed to all other nodes in the case of a cluster. You can lookup such actors with the key that was used when they were registered. The reply to such a Find request is a Listing, which contains a Set of actor references that are registered for the key. Note that several actors can be registered to the same key.\nThe registry is dynamic. New actors can be registered during the lifecycle of the system. Entries are removed when registered actors are stopped, manually deregistered or the node they live on is removed from the Cluster. To facilitate this dynamic aspect you can also subscribe to changes with the Receptionist.Subscribe message. It will send Listing messages to the subscriber, first with the set of entries upon subscription, then whenever the entries for a key are changed.\nThese imports are used in the following example:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.receptionist.Receptionist\nimport pekko.actor.typed.receptionist.ServiceKey\nimport pekko.actor.typed.scaladsl.Behaviors Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.receptionist.Receptionist;\nimport org.apache.pekko.actor.typed.receptionist.ServiceKey;\nFirst we create a PingService actor and register it with the Receptionist against a ServiceKeyServiceKey that will later be used to lookup the reference:\nScala copysourceobject PingService {\n  val PingServiceKey = ServiceKey[Ping](\"pingService\")\n\n  final case class Ping(replyTo: ActorRef[Pong.type])\n  case object Pong\n\n  def apply(): Behavior[Ping] = {\n    Behaviors.setup { context =>\n      context.system.receptionist ! Receptionist.Register(PingServiceKey, context.self)\n\n      Behaviors.receiveMessage {\n        case Ping(replyTo) =>\n          context.log.info(\"Pinged by {}\", replyTo)\n          replyTo ! Pong\n          Behaviors.same\n      }\n    }\n  }\n} Java copysourcepublic class PingService {\n\n  public static final ServiceKey<Ping> pingServiceKey =\n      ServiceKey.create(Ping.class, \"pingService\");\n\n  public static class Pong {}\n\n  public static class Ping {\n    private final ActorRef<Pong> replyTo;\n\n    public Ping(ActorRef<Pong> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static Behavior<Ping> create() {\n    return Behaviors.setup(\n        context -> {\n          context\n              .getSystem()\n              .receptionist()\n              .tell(Receptionist.register(pingServiceKey, context.getSelf()));\n\n          return new PingService(context).behavior();\n        });\n  }\n\n  private final ActorContext<Ping> context;\n\n  private PingService(ActorContext<Ping> context) {\n    this.context = context;\n  }\n\n  private Behavior<Ping> behavior() {\n    return Behaviors.receive(Ping.class).onMessage(Ping.class, this::onPing).build();\n  }\n\n  private Behavior<Ping> onPing(Ping msg) {\n    context.getLog().info(\"Pinged by {}\", msg.replyTo);\n    msg.replyTo.tell(new Pong());\n    return Behaviors.same();\n  }\n}\nThen we have another actor that requires a PingService to be constructed:\nScala copysourceobject Pinger {\n  def apply(pingService: ActorRef[PingService.Ping]): Behavior[PingService.Pong.type] = {\n    Behaviors.setup { context =>\n      pingService ! PingService.Ping(context.self)\n\n      Behaviors.receiveMessage { _ =>\n        context.log.info(\"{} was ponged!!\", context.self)\n        Behaviors.stopped\n      }\n    }\n  }\n} Java copysourcepublic class Pinger {\n  private final ActorContext<PingService.Pong> context;\n  private final ActorRef<PingService.Ping> pingService;\n\n  private Pinger(ActorContext<PingService.Pong> context, ActorRef<PingService.Ping> pingService) {\n    this.context = context;\n    this.pingService = pingService;\n  }\n\n  public static Behavior<PingService.Pong> create(ActorRef<PingService.Ping> pingService) {\n    return Behaviors.setup(\n        ctx -> {\n          pingService.tell(new PingService.Ping(ctx.getSelf()));\n          return new Pinger(ctx, pingService).behavior();\n        });\n  }\n\n  private Behavior<PingService.Pong> behavior() {\n    return Behaviors.receive(PingService.Pong.class)\n        .onMessage(PingService.Pong.class, this::onPong)\n        .build();\n  }\n\n  private Behavior<PingService.Pong> onPong(PingService.Pong msg) {\n    context.getLog().info(\"{} was ponged!!\", context.getSelf());\n    return Behaviors.stopped();\n  }\n}\nFinally in the guardian actor we spawn the service as well as subscribing to any actors registering against the ServiceKeyServiceKey. Subscribing means that the guardian actor will be informed of any new registrations via a Listing message:\nScala copysourceobject Guardian {\n  def apply(): Behavior[Nothing] = {\n    Behaviors\n      .setup[Receptionist.Listing] { context =>\n        context.spawnAnonymous(PingService())\n        context.system.receptionist ! Receptionist.Subscribe(PingService.PingServiceKey, context.self)\n\n        Behaviors.receiveMessagePartial[Receptionist.Listing] {\n          case PingService.PingServiceKey.Listing(listings) =>\n            listings.foreach(ps => context.spawnAnonymous(Pinger(ps)))\n            Behaviors.same\n        }\n      }\n      .narrow\n  }\n} Java copysourcepublic class Guardian {\n\n  public static Behavior<Void> create() {\n    return Behaviors.setup(\n            (ActorContext<Receptionist.Listing> context) -> {\n              context\n                  .getSystem()\n                  .receptionist()\n                  .tell(\n                      Receptionist.subscribe(\n                          PingService.pingServiceKey, context.getSelf().narrow()));\n              context.spawnAnonymous(PingService.create());\n\n              return new Guardian(context).behavior();\n            })\n        .unsafeCast(); // Void\n  }\n\n  private final ActorContext<Receptionist.Listing> context;\n\n  private Guardian(ActorContext<Receptionist.Listing> context) {\n    this.context = context;\n  }\n\n  private Behavior<Receptionist.Listing> behavior() {\n    return Behaviors.receive(Receptionist.Listing.class)\n        .onMessage(Receptionist.Listing.class, this::onListing)\n        .build();\n  }\n\n  private Behavior<Receptionist.Listing> onListing(Receptionist.Listing msg) {\n    msg.getServiceInstances(PingService.pingServiceKey)\n        .forEach(pingService -> context.spawnAnonymous(Pinger.create(pingService)));\n    return Behaviors.same();\n  }\n}\nEach time a new (which is just a single time in this example) PingService is registered the guardian actor spawns a Pinger for each currently known PingService. The Pinger sends a Ping message and when receiving the Pong reply it stops.\nIn above example we used Receptionist.Subscribe, but it’s also possible to request a single Listing of the current state without receiving further updates by sending the Receptionist.Find message to the receptionist. An example of using Receptionist.Find:\nScala copysourceobject PingManager {\n  sealed trait Command\n  case object PingAll extends Command\n  private case class ListingResponse(listing: Receptionist.Listing) extends Command\n\n  def apply(): Behavior[Command] = {\n    Behaviors.setup[Command] { context =>\n      val listingResponseAdapter = context.messageAdapter[Receptionist.Listing](ListingResponse.apply)\n\n      context.spawnAnonymous(PingService())\n\n      Behaviors.receiveMessagePartial {\n        case PingAll =>\n          context.system.receptionist ! Receptionist.Find(PingService.PingServiceKey, listingResponseAdapter)\n          Behaviors.same\n        case ListingResponse(PingService.PingServiceKey.Listing(listings)) =>\n          listings.foreach(ps => context.spawnAnonymous(Pinger(ps)))\n          Behaviors.same\n      }\n    }\n  }\n} Java copysourcepublic class PingManager {\n\n  interface Command {}\n\n  enum PingAll implements Command {\n    INSTANCE\n  }\n\n  private static class ListingResponse implements Command {\n    final Receptionist.Listing listing;\n\n    private ListingResponse(Receptionist.Listing listing) {\n      this.listing = listing;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(context -> new PingManager(context).behavior());\n  }\n\n  private final ActorContext<Command> context;\n  private final ActorRef<Receptionist.Listing> listingResponseAdapter;\n\n  private PingManager(ActorContext<Command> context) {\n    this.context = context;\n    this.listingResponseAdapter =\n        context.messageAdapter(Receptionist.Listing.class, ListingResponse::new);\n\n    context.spawnAnonymous(PingService.create());\n  }\n\n  private Behavior<Command> behavior() {\n    return Behaviors.receive(Command.class)\n        .onMessage(PingAll.class, notUsed -> onPingAll())\n        .onMessage(ListingResponse.class, response -> onListing(response.listing))\n        .build();\n  }\n\n  private Behavior<Command> onPingAll() {\n    context\n        .getSystem()\n        .receptionist()\n        .tell(Receptionist.find(PingService.pingServiceKey, listingResponseAdapter));\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> onListing(Receptionist.Listing msg) {\n    msg.getServiceInstances(PingService.pingServiceKey)\n        .forEach(pingService -> context.spawnAnonymous(Pinger.create(pingService)));\n    return Behaviors.same();\n  }\n}\nAlso note how a messageAdaptermessageAdapter is used to convert the Receptionist.Listing to a message type that the PingManager understands.\nIf a server no longer wish to be associated with a service key it can deregister using the command Receptionist.Deregister which will remove the association and inform all subscribers.\nThe command can optionally send an acknowledgement once the local receptionist has removed the registration. The acknowledgement does not guarantee that all subscribers has seen that the instance has been removed, it may still receive messages from subscribers for some time after this.\nScala copysourcecontext.system.receptionist ! Receptionist.Deregister(PingService.PingServiceKey, context.self) Java copysourcecontext\n    .getSystem()\n    .receptionist()\n    .tell(Receptionist.deregister(PingService.pingServiceKey, context.getSelf()));","title":"Receptionist"},{"location":"/typed/actor-discovery.html#cluster-receptionist","text":"The Receptionist also works in a cluster, an actor registered to the receptionist will appear in the receptionist of the other nodes of the cluster.\nThe state for the receptionist is propagated via distributed data which means that each node will eventually reach the same set of actors per ServiceKey.\nSubscriptions and Find queries to a clustered receptionist will keep track of cluster reachability and only list registered actors that are reachable. The full set of actors, including unreachable ones, is available through Listing.allServiceInstancesListing.getAllServiceInstances.\nOne important difference from local only receptions are the serialization concerns, all messages sent to and back from an actor on another node must be serializable, see serialization.","title":"Cluster Receptionist"},{"location":"/typed/actor-discovery.html#receptionist-scalability","text":"The receptionist does not scale up to any number of services or very high turnaround of services. It will likely handle up to thousands or tens of thousands of services. Use cases with higher demands the receptionist for initial contact between actors on the nodes while the actual logic of those is up to the applications own actors.","title":"Receptionist Scalability"},{"location":"/typed/routers.html","text":"","title":"Routers"},{"location":"/typed/routers.html#routers","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Routing.","title":"Routers"},{"location":"/typed/routers.html#dependency","text":"To use Pekko Actor Typed, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/routers.html#introduction","text":"In some cases it is useful to distribute messages of the same type over a set of actors, so that messages can be processed in parallel - a single actor will only process one message at a time.\nThe router itself is a behavior that is spawned into a running actor that will then forward any message sent to it to one final recipient out of the set of routees.\nThere are two kinds of routers included in Pekko Typed - the pool router and the group router.","title":"Introduction"},{"location":"/typed/routers.html#pool-router","text":"The pool router is created with a routee Behavior and spawns a number of children with that behavior which it will then forward messages to.\nIf a child is stopped the pool router removes it from its set of routees. When the last child stops the router itself stops. To make a resilient router that deals with failures the routee Behavior must be supervised.\nAs actor children are always local the routees are never spread across a cluster with a pool router.\nLet’s first introduce the routee:\nScala copysourceobject Worker {\n  sealed trait Command\n  case class DoLog(text: String) extends Command\n\n  def apply(): Behavior[Command] = Behaviors.setup { context =>\n    context.log.info(\"Starting worker\")\n\n    Behaviors.receiveMessage {\n      case DoLog(text) =>\n        context.log.info(\"Got message {}\", text)\n        Behaviors.same\n    }\n  }\n}\n Java copysourceclass Worker {\n  interface Command {}\n\n  static class DoLog implements Command {\n    public final String text;\n\n    public DoLog(String text) {\n      this.text = text;\n    }\n  }\n\n  static final Behavior<Command> create() {\n    return Behaviors.setup(\n        context -> {\n          context.getLog().info(\"Starting worker\");\n\n          return Behaviors.receive(Command.class)\n              .onMessage(DoLog.class, doLog -> onDoLog(context, doLog))\n              .build();\n        });\n  }\n\n  private static Behavior<Command> onDoLog(ActorContext<Command> context, DoLog doLog) {\n    context.getLog().info(\"Got message {}\", doLog.text);\n    return Behaviors.same();\n  }\n}\n\nstatic class Proxy {\n\n  public final ServiceKey<Message> registeringKey =\n      ServiceKey.create(Message.class, \"aggregator-key\");\n\n  public String mapping(Message message) {\n    return message.getId();\n  }\n\n  static class Message {\n\n    public Message(String id, String content) {\n      this.id = id;\n      this.content = content;\n    }\n\n    private String content;\n    private String id;\n\n    public final String getContent() {\n      return content;\n    }\n\n    public final String getId() {\n      return id;\n    }\n  }\n\n  static Behavior<Message> create(ActorRef<String> monitor) {\n    return Behaviors.receive(Message.class)\n        .onMessage(Message.class, in -> onMyMessage(monitor, in))\n        .build();\n  }\n\n  private static Behavior<Message> onMyMessage(ActorRef<String> monitor, Message message) {\n    monitor.tell(message.getId());\n    return Behaviors.same();\n  }\n}\nAfter having defined the routee, we can now concentrate on configuring the router itself. Note again the the router is an Actor in itself:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.testkit.typed.scaladsl.{ LogCapturing, ScalaTestWithActorTestKit }\nimport pekko.actor.typed.{ ActorRef, Behavior, SupervisorStrategy }\nimport pekko.actor.typed.receptionist.{ Receptionist, ServiceKey }\nimport pekko.actor.typed.scaladsl.{ Behaviors, Routers }\n\n// This would be defined within your actor object\nBehaviors.setup[Unit] { ctx =>\n  val pool = Routers.pool(poolSize = 4) {\n    // make sure the workers are restarted if they fail\n    Behaviors.supervise(Worker()).onFailure[Exception](SupervisorStrategy.restart)\n  }\n  val router = ctx.spawn(pool, \"worker-pool\")\n\n  (0 to 10).foreach { n =>\n    router ! Worker.DoLog(s\"msg $n\")\n  }\n\n  val poolWithBroadcast = pool.withBroadcastPredicate(_.isInstanceOf[DoBroadcastLog])\n  val routerWithBroadcast = ctx.spawn(poolWithBroadcast, \"pool-with-broadcast\")\n  // this will be sent to all 4 routees\n  routerWithBroadcast ! DoBroadcastLog(\"msg\")\n  Behaviors.empty\n} Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.SupervisorStrategy;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.GroupRouter;\nimport org.apache.pekko.actor.typed.javadsl.PoolRouter;\nimport org.apache.pekko.actor.typed.javadsl.Routers;\nimport org.apache.pekko.actor.typed.receptionist.Receptionist;\nimport org.apache.pekko.actor.typed.receptionist.ServiceKey;\n\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestKitJunitResource;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestProbe;\n\nimport org.junit.ClassRule;\nimport org.junit.Test;\nimport org.scalatestplus.junit.JUnitSuite;\n\n// This would be defined within your actor class\nBehaviors.setup(\n    context -> {\n      int poolSize = 4;\n      PoolRouter<Worker.Command> pool =\n          Routers.pool(\n              poolSize,\n              // make sure the workers are restarted if they fail\n              Behaviors.supervise(Worker.create()).onFailure(SupervisorStrategy.restart()));\n      ActorRef<Worker.Command> router = context.spawn(pool, \"worker-pool\");\n\n      for (int i = 0; i < 10; i++) {\n        router.tell(new Worker.DoLog(\"msg \" + i));\n      }\n    });","title":"Pool Router"},{"location":"/typed/routers.html#configuring-dispatchers","text":"Since the router itself is spawned as an actor the dispatcher used for it can be configured directly in the call to spawn. The routees, however, are spawned by the router. Therefore, the PoolRouter has a property to configure the Props of its routees:\nScala copysource// make sure workers use the default blocking IO dispatcher\nval blockingPool = pool.withRouteeProps(routeeProps = DispatcherSelector.blocking())\n// spawn head router using the same executor as the parent\nval blockingRouter = ctx.spawn(blockingPool, \"blocking-pool\", DispatcherSelector.sameAsParent()) Java copysource// make sure workers use the default blocking IO dispatcher\nPoolRouter<Worker.Command> blockingPool =\n    pool.withRouteeProps(DispatcherSelector.blocking());\n// spawn head router using the same executor as the parent\nActorRef<Worker.Command> blockingRouter =\n    context.spawn(blockingPool, \"blocking-pool\", DispatcherSelector.sameAsParent());","title":"Configuring Dispatchers"},{"location":"/typed/routers.html#broadcasting-a-message-to-all-routees","text":"Pool routers can be configured to identify messages intended to be broad-casted to all routees. Therefore, the PoolRouter has a property to configure its broadcastPredicate:\nScala copysourceval poolWithBroadcast = pool.withBroadcastPredicate(_.isInstanceOf[DoBroadcastLog])\nval routerWithBroadcast = ctx.spawn(poolWithBroadcast, \"pool-with-broadcast\")\n// this will be sent to all 4 routees\nrouterWithBroadcast ! DoBroadcastLog(\"msg\")\nBehaviors.empty Java copysourcePoolRouter<Worker.Command> broadcastingPool =\n    pool.withBroadcastPredicate(msg -> msg instanceof DoBroadcastLog);","title":"Broadcasting a message to all routees"},{"location":"/typed/routers.html#group-router","text":"The group router is created with a ServiceKey and uses the receptionist (see Receptionist) to discover available actors for that key and routes messages to one of the currently known registered actors for a key.\nSince the receptionist is used this means the group router is cluster-aware out of the box. The router sends messages to registered actors on any node in the cluster that is reachable. If no reachable actor exists the router will fallback and route messages to actors on nodes marked as unreachable.\nThat the receptionist is used also means that the set of routees is eventually consistent, and that immediately when the group router is started the set of routees it knows about is empty, until it has seen a listing from the receptionist it stashes incoming messages and forwards them as soon as it gets a listing from the receptionist.\nWhen the router has received a listing from the receptionist and the set of registered actors is empty the router will drop them (publish them to the event stream as org.apache.pekko.actor.Dropped).\nScala copysourceval serviceKey = ServiceKey[Worker.Command](\"log-worker\")\n\nBehaviors.setup[Unit] { ctx =>\n  // this would likely happen elsewhere - if we create it locally we\n  // can just as well use a pool\n  val worker = ctx.spawn(Worker(), \"worker\")\n  ctx.system.receptionist ! Receptionist.Register(serviceKey, worker)\n\n  val group = Routers.group(serviceKey)\n  val router = ctx.spawn(group, \"worker-group\")\n\n  // the group router will stash messages until it sees the first listing of registered\n  // services from the receptionist, so it is safe to send messages right away\n  (0 to 10).foreach { n =>\n    router ! Worker.DoLog(s\"msg $n\")\n  }\n\n  Behaviors.empty\n} Java copysourceServiceKey<Worker.Command> serviceKey = ServiceKey.create(Worker.Command.class, \"log-worker\");\n\nBehaviors.setup(\n    context -> {\n\n      // this would likely happen elsewhere - if we create it locally we\n      // can just as well use a pool\n      ActorRef<Worker.Command> worker = context.spawn(Worker.create(), \"worker\");\n      context.getSystem().receptionist().tell(Receptionist.register(serviceKey, worker));\n\n      GroupRouter<Worker.Command> group = Routers.group(serviceKey);\n      ActorRef<Worker.Command> router = context.spawn(group, \"worker-group\");\n\n      // the group router will stash messages until it sees the first listing of\n      // registered\n      // services from the receptionist, so it is safe to send messages right away\n      for (int i = 0; i < 10; i++) {\n        router.tell(new Worker.DoLog(\"msg \" + i));\n      }\n\n      return Behaviors.empty();\n    });","title":"Group Router"},{"location":"/typed/routers.html#routing-strategies","text":"There are three different strategies for selecting which routee a message is forwarded to that can be selected from the router before spawning it:\nScala copysourceval alternativePool = pool.withPoolSize(2).withRoundRobinRouting() Java copysourcePoolRouter<Worker.Command> alternativePool = pool.withPoolSize(2).withRoundRobinRouting();","title":"Routing strategies"},{"location":"/typed/routers.html#round-robin","text":"Rotates over the set of routees making sure that if there are n routees, then for n messages sent through the router, each actor is forwarded one message.\nRound robin gives fair routing where every available routee gets the same amount of messages as long as the set of routees stays relatively stable, but may be unfair if the set of routees changes a lot.\nThis is the default for pool routers as the pool of routees is expected to remain the same.\nAn optional parameter preferLocalRoutees can be used for this strategy. Routers will only use routees located in local actor system if preferLocalRoutees is true and local routees do exist. The default value for this parameter is false.","title":"Round Robin"},{"location":"/typed/routers.html#random","text":"Randomly selects a routee when a message is sent through the router.\nThis is the default for group routers as the group of routees is expected to change as nodes join and leave the cluster.\nAn optional parameter preferLocalRoutees can be used for this strategy. Routers will only use routees located in local actor system if preferLocalRoutees is true and local routees do exist. The default value for this parameter is false.","title":"Random"},{"location":"/typed/routers.html#consistent-hashing","text":"Uses consistent hashing to select a routee based on the sent message. This article gives good insight into how consistent hashing is implemented.\nCurrently you have to define hashMapping of the router to map incoming messages to their consistent hash key. This makes the decision transparent for the sender.\nConsistent hashing delivers messages with the same hash to the same routee as long as the set of routees stays the same. When the set of routees changes, consistent hashing tries to make sure, but does not guarantee, that messages with the same hash are routed to the same routee.\nScala copysourceval router = spawn(Routers.group(Proxy.RegisteringKey).withConsistentHashingRouting(10, Proxy.mapping))\n\nrouter ! Proxy.Message(\"123\", \"Text1\")\nrouter ! Proxy.Message(\"123\", \"Text2\")\n\nrouter ! Proxy.Message(\"zh3\", \"Text3\")\nrouter ! Proxy.Message(\"zh3\", \"Text4\")\n// the hash is calculated over the Proxy.Message first parameter obtained through the Proxy.mapping function Java copysourceActorRef<Proxy.Message> router =\n    testKit.spawn(\n        Routers.group(proxy.registeringKey)\n            .withConsistentHashingRouting(10, command -> proxy.mapping(command)));\n\nrouter.tell(new Proxy.Message(\"123\", \"Text1\"));\nrouter.tell(new Proxy.Message(\"123\", \"Text2\"));\n\nrouter.tell(new Proxy.Message(\"zh3\", \"Text3\"));\nrouter.tell(new Proxy.Message(\"zh3\", \"Text4\"));\n// the hash is calculated over the Proxy.Message first parameter obtained through the\n// Proxy.mapping function\nSee also Pekko Cluster Sharding which provides stable routing and rebalancing of the routee actors.","title":"Consistent Hashing"},{"location":"/typed/routers.html#routers-and-performance","text":"Note that if the routees are sharing a resource, the resource will determine if increasing the number of actors will actually give higher throughput or faster answers. For example if the routees are CPU bound actors it will not give better performance to create more routees than there are threads to execute the actors.\nSince the router itself is an actor and has a mailbox this means that messages are routed sequentially to the routees where it can be processed in parallel (depending on the available threads in the dispatcher). In a high throughput use cases the sequential routing could become a bottle neck. Pekko Typed does not provide an optimized tool for this.","title":"Routers and performance"},{"location":"/typed/stash.html","text":"","title":"Stash"},{"location":"/typed/stash.html#stash","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Actors.","title":"Stash"},{"location":"/typed/stash.html#dependency","text":"To use Pekko Actor Typed, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/stash.html#introduction","text":"Stashing enables an actor to temporarily buffer all or some messages that cannot or should not be handled using the actor’s current behavior.\nA typical example when this is useful is if the actor has to load some initial state or initialize some resources before it can accept the first real message. Another example is when the actor is waiting for something to complete before processing the next message.\nLet’s illustrate these two with an example. The DataAccess actor below is used like a single access point to a value stored in a database. When it’s started it loads current state from the database, and while waiting for that initial value all incoming messages are stashed.\nWhen a new state is saved in the database it also stashes incoming messages to make the processing sequential, one after the other without multiple pending writes.\nScala copysourceimport scala.concurrent.Future\nimport scala.util.Failure\nimport scala.util.Success\n\nimport org.apache.pekko\nimport pekko.Done\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.scaladsl.StashBuffer\n\ntrait DB {\n  def save(id: String, value: String): Future[Done]\n  def load(id: String): Future[String]\n}\n\nobject DataAccess {\n  sealed trait Command\n  final case class Save(value: String, replyTo: ActorRef[Done]) extends Command\n  final case class Get(replyTo: ActorRef[String]) extends Command\n  private final case class InitialState(value: String) extends Command\n  private case object SaveSuccess extends Command\n  private final case class DBError(cause: Throwable) extends Command\n\n  def apply(id: String, db: DB): Behavior[Command] = {\n    Behaviors.withStash(100) { buffer =>\n      Behaviors.setup[Command] { context =>\n        new DataAccess(context, buffer, id, db).start()\n      }\n    }\n  }\n}\n\nclass DataAccess(\n    context: ActorContext[DataAccess.Command],\n    buffer: StashBuffer[DataAccess.Command],\n    id: String,\n    db: DB) {\n  import DataAccess._\n\n  private def start(): Behavior[Command] = {\n    context.pipeToSelf(db.load(id)) {\n      case Success(value) => InitialState(value)\n      case Failure(cause) => DBError(cause)\n    }\n\n    Behaviors.receiveMessage {\n      case InitialState(value) =>\n        // now we are ready to handle stashed messages if any\n        buffer.unstashAll(active(value))\n      case DBError(cause) =>\n        throw cause\n      case other =>\n        // stash all other messages for later processing\n        buffer.stash(other)\n        Behaviors.same\n    }\n  }\n\n  private def active(state: String): Behavior[Command] = {\n    Behaviors.receiveMessagePartial {\n      case Get(replyTo) =>\n        replyTo ! state\n        Behaviors.same\n      case Save(value, replyTo) =>\n        context.pipeToSelf(db.save(id, value)) {\n          case Success(_)     => SaveSuccess\n          case Failure(cause) => DBError(cause)\n        }\n        saving(value, replyTo)\n    }\n  }\n\n  private def saving(state: String, replyTo: ActorRef[Done]): Behavior[Command] = {\n    Behaviors.receiveMessage {\n      case SaveSuccess =>\n        replyTo ! Done\n        buffer.unstashAll(active(state))\n      case DBError(cause) =>\n        throw cause\n      case other =>\n        buffer.stash(other)\n        Behaviors.same\n    }\n  }\n\n} Java copysourceimport org.apache.pekko.Done;\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.StashBuffer;\nimport java.util.concurrent.CompletionStage;\n\n\ninterface DB {\n  CompletionStage<Done> save(String id, String value);\n\n  CompletionStage<String> load(String id);\n}\n\npublic class DataAccess {\n\n  public interface Command {}\n\n  public static class Save implements Command {\n    public final String payload;\n    public final ActorRef<Done> replyTo;\n\n    public Save(String payload, ActorRef<Done> replyTo) {\n      this.payload = payload;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Get implements Command {\n    public final ActorRef<String> replyTo;\n\n    public Get(ActorRef<String> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  private static class InitialState implements Command {\n    public final String value;\n\n    InitialState(String value) {\n      this.value = value;\n    }\n  }\n\n  private enum SaveSuccess implements Command {\n    INSTANCE\n  }\n\n  private static class DBError implements Command {\n    public final RuntimeException cause;\n\n    DBError(RuntimeException cause) {\n      this.cause = cause;\n    }\n  }\n\n  private final ActorContext<Command> context;\n  private final StashBuffer<Command> buffer;\n  private final String id;\n  private final DB db;\n\n  private DataAccess(\n      ActorContext<Command> context, StashBuffer<Command> buffer, String id, DB db) {\n    this.context = context;\n    this.buffer = buffer;\n    this.id = id;\n    this.db = db;\n  }\n\n  public static Behavior<Command> create(String id, DB db) {\n    return Behaviors.withStash(\n        100,\n        stash ->\n            Behaviors.setup(\n                ctx -> {\n                  ctx.pipeToSelf(\n                      db.load(id),\n                      (value, cause) -> {\n                        if (cause == null) return new InitialState(value);\n                        else return new DBError(asRuntimeException(cause));\n                      });\n\n                  return new DataAccess(ctx, stash, id, db).start();\n                }));\n  }\n\n  private Behavior<Command> start() {\n    return Behaviors.receive(Command.class)\n        .onMessage(InitialState.class, this::onInitialState)\n        .onMessage(DBError.class, this::onDBError)\n        .onMessage(Command.class, this::stashOtherCommand)\n        .build();\n  }\n\n  private Behavior<Command> onInitialState(InitialState message) {\n    // now we are ready to handle stashed messages if any\n    return buffer.unstashAll(active(message.value));\n  }\n\n  private Behavior<Command> onDBError(DBError message) {\n    throw message.cause;\n  }\n\n  private Behavior<Command> stashOtherCommand(Command message) {\n    // stash all other messages for later processing\n    buffer.stash(message);\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> active(String state) {\n    return Behaviors.receive(Command.class)\n        .onMessage(Get.class, message -> onGet(state, message))\n        .onMessage(Save.class, this::onSave)\n        .build();\n  }\n\n  private Behavior<Command> onGet(String state, Get message) {\n    message.replyTo.tell(state);\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> onSave(Save message) {\n    context.pipeToSelf(\n        db.save(id, message.payload),\n        (value, cause) -> {\n          if (cause == null) return SaveSuccess.INSTANCE;\n          else return new DBError(asRuntimeException(cause));\n        });\n    return saving(message.payload, message.replyTo);\n  }\n\n  private Behavior<Command> saving(String state, ActorRef<Done> replyTo) {\n    return Behaviors.receive(Command.class)\n        .onMessage(SaveSuccess.class, message -> onSaveSuccess(state, replyTo))\n        .onMessage(DBError.class, this::onDBError)\n        .onMessage(Command.class, this::stashOtherCommand)\n        .build();\n  }\n\n  private Behavior<Command> onSaveSuccess(String state, ActorRef<Done> replyTo) {\n    replyTo.tell(Done.getInstance());\n    return buffer.unstashAll(active(state));\n  }\n\n  private static RuntimeException asRuntimeException(Throwable t) {\n    // can't throw Throwable in lambdas\n    if (t instanceof RuntimeException) {\n      return (RuntimeException) t;\n    } else {\n      return new RuntimeException(t);\n    }\n  }\n}\nOne important thing to be aware of is that the StashBufferStashBuffer is a buffer and stashed messages will be kept in memory until they are unstashed (or the actor is stopped and garbage collected). It’s recommended to avoid stashing too many messages to avoid too much memory usage and even risking OutOfMemoryError if many actors are stashing many messages. Therefore the StashBufferStashBuffer is bounded and the capacity of how many messages it can hold must be specified when it’s created.\nIf you try to stash more messages than the capacity a StashOverflowExceptionStashOverflowException will be thrown. You can use StashBuffer.isFullStashBuffer.isFull before stashing a message to avoid that and take other actions, such as dropping the message.\nWhen unstashing the buffered messages by calling unstashAllunstashAll the messages will be processed sequentially in the order they were added and all are processed unless an exception is thrown. The actor is unresponsive to other new messages until unstashAllunstashAll is completed. That is another reason for keeping the number of stashed messages low. Actors that hog the message processing thread for too long can result in starvation of other actors.\nThat can be mitigated by using the StashBuffer.unstashStashBuffer.unstash with numberOfMessages parameter and then send a message to context.selfcontext.getSelf before continuing unstashing more. That means that other new messages may arrive in-between and those must be stashed to keep the original order of messages. It becomes more complicated, so better keep the number of stashed messages low.","title":"Introduction"},{"location":"/typed/fsm.html","text":"","title":"Behaviors as finite state machines"},{"location":"/typed/fsm.html#behaviors-as-finite-state-machines","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic FSM.\nAn actor can be used to model a Finite State Machine (FSM).\nTo demonstrate this, consider an actor which shall receive and queue messages while they arrive in a burst and send them on after the burst ended or a flush request is received.\nThis example demonstrates how to:\nModel states using different behaviors Model storing data at each state by representing the behavior as a method Implement state timeouts\nThe events the FSM can receive become the type of message the Actor can receive:\nScala copysourceobject Buncher {\n\n  // FSM event becomes the type of the message Actor supports\n  sealed trait Event\n  final case class SetTarget(ref: ActorRef[Batch]) extends Event\n  final case class Queue(obj: Any) extends Event\n  case object Flush extends Event\n  private case object Timeout extends Event\n} Java copysourcepublic abstract class Buncher {\n\n  public interface Event {}\n\n  public static final class SetTarget implements Event {\n    public final ActorRef<Batch> ref;\n\n    public SetTarget(ActorRef<Batch> ref) {\n      this.ref = ref;\n    }\n  }\n\n  private enum Timeout implements Event {\n    INSTANCE\n  }\n\n  public enum Flush implements Event {\n    INSTANCE\n  }\n\n  public static final class Queue implements Event {\n    public final Object obj;\n\n    public Queue(Object obj) {\n      this.obj = obj;\n    }\n  }\n}\nSetTarget is needed for starting it up, setting the destination for the Batches to be passed on; Queue will add to the internal queue while Flush will mark the end of a burst.\nScala copysourcesealed trait Data\ncase object Uninitialized extends Data\nfinal case class Todo(target: ActorRef[Batch], queue: immutable.Seq[Any]) extends Data\n\nfinal case class Batch(obj: immutable.Seq[Any]) Java copysourceinterface Data {}\n\npublic static final class Todo implements Data {\n  public final ActorRef<Batch> target;\n  public final List<Object> queue;\n\n  public Todo(ActorRef<Batch> target, List<Object> queue) {\n    this.target = target;\n    this.queue = queue;\n  }\n\n}\n\npublic static final class Batch {\n  public final List<Object> list;\n\n  public Batch(List<Object> list) {\n    this.list = list;\n  }\n\n}\nEach state becomes a distinct behavior and after processing a message the next state in the form of a Behavior is returned.\nScala copysourceobject Buncher {\n  // states of the FSM represented as behaviors\n\n  // initial state\n  def apply(): Behavior[Event] = idle(Uninitialized)\n\n  private def idle(data: Data): Behavior[Event] = Behaviors.receiveMessage[Event] { message =>\n    (message, data) match {\n      case (SetTarget(ref), Uninitialized) =>\n        idle(Todo(ref, Vector.empty))\n      case (Queue(obj), t @ Todo(_, v)) =>\n        active(t.copy(queue = v :+ obj))\n      case _ =>\n        Behaviors.unhandled\n    }\n  }\n\n  private def active(data: Todo): Behavior[Event] =\n    Behaviors.withTimers[Event] { timers =>\n      // instead of FSM state timeout\n      timers.startSingleTimer(Timeout, 1.second)\n      Behaviors.receiveMessagePartial {\n        case Flush | Timeout =>\n          data.target ! Batch(data.queue)\n          idle(data.copy(queue = Vector.empty))\n        case Queue(obj) =>\n          active(data.copy(queue = data.queue :+ obj))\n      }\n    }\n\n} Java copysourcepublic abstract class Buncher {\n  // FSM states represented as behaviors\n\n  // initial state\n  public static Behavior<Event> create() {\n    return uninitialized();\n  }\n\n  private static Behavior<Event> uninitialized() {\n    return Behaviors.receive(Event.class)\n        .onMessage(\n            SetTarget.class, message -> idle(new Todo(message.ref, Collections.emptyList())))\n        .build();\n  }\n\n  private static Behavior<Event> idle(Todo data) {\n    return Behaviors.receive(Event.class)\n        .onMessage(Queue.class, message -> active(data.addElement(message)))\n        .build();\n  }\n\n  private static Behavior<Event> active(Todo data) {\n    return Behaviors.withTimers(\n        timers -> {\n          // State timeouts done with withTimers\n          timers.startSingleTimer(\"Timeout\", Timeout.INSTANCE, Duration.ofSeconds(1));\n          return Behaviors.receive(Event.class)\n              .onMessage(Queue.class, message -> active(data.addElement(message)))\n              .onMessage(Flush.class, message -> activeOnFlushOrTimeout(data))\n              .onMessage(Timeout.class, message -> activeOnFlushOrTimeout(data))\n              .build();\n        });\n  }\n\n  private static Behavior<Event> activeOnFlushOrTimeout(Todo data) {\n    data.target.tell(new Batch(data.queue));\n    return idle(data.copy(new ArrayList<>()));\n  }\n\n}\nThe method idle above makes use of Behaviors.unhandled which advises the system to reuse the previous behavior, including the hint that the message has not been handled. There are two related behaviors: return Behaviors.empty as next behavior in case you reached a state where you don’t expect messages any more. For instance if an actor only waits until all spawned child actors stopped. Unhandled messages are still logged with this behavior. return Behaviors.ignore as next behavior in case you don’t care about unhandled messages. All messages sent to an actor with such a behavior are simply dropped and ignored (without logging)\nTo set state timeouts use Behaviors.withTimers along with a startSingleTimer.","title":"Behaviors as finite state machines"},{"location":"/typed/fsm.html#example-project","text":"FSM example project FSM example project is an example project that can be downloaded, and with instructions of how to run.\nThis project contains a Dining Hakkers sample illustrating how to model a Finite State Machine (FSM) with actors.","title":"Example project"},{"location":"/coordinated-shutdown.html","text":"","title":"Coordinated Shutdown"},{"location":"/coordinated-shutdown.html#coordinated-shutdown","text":"Under normal conditions, when an ActorSystem is terminated or the JVM process is shut down, certain actors and services will be stopped in a specific order.\nThe CoordinatedShutdownCoordinatedShutdown extension registers internal and user-defined tasks to be executed during the shutdown process. The tasks are grouped in configuration-defined “phases” which define the shutdown order.\nEspecially the phases before-service-unbind, before-cluster-shutdown and before-actor-system-terminate are intended for application specific phases or tasks.\nThe order of the shutdown phases is defined in configuration pekko.coordinated-shutdown.phases. See the default phases in the reference.conf tab:\nMost relevant default phases Phase Description before-service-unbind The first pre-defined phase during shutdown. before-cluster-shutdown Phase for custom application tasks that are to be run after service shutdown and before cluster shutdown. before-actor-system-terminate Phase for custom application tasks that are to be run after cluster shutdown and before ActorSystem termination. reference.conf (HOCON) copysource# CoordinatedShutdown is enabled by default and will run the tasks that\n# are added to these phases by individual Pekko modules and user logic.\n#\n# The phases are ordered as a DAG by defining the dependencies between the phases\n# to make sure shutdown tasks are run in the right order.\n#\n# In general user tasks belong in the first few phases, but there may be use\n# cases where you would want to hook in new phases or register tasks later in\n# the DAG.\n#\n# Each phase is defined as a named config section with the\n# following optional properties:\n# - timeout=15s: Override the default-phase-timeout for this phase.\n# - recover=off: If the phase fails the shutdown is aborted\n#                and depending phases will not be executed.\n# - enabled=off: Skip all tasks registered in this phase. DO NOT use\n#                this to disable phases unless you are absolutely sure what the\n#                consequences are. Many of the built in tasks depend on other tasks\n#                having been executed in earlier phases and may break if those are disabled.\n# depends-on=[]: Run the phase after the given phases\nphases {\n\n  # The first pre-defined phase that applications can add tasks to.\n  # Note that more phases can be added in the application's\n  # configuration by overriding this phase with an additional\n  # depends-on.\n  before-service-unbind {\n  }\n\n  # Stop accepting new incoming connections.\n  # This is where you can register tasks that makes a server stop accepting new connections. Already\n  # established connections should be allowed to continue and complete if possible.\n  service-unbind {\n    depends-on = [before-service-unbind]\n  }\n\n  # Wait for requests that are in progress to be completed.\n  # This is where you register tasks that will wait for already established connections to complete, potentially\n  # also first telling them that it is time to close down.\n  service-requests-done {\n    depends-on = [service-unbind]\n  }\n\n  # Final shutdown of service endpoints.\n  # This is where you would add tasks that forcefully kill connections that are still around.\n  service-stop {\n    depends-on = [service-requests-done]\n  }\n\n  # Phase for custom application tasks that are to be run\n  # after service shutdown and before cluster shutdown.\n  before-cluster-shutdown {\n    depends-on = [service-stop]\n  }\n\n  # Graceful shutdown of the Cluster Sharding regions.\n  # This phase is not meant for users to add tasks to.\n  cluster-sharding-shutdown-region {\n    timeout = 10 s\n    depends-on = [before-cluster-shutdown]\n  }\n\n  # Emit the leave command for the node that is shutting down.\n  # This phase is not meant for users to add tasks to.\n  cluster-leave {\n    depends-on = [cluster-sharding-shutdown-region]\n  }\n\n  # Shutdown cluster singletons\n  # This is done as late as possible to allow the shard region shutdown triggered in\n  # the \"cluster-sharding-shutdown-region\" phase to complete before the shard coordinator is shut down.\n  # This phase is not meant for users to add tasks to.\n  cluster-exiting {\n    timeout = 10 s\n    depends-on = [cluster-leave]\n  }\n\n  # Wait until exiting has been completed\n  # This phase is not meant for users to add tasks to.\n  cluster-exiting-done {\n    depends-on = [cluster-exiting]\n  }\n\n  # Shutdown the cluster extension\n  # This phase is not meant for users to add tasks to.\n  cluster-shutdown {\n    depends-on = [cluster-exiting-done]\n  }\n\n  # Phase for custom application tasks that are to be run\n  # after cluster shutdown and before ActorSystem termination.\n  before-actor-system-terminate {\n    depends-on = [cluster-shutdown]\n  }\n\n  # Last phase. See terminate-actor-system and exit-jvm above.\n  # Don't add phases that depends on this phase because the\n  # dispatcher and scheduler of the ActorSystem have been shutdown.\n  # This phase is not meant for users to add tasks to.\n  actor-system-terminate {\n    timeout = 10 s\n    depends-on = [before-actor-system-terminate]\n  }\n}\nMore phases can be added in the application’s application.conf if needed by overriding a phase with an additional depends-on.\nThe default phases are defined in a single linear order, but the phases can be ordered as a directed acyclic graph (DAG) by defining the dependencies between the phases. The phases are ordered with topological sort of the DAG.\nTasks can be added to a phase like in this example which allows a certain actor to react before termination starts:\nScala copysourceobject MyActor {\n\n  trait Messages\n  case class Stop(replyTo: ActorRef[Done]) extends Messages\n\n  def behavior: Behavior[Messages] =\n    Behaviors.receiveMessage {\n      // ...\n      case Stop(replyTo) =>\n        // shut down the actor internals\n        // ..\n        replyTo.tell(Done)\n        Behaviors.stopped\n    }\n}\n\n  CoordinatedShutdown(context.system).addTask(CoordinatedShutdown.PhaseBeforeServiceUnbind, \"someTaskName\") { () =>\n    implicit val timeout: Timeout = 5.seconds\n    myActor.ask(MyActor.Stop(_))\n  } Java copysourceimport static org.apache.pekko.actor.typed.javadsl.AskPattern.ask;\n\npublic static class MyActor extends AbstractBehavior<MyActor.Messages> {\n  interface Messages {}\n\n  // ...\n\n  static final class Stop implements Messages {\n    final ActorRef<Done> replyTo;\n\n    Stop(ActorRef<Done> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n  @Override\n  public Receive<Messages> createReceive() {\n    return newReceiveBuilder().onMessage(Stop.class, this::stop).build();\n  }\n\n  private Behavior<Messages> stop(Stop stop) {\n    // shut down the actor internal\n    // ...\n    stop.replyTo.tell(Done.done());\n    return Behaviors.stopped();\n  }\n}\n\n          CoordinatedShutdown.get(system)\n              .addTask(\n                  CoordinatedShutdown.PhaseBeforeServiceUnbind(),\n                  \"someTaskName\",\n                  () ->\n                      ask(myActor, MyActor.Stop::new, Duration.ofSeconds(5), system.scheduler()));\nThe returned Future[Done] CompletionStage<Done> should be completed when the task is completed. The task name parameter is only used for debugging/logging.\nTasks added to the same phase are executed in parallel without any ordering assumptions. Next phase will not start until all tasks of previous phase have been completed.\nIf tasks are not completed within a configured timeout (see reference.conf) the next phase will be started anyway. It is possible to configure recover=off for a phase to abort the rest of the shutdown process if a task fails or is not completed within the timeout.\nIf cancellation of previously added tasks is required:\nScala copysourceval c: Cancellable =\n  CoordinatedShutdown(system).addCancellableTask(CoordinatedShutdown.PhaseBeforeServiceUnbind, \"cleanup\") { () =>\n    Future {\n      cleanup()\n      Done\n    }\n  }\n\n// much later...\nc.cancel() Java copysourceCancellable cancellable =\n    CoordinatedShutdown.get(system)\n        .addCancellableTask(\n            CoordinatedShutdown.PhaseBeforeServiceUnbind(), \"someTaskCleanup\", () -> cleanup());\n// much later...\ncancellable.cancel();\nIn the above example, it may be more convenient to simply stop the actor when it’s done shutting down, rather than send back a done message, and for the shutdown task to not complete until the actor is terminated. A convenience method is provided that adds a task that sends a message to the actor and then watches its termination (there is currently no corresponding functionality for the new actors API see #29056):\nScala copysourceCoordinatedShutdown(system).addActorTerminationTask(\n  CoordinatedShutdown.PhaseBeforeServiceUnbind,\n  \"someTaskName\",\n  someActor,\n  Some(\"stop\")) Java copysourceCoordinatedShutdown.get(system)\n    .addActorTerminationTask(\n        CoordinatedShutdown.PhaseBeforeServiceUnbind(),\n        \"someTaskName\",\n        someActor,\n        Optional.of(\"stop\"));\nTasks should typically be registered as early as possible after system startup. When running the coordinated shutdown tasks that have been registered will be performed but tasks that are added too late will not be run.\nTo start the coordinated shutdown process you can either invoke terminate() on the ActorSystem, or run runAll on the CoordinatedShutdown extension and pass it a class implementing CoordinatedShutdown.ReasonCoordinatedShutdown.Reason for informational purposes:\nScala copysource// shut down with `ActorSystemTerminateReason`\nsystem.terminate()\n\n// or define a specific reason\ncase object UserInitiatedShutdown extends CoordinatedShutdown.Reason\n\nval done: Future[Done] = CoordinatedShutdown(system).run(UserInitiatedShutdown) Java copysource// shut down with `ActorSystemTerminateReason`\nsystem.terminate();\n\n// or define a specific reason\nclass UserInitiatedShutdown implements CoordinatedShutdown.Reason {\n  @Override\n  public String toString() {\n    return \"UserInitiatedShutdown\";\n  }\n}\n\nCompletionStage<Done> done =\n    CoordinatedShutdown.get(system).runAll(new UserInitiatedShutdown());\nIt’s safe to call the run runAll method multiple times. It will only run once.\nThat also means that the ActorSystem will be terminated in the last phase. By default, the JVM is not forcefully stopped (it will be stopped if all non-daemon threads have been terminated). To enable a hard System.exit as a final action you can configure:\npekko.coordinated-shutdown.exit-jvm = on\nThe coordinated shutdown process is also started once the actor system’s root actor is stopped.\nWhen using Pekko Cluster the CoordinatedShutdown will automatically run when the cluster node sees itself as Exiting, i.e. leaving from another node will trigger the shutdown process on the leaving node. Tasks for graceful leaving of cluster including graceful shutdown of Cluster Singletons and Cluster Sharding are added automatically when Pekko Cluster is used, i.e. running the shutdown process will also trigger the graceful leaving if it’s not already in progress.\nBy default, the CoordinatedShutdown will be run when the JVM process exits, e.g. via kill SIGTERM signal (SIGINT ctrl-c doesn’t work). This behavior can be disabled with:\npekko.coordinated-shutdown.run-by-jvm-shutdown-hook=off\nIf you have application specific JVM shutdown hooks it’s recommended that you register them via the CoordinatedShutdown so that they are running before Pekko internal shutdown hooks, e.g. those shutting down Pekko Remoting (Artery).\nScala copysourceCoordinatedShutdown(system).addJvmShutdownHook {\n  println(\"custom JVM shutdown hook...\")\n} Java copysourceCoordinatedShutdown.get(system)\n    .addJvmShutdownHook(() -> System.out.println(\"custom JVM shutdown hook...\"));\nFor some tests it might be undesired to terminate the ActorSystem via CoordinatedShutdown. You can disable that by adding the following to the configuration of the ActorSystem that is used in the test:\n# Don't terminate ActorSystem via CoordinatedShutdown in tests\npekko.coordinated-shutdown.terminate-actor-system = off\npekko.coordinated-shutdown.run-by-actor-system-terminate = off\npekko.coordinated-shutdown.run-by-jvm-shutdown-hook = off\npekko.cluster.run-coordinated-shutdown-when-down = off","title":"Coordinated Shutdown"},{"location":"/typed/dispatchers.html","text":"","title":"Dispatchers"},{"location":"/typed/dispatchers.html#dispatchers","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Dispatchers.","title":"Dispatchers"},{"location":"/typed/dispatchers.html#dependency","text":"Dispatchers are part of core Pekko, which means that they are part of the pekko-actor dependency. This page describes how to use dispatchers with pekko-actor-typed, which has dependency:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/dispatchers.html#introduction","text":"A Pekko MessageDispatcher is what makes Pekko Actors “tick”, it is the engine of the machine so to speak. All MessageDispatcher implementations are also an ExecutionContextExecutor, which means that they can be used to execute arbitrary code, for instance FuturesCompletableFutures.","title":"Introduction"},{"location":"/typed/dispatchers.html#default-dispatcher","text":"Every ActorSystem will have a default dispatcher that will be used in case nothing else is configured for an Actor. The default dispatcher can be configured, and is by default a Dispatcher with the configured pekko.actor.default-dispatcher.executor. If no executor is selected a “fork-join-executor” is selected, which gives excellent performance in most cases.","title":"Default dispatcher"},{"location":"/typed/dispatchers.html#internal-dispatcher","text":"To protect the internal Actors that are spawned by the various Pekko modules, a separate internal dispatcher is used by default. The internal dispatcher can be tuned in a fine-grained way with the setting pekko.actor.internal-dispatcher, it can also be replaced by another dispatcher by making pekko.actor.internal-dispatcher an alias.","title":"Internal dispatcher"},{"location":"/typed/dispatchers.html#looking-up-a-dispatcher","text":"Dispatchers implement the ExecutionContextExecutor interface and can thus be used to run FutureCompletableFuture invocations etc.\nScala copysource// for use with Futures, Scheduler, etc.\nimport org.apache.pekko.actor.typed.DispatcherSelector\nimplicit val executionContext = context.system.dispatchers.lookup(DispatcherSelector.fromConfig(\"my-dispatcher\")) Java copysource// this is scala.concurrent.ExecutionContextExecutor, which implements\n// both scala.concurrent.ExecutionContext (for use with Futures, Scheduler, etc.)\n// and java.util.concurrent.Executor (for use with CompletableFuture etc.)\nfinal ExecutionContextExecutor ex =\n    system.dispatchers().lookup(DispatcherSelector.fromConfig(\"my-dispatcher\"));","title":"Looking up a Dispatcher"},{"location":"/typed/dispatchers.html#selecting-a-dispatcher","text":"A default dispatcher is used for all actors that are spawned without specifying a custom dispatcher. This is suitable for all actors that don’t block. Blocking in actors needs to be carefully managed, more details here.\nTo select a dispatcher use DispatcherSelector to create a Props instance for spawning your actor:\nScala copysourceimport org.apache.pekko.actor.typed.DispatcherSelector\n\ncontext.spawn(yourBehavior, \"DefaultDispatcher\")\ncontext.spawn(yourBehavior, \"ExplicitDefaultDispatcher\", DispatcherSelector.default())\ncontext.spawn(yourBehavior, \"BlockingDispatcher\", DispatcherSelector.blocking())\ncontext.spawn(yourBehavior, \"ParentDispatcher\", DispatcherSelector.sameAsParent())\ncontext.spawn(yourBehavior, \"DispatcherFromConfig\", DispatcherSelector.fromConfig(\"your-dispatcher\")) Java copysourcecontext.spawn(behavior, \"DefaultDispatcher\");\ncontext.spawn(behavior, \"ExplicitDefaultDispatcher\", DispatcherSelector.defaultDispatcher());\ncontext.spawn(behavior, \"BlockingDispatcher\", DispatcherSelector.blocking());\ncontext.spawn(behavior, \"ParentDispatcher\", DispatcherSelector.sameAsParent());\ncontext.spawn(\n    behavior, \"DispatcherFromConfig\", DispatcherSelector.fromConfig(\"your-dispatcher\"));\nDispatcherSelector has a few convenience methods:\nDispatcherSelector.defaultDispatcherSelector.defaultDispatcher to look up the default dispatcher DispatcherSelector.blocking can be used to execute actors that block e.g. a legacy database API that does not support FutureCompletionStages DispatcherSelector.sameAsParent to use the same dispatcher as the parent actor\nThe final example shows how to load a custom dispatcher from configuration and relies on this being in your application.conf:\ncopysourceyour-dispatcher {\n  type = Dispatcher\n  executor = \"thread-pool-executor\"\n  thread-pool-executor {\n    fixed-pool-size = 32\n  }\n  throughput = 1\n}","title":"Selecting a dispatcher"},{"location":"/typed/dispatchers.html#types-of-dispatchers","text":"There are 2 different types of message dispatchers:\nDispatcher This is an event-based dispatcher that binds a set of Actors to a thread pool. The default dispatcher is used if no other is specified. Shareability: Unlimited Mailboxes: Any, creates one per Actor Use cases: Default dispatcher, Bulkheading Driven by: java.util.concurrent.ExecutorService. Specify using “executor” using “fork-join-executor”, “thread-pool-executor” or the fully-qualified class name of an org.apache.pekko.dispatcher.ExecutorServiceConfigurator implementation. PinnedDispatcher This dispatcher dedicates a unique thread for each actor using it; i.e. each actor will have its own thread pool with only one thread in the pool. Shareability: None Mailboxes: Any, creates one per Actor Use cases: Bulkheading Driven by: Any org.apache.pekko.dispatch.ThreadPoolExecutorConfigurator. By default a “thread-pool-executor”.\nHere is an example configuration of a Fork Join Pool dispatcher:\ncopysourcemy-dispatcher {\n  # Dispatcher is the name of the event-based dispatcher\n  type = Dispatcher\n  # What kind of ExecutionService to use\n  executor = \"fork-join-executor\"\n  # Configuration for the fork join pool\n  fork-join-executor {\n    # Min number of threads to cap factor-based parallelism number to\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads to cap factor-based parallelism number to\n    parallelism-max = 10\n  }\n  # Throughput defines the maximum number of messages to be\n  # processed per actor before the thread jumps to the next actor.\n  # Set to 1 for as fair as possible.\n  throughput = 100\n}\nFor more configuration options, see the More dispatcher configuration examples section and the default-dispatcher section of the configuration.\nNote The parallelism-max for the fork-join-executor does not set the upper bound on the total number of threads allocated by the ForkJoinPool. It is a setting specifically talking about the number of hot threads the pool will keep running in order to reduce the latency of handling a new incoming task. You can read more about parallelism in the JDK’s ForkJoinPool documentation.\nNote The thread-pool-executor dispatcher is implemented using by a java.util.concurrent.ThreadPoolExecutor. You can read more about it in the JDK’s ThreadPoolExecutor documentation.","title":"Types of dispatchers"},{"location":"/typed/dispatchers.html#dispatcher-aliases","text":"When a dispatcher is looked up, and the given setting contains a string rather than a dispatcher config block, the lookup will treat it as an alias, and follow that string to an alternate location for a dispatcher config. If the dispatcher config is referenced both through an alias and through the absolute path only one dispatcher will be used and shared among the two ids.\nExample: configuring internal-dispatcher to be an alias for default-dispatcher:\npekko.actor.internal-dispatcher = pekko.actor.default-dispatcher","title":"Dispatcher aliases"},{"location":"/typed/dispatchers.html#blocking-needs-careful-management","text":"In some cases it is unavoidable to do blocking operations, i.e. to put a thread to sleep for an indeterminate time, waiting for an external event to occur. Examples are legacy RDBMS drivers or messaging APIs, and the underlying reason is typically that (network) I/O occurs under the covers.\nThe Managing Blocking in Akka video explains why it is bad to block inside an actor, and how you can use custom dispatchers to manage blocking when you cannot avoid it. The same principle applies with Pekko actors.","title":"Blocking Needs Careful Management"},{"location":"/typed/dispatchers.html#problem-blocking-on-default-dispatcher","text":"Simply adding blocking calls to your actor message processing like this is problematic:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject BlockingActor {\n  def apply(): Behavior[Int] =\n    Behaviors.receiveMessage { i =>\n      // DO NOT DO THIS HERE: this is an example of incorrect code,\n      // better alternatives are described further on.\n\n      // block for 5 seconds, representing blocking I/O, etc\n      Thread.sleep(5000)\n      println(s\"Blocking operation finished: $i\")\n      Behaviors.same\n    }\n} Java copysourceimport org.apache.pekko.actor.typed.*;\nimport org.apache.pekko.actor.typed.javadsl.*;\n\npublic class BlockingActor extends AbstractBehavior<Integer> {\n  public static Behavior<Integer> create() {\n    return Behaviors.setup(BlockingActor::new);\n  }\n\n  private BlockingActor(ActorContext<Integer> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Integer> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(\n            Integer.class,\n            i -> {\n              // DO NOT DO THIS HERE: this is an example of incorrect code,\n              // better alternatives are described further on.\n\n              // block for 5 seconds, representing blocking I/O, etc\n              Thread.sleep(5000);\n              System.out.println(\"Blocking operation finished: \" + i);\n              return Behaviors.same();\n            })\n        .build();\n  }\n}\nWithout any further configuration the default dispatcher runs this actor along with all other actors. This is very efficient when all actor message processing is non-blocking. When all of the available threads are blocked, however, then all the actors on the same dispatcher will starve for threads and will not be able to process incoming messages.\nNote Blocking APIs should also be avoided if possible. Try to find or build Reactive APIs, such that blocking is minimised, or moved over to dedicated dispatchers. Often when integrating with existing libraries or systems it is not possible to avoid blocking APIs. The following solution explains how to handle blocking operations properly. Note that the same hints apply to managing blocking operations anywhere in Pekko, including Streams, HTTP and other reactive libraries built on top of it.\nTo demonstrate this problem, let’s set up an application with the above BlockingActor and the following PrintActor:\nScala copysourceobject PrintActor {\n  def apply(): Behavior[Integer] =\n    Behaviors.receiveMessage { i =>\n      println(s\"PrintActor: $i\")\n      Behaviors.same\n    }\n} Java copysourceclass PrintActor extends AbstractBehavior<Integer> {\n\n  public static Behavior<Integer> create() {\n    return Behaviors.setup(PrintActor::new);\n  }\n\n  private PrintActor(ActorContext<Integer> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Integer> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(\n            Integer.class,\n            i -> {\n              System.out.println(\"PrintActor: \" + i);\n              return Behaviors.same();\n            })\n        .build();\n  }\n}\nScala copysourceval root = Behaviors.setup[Nothing] { context =>\n  for (i <- 1 to 100) {\n    context.spawn(BlockingFutureActor(), s\"futureActor-$i\") ! i\n    context.spawn(PrintActor(), s\"printActor-$i\") ! i\n  }\n  Behaviors.empty\n}\nval system = ActorSystem[Nothing](root, \"BlockingDispatcherSample\") Java copysourceBehavior<Void> root =\n    Behaviors.setup(\n        context -> {\n          for (int i = 0; i < 100; i++) {\n            context.spawn(BlockingActor.create(), \"BlockingActor-\" + i).tell(i);\n            context.spawn(PrintActor.create(), \"PrintActor-\" + i).tell(i);\n          }\n          return Behaviors.ignore();\n        });\nHere the app is sending 100 messages to BlockingActors and PrintActors and large numbers of pekko.actor.default-dispatcher threads are handling requests. When you run the above code, you will likely to see the entire application gets stuck somewhere like this:\n>　PrintActor: 44\n>　PrintActor: 45\nPrintActor is considered non-blocking, however it is not able to proceed with handling the remaining messages, since all the threads are occupied and blocked by the other blocking actors - thus leading to thread starvation.\nIn the thread state diagrams below the colours have the following meaning:\nTurquoise - Sleeping state Orange - Waiting state Green - Runnable state\nThe thread information was recorded using the YourKit profiler, however any good JVM profiler has this feature (including the free and bundled with the Oracle JDK VisualVM, as well as Java Mission Control).\nThe orange portion of the thread shows that it is idle. Idle threads are fine - they’re ready to accept new work. However, a large number of turquoise (blocked, or sleeping as in our example) threads leads to thread starvation.\nIn the above example we put the code under load by sending hundreds of messages to blocking actors which causes threads of the default dispatcher to be blocked. The fork join pool based dispatcher in Pekko then attempts to compensate for this blocking by adding more threads to the pool (default-pekko.actor.default-dispatcher 18,19,20,...). This however is not able to help if those too will immediately get blocked, and eventually the blocking operations will dominate the entire dispatcher.\nIn essence, the Thread.sleep operation has dominated all threads and caused anything executing on the default dispatcher to starve for resources (including any actor that you have not configured an explicit dispatcher for).\nNon-solution: Wrapping in a Future When facing this, you may be tempted to wrap the blocking call inside a Future and work with that instead, but this strategy is too simplistic: you are quite likely to find bottlenecks or run out of memory or threads when the application runs under increased load. Scala copysourceobject BlockingFutureActor {\n  def apply(): Behavior[Int] =\n    Behaviors.setup { context =>\n      implicit val executionContext: ExecutionContext = context.executionContext\n\n      Behaviors.receiveMessage { i =>\n        triggerFutureBlockingOperation(i)\n        Behaviors.same\n      }\n    }\n\n  def triggerFutureBlockingOperation(i: Int)(implicit ec: ExecutionContext): Future[Unit] = {\n    println(s\"Calling blocking Future: $i\")\n    Future {\n      Thread.sleep(5000) // block for 5 seconds\n      println(s\"Blocking future finished $i\")\n    }\n  }\n} The key problematic line here is this: implicit val executionContext: ExecutionContext = context.executionContext\n Using context.executionContext as the dispatcher on which the blocking Future executes can still be a problem, since this dispatcher is by default used for all other actor processing unless you set up a separate dispatcher for the actor.","title":"Problem: Blocking on default dispatcher"},{"location":"/typed/dispatchers.html#solution-dedicated-dispatcher-for-blocking-operations","text":"An efficient method of isolating the blocking behavior, such that it does not impact the rest of the system, is to prepare and use a dedicated dispatcher for all those blocking operations. This technique is often referred to as “bulk-heading” or simply “isolating blocking”.\nIn application.conf, the dispatcher dedicated to blocking behavior should be configured as follows:\ncopysourcemy-blocking-dispatcher {\n  type = Dispatcher\n  executor = \"thread-pool-executor\"\n  thread-pool-executor {\n    fixed-pool-size = 16\n  }\n  throughput = 1\n}\nA thread-pool-executor based dispatcher allows us to limit the number of threads it will host, and this way we gain tight control over the maximum number of blocked threads the system may use.\nThe exact size should be fine tuned depending on the workload you’re expecting to run on this dispatcher.\nWhenever blocking has to be done, use the above configured dispatcher instead of the default one:\nScala copysourceobject SeparateDispatcherFutureActor {\n  def apply(): Behavior[Int] =\n    Behaviors.setup { context =>\n      implicit val executionContext: ExecutionContext =\n        context.system.dispatchers.lookup(DispatcherSelector.fromConfig(\"my-blocking-dispatcher\"))\n\n      Behaviors.receiveMessage { i =>\n        triggerFutureBlockingOperation(i)\n        Behaviors.same\n      }\n    }\n\n  def triggerFutureBlockingOperation(i: Int)(implicit ec: ExecutionContext): Future[Unit] = {\n    println(s\"Calling blocking Future: $i\")\n    Future {\n      Thread.sleep(5000) // block for 5 seconds\n      println(s\"Blocking future finished $i\")\n    }\n  }\n} Java copysourceclass SeparateDispatcherFutureActor extends AbstractBehavior<Integer> {\n  private final Executor ec;\n\n  public static Behavior<Integer> create() {\n    return Behaviors.setup(SeparateDispatcherFutureActor::new);\n  }\n\n  private SeparateDispatcherFutureActor(ActorContext<Integer> context) {\n    super(context);\n    ec =\n        context\n            .getSystem()\n            .dispatchers()\n            .lookup(DispatcherSelector.fromConfig(\"my-blocking-dispatcher\"));\n  }\n\n  @Override\n  public Receive<Integer> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(\n            Integer.class,\n            i -> {\n              triggerFutureBlockingOperation(i, ec);\n              return Behaviors.same();\n            })\n        .build();\n  }\n\n  private static void triggerFutureBlockingOperation(Integer i, Executor ec) {\n    System.out.println(\"Calling blocking Future on separate dispatcher: \" + i);\n    CompletableFuture<Integer> f =\n        CompletableFuture.supplyAsync(\n            () -> {\n              try {\n                Thread.sleep(5000);\n                System.out.println(\"Blocking future finished: \" + i);\n                return i;\n              } catch (InterruptedException e) {\n                return -1;\n              }\n            },\n            ec);\n  }\n}\nThe thread pool behavior is shown in the below diagram.\nMessages sent to SeparateDispatcherFutureActorSeparateDispatcherCompletionStageActor and PrintActor are handled by the default dispatcher - the green lines, which represent the actual execution.\nWhen blocking operations are run on the my-blocking-dispatcher, it uses the threads (up to the configured limit) to handle these operations. The sleeping in this case is nicely isolated to just this dispatcher, and the default one remains unaffected, allowing the rest of the application to proceed as if nothing bad was happening. After a certain period of idleness, threads started by this dispatcher will be shut down.\nIn this case, the throughput of other actors was not impacted - they were still served on the default dispatcher.\nThis is the recommended way of dealing with any kind of blocking in reactive applications.\nFor a similar discussion specifically about Pekko HTTP, refer to Handling blocking operations in Pekko HTTP.","title":"Solution: Dedicated dispatcher for blocking operations"},{"location":"/typed/dispatchers.html#available-solutions-to-blocking-operations","text":"The non-exhaustive list of adequate solutions to the “blocking problem” includes the following suggestions:\nDo the blocking call within a FutureCompletionStage, ensuring an upper bound on the number of such calls at any point in time (submitting an unbounded number of tasks of this nature will exhaust your memory or thread limits). Do the blocking call within a Future, providing a thread pool with an upper limit on the number of threads which is appropriate for the hardware on which the application runs, as explained in detail in this section. Dedicate a single thread to manage a set of blocking resources (e.g. a NIO selector driving multiple channels) and dispatch events as they occur as actor messages. Do the blocking call within an actor (or a set of actors) managed by a router, making sure to configure a thread pool which is either dedicated for this purpose or sufficiently sized.\nThe last possibility is especially well-suited for resources which are single-threaded in nature, like database handles which traditionally can only execute one outstanding query at a time and use internal synchronization to ensure this. A common pattern is to create a router for N actors, each of which wraps a single DB connection and handles queries as sent to the router. The number N must then be tuned for maximum throughput, which will vary depending on which DBMS is deployed on what hardware.\nNote Configuring thread pools is a task best delegated to Pekko, configure it in application.conf and instantiate through an ActorSystem","title":"Available solutions to blocking operations"},{"location":"/typed/dispatchers.html#more-dispatcher-configuration-examples","text":"","title":"More dispatcher configuration examples"},{"location":"/typed/dispatchers.html#fixed-pool-size","text":"Configuring a dispatcher with fixed thread pool size, e.g. for actors that perform blocking IO:\ncopysourceblocking-io-dispatcher {\n  type = Dispatcher\n  executor = \"thread-pool-executor\"\n  thread-pool-executor {\n    fixed-pool-size = 32\n  }\n  throughput = 1\n}","title":"Fixed pool size"},{"location":"/typed/dispatchers.html#cores","text":"Another example that uses the thread pool based on the number of cores (e.g. for CPU bound tasks)\ncopysourcemy-thread-pool-dispatcher {\n  # Dispatcher is the name of the event-based dispatcher\n  type = Dispatcher\n  # What kind of ExecutionService to use\n  executor = \"thread-pool-executor\"\n  # Configuration for the thread pool\n  thread-pool-executor {\n    # minimum number of threads to cap factor-based core number to\n    core-pool-size-min = 2\n    # No of core threads ... ceil(available processors * factor)\n    core-pool-size-factor = 2.0\n    # maximum number of threads to cap factor-based number to\n    core-pool-size-max = 10\n  }\n  # Throughput defines the maximum number of messages to be\n  # processed per actor before the thread jumps to the next actor.\n  # Set to 1 for as fair as possible.\n  throughput = 100\n}","title":"Cores"},{"location":"/typed/dispatchers.html#pinned","text":"A separate thread is dedicated for each actor that is configured to use the pinned dispatcher.\nConfiguring a PinnedDispatcher:\ncopysourcemy-pinned-dispatcher {\n  executor = \"thread-pool-executor\"\n  type = PinnedDispatcher\n}\nNote that thread-pool-executor configuration as per the above my-thread-pool-dispatcher example is NOT applicable. This is because every actor will have its own thread pool when using PinnedDispatcher, and that pool will have only one thread.\nNote that it’s not guaranteed that the same thread is used over time, since the core pool timeout is used for PinnedDispatcher to keep resource usage down in case of idle actors. To use the same thread all the time you need to add thread-pool-executor.allow-core-timeout=off to the configuration of the PinnedDispatcher.","title":"Pinned"},{"location":"/typed/dispatchers.html#thread-shutdown-timeout","text":"Both the fork-join-executor and thread-pool-executor may shutdown threads when they are not used. If it’s desired to keep the threads alive longer there are some timeout settings that can be adjusted.\ncopysourcemy-dispatcher-with-timeouts {\n  type = Dispatcher\n  executor = \"thread-pool-executor\"\n  thread-pool-executor {\n    fixed-pool-size = 16\n    # Keep alive time for threads\n    keep-alive-time = 60s\n    # Allow core threads to time out\n    allow-core-timeout = off\n  }\n  # How long time the dispatcher will wait for new actors until it shuts down\n  shutdown-timeout = 60s\n}\nWhen using the dispatcher as an ExecutionContext without assigning actors to it the shutdown-timeout should typically be increased, since the default of 1 second may cause too frequent shutdown of the entire thread pool.","title":"Thread shutdown timeout"},{"location":"/typed/mailboxes.html","text":"","title":"Mailboxes"},{"location":"/typed/mailboxes.html#mailboxes","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Mailboxes.","title":"Mailboxes"},{"location":"/typed/mailboxes.html#dependency","text":"Mailboxes are part of core Pekko, which means that they are part of the pekko-actor dependency. This page describes how to use mailboxes with pekko-actor-typed, which has dependency:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/mailboxes.html#introduction","text":"Each actor in Pekko has a Mailbox, this is where the messages are enqueued before being processed by the actor.\nBy default an unbounded mailbox is used, this means any number of messages can be enqueued into the mailbox.\nThe unbounded mailbox is a convenient default but in a scenario where messages are added to the mailbox faster than the actor can process them, this can lead to the application running out of memory. For this reason a bounded mailbox can be specified, the bounded mailbox will pass new messages to deadletters when the mailbox is full.\nFor advanced use cases it is also possible to defer mailbox selection to config by pointing to a config path.","title":"Introduction"},{"location":"/typed/mailboxes.html#selecting-what-mailbox-is-used","text":"","title":"Selecting what mailbox is used"},{"location":"/typed/mailboxes.html#selecting-a-mailbox-type-for-an-actor","text":"To select a specific mailbox for an actor use MailboxSelectorMailboxSelector to create a PropsProps instance for spawning your actor:\nScala copysourcecontext.spawn(childBehavior, \"bounded-mailbox-child\", MailboxSelector.bounded(100))\n\nval props = MailboxSelector.fromConfig(\"my-app.my-special-mailbox\")\ncontext.spawn(childBehavior, \"from-config-mailbox-child\", props) Java copysourcecontext.spawn(childBehavior, \"bounded-mailbox-child\", MailboxSelector.bounded(100));\n\ncontext.spawn(\n    childBehavior,\n    \"from-config-mailbox-child\",\n    MailboxSelector.fromConfig(\"my-app.my-special-mailbox\"));\nfromConfigfromConfig takes an absolute config path to a block defining the dispatcher in the config file:\ncopysourcemy-app {\n  my-special-mailbox {\n    mailbox-type = \"org.apache.pekko.dispatch.SingleConsumerOnlyUnboundedMailbox\"\n  }\n}","title":"Selecting a Mailbox Type for an Actor"},{"location":"/typed/mailboxes.html#default-mailbox","text":"The default mailbox is used when the mailbox is not specified and is the SingleConsumerOnlyUnboundedMailboxSingleConsumerOnlyUnboundedMailbox","title":"Default Mailbox"},{"location":"/typed/mailboxes.html#which-configuration-is-passed-to-the-mailbox-type","text":"Each mailbox type is implemented by a class which extends MailboxTypeMailboxType and takes two constructor arguments: a ActorSystem.SettingsActorSystem.Settings object and a Config section. The latter is computed by obtaining the named configuration section from the ActorSystemActorSystem configuration, overriding its id key with the configuration path of the mailbox type and adding a fall-back to the default mailbox configuration section.","title":"Which Configuration is passed to the Mailbox Type"},{"location":"/typed/mailboxes.html#mailbox-implementations","text":"Pekko ships with a number of mailbox implementations:\nSingleConsumerOnlyUnboundedMailboxSingleConsumerOnlyUnboundedMailbox (default) This is the default Backed by a Multiple-Producer Single-Consumer queue, cannot be used with BalancingDispatcher Blocking: No Bounded: No Configuration name: \"org.apache.pekko.dispatch.SingleConsumerOnlyUnboundedMailbox\" UnboundedMailboxUnboundedMailbox Backed by a java.util.concurrent.ConcurrentLinkedQueue Blocking: No Bounded: No Configuration name: \"unbounded\" or \"org.apache.pekko.dispatch.UnboundedMailbox\" NonBlockingBoundedMailboxNonBlockingBoundedMailbox Backed by a very efficient Multiple-Producer Single-Consumer queue Blocking: No (discards overflowing messages into deadLetters) Bounded: Yes Configuration name: \"org.apache.pekko.dispatch.NonBlockingBoundedMailbox\" UnboundedControlAwareMailboxUnboundedControlAwareMailbox Delivers messages that extend dispatch.ControlMessagedispatch.ControlMessage with higher priority Backed by two java.util.concurrent.ConcurrentLinkedQueue Blocking: No Bounded: No Configuration name: \"org.apache.pekko.dispatch.UnboundedControlAwareMailbox\" UnboundedPriorityMailboxUnboundedPriorityMailbox Backed by a java.util.concurrent.PriorityBlockingQueue Delivery order for messages of equal priority is undefined - contrast with the UnboundedStablePriorityMailbox Blocking: No Bounded: No Configuration name: \"org.apache.pekko.dispatch.UnboundedPriorityMailbox\" UnboundedStablePriorityMailboxUnboundedStablePriorityMailbox Backed by a java.util.concurrent.PriorityBlockingQueue wrapped in an util.PriorityQueueStabilizerutil.PriorityQueueStabilizer FIFO order is preserved for messages of equal priority - contrast with the UnboundedPriorityMailbox Blocking: No Bounded: No Configuration name: \"org.apache.pekko.dispatch.UnboundedStablePriorityMailbox\"\nOther bounded mailbox implementations which will block the sender if the capacity is reached and configured with non-zero mailbox-push-timeout-time.\nNote The following mailboxes should only be used with zero mailbox-push-timeout-time.\nBoundedMailboxBoundedMailbox Backed by a java.util.concurrent.LinkedBlockingQueue Blocking: Yes if used with non-zero mailbox-push-timeout-time, otherwise No Bounded: Yes Configuration name: \"bounded\" or \"org.apache.pekko.dispatch.BoundedMailbox\" BoundedPriorityMailboxBoundedPriorityMailbox Backed by a java.util.PriorityQueue wrapped in an util.BoundedBlockingQueueutil.BoundedBlockingQueue Delivery order for messages of equal priority is undefined - contrast with the BoundedStablePriorityMailbox Blocking: Yes if used with non-zero mailbox-push-timeout-time, otherwise No Bounded: Yes Configuration name: \"org.apache.pekko.dispatch.BoundedPriorityMailbox\" BoundedStablePriorityMailboxBoundedStablePriorityMailbox Backed by a java.util.PriorityQueue wrapped in an util.PriorityQueueStabilizerutil.PriorityQueueStabilizer and an util.BoundedBlockingQueueutil.BoundedBlockingQueue FIFO order is preserved for messages of equal priority - contrast with the BoundedPriorityMailbox Blocking: Yes if used with non-zero mailbox-push-timeout-time, otherwise No Bounded: Yes Configuration name: \"org.apache.pekko.dispatch.BoundedStablePriorityMailbox\" BoundedControlAwareMailboxBoundedControlAwareMailbox Delivers messages that extend dispatch.ControlMessagedispatch.ControlMessage with higher priority Backed by two java.util.concurrent.ConcurrentLinkedQueue and blocking on enqueue if capacity has been reached Blocking: Yes if used with non-zero mailbox-push-timeout-time, otherwise No Bounded: Yes Configuration name: \"org.apache.pekko.dispatch.BoundedControlAwareMailbox\"","title":"Mailbox Implementations"},{"location":"/typed/mailboxes.html#custom-mailbox-type","text":"The best way to show how to create your own Mailbox type is by example:\nScala copysource// Marker trait used for mailbox requirements mapping\ntrait MyUnboundedMessageQueueSemantics Java copysource// Marker interface used for mailbox requirements mapping\npublic interface MyUnboundedMessageQueueSemantics {}\nScala copysourceimport org.apache.pekko\nimport pekko.actor.ActorRef\nimport pekko.actor.ActorSystem\nimport pekko.dispatch.Envelope\nimport pekko.dispatch.MailboxType\nimport pekko.dispatch.MessageQueue\nimport pekko.dispatch.ProducesMessageQueue\nimport com.typesafe.config.Config\nimport java.util.concurrent.ConcurrentLinkedQueue\nimport scala.Option\n\nobject MyUnboundedMailbox {\n  // This is the MessageQueue implementation\n  class MyMessageQueue extends MessageQueue with MyUnboundedMessageQueueSemantics {\n\n    private final val queue = new ConcurrentLinkedQueue[Envelope]()\n\n    // these should be implemented; queue used as example\n    def enqueue(receiver: ActorRef, handle: Envelope): Unit =\n      queue.offer(handle)\n    def dequeue(): Envelope = queue.poll()\n    def numberOfMessages: Int = queue.size\n    def hasMessages: Boolean = !queue.isEmpty\n    def cleanUp(owner: ActorRef, deadLetters: MessageQueue): Unit = {\n      while (hasMessages) {\n        deadLetters.enqueue(owner, dequeue())\n      }\n    }\n  }\n}\n\n// This is the Mailbox implementation\nclass MyUnboundedMailbox extends MailboxType with ProducesMessageQueue[MyUnboundedMailbox.MyMessageQueue] {\n\n  import MyUnboundedMailbox._\n\n  // This constructor signature must exist, it will be called by Pekko\n  def this(settings: ActorSystem.Settings, config: Config) = {\n    // put your initialization code here\n    this()\n  }\n\n  // The create method is called to create the MessageQueue\n  final override def create(owner: Option[ActorRef], system: Option[ActorSystem]): MessageQueue =\n    new MyMessageQueue()\n} Java copysourceimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.dispatch.Envelope;\nimport org.apache.pekko.dispatch.MailboxType;\nimport org.apache.pekko.dispatch.MessageQueue;\nimport org.apache.pekko.dispatch.ProducesMessageQueue;\nimport com.typesafe.config.Config;\nimport java.util.concurrent.ConcurrentLinkedQueue;\nimport java.util.Queue;\nimport scala.Option;\n\npublic class MyUnboundedMailbox\n    implements MailboxType, ProducesMessageQueue<MyUnboundedMailbox.MyMessageQueue> {\n\n  // This is the MessageQueue implementation\n  public static class MyMessageQueue implements MessageQueue, MyUnboundedMessageQueueSemantics {\n    private final Queue<Envelope> queue = new ConcurrentLinkedQueue<Envelope>();\n\n    // these must be implemented; queue used as example\n    public void enqueue(ActorRef receiver, Envelope handle) {\n      queue.offer(handle);\n    }\n\n    public Envelope dequeue() {\n      return queue.poll();\n    }\n\n    public int numberOfMessages() {\n      return queue.size();\n    }\n\n    public boolean hasMessages() {\n      return !queue.isEmpty();\n    }\n\n    public void cleanUp(ActorRef owner, MessageQueue deadLetters) {\n      while (!queue.isEmpty()) {\n        deadLetters.enqueue(owner, dequeue());\n      }\n    }\n  }\n\n  // This constructor signature must exist, it will be called by Pekko\n  public MyUnboundedMailbox(ActorSystem.Settings settings, Config config) {\n    // put your initialization code here\n  }\n\n  // The create method is called to create the MessageQueue\n  public MessageQueue create(Option<ActorRef> owner, Option<ActorSystem> system) {\n    return new MyMessageQueue();\n  }\n}\nAnd then you specify the FQCN of your MailboxType as the value of the “mailbox-type” in the dispatcher configuration, or the mailbox configuration.\nNote Make sure to include a constructor which takes actor.ActorSystem.Settingsactor.ActorSystem.Settings and com.typesafe.config.Config arguments, as this constructor is invoked reflectively to construct your mailbox type. The config passed in as second argument is that section from the configuration which describes the dispatcher or mailbox setting using this mailbox type; the mailbox type will be instantiated once for each dispatcher or mailbox setting using it.","title":"Custom Mailbox type"},{"location":"/typed/testing.html","text":"","title":"Testing"},{"location":"/typed/testing.html#testing","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Testing.","title":"Testing"},{"location":"/typed/testing.html#module-info","text":"To use Actor TestKit add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-testkit-typed\" % PekkoVersion % Test Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-testkit-typed_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  testImplementation \"org.apache.pekko:pekko-actor-testkit-typed_${versions.ScalaBinary}\"\n}\nWe recommend using Pekko TestKit with ScalaTest: sbt libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.1.4\" % Test Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencies>\n  <dependency>\n    <groupId>org.scalatest</groupId>\n    <artifactId>scalatest_${scala.binary.version}</artifactId>\n    <version>3.1.4</version>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  testImplementation \"org.scalatest:scalatest_${versions.ScalaBinary}:3.1.4\"\n}\nProject Info: Pekko Actor Testkit (typed) Artifact org.apache.pekko pekko-actor-testkit-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.actor.testkit.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/testing.html#introduction","text":"Testing can either be done asynchronously using a real ActorSystemActorSystem or synchronously on the testing thread using the BehaviorTestKitBehaviorTestKit.\nFor testing logic in a BehaviorBehavior in isolation synchronous testing is preferred, but the features that can be tested are limited. For testing interactions between multiple actors a more realistic asynchronous test is preferred.\nThose two testing approaches are described in:\nAsynchronous testing Basic example Observing mocked behavior Test framework integration Configuration Controlling the scheduler Test of logging Silence logging output from tests Synchronous behavior testing Spawning children Sending messages Testing other effects Checking for Log Messages","title":"Introduction"},{"location":"/typed/testing-async.html","text":"","title":"Asynchronous testing"},{"location":"/typed/testing-async.html#asynchronous-testing","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Testing.\nAsynchronous testing uses a real ActorSystemActorSystem that allows you to test your Actors in a more realistic environment.\nThe minimal setup consists of the test procedure, which provides the desired stimuli, the actor under test, and an actor receiving replies. Bigger systems replace the actor under test with a network of actors, apply stimuli at varying injection points and arrange results to be sent from different emission points, but the basic principle stays the same in that a single procedure drives the test.","title":"Asynchronous testing"},{"location":"/typed/testing-async.html#basic-example","text":"Actor under test:\nScala copysourceobject Echo {\n  case class Ping(message: String, response: ActorRef[Pong])\n  case class Pong(message: String)\n\n  def apply(): Behavior[Ping] = Behaviors.receiveMessage {\n    case Ping(m, replyTo) =>\n      replyTo ! Pong(m)\n      Behaviors.same\n  }\n} Java copysourcepublic static class Echo {\n  public static class Ping {\n    public final String message;\n    public final ActorRef<Pong> replyTo;\n\n    public Ping(String message, ActorRef<Pong> replyTo) {\n      this.message = message;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Pong {\n    public final String message;\n\n    public Pong(String message) {\n      this.message = message;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n      if (this == o) return true;\n      if (!(o instanceof Pong)) return false;\n      Pong pong = (Pong) o;\n      return message.equals(pong.message);\n    }\n\n    @Override\n    public int hashCode() {\n      return Objects.hash(message);\n    }\n  }\n\n  public static Behavior<Ping> create() {\n    return Behaviors.receive(Ping.class)\n        .onMessage(\n            Ping.class,\n            ping -> {\n              ping.replyTo.tell(new Pong(ping.message));\n              return Behaviors.same();\n            })\n        .build();\n  }\n}\nTests create an instance of ActorTestKitActorTestKit. This provides access to:\nAn ActorSystem Methods for spawning Actors. These are created under the special testkit user guardian A method to shut down the ActorSystem from the test suite\nThis first example is using the “raw” ActorTestKit but if you are using ScalaTestJUnit you can simplify the tests by using the Test framework integration. It’s still good to read this section to understand how it works.\nScala copysourceimport org.apache.pekko.actor.testkit.typed.scaladsl.ActorTestKit\n\nimport org.scalatest.BeforeAndAfterAll\nimport org.scalatest.matchers.should.Matchers\nimport org.scalatest.wordspec.AnyWordSpec\n\nclass AsyncTestingExampleSpec\n    extends AnyWordSpec\n    with BeforeAndAfterAll\n    with Matchers {\n  val testKit = ActorTestKit()\n} Java copysourceimport org.apache.pekko.actor.testkit.typed.javadsl.ActorTestKit;\n\npublic class AsyncTestingExampleTest\n{\n  static final ActorTestKit testKit = ActorTestKit.create();\n}\nYour test is responsible for shutting down the ActorSystemActorSystem e.g. using BeforeAndAfterAll when using ScalaTest@AfterClass when using JUnit.\nScala copysourceoverride def afterAll(): Unit = testKit.shutdownTestKit() Java copysource@AfterClass\npublic static void cleanup() {\n  testKit.shutdownTestKit();\n}\nThe following demonstrates:\nCreating an actor from the TestKit’s system using spawn Creating a TestProbe Verifying that the actor under test responds via the TestProbe\nNote that it is possible to use a TestProbe directly as a RecipientRefRecipientRef (a common supertype of ActorRef and Cluster Sharding EntityRef), in cases where a message protocol uses RecipientRef instead of specifying ActorRef or EntityRef.\nScala copysourceval pinger = testKit.spawn(Echo(), \"ping\")\nval probe = testKit.createTestProbe[Echo.Pong]()\npinger ! Echo.Ping(\"hello\", probe.ref)\nprobe.expectMessage(Echo.Pong(\"hello\")) Java copysourceActorRef<Echo.Ping> pinger = testKit.spawn(Echo.create(), \"ping\");\nTestProbe<Echo.Pong> probe = testKit.createTestProbe();\npinger.tell(new Echo.Ping(\"hello\", probe.ref()));\nprobe.expectMessage(new Echo.Pong(\"hello\"));\nActors can also be spawned anonymously:\nScala copysourceval pinger = testKit.spawn(Echo()) Java copysourceActorRef<Echo.Ping> pinger = testKit.spawn(Echo.create());\nNote that you can add import testKit._ to get access to the spawn and createTestProbe methods at the top level without prefixing them with testKit.","title":"Basic example"},{"location":"/typed/testing-async.html#stopping-actors","text":"The method will wait until the actor stops or throw an assertion error in case of a timeout.\nScala copysourceval pinger1 = testKit.spawn(Echo(), \"pinger\")\npinger1 ! Echo.Ping(\"hello\", probe.ref)\nprobe.expectMessage(Echo.Pong(\"hello\"))\ntestKit.stop(pinger1) // Uses default timeout\n\n// Immediately creating an actor with the same name\nval pinger2 = testKit.spawn(Echo(), \"pinger\")\npinger2 ! Echo.Ping(\"hello\", probe.ref)\nprobe.expectMessage(Echo.Pong(\"hello\"))\ntestKit.stop(pinger2, 10.seconds) // Custom timeout Java copysourceActorRef<Echo.Ping> pinger1 = testKit.spawn(Echo.create(), \"pinger\");\npinger1.tell(new Echo.Ping(\"hello\", probe.ref()));\nprobe.expectMessage(new Echo.Pong(\"hello\"));\ntestKit.stop(pinger1);\n\n// Immediately creating an actor with the same name\nActorRef<Echo.Ping> pinger2 = testKit.spawn(Echo.create(), \"pinger\");\npinger2.tell(new Echo.Ping(\"hello\", probe.ref()));\nprobe.expectMessage(new Echo.Pong(\"hello\"));\ntestKit.stop(pinger2, Duration.ofSeconds(10));\nThe stop method can only be used for actors that were spawned by the same ActorTestKitActorTestKit. Other actors will not be stopped by that method.","title":"Stopping actors"},{"location":"/typed/testing-async.html#observing-mocked-behavior","text":"When testing a component (which may be an actor or not) that interacts with other actors it can be useful to not have to run the other actors it depends on. Instead, you might want to create mock behaviors that accept and possibly respond to messages in the same way the other actor would do but without executing any actual logic. In addition to this it can also be useful to observe those interactions to assert that the component under test did send the expected messages. This allows the same kinds of tests as classic TestActor/Autopilot.\nAs an example, let’s assume we’d like to test the following component:\nScala copysourcecase class Message(i: Int, replyTo: ActorRef[Try[Int]])\n\nclass Producer(publisher: ActorRef[Message])(implicit scheduler: Scheduler) {\n\n  def produce(messages: Int)(implicit timeout: Timeout): Unit = {\n    (0 until messages).foreach(publish)\n  }\n\n  private def publish(i: Int)(implicit timeout: Timeout): Future[Try[Int]] = {\n    publisher.ask(ref => Message(i, ref))\n  }\n\n} Java copysource static class Message {\n  int i;\n  ActorRef<Integer> replyTo;\n\n  Message(int i, ActorRef<Integer> replyTo) {\n    this.i = i;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class Producer {\n\n  private Scheduler scheduler;\n  private ActorRef<Message> publisher;\n\n  Producer(Scheduler scheduler, ActorRef<Message> publisher) {\n    this.scheduler = scheduler;\n    this.publisher = publisher;\n  }\n\n  public void produce(int messages) {\n    IntStream.range(0, messages).forEach(this::publish);\n  }\n\n  private CompletionStage<Integer> publish(int i) {\n    return AskPattern.ask(\n        publisher,\n        (ActorRef<Integer> ref) -> new Message(i, ref),\n        Duration.ofSeconds(3),\n        scheduler);\n  }\n}\nIn our test, we create a mocked publisher actor. Additionally we use Behaviors.monitor with a TestProbe in order to be able to verify the interaction of the producer with the publisher:\nScala copysourceimport testKit._\n\n// simulate the happy path\nval mockedBehavior = Behaviors.receiveMessage[Message] { msg =>\n  msg.replyTo ! Success(msg.i)\n  Behaviors.same\n}\nval probe = testKit.createTestProbe[Message]()\nval mockedPublisher = testKit.spawn(Behaviors.monitor(probe.ref, mockedBehavior))\n\n// test our component\nval producer = new Producer(mockedPublisher)\nval messages = 3\nproducer.produce(messages)\n\n// verify expected behavior\nfor (i <- 0 until messages) {\n  val msg = probe.expectMessageType[Message]\n  msg.i shouldBe i\n} Java copysource// simulate the happy path\nBehavior<Message> mockedBehavior =\n    Behaviors.receiveMessage(\n        message -> {\n          message.replyTo.tell(message.i);\n          return Behaviors.same();\n        });\nTestProbe<Message> probe = testKit.createTestProbe();\nActorRef<Message> mockedPublisher =\n    testKit.spawn(Behaviors.monitor(Message.class, probe.ref(), mockedBehavior));\n\n// test our component\nProducer producer = new Producer(testKit.scheduler(), mockedPublisher);\nint messages = 3;\nproducer.produce(messages);\n\n// verify expected behavior\nIntStream.range(0, messages)\n    .forEach(\n        i -> {\n          Message msg = probe.expectMessageClass(Message.class);\n          assertEquals(i, msg.i);\n        });","title":"Observing mocked behavior"},{"location":"/typed/testing-async.html#test-framework-integration","text":"If you are using JUnit you can use TestKitJunitResource to have the async test kit automatically shutdown when the test is complete. Note that the dependency on JUnit is marked as optional from the test kit module, so your project must explicitly include a dependency on JUnit to use this.\nIf you are using ScalaTest you can extend ScalaTestWithActorTestKit to have the async test kit automatically shutdown when the test is complete. This is done in afterAll from the BeforeAndAfterAll trait. If you override that method you should call super.afterAll to shutdown the test kit. Note that the dependency on ScalaTest is marked as optional from the test kit module, so your project must explicitly include a dependency on ScalaTest to use this.\nScala copysourceimport pekko.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit\nimport org.scalatest.wordspec.AnyWordSpecLike\n\nclass ScalaTestIntegrationExampleSpec extends ScalaTestWithActorTestKit with AnyWordSpecLike {\n\n  \"Something\" must {\n    \"behave correctly\" in {\n      val pinger = testKit.spawn(Echo(), \"ping\")\n      val probe = testKit.createTestProbe[Echo.Pong]()\n      pinger ! Echo.Ping(\"hello\", probe.ref)\n      probe.expectMessage(Echo.Pong(\"hello\"))\n    }\n  }\n} Java copysourceimport org.apache.pekko.actor.testkit.typed.javadsl.TestKitJunitResource;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestProbe;\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.junit.ClassRule;\nimport org.junit.Test;\n\npublic class JunitIntegrationExampleTest {\n\n  @ClassRule public static final TestKitJunitResource testKit = new TestKitJunitResource();\n\n\n  @Test\n  public void testSomething() {\n    ActorRef<Echo.Ping> pinger = testKit.spawn(Echo.create(), \"ping\");\n    TestProbe<Echo.Pong> probe = testKit.createTestProbe();\n    pinger.tell(new Echo.Ping(\"hello\", probe.ref()));\n    probe.expectMessage(new Echo.Pong(\"hello\"));\n  }\n}","title":"Test framework integration"},{"location":"/typed/testing-async.html#configuration","text":"By default the ActorTestKit loads configuration from application-test.conf if that exists, otherwise it is using default configuration from the reference.conf resources that ship with the Pekko libraries. The application.conf of your project is not used in this case. A specific configuration can be given as parameter when creating the TestKit.\nIf you prefer to use application.conf you can pass that as the configuration parameter to the TestKit. It’s loaded with:\nScala copysourceimport com.typesafe.config.ConfigFactory\n\nConfigFactory.load() Java copysourceimport com.typesafe.config.ConfigFactory;\n\nConfigFactory.load()\nIt’s often convenient to define configuration for a specific test as a String in the test itself and use that as the configuration parameter to the TestKit. ConfigFactory.parseString can be used for that:\nScala copysourceConfigFactory.parseString(\"\"\"\n  pekko.loglevel = DEBUG\n  pekko.log-config-on-start = on\n  \"\"\") Java copysourceConfigFactory.parseString(\"pekko.loglevel = DEBUG \\n\" + \"pekko.log-config-on-start = on \\n\")\nCombining those approaches using withFallback:\nScala copysourceConfigFactory.parseString(\"\"\"\n  pekko.loglevel = DEBUG\n  pekko.log-config-on-start = on\n  \"\"\").withFallback(ConfigFactory.load()) Java copysourceConfigFactory.parseString(\"pekko.loglevel = DEBUG \\n\" + \"pekko.log-config-on-start = on \\n\")\n    .withFallback(ConfigFactory.load())\nMore information can be found in the documentation of the configuration library.\nNote Note that reference.conf files are intended for libraries to define default values and shouldn’t be used in an application. It’s not supported to override a config property owned by one library in a reference.conf of another library.","title":"Configuration"},{"location":"/typed/testing-async.html#controlling-the-scheduler","text":"It can be hard to reliably unit test specific scenario’s when your actor relies on timing: especially when running many tests in parallel it can be hard to get the timing just right. Making such tests more reliable by using generous timeouts make the tests take a long time to run.\nFor such situations, we provide a scheduler where you can manually, explicitly advance the clock.\nScala copysourceimport scala.concurrent.duration._\nimport org.apache.pekko\nimport pekko.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit\nimport pekko.actor.testkit.typed.scaladsl.ManualTime\nimport pekko.actor.testkit.typed.scaladsl.TestProbe\nimport pekko.actor.testkit.typed.scaladsl.LogCapturing\nimport pekko.actor.typed.scaladsl.Behaviors\nimport org.scalatest.wordspec.AnyWordSpecLike\n\nclass ManualTimerExampleSpec\n    extends ScalaTestWithActorTestKit(ManualTime.config)\n    with AnyWordSpecLike\n    with LogCapturing {\n\n  val manualTime: ManualTime = ManualTime()\n\n  \"A timer\" must {\n    \"schedule non-repeated ticks\" in {\n      case object Tick\n      case object Tock\n\n      val probe = TestProbe[Tock.type]()\n      val behavior = Behaviors.withTimers[Tick.type] { timer =>\n        timer.startSingleTimer(Tick, 10.millis)\n        Behaviors.receiveMessage { _ =>\n          probe.ref ! Tock\n          Behaviors.same\n        }\n      }\n\n      spawn(behavior)\n\n      manualTime.expectNoMessageFor(9.millis, probe)\n\n      manualTime.timePasses(2.millis)\n      probe.expectMessage(Tock)\n\n      manualTime.expectNoMessageFor(10.seconds, probe)\n    }\n  }\n} Java copysource import org.apache.pekko.actor.testkit.typed.javadsl.LogCapturing;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.testkit.typed.javadsl.ManualTime;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestKitJunitResource;\nimport org.junit.ClassRule;\nimport org.junit.Rule;\nimport org.scalatestplus.junit.JUnitSuite;\nimport java.time.Duration;\n\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\n\nimport org.junit.Test;\n\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestProbe;\n\npublic class ManualTimerExampleTest extends JUnitSuite {\n\n  @ClassRule\n  public static final TestKitJunitResource testKit = new TestKitJunitResource(ManualTime.config());\n\n  @Rule public final LogCapturing logCapturing = new LogCapturing();\n\n  private final ManualTime manualTime = ManualTime.get(testKit.system());\n\n  static final class Tick {\n    private Tick() {}\n\n    static final Tick INSTANCE = new Tick();\n  }\n\n  static final class Tock {}\n\n  @Test\n  public void testScheduleNonRepeatedTicks() {\n    TestProbe<Tock> probe = testKit.createTestProbe();\n    Behavior<Tick> behavior =\n        Behaviors.withTimers(\n            timer -> {\n              timer.startSingleTimer(Tick.INSTANCE, Duration.ofMillis(10));\n              return Behaviors.receiveMessage(\n                  tick -> {\n                    probe.ref().tell(new Tock());\n                    return Behaviors.same();\n                  });\n            });\n\n    testKit.spawn(behavior);\n\n    manualTime.expectNoMessageFor(Duration.ofMillis(9), probe);\n\n    manualTime.timePasses(Duration.ofMillis(2));\n    probe.expectMessageClass(Tock.class);\n\n    manualTime.expectNoMessageFor(Duration.ofSeconds(10), probe);\n  }\n}","title":"Controlling the scheduler"},{"location":"/typed/testing-async.html#test-of-logging","text":"To verify that certain logging events are emitted there is a utility called LoggingTestKitLoggingTestKit . You define a criteria of the expected logging events and it will assert that the given number of occurrences of matching logging events are emitted within a block of code.\nNote The LoggingTestKitLoggingTestKit implementation requires Logback dependency.\nFor example, a criteria that verifies that an INFO level event with a message containing “Received message” is logged:\nScala copysourceimport org.apache.pekko.actor.testkit.typed.scaladsl.LoggingTestKit\n\n// implicit ActorSystem is needed, but that is given by ScalaTestWithActorTestKit\n// implicit val system: ActorSystem[_]\n\nLoggingTestKit.info(\"Received message\").expect {\n  ref ! Message(\"hello\")\n} Java copysourceimport org.apache.pekko.actor.testkit.typed.javadsl.LoggingTestKit;\n\nLoggingTestKit.info(\"Received message\")\n    .expect(\n        system,\n        () -> {\n          ref.tell(new Message(\"hello\"));\n          return null;\n        });\nMore advanced criteria can be built by chaining conditions that all must be satisfied for a matching event.\nScala copysourceLoggingTestKit\n  .error[IllegalArgumentException]\n  .withMessageRegex(\".*was rejected.*expecting ascii input.*\")\n  .withCustom { event =>\n    event.marker match {\n      case Some(m) => m.getName == \"validation\"\n      case None    => false\n    }\n  }\n  .withOccurrences(2)\n  .expect {\n    ref ! Message(\"hellö\")\n    ref ! Message(\"hejdå\")\n  } Java copysourceLoggingTestKit.error(IllegalArgumentException.class)\n    .withMessageRegex(\".*was rejected.*expecting ascii input.*\")\n    .withCustom(\n        event ->\n            event.getMarker().isPresent()\n                && event.getMarker().get().getName().equals(\"validation\"))\n    .withOccurrences(2)\n    .expect(\n        system,\n        () -> {\n          ref.tell(new Message(\"hellö\"));\n          ref.tell(new Message(\"hejdå\"));\n          return null;\n        });\nSee LoggingTestKitLoggingTestKit for more details.","title":"Test of logging"},{"location":"/typed/testing-async.html#silence-logging-output-from-tests","text":"When running tests, it’s typically preferred to have the output to standard out, together with the output from the testing framework (ScalaTestJUnit). On one hand you want the output to be clean without logging noise, but on the other hand you want as much information as possible if there is a test failure (for example in CI builds).\nThe Pekko TestKit provides a LogCapturing utility to support this with ScalaTest or JUnit. It will buffer log events instead of emitting them to the ConsoleAppender immediately (or whatever Logback appender that is configured). When there is a test failure the buffered events are flushed to the target appenders, typically a ConsoleAppender.\nNote The LogCapturing utility requires Logback dependency.\nMix LogCapturing trait into the ScalaTestAdd a LogCapturing @Rule in the JUnit test like this:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.testkit.typed.scaladsl.LogCapturing\nimport pekko.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit\nimport org.scalatest.wordspec.AnyWordSpecLike\n\nclass LogCapturingExampleSpec extends ScalaTestWithActorTestKit with AnyWordSpecLike with LogCapturing {\n\n  \"Something\" must {\n    \"behave correctly\" in {\n      val pinger = testKit.spawn(Echo(), \"ping\")\n      val probe = testKit.createTestProbe[Echo.Pong]()\n      pinger ! Echo.Ping(\"hello\", probe.ref)\n      probe.expectMessage(Echo.Pong(\"hello\"))\n    }\n  }\n} Java copysourceimport org.apache.pekko.actor.testkit.typed.javadsl.LogCapturing;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestKitJunitResource;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestProbe;\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.junit.ClassRule;\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class LogCapturingExampleTest {\n\n  @ClassRule public static final TestKitJunitResource testKit = new TestKitJunitResource();\n\n  @Rule public final LogCapturing logCapturing = new LogCapturing();\n\n  @Test\n  public void testSomething() {\n    ActorRef<Echo.Ping> pinger = testKit.spawn(Echo.create(), \"ping\");\n    TestProbe<Echo.Pong> probe = testKit.createTestProbe();\n    pinger.tell(new Echo.Ping(\"hello\", probe.ref()));\n    probe.expectMessage(new Echo.Pong(\"hello\"));\n  }\n}\nThen you also need to configure the CapturingAppender and CapturingAppenderDelegate in src/test/resources/logback-test.xml:\ncopysource<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n\n    <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\">\n            <level>INFO</level>\n        </filter>\n        <encoder>\n            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n        </encoder>\n    </appender>\n\n    <!--\n    Logging from tests are silenced by this appender. When there is a test failure\n    the captured logging events are flushed to the appenders defined for the\n    org.apache.pekko.actor.testkit.typed.internal.CapturingAppenderDelegate logger.\n    -->\n    <appender name=\"CapturingAppender\" class=\"org.apache.pekko.actor.testkit.typed.internal.CapturingAppender\" />\n\n    <!--\n    The appenders defined for this CapturingAppenderDelegate logger are used\n    when there is a test failure and all logging events from the test are\n    flushed to these appenders.\n    -->\n    <logger name=\"org.apache.pekko.actor.testkit.typed.internal.CapturingAppenderDelegate\" >\n      <appender-ref ref=\"STDOUT\"/>\n    </logger>\n\n    <root level=\"DEBUG\">\n        <appender-ref ref=\"CapturingAppender\"/>\n    </root>\n</configuration>","title":"Silence logging output from tests"},{"location":"/typed/testing-sync.html","text":"","title":"Synchronous behavior testing"},{"location":"/typed/testing-sync.html#synchronous-behavior-testing","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Testing.\nThe BehaviorTestKit provides a very nice way of unit testing a Behavior in a deterministic way, but it has some limitations to be aware of.\nCertain BehaviorBehaviors will be hard to test synchronously and the BehaviorTestKit doesn’t support testing of all features. In those cases the asynchronous ActorTestKit is recommended. Example of limitations:\nSpawning of FutureCompletionStage or other asynchronous task and you rely on a callback to complete before observing the effect you want to test. Usage of scheduler is not supported. EventSourcedBehavior can’t be tested. Interactions with other actors must be stubbed. Blackbox testing style. Supervision is not supported.\nThe BehaviorTestKit will be improved and some of these problems will be removed but it will always have limitations.\nThe following demonstrates how to test:\nSpawning child actors Spawning child actors anonymously Sending a message either as a reply or to another actor Sending a message to a child actor\nThe examples below require the following imports:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.testkit.typed.CapturedLogEvent\nimport pekko.actor.testkit.typed.Effect._\nimport pekko.actor.testkit.typed.scaladsl.BehaviorTestKit\nimport pekko.actor.testkit.typed.scaladsl.TestInbox\nimport pekko.actor.typed._\nimport pekko.actor.typed.scaladsl._\nimport com.typesafe.config.ConfigFactory\nimport org.slf4j.event.Level\n Java copysourceimport org.apache.pekko.actor.testkit.typed.CapturedLogEvent;\nimport org.apache.pekko.actor.testkit.typed.Effect;\nimport org.apache.pekko.actor.testkit.typed.javadsl.BehaviorTestKit;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestInbox;\nimport org.apache.pekko.actor.typed.*;\nimport org.apache.pekko.actor.typed.javadsl.*;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Optional;\n\nimport com.typesafe.config.Config;\nimport org.slf4j.event.Level;\nEach of the tests are testing an actor that based on the message executes a different effect to be tested:\nScala copysourceobject Hello {\n  sealed trait Command\n  case object CreateAnonymousChild extends Command\n  case class CreateChild(childName: String) extends Command\n  case class SayHelloToChild(childName: String) extends Command\n  case object SayHelloToAnonymousChild extends Command\n  case class SayHello(who: ActorRef[String]) extends Command\n  case class LogAndSayHello(who: ActorRef[String]) extends Command\n\n  def apply(): Behaviors.Receive[Command] = Behaviors.receivePartial {\n    case (context, CreateChild(name)) =>\n      context.spawn(childActor, name)\n      Behaviors.same\n    case (context, CreateAnonymousChild) =>\n      context.spawnAnonymous(childActor)\n      Behaviors.same\n    case (context, SayHelloToChild(childName)) =>\n      val child: ActorRef[String] = context.spawn(childActor, childName)\n      child ! \"hello\"\n      Behaviors.same\n    case (context, SayHelloToAnonymousChild) =>\n      val child: ActorRef[String] = context.spawnAnonymous(childActor)\n      child ! \"hello stranger\"\n      Behaviors.same\n    case (_, SayHello(who)) =>\n      who ! \"hello\"\n      Behaviors.same\n    case (context, LogAndSayHello(who)) =>\n      context.log.info(\"Saying hello to {}\", who.path.name)\n      who ! \"hello\"\n      Behaviors.same\n  } Java copysourcepublic static class Hello extends AbstractBehavior<Hello.Command> {\n\n  public interface Command {}\n\n  public static class CreateAChild implements Command {\n    public final String childName;\n\n    public CreateAChild(String childName) {\n      this.childName = childName;\n    }\n  }\n\n  public enum CreateAnAnonymousChild implements Command {\n    INSTANCE\n  }\n\n  public static class SayHelloToChild implements Command {\n    public final String childName;\n\n    public SayHelloToChild(String childName) {\n      this.childName = childName;\n    }\n  }\n\n  public enum SayHelloToAnonymousChild implements Command {\n    INSTANCE\n  }\n\n  public static class SayHello implements Command {\n    public final ActorRef<String> who;\n\n    public SayHello(ActorRef<String> who) {\n      this.who = who;\n    }\n  }\n\n  public static class LogAndSayHello implements Command {\n    public final ActorRef<String> who;\n\n    public LogAndSayHello(ActorRef<String> who) {\n      this.who = who;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(Hello::new);\n  }\n\n  private Hello(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(CreateAChild.class, this::onCreateAChild)\n        .onMessage(CreateAnAnonymousChild.class, this::onCreateAnonymousChild)\n        .onMessage(SayHelloToChild.class, this::onSayHelloToChild)\n        .onMessage(SayHelloToAnonymousChild.class, this::onSayHelloToAnonymousChild)\n        .onMessage(SayHello.class, this::onSayHello)\n        .onMessage(LogAndSayHello.class, this::onLogAndSayHello)\n        .build();\n  }\n\n  private Behavior<Command> onCreateAChild(CreateAChild message) {\n    getContext().spawn(Child.create(), message.childName);\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> onCreateAnonymousChild(CreateAnAnonymousChild message) {\n    getContext().spawnAnonymous(Child.create());\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> onSayHelloToChild(SayHelloToChild message) {\n    ActorRef<String> child = getContext().spawn(Child.create(), message.childName);\n    child.tell(\"hello\");\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> onSayHelloToAnonymousChild(SayHelloToAnonymousChild message) {\n    ActorRef<String> child = getContext().spawnAnonymous(Child.create());\n    child.tell(\"hello stranger\");\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> onSayHello(SayHello message) {\n    message.who.tell(\"hello\");\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> onLogAndSayHello(LogAndSayHello message) {\n    getContext().getLog().info(\"Saying hello to {}\", message.who.path().name());\n    message.who.tell(\"hello\");\n    return Behaviors.same();\n  }\n}\nFor creating a child actor a noop actor is created:\nScala copysourceval childActor = Behaviors.receiveMessage[String] { _ =>\n  Behaviors.same[String]\n} Java copysourcepublic static class Child {\n  public static Behavior<String> create() {\n    return Behaviors.receive((context, message) -> Behaviors.same());\n  }\n}\nAll of the tests make use of the BehaviorTestKitBehaviorTestKit to avoid the need for a real ActorContext. Some of the tests make use of the TestInboxTestInbox which allows the creation of an ActorRefActorRef that can be used for synchronous testing, similar to the TestProbe used for asynchronous testing.","title":"Synchronous behavior testing"},{"location":"/typed/testing-sync.html#spawning-children","text":"With a name:\nScala copysourceval testKit = BehaviorTestKit(Hello())\ntestKit.run(Hello.CreateChild(\"child\"))\ntestKit.expectEffect(Spawned(childActor, \"child\")) Java copysourceBehaviorTestKit<Hello.Command> test = BehaviorTestKit.create(Hello.create());\ntest.run(new Hello.CreateAChild(\"child\"));\nassertEquals(\"child\", test.expectEffectClass(Effect.Spawned.class).childName());\nAnonymously:\nScala copysourceval testKit = BehaviorTestKit(Hello())\ntestKit.run(Hello.CreateAnonymousChild)\ntestKit.expectEffect(SpawnedAnonymous(childActor)) Java copysourceBehaviorTestKit<Hello.Command> test = BehaviorTestKit.create(Hello.create());\ntest.run(Hello.CreateAnAnonymousChild.INSTANCE);\ntest.expectEffectClass(Effect.SpawnedAnonymous.class);","title":"Spawning children"},{"location":"/typed/testing-sync.html#sending-messages","text":"For testing sending a message a TestInboxTestInbox is created that provides an ActorRefActorRef and methods to assert against the messages that have been sent to it.\nScala copysourceval testKit = BehaviorTestKit(Hello())\nval inbox = TestInbox[String]()\ntestKit.run(Hello.SayHello(inbox.ref))\ninbox.expectMessage(\"hello\") Java copysourceBehaviorTestKit<Hello.Command> test = BehaviorTestKit.create(Hello.create());\nTestInbox<String> inbox = TestInbox.create();\ntest.run(new Hello.SayHello(inbox.getRef()));\ninbox.expectMessage(\"hello\");\nAnother use case is sending a message to a child actor you can do this by looking up the TestInboxTestInbox for a child actor from the BehaviorTestKitBehaviorTestKit:\nScala copysourceval testKit = BehaviorTestKit(Hello())\ntestKit.run(Hello.SayHelloToChild(\"child\"))\nval childInbox = testKit.childInbox[String](\"child\")\nchildInbox.expectMessage(\"hello\") Java copysourceBehaviorTestKit<Hello.Command> testKit = BehaviorTestKit.create(Hello.create());\ntestKit.run(new Hello.SayHelloToChild(\"child\"));\nTestInbox<String> childInbox = testKit.childInbox(\"child\");\nchildInbox.expectMessage(\"hello\");\nFor anonymous children the actor names are generated in a deterministic way:\nScala copysourceval testKit = BehaviorTestKit(Hello())\ntestKit.run(Hello.SayHelloToAnonymousChild)\nval child = testKit.expectEffectType[SpawnedAnonymous[String]]\n\nval childInbox = testKit.childInbox(child.ref)\nchildInbox.expectMessage(\"hello stranger\") Java copysourceBehaviorTestKit<Hello.Command> testKit = BehaviorTestKit.create(Hello.create());\ntestKit.run(Hello.SayHelloToAnonymousChild.INSTANCE);\n// Anonymous actors are created as: $a $b etc\nTestInbox<String> childInbox = testKit.childInbox(\"$a\");\nchildInbox.expectMessage(\"hello stranger\");","title":"Sending messages"},{"location":"/typed/testing-sync.html#testing-other-effects","text":"The BehaviorTestKitBehaviorTestKit keeps track other effects you can verify, look at the sub-classes of EffectEffect\nSpawnedAdapter Stopped Watched WatchedWith Unwatched Scheduled TimerScheduled TimerCancelled","title":"Testing other effects"},{"location":"/typed/testing-sync.html#checking-for-log-messages","text":"The BehaviorTestKitBehaviorTestKit also keeps track of everything that is being logged. Here, you can see an example on how to check if the behavior logged certain messages:\nScala copysourceval testKit = BehaviorTestKit(Hello())\nval inbox = TestInbox[String](\"Inboxer\")\ntestKit.run(Hello.LogAndSayHello(inbox.ref))\ntestKit.logEntries() shouldBe Seq(CapturedLogEvent(Level.INFO, \"Saying hello to Inboxer\")) Java copysourceBehaviorTestKit<Hello.Command> test = BehaviorTestKit.create(Hello.create());\nTestInbox<String> inbox = TestInbox.create(\"Inboxer\");\ntest.run(new Hello.LogAndSayHello(inbox.getRef()));\n\nList<CapturedLogEvent> allLogEntries = test.getAllLogEntries();\nassertEquals(1, allLogEntries.size());\nCapturedLogEvent expectedLogEvent =\n    new CapturedLogEvent(\n        Level.INFO,\n        \"Saying hello to Inboxer\",\n        Optional.empty(),\n        Optional.empty(),\n        new HashMap<>());\nassertEquals(expectedLogEvent, allLogEntries.get(0));\nSee the other public methods and API documentation on BehaviorTestKitBehaviorTestKit for other types of verification.","title":"Checking for Log Messages"},{"location":"/typed/coexisting.html","text":"","title":"Coexistence"},{"location":"/typed/coexisting.html#coexistence","text":"","title":"Coexistence"},{"location":"/typed/coexisting.html#dependency","text":"To use Pekko Actor Typed, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/coexisting.html#introduction","text":"We believe Pekko Typed will be adopted in existing systems gradually and therefore it’s important to be able to use typed and classic actors together, within the same ActorSystem. Also, we will not be able to integrate with all existing modules in one big bang release and that is another reason for why these two ways of writing actors must be able to coexist.\nThere are two different ActorSystems: actor.ActorSystemactor.ActorSystem and actor.typed.ActorSystemactor.typed.ActorSystem.\nCurrently the typed actor system is implemented using the classic actor system under the hood. This may change in the future.\nTyped and classic can interact the following ways:\nclassic actor systems can create typed actors typed actors can send messages to classic actors, and opposite spawn and supervise typed child from classic parent, and opposite watch typed from classic, and opposite classic actor system can be converted to a typed actor system\nIn the examples the pekko.actor package is aliased to classic. Scala copysourceimport org.apache.pekko.{ actor => classic }\nThe examples use fully qualified class names for the classic classes to distinguish between typed and classic classes with the same name.","title":"Introduction"},{"location":"/typed/coexisting.html#classic-to-typed","text":"While coexisting your application will likely still have a classic ActorSystem. This can be converted to a typed ActorSystem so that new code and migrated parts don’t rely on the classic system:\nScala copysource// adds support for actors to a classic actor system and context\nimport org.apache.pekko.actor.typed.scaladsl.adapter._\n\nval system = pekko.actor.ActorSystem(\"ClassicToTypedSystem\")\nval typedSystem: ActorSystem[Nothing] = system.toTyped Java copysource// In java use the static methods on Adapter to convert from typed to classic\nimport org.apache.pekko.actor.typed.javadsl.Adapter;\norg.apache.pekko.actor.ActorSystem classicActorSystem =\n    org.apache.pekko.actor.ActorSystem.create();\nActorSystem<Void> typedActorSystem = Adapter.toTyped(classicActorSystem);\nThen for new typed actors here’s how you create, watch and send messages to it from a classic actor.\nScala copysourceobject Typed {\n  sealed trait Command\n  final case class Ping(replyTo: ActorRef[Pong.type]) extends Command\n  case object Pong\n\n  def apply(): Behavior[Command] =\n    Behaviors.receive { (context, message) =>\n      message match {\n        case Ping(replyTo) =>\n          context.log.info(s\"${context.self} got Ping from $replyTo\")\n          // replyTo is a classic actor that has been converted for coexistence\n          replyTo ! Pong\n          Behaviors.same\n      }\n    }\n} Java copysourcepublic abstract static class Typed {\n  interface Command {}\n\n  public static class Ping implements Command {\n    public final org.apache.pekko.actor.typed.ActorRef<Pong> replyTo;\n\n    public Ping(ActorRef<Pong> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Pong {}\n\n  public static Behavior<Command> behavior() {\n    return Behaviors.receive(Typed.Command.class)\n        .onMessage(\n            Typed.Ping.class,\n            message -> {\n              message.replyTo.tell(new Pong());\n              return same();\n            })\n        .build();\n  }\n}\nThe top level classic actor is created in the usual way:\nScala copysourceval classicActor = system.actorOf(Classic.props()) Java copysourceorg.apache.pekko.actor.ActorSystem as = org.apache.pekko.actor.ActorSystem.create();\norg.apache.pekko.actor.ActorRef classic = as.actorOf(Classic.props());\nThen it can create a typed actor, watch it, and send a message to it:\nScala copysourceclass Classic extends classic.Actor with ActorLogging {\n  // context.spawn is an implicit extension method\n  val second: ActorRef[Typed.Command] =\n    context.spawn(Typed(), \"second\")\n\n  // context.watch is an implicit extension method\n  context.watch(second)\n\n  // self can be used as the `replyTo` parameter here because\n  // there is an implicit conversion from org.apache.pekko.actor.ActorRef to\n  // org.apache.pekko.actor.typed.ActorRef\n  // An equal alternative would be `self.toTyped`\n  second ! Typed.Ping(self)\n\n  override def receive = {\n    case Typed.Pong =>\n      log.info(s\"$self got Pong from ${sender()}\")\n      // context.stop is an implicit extension method\n      context.stop(second)\n    case classic.Terminated(ref) =>\n      log.info(s\"$self observed termination of $ref\")\n      context.stop(self)\n  }\n} Java copysourcepublic static class Classic extends AbstractActor {\n  public static org.apache.pekko.actor.Props props() {\n    return org.apache.pekko.actor.Props.create(Classic.class);\n  }\n\n  private final org.apache.pekko.actor.typed.ActorRef<Typed.Command> second =\n      Adapter.spawn(getContext(), Typed.behavior(), \"second\");\n\n  @Override\n  public void preStart() {\n    Adapter.watch(getContext(), second);\n    second.tell(new Typed.Ping(Adapter.toTyped(getSelf())));\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Typed.Pong.class,\n            message -> {\n              Adapter.stop(getContext(), second);\n            })\n        .match(\n            org.apache.pekko.actor.Terminated.class,\n            t -> {\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n}\nThere is one import that is needed to make that work. We import the Adapter class and call static methods for conversion.\nScala copysource// adds support for actors to a classic actor system and context\nimport org.apache.pekko.actor.typed.scaladsl.adapter._ Java copysource// In java use the static methods on Adapter to convert from typed to classic\nimport org.apache.pekko.actor.typed.javadsl.Adapter;\nThat adds some implicit extension methods that are added to classic and typed ActorSystem, ActorContext and ActorRef in both directions. To convert between typed and classic ActorSystem, ActorContext and ActorRef in both directions there are adapter methods in pekko.actor.typed.javadsl.Adapter. Note the inline comments in the example above.\nThis method of using a top level classic actor is the suggested path for this type of co-existence. However, if you prefer to start with a typed top level actor then you can use the implicit spawn -methodAdapter.spawn directly from the typed system:\nScala copysourceval system = classic.ActorSystem(\"TypedWatchingClassic\")\nval typed = system.spawn(Typed.behavior, \"Typed\") Java copysourceActorSystem as = ActorSystem.create();\nActorRef<Typed.Command> typed = Adapter.spawn(as, Typed.create(), \"Typed\");\nThe above classic-typed difference is further elaborated in the ActorSystem section of “Learning Pekko Typed from Classic”.","title":"Classic to typed"},{"location":"/typed/coexisting.html#typed-to-classic","text":"Let’s turn the example upside down and first start the typed actor and then the classic as a child.\nThe following will show how to create, watch and send messages back and forth from a typed actor to this classic actor:\nScala copysourceobject Classic {\n  def props(): classic.Props = classic.Props(new Classic)\n}\nclass Classic extends classic.Actor {\n  override def receive = {\n    case Typed.Ping(replyTo) =>\n      replyTo ! Typed.Pong\n  }\n} Java copysourcepublic static class Classic extends AbstractActor {\n  public static org.apache.pekko.actor.Props props() {\n    return org.apache.pekko.actor.Props.create(Classic.class);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(Typed.Ping.class, this::onPing).build();\n  }\n\n  private void onPing(Typed.Ping message) {\n    message.replyTo.tell(Typed.Pong.INSTANCE);\n  }\n}\nCreating the actor system and the typed actor:\nScala copysourceval system = classic.ActorSystem(\"TypedWatchingClassic\")\nval typed = system.spawn(Typed.behavior, \"Typed\") Java copysourceActorSystem as = ActorSystem.create();\nActorRef<Typed.Command> typed = Adapter.spawn(as, Typed.create(), \"Typed\");\nThen the typed actor creates the classic actor, watches it and sends and receives a response:\nScala copysourceobject Typed {\n  final case class Ping(replyTo: pekko.actor.typed.ActorRef[Pong.type])\n  sealed trait Command\n  case object Pong extends Command\n\n  val behavior: Behavior[Command] =\n    Behaviors.setup { context =>\n      // context.actorOf is an implicit extension method\n      val classic = context.actorOf(Classic.props(), \"second\")\n\n      // context.watch is an implicit extension method\n      context.watch(classic)\n\n      // illustrating how to pass sender, toClassic is an implicit extension method\n      classic.tell(Typed.Ping(context.self), context.self.toClassic)\n\n      Behaviors\n        .receivePartial[Command] {\n          case (context, Pong) =>\n            // it's not possible to get the sender, that must be sent in message\n            // context.stop is an implicit extension method\n            context.stop(classic)\n            Behaviors.same\n        }\n        .receiveSignal {\n          case (_, pekko.actor.typed.Terminated(_)) =>\n            Behaviors.stopped\n        }\n    }\n} Java copysourcepublic static class Typed extends AbstractBehavior<Typed.Command> {\n\n  public static class Ping {\n    public final org.apache.pekko.actor.typed.ActorRef<Pong> replyTo;\n\n    public Ping(ActorRef<Pong> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  interface Command {}\n\n  public enum Pong implements Command {\n    INSTANCE\n  }\n\n  private final org.apache.pekko.actor.ActorRef second;\n\n  private Typed(ActorContext<Command> context, org.apache.pekko.actor.ActorRef second) {\n    super(context);\n    this.second = second;\n  }\n\n  public static Behavior<Command> create() {\n    return org.apache.pekko.actor.typed.javadsl.Behaviors.setup(\n        context -> {\n          org.apache.pekko.actor.ActorRef second =\n              Adapter.actorOf(context, Classic.props(), \"second\");\n\n          Adapter.watch(context, second);\n\n          second.tell(\n              new Typed.Ping(context.getSelf().narrow()), Adapter.toClassic(context.getSelf()));\n\n          return new Typed(context, second);\n        });\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Typed.Pong.class, message -> onPong())\n        .onSignal(org.apache.pekko.actor.typed.Terminated.class, sig -> Behaviors.stopped())\n        .build();\n  }\n\n  private Behavior<Command> onPong() {\n    Adapter.stop(getContext(), second);\n    return this;\n  }\n}\nNote that when sending from a typed actor to a classic ActorRefActorRef there is no sender in scope as in classic. The typed sender should use its own ActorContext[T].self explicitly, as shown in the snippet.","title":"Typed to classic"},{"location":"/typed/coexisting.html#supervision","text":"The default supervision for classic actors is to restart whereas for typed it is to stop. When combining classic and typed actors the default supervision is based on the default behavior of the child, for example if a classic actor creates a typed child, its default supervision will be to stop. If a typed actor creates a classic child, its default supervision will be to restart.","title":"Supervision"},{"location":"/typed/style-guide.html","text":"","title":"Style guide"},{"location":"/typed/style-guide.html#style-guide","text":"This is a style guide with recommendations of idioms and patterns for writing Pekko actors. Note that this guide does not cover the classic actor API.\nAs with all style guides, treat this as a list of rules to be broken. There are certainly times when alternative styles should be preferred over the ones given here.","title":"Style guide"},{"location":"/typed/style-guide.html#functional-versus-object-oriented-style","text":"There are two flavors of the Actor APIs.\nThe functional programming style where you pass a function to a factory which then constructs a behavior, for stateful actors this means passing immutable state around as parameters and switching to a new behavior whenever you need to act on a changed state. The object-oriented style where a concrete class for the actor behavior is defined and mutable state is kept inside of it as fields.\nAn example of a counter actor implemented in the functional style:\nScala copysource import org.apache.pekko\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n\n  def apply(): Behavior[Command] =\n    counter(0)\n\n  private def counter(n: Int): Behavior[Command] =\n    Behaviors.receive { (context, message) =>\n      message match {\n        case Increment =>\n          val newValue = n + 1\n          context.log.debug(\"Incremented counter to [{}]\", newValue)\n          counter(newValue)\n        case GetValue(replyTo) =>\n          replyTo ! Value(n)\n          Behaviors.same\n      }\n    }\n} Java copysourceimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.SupervisorStrategy;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\n\npublic class Counter {\n  public interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    public final ActorRef<Value> replyTo;\n\n    public GetValue(ActorRef<Value> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Value {\n    public final int value;\n\n    public Value(int value) {\n      this.value = value;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(context -> counter(context, 0));\n  }\n\n  private static Behavior<Command> counter(final ActorContext<Command> context, final int n) {\n\n    return Behaviors.receive(Command.class)\n        .onMessage(Increment.class, notUsed -> onIncrement(context, n))\n        .onMessage(GetValue.class, command -> onGetValue(n, command))\n        .build();\n  }\n\n  private static Behavior<Command> onIncrement(ActorContext<Command> context, int n) {\n    int newValue = n + 1;\n    context.getLog().debug(\"Incremented counter to [{}]\", newValue);\n    return counter(context, newValue);\n  }\n\n  private static Behavior<Command> onGetValue(int n, GetValue command) {\n    command.replyTo.tell(new Value(n));\n    return Behaviors.same();\n  }\n}\nCorresponding actor implemented in the object-oriented style:\nScala copysource import org.apache.pekko\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\nimport org.apache.pekko.actor.typed.scaladsl.AbstractBehavior\nimport org.slf4j.Logger\n\nobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n\n  def apply(): Behavior[Command] = {\n    Behaviors.setup(context => new Counter(context))\n  }\n}\n\nclass Counter(context: ActorContext[Counter.Command]) extends AbstractBehavior[Counter.Command](context) {\n  import Counter._\n\n  private var n = 0\n\n  override def onMessage(msg: Command): Behavior[Counter.Command] = {\n    msg match {\n      case Increment =>\n        n += 1\n        context.log.debug(\"Incremented counter to [{}]\", n)\n        this\n      case GetValue(replyTo) =>\n        replyTo ! Value(n)\n        this\n    }\n  }\n} Java copysourceimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.SupervisorStrategy;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\npublic class Counter extends AbstractBehavior<Counter.Command> {\n\n  public interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    public final ActorRef<Value> replyTo;\n\n    public GetValue(ActorRef<Value> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Value {\n    public final int value;\n\n    public Value(int value) {\n      this.value = value;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(Counter::new);\n  }\n\n  private int n;\n\n  private Counter(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Increment.class, notUsed -> onIncrement())\n        .onMessage(GetValue.class, this::onGetValue)\n        .build();\n  }\n\n  private Behavior<Command> onIncrement() {\n    n++;\n    getContext().getLog().debug(\"Incremented counter to [{}]\", n);\n    return this;\n  }\n\n  private Behavior<Command> onGetValue(GetValue command) {\n    command.replyTo.tell(new Value(n));\n    return this;\n  }\n}\nSome similarities to note:\nMessages are defined in the same way. Both have an apply factory method in the companion objecta static create factory method to create the initial behavior, i.e. from the outside they are used in the same way. Pattern matchingMatching and handling of the messages are done in the same way. The ActorContext API is the same.\nA few differences to note:\nThere is no class in the functional style, but that is not strictly a requirement and sometimes it’s convenient to use a class also with the functional style to reduce number of parameters in the methods. Mutable state, such as the var nint n is typically used in the object-oriented style. In the functional style the state is is updated by returning a new behavior that holds the new immutable state, the n: Intfinal int n parameter of the counter method. The object-oriented style must use a new instance of the initial Behavior for each spawned actor instance, since the state in AbstractBehavior instance must not be shared between actor instances. This is “hidden” in the functional style since the immutable state is captured by the function. In the object-oriented style one can return this to stay with the same behavior for next message. In the functional style there is no this so Behaviors.same is used instead. The ActorContext is accessed in different ways. In the object-oriented style it’s retrieved from Behaviors.setup and kept as an instance field, while in the functional style it’s passed in alongside the message. That said, Behaviors.setup is often used in the functional style as well, and then often together with Behaviors.receiveMessage that doesn’t pass in the context with the message. The ActorContext is accessed with Behaviors.setup but then kept in different ways. As an instance field versus a method parameter.\nWhich style you choose to use is a matter of taste and both styles can be mixed depending on which is best for a specific actor. An actor can switch between behaviors implemented in different styles. For example, it may have an initial behavior that is only stashing messages until some initial query has been completed and then switching over to its main active behavior that is maintaining some mutable state. Such initial behavior is nice in the functional style and the active behavior may be better with the object-oriented style.\nWe would recommend using the tool that is best for the job. The APIs are similar in many ways to make it easy to learn both. You may of course also decide to just stick to one style for consistency and familiarity reasons.\nWhen developing in Scala the functional style will probably be the choice for many. Some reasons why you may want to use the functional style: You are familiar with a functional approach of structuring the code. Note that this API is still not using any advanced functional programming or type theory constructs. The state is immutable and can be passed to “next” behavior. The Behavior is stateless. The actor lifecycle has several different phases that can be represented by switching between different behaviors, like a finite state machine. This is also supported with the object-oriented style, but it’s typically nicer with the functional style. It’s less risk of accessing mutable state in the actor from other threads, like Future or Streams callbacks. Some reasons why you may want to use the object-oriented style: You are more familiar with an object-oriented style of structuring the code with methods in a class rather than functions. Some state is not immutable. It could be more familiar and easier to upgrade existing classic actors to this style. Mutable state can sometimes have better performance, e.g. mutable collections and avoiding allocating new instance for next behavior (be sure to benchmark if this is your motivation).\nWhen developing in Java the object-oriented style will probably be the choice for many. Some reasons why you may want to use the object-oriented style: You are more familiar with an object-oriented style of structuring the code with methods in a class rather than functions. Java lambdas can only close over final or effectively final fields, making it impractical to use the functional style in behaviors that mutate their fields. Some state is not immutable, e.g. immutable collections are not widely used in Java. It is OK to use mutable state also with the functional style but you must make sure that it’s not shared between different actor instances. It could be more familiar and easier to upgrade existing classic actors to this style. Mutable state can sometimes have better performance, e.g. mutable collections and avoiding allocating new instance for next behavior (be sure to benchmark if this is your motivation). Some reasons why you may want to use the functional style: You are familiar with a functional approach of structuring the code. Note that this API is still not using any advanced functional programming or type theory constructs. The state is immutable and can be passed to “next” behavior. The Behavior is stateless. The actor lifecycle has several different phases that can be represented by switching between different behaviors, like a finite state machine. This is also supported with the object-oriented style, but it’s typically nicer with the functional style. It’s less risk of accessing mutable state in the actor from other threads, like CompletionStage or Streams callbacks.","title":"Functional versus object-oriented style"},{"location":"/typed/style-guide.html#passing-around-too-many-parameters","text":"One thing you will quickly run into when using the functional style is that you need to pass around many parameters.\nLet’s add name parameter and timers to the previous Counter example. A first approach would be to just add those as separate parameters:\nScala copysource // this is an anti-example, better solutions exists\nobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class IncrementRepeatedly(interval: FiniteDuration) extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n\n  def apply(name: String): Behavior[Command] =\n    Behaviors.withTimers { timers =>\n      counter(name, timers, 0)\n    }\n\n  private def counter(name: String, timers: TimerScheduler[Command], n: Int): Behavior[Command] =\n    Behaviors.receive { (context, message) =>\n      message match {\n        case IncrementRepeatedly(interval) =>\n          context.log.debug(\n            \"[{}] Starting repeated increments with interval [{}], current count is [{}]\",\n            name,\n            interval.toString,\n            n.toString)\n          timers.startTimerWithFixedDelay(Increment, interval)\n          Behaviors.same\n        case Increment =>\n          val newValue = n + 1\n          context.log.debug2(\"[{}] Incremented counter to [{}]\", name, newValue)\n          counter(name, timers, newValue)\n        case GetValue(replyTo) =>\n          replyTo ! Value(n)\n          Behaviors.same\n      }\n    }\n} Java copysource // this is an anti-example, better solutions exists\npublic class Counter {\n  public interface Command {}\n\n  public static class IncrementRepeatedly implements Command {\n    public final Duration interval;\n\n    public IncrementRepeatedly(Duration interval) {\n      this.interval = interval;\n    }\n  }\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    public final ActorRef<Value> replyTo;\n\n    public GetValue(ActorRef<Value> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Value {\n    public final int value;\n\n    public Value(int value) {\n      this.value = value;\n    }\n  }\n\n  public static Behavior<Command> create(String name) {\n    return Behaviors.setup(\n        context -> Behaviors.withTimers(timers -> counter(name, context, timers, 0)));\n  }\n\n  private static Behavior<Command> counter(\n      final String name,\n      final ActorContext<Command> context,\n      final TimerScheduler<Command> timers,\n      final int n) {\n\n    return Behaviors.receive(Command.class)\n        .onMessage(\n            IncrementRepeatedly.class,\n            command -> onIncrementRepeatedly(name, context, timers, n, command))\n        .onMessage(Increment.class, notUsed -> onIncrement(name, context, timers, n))\n        .onMessage(GetValue.class, command -> onGetValue(n, command))\n        .build();\n  }\n\n  private static Behavior<Command> onIncrementRepeatedly(\n      String name,\n      ActorContext<Command> context,\n      TimerScheduler<Command> timers,\n      int n,\n      IncrementRepeatedly command) {\n    context\n        .getLog()\n        .debug(\n            \"[{}] Starting repeated increments with interval [{}], current count is [{}]\",\n            name,\n            command.interval,\n            n);\n    timers.startTimerWithFixedDelay(Increment.INSTANCE, command.interval);\n    return Behaviors.same();\n  }\n\n  private static Behavior<Command> onIncrement(\n      String name, ActorContext<Command> context, TimerScheduler<Command> timers, int n) {\n    int newValue = n + 1;\n    context.getLog().debug(\"[{}] Incremented counter to [{}]\", name, newValue);\n    return counter(name, context, timers, newValue);\n  }\n\n  private static Behavior<Command> onGetValue(int n, GetValue command) {\n    command.replyTo.tell(new Value(n));\n    return Behaviors.same();\n  }\n}\nOuch, that doesn’t look good. More things may be needed, such as stashing or application specific “constructor” parameters. As you can imagine, that will be too much boilerplate.\nAs a first step we can place all these parameters in a class so that we at least only have to pass around one thing. Still good to have the “changing” state, the n: Intfinal int n here, as a separate parameter.\nScala copysource // this is better than previous example, but even better solution exists\nobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class IncrementRepeatedly(interval: FiniteDuration) extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n\n  private case class Setup(name: String, context: ActorContext[Command], timers: TimerScheduler[Command])\n\n  def apply(name: String): Behavior[Command] =\n    Behaviors.setup { context =>\n      Behaviors.withTimers { timers =>\n        counter(Setup(name, context, timers), 0)\n      }\n    }\n\n  private def counter(setup: Setup, n: Int): Behavior[Command] =\n    Behaviors.receiveMessage {\n      case IncrementRepeatedly(interval) =>\n        setup.context.log.debugN(\n          \"[{}] Starting repeated increments with interval [{}], current count is [{}]\",\n          setup.name,\n          interval,\n          n)\n        setup.timers.startTimerWithFixedDelay(Increment, interval)\n        Behaviors.same\n      case Increment =>\n        val newValue = n + 1\n        setup.context.log.debug2(\"[{}] Incremented counter to [{}]\", setup.name, newValue)\n        counter(setup, newValue)\n      case GetValue(replyTo) =>\n        replyTo ! Value(n)\n        Behaviors.same\n    }\n} Java copysource // this is better than previous example, but even better solution exists\npublic class Counter {\n  // messages omitted for brevity, same messages as above example\n\n  private static class Setup {\n    final String name;\n    final ActorContext<Command> context;\n    final TimerScheduler<Command> timers;\n\n    private Setup(String name, ActorContext<Command> context, TimerScheduler<Command> timers) {\n      this.name = name;\n      this.context = context;\n      this.timers = timers;\n    }\n  }\n\n  public static Behavior<Command> create(String name) {\n    return Behaviors.setup(\n        context ->\n            Behaviors.withTimers(timers -> counter(new Setup(name, context, timers), 0)));\n  }\n\n  private static Behavior<Command> counter(final Setup setup, final int n) {\n\n    return Behaviors.receive(Command.class)\n        .onMessage(\n            IncrementRepeatedly.class, command -> onIncrementRepeatedly(setup, n, command))\n        .onMessage(Increment.class, notUsed -> onIncrement(setup, n))\n        .onMessage(GetValue.class, command -> onGetValue(n, command))\n        .build();\n  }\n\n  private static Behavior<Command> onIncrementRepeatedly(\n      Setup setup, int n, IncrementRepeatedly command) {\n    setup\n        .context\n        .getLog()\n        .debug(\n            \"[{}] Starting repeated increments with interval [{}], current count is [{}]\",\n            setup.name,\n            command.interval,\n            n);\n    setup.timers.startTimerWithFixedDelay(Increment.INSTANCE, command.interval);\n    return Behaviors.same();\n  }\n\n  private static Behavior<Command> onIncrement(Setup setup, int n) {\n    int newValue = n + 1;\n    setup.context.getLog().debug(\"[{}] Incremented counter to [{}]\", setup.name, newValue);\n    return counter(setup, newValue);\n  }\n\n  private static Behavior<Command> onGetValue(int n, GetValue command) {\n    command.replyTo.tell(new Value(n));\n    return Behaviors.same();\n  }\n}\nThat’s better. Only one thing to carry around and easy to add more things to it without rewriting everything. Note that we also placed the ActorContext in the Setup class, and therefore switched from Behaviors.receive to Behaviors.receiveMessage since we already have access to the context.\nIt’s still rather annoying to have to pass the same thing around everywhere.\nWe can do better by introducing an enclosing class, even though it’s still using the functional style. The “constructor” parameters can be immutablefinal instance fields and can be accessed from member methods.\nScala copysource // this is better than previous examples\nobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class IncrementRepeatedly(interval: FiniteDuration) extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n\n  def apply(name: String): Behavior[Command] =\n    Behaviors.setup { context =>\n      Behaviors.withTimers { timers =>\n        new Counter(name, context, timers).counter(0)\n      }\n    }\n}\n\nclass Counter private (\n    name: String,\n    context: ActorContext[Counter.Command],\n    timers: TimerScheduler[Counter.Command]) {\n  import Counter._\n\n  private def counter(n: Int): Behavior[Command] =\n    Behaviors.receiveMessage {\n      case IncrementRepeatedly(interval) =>\n        context.log.debugN(\n          \"[{}] Starting repeated increments with interval [{}], current count is [{}]\",\n          name,\n          interval,\n          n)\n        timers.startTimerWithFixedDelay(Increment, interval)\n        Behaviors.same\n      case Increment =>\n        val newValue = n + 1\n        context.log.debug2(\"[{}] Incremented counter to [{}]\", name, newValue)\n        counter(newValue)\n      case GetValue(replyTo) =>\n        replyTo ! Value(n)\n        Behaviors.same\n    }\n} Java copysource // this is better than previous examples\npublic class Counter {\n  // messages omitted for brevity, same messages as above example\n\n  public static Behavior<Command> create(String name) {\n    return Behaviors.setup(\n        context ->\n            Behaviors.withTimers(timers -> new Counter(name, context, timers).counter(0)));\n  }\n\n  private final String name;\n  private final ActorContext<Command> context;\n  private final TimerScheduler<Command> timers;\n\n  private Counter(String name, ActorContext<Command> context, TimerScheduler<Command> timers) {\n    this.name = name;\n    this.context = context;\n    this.timers = timers;\n  }\n\n  private Behavior<Command> counter(final int n) {\n    return Behaviors.receive(Command.class)\n        .onMessage(IncrementRepeatedly.class, command -> onIncrementRepeatedly(n, command))\n        .onMessage(Increment.class, notUsed -> onIncrement(n))\n        .onMessage(GetValue.class, command -> onGetValue(n, command))\n        .build();\n  }\n\n  private Behavior<Command> onIncrementRepeatedly(int n, IncrementRepeatedly command) {\n    context\n        .getLog()\n        .debug(\n            \"[{}] Starting repeated increments with interval [{}], current count is [{}]\",\n            name,\n            command.interval,\n            n);\n    timers.startTimerWithFixedDelay(Increment.INSTANCE, command.interval);\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> onIncrement(int n) {\n    int newValue = n + 1;\n    context.getLog().debug(\"[{}] Incremented counter to [{}]\", name, newValue);\n    return counter(newValue);\n  }\n\n  private Behavior<Command> onGetValue(int n, GetValue command) {\n    command.replyTo.tell(new Value(n));\n    return Behaviors.same();\n  }\n}\nThat’s nice. One thing to be cautious with here is that it’s important that you create a new instance for each spawned actor, since those parameters must not be shared between different actor instances. That comes natural when creating the instance from Behaviors.setup as in the above example. Having a apply factory method in the companion object and making the constructor private is recommended. static create factory method and making the constructor private is recommended.\nThis can also be useful when testing the behavior by creating a test subclass that overrides certain methods in the class. The test would create the instance without the apply factory methodstatic create factory method. Then you need to relax the visibility constraints of the constructor and methods.\nIt’s not recommended to place mutable state and var membersnon-final members in the enclosing class. It would be correct from an actor thread-safety perspective as long as the same instance of the enclosing class is not shared between different actor instances, but if that is what you need you should rather use the object-oriented style with the AbstractBehavior class.\nSimilar can be achieved without an enclosing class by placing the def counter inside the Behaviors.setup block. That works fine, but for more complex behaviors it can be better to structure the methods in a class. For completeness, here is how it would look like: Scala copysource // this works, but previous example is better for structuring more complex behaviors\nobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class IncrementRepeatedly(interval: FiniteDuration) extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n\n  def apply(name: String): Behavior[Command] =\n    Behaviors.setup { context =>\n      Behaviors.withTimers { timers =>\n        def counter(n: Int): Behavior[Command] =\n          Behaviors.receiveMessage {\n            case IncrementRepeatedly(interval) =>\n              context.log.debugN(\n                \"[{}] Starting repeated increments with interval [{}], current count is [{}]\",\n                name,\n                interval,\n                n)\n              timers.startTimerWithFixedDelay(Increment, interval)\n              Behaviors.same\n            case Increment =>\n              val newValue = n + 1\n              context.log.debug2(\"[{}] Incremented counter to [{}]\", name, newValue)\n              counter(newValue)\n            case GetValue(replyTo) =>\n              replyTo ! Value(n)\n              Behaviors.same\n          }\n\n        counter(0)\n      }\n    }\n}","title":"Passing around too many parameters"},{"location":"/typed/style-guide.html#behavior-factory-method","text":"The initial behavior should be created via a factory method in the companion objecta static factory method. Thereby the usage of the behavior doesn’t change when the implementation is changed, for example if changing between object-oriented and function style.\nThe factory method is a good place for retrieving resources like Behaviors.withTimers, Behaviors.withStash and ActorContext with Behaviors.setup.\nWhen using the object-oriented style, AbstractBehavior, a new instance should be created from a Behaviors.setup block in this factory method even though the ActorContext is not needed. This is important because a new instance should be created when restart supervision is used. Typically, the ActorContext is needed anyway.\nThe naming convention for the factory method is apply (when using Scala)create (when using Java). Consistent naming makes it easier for readers of the code to find the “starting point” of the behavior.\nIn the functional style the factory could even have been defined as a valstatic field if all state is immutable and captured by the function, but since most behaviors need some initialization parameters it is preferred to consistently use a method (def) for the factory.\nExample:\nScala copysourceobject CountDown {\n  sealed trait Command\n  case object Down extends Command\n\n  // factory for the initial `Behavior`\n  def apply(countDownFrom: Int, notifyWhenZero: ActorRef[Done]): Behavior[Command] =\n    new CountDown(notifyWhenZero).counter(countDownFrom)\n}\n\nprivate class CountDown(notifyWhenZero: ActorRef[Done]) {\n  import CountDown._\n\n  private def counter(remaining: Int): Behavior[Command] = {\n    Behaviors.receiveMessage {\n      case Down =>\n        if (remaining == 1) {\n          notifyWhenZero.tell(Done)\n          Behaviors.stopped\n        } else\n          counter(remaining - 1)\n    }\n  }\n\n} Java copysourcepublic class CountDown extends AbstractBehavior<CountDown.Command> {\n\n  public interface Command {}\n\n  public enum Down implements Command {\n    INSTANCE\n  }\n\n  // factory for the initial `Behavior`\n  public static Behavior<Command> create(int countDownFrom, ActorRef<Done> notifyWhenZero) {\n    return Behaviors.setup(context -> new CountDown(context, countDownFrom, notifyWhenZero));\n  }\n\n  private final ActorRef<Done> notifyWhenZero;\n  private int remaining;\n\n  private CountDown(\n      ActorContext<Command> context, int countDownFrom, ActorRef<Done> notifyWhenZero) {\n    super(context);\n    this.remaining = countDownFrom;\n    this.notifyWhenZero = notifyWhenZero;\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder().onMessage(Down.class, notUsed -> onDown()).build();\n  }\n\n  private Behavior<Command> onDown() {\n    remaining--;\n    if (remaining == 0) {\n      notifyWhenZero.tell(Done.getInstance());\n      return Behaviors.stopped();\n    } else {\n      return this;\n    }\n  }\n}\nWhen spawning an actor from this initial behavior it looks like:\nScala copysourceval countDown = context.spawn(CountDown(100, doneRef), \"countDown\") Java copysourceActorRef<CountDown.Command> countDown =\n    context.spawn(CountDown.create(100, doneRef), \"countDown\");","title":"Behavior factory method"},{"location":"/typed/style-guide.html#where-to-define-messages","text":"When sending or receiving actor messages they should be prefixed with the name of the actor/behavior that defines them to avoid ambiguities.\nScala copysourcecountDown ! CountDown.Down Java copysourcecountDown.tell(CountDown.Down.INSTANCE);\nSuch a style is preferred over using importing Down and using countDown ! Down importing Down and using countDown.tell(Down.INSTANCE);. However, within the Behavior that handle these messages the short names can be used.\nTherefore it is not recommended to define messages as top-level classes.\nFor the majority of cases it’s good style to define the messages in the companion objectas static inner classes together with the Behavior.\nScala copysourceobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n} Java copysourcepublic class Counter extends AbstractBehavior<Counter.Command> {\n\n  public interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    public final ActorRef<Value> replyTo;\n\n    public GetValue(ActorRef<Value> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Value {\n    public final int value;\n\n    public Value(int value) {\n      this.value = value;\n    }\n  }\n}\nIf several actors share the same message protocol, it’s recommended to define those messages in a separate objectinterface for that protocol.\nHere’s an example of a shared message protocol setup:\nScala copysourceobject CounterProtocol {\n  sealed trait Command\n\n  final case class Increment(delta: Int, replyTo: ActorRef[OperationResult]) extends Command\n  final case class Decrement(delta: Int, replyTo: ActorRef[OperationResult]) extends Command\n\n  sealed trait OperationResult\n  case object Confirmed extends OperationResult\n  final case class Rejected(reason: String) extends OperationResult\n} Java copysourceinterface CounterProtocol {\n  interface Command {}\n\n  public static class Increment implements Command {\n    public final int delta;\n    private final ActorRef<OperationResult> replyTo;\n\n    public Increment(int delta, ActorRef<OperationResult> replyTo) {\n      this.delta = delta;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Decrement implements Command {\n    public final int delta;\n    private final ActorRef<OperationResult> replyTo;\n\n    public Decrement(int delta, ActorRef<OperationResult> replyTo) {\n      this.delta = delta;\n      this.replyTo = replyTo;\n    }\n  }\n\n  interface OperationResult {}\n\n  enum Confirmed implements OperationResult {\n    INSTANCE\n  }\n\n  public static class Rejected implements OperationResult {\n    public final String reason;\n\n    public Rejected(String reason) {\n      this.reason = reason;\n    }\n  }\n}\nNote that the response message hierarchy in this case could be completely avoided by using the API instead (see Generic Response Wrapper).","title":"Where to define messages"},{"location":"/typed/style-guide.html#public-versus-private-messages","text":"Often an actor has some messages that are only for its internal implementation and not part of the public message protocol, such as timer messages or wrapper messages for ask or messageAdapter.\nSuch messages should be declared private so they can’t be accessed and sent from the outside of the actor. Note that they must still extendimplement the public Command traitinterface.\nHere is an example of using private for an internal message:\nScala copysourceobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n\n  // Tick is private so can't be sent from the outside\n  private case object Tick extends Command\n\n  def apply(name: String, tickInterval: FiniteDuration): Behavior[Command] =\n    Behaviors.setup { context =>\n      Behaviors.withTimers { timers =>\n        timers.startTimerWithFixedDelay(Tick, tickInterval)\n        new Counter(name, context).counter(0)\n      }\n    }\n}\n\nclass Counter private (name: String, context: ActorContext[Counter.Command]) {\n  import Counter._\n\n  private def counter(n: Int): Behavior[Command] =\n    Behaviors.receiveMessage {\n      case Increment =>\n        val newValue = n + 1\n        context.log.debug2(\"[{}] Incremented counter to [{}]\", name, newValue)\n        counter(newValue)\n      case Tick =>\n        val newValue = n + 1\n        context.log.debug2(\"[{}] Incremented counter by background tick to [{}]\", name, newValue)\n        counter(newValue)\n      case GetValue(replyTo) =>\n        replyTo ! Value(n)\n        Behaviors.same\n    }\n} Java copysourcepublic class Counter extends AbstractBehavior<Counter.Command> {\n\n  public interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    public final ActorRef<Value> replyTo;\n\n    public GetValue(ActorRef<Value> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Value {\n    public final int value;\n\n    public Value(int value) {\n      this.value = value;\n    }\n  }\n\n  // Tick is private so can't be sent from the outside\n  private enum Tick implements Command {\n    INSTANCE\n  }\n\n  public static Behavior<Command> create(String name, Duration tickInterval) {\n    return Behaviors.setup(\n        context ->\n            Behaviors.withTimers(\n                timers -> {\n                  timers.startTimerWithFixedDelay(Tick.INSTANCE, tickInterval);\n                  return new Counter(name, context);\n                }));\n  }\n\n  private final String name;\n  private int count;\n\n  private Counter(String name, ActorContext<Command> context) {\n    super(context);\n    this.name = name;\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Increment.class, notUsed -> onIncrement())\n        .onMessage(Tick.class, notUsed -> onTick())\n        .onMessage(GetValue.class, this::onGetValue)\n        .build();\n  }\n\n\n  private Behavior<Command> onIncrement() {\n    count++;\n    getContext().getLog().debug(\"[{}] Incremented counter to [{}]\", name, count);\n    return this;\n  }\n\n  private Behavior<Command> onTick() {\n    count++;\n    getContext()\n        .getLog()\n        .debug(\"[{}] Incremented counter by background tick to [{}]\", name, count);\n    return this;\n  }\n\n  private Behavior<Command> onGetValue(GetValue command) {\n    command.replyTo.tell(new Value(count));\n    return this;\n  }\n\n}\nAn alternative approach is using a type hierarchy and narrow to have a super-type for the public messages as a distinct type from the super-type of all actor messages. The former approach is recommended but it is good to know this alternative as it can be useful when using shared message protocol classes as described in Where to define messages.\nHere’s an example of using a type hierarchy to separate public and private messages:\nScala copysource// above example is preferred, but this is possible and not wrong\nobject Counter {\n  // The type of all public and private messages the Counter actor handles\n  sealed trait Message\n\n  /** Counter's public message protocol type. */\n  sealed trait Command extends Message\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Value]) extends Command\n  final case class Value(n: Int)\n\n  // The type of the Counter actor's internal messages.\n  sealed trait PrivateCommand extends Message\n  // Tick is a private command so can't be sent to an ActorRef[Command]\n  case object Tick extends PrivateCommand\n\n  def apply(name: String, tickInterval: FiniteDuration): Behavior[Command] = {\n    Behaviors\n      .setup[Counter.Message] { context =>\n        Behaviors.withTimers { timers =>\n          timers.startTimerWithFixedDelay(Tick, tickInterval)\n          new Counter(name, context).counter(0)\n        }\n      }\n      .narrow // note narrow here\n  }\n}\n\nclass Counter private (name: String, context: ActorContext[Counter.Message]) {\n  import Counter._\n\n  private def counter(n: Int): Behavior[Message] =\n    Behaviors.receiveMessage {\n      case Increment =>\n        val newValue = n + 1\n        context.log.debug2(\"[{}] Incremented counter to [{}]\", name, newValue)\n        counter(newValue)\n      case Tick =>\n        val newValue = n + 1\n        context.log.debug2(\"[{}] Incremented counter by background tick to [{}]\", name, newValue)\n        counter(newValue)\n      case GetValue(replyTo) =>\n        replyTo ! Value(n)\n        Behaviors.same\n    }\n} Java copysource// above example is preferred, but this is possible and not wrong\npublic class Counter extends AbstractBehavior<Counter.Message> {\n\n  // The type of all public and private messages the Counter actor handles\n  public interface Message {}\n\n  /** Counter's public message protocol type. */\n  public interface Command extends Message {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    public final ActorRef<Value> replyTo;\n\n    public GetValue(ActorRef<Value> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Value {\n    public final int value;\n\n    public Value(int value) {\n      this.value = value;\n    }\n  }\n\n  // The type of the Counter actor's internal messages.\n  interface PrivateCommand extends Message {}\n\n  // Tick is a private command so can't be sent to an ActorRef<Command>\n  enum Tick implements PrivateCommand {\n    INSTANCE\n  }\n\n  public static Behavior<Command> create(String name, Duration tickInterval) {\n    return Behaviors.setup(\n            (ActorContext<Message> context) ->\n                Behaviors.withTimers(\n                    timers -> {\n                      timers.startTimerWithFixedDelay(Tick.INSTANCE, tickInterval);\n                      return new Counter(name, context);\n                    }))\n        .narrow(); // note narrow here\n  }\n\n  private final String name;\n  private int count;\n\n  private Counter(String name, ActorContext<Message> context) {\n    super(context);\n    this.name = name;\n  }\n\n  @Override\n  public Receive<Message> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Increment.class, notUsed -> onIncrement())\n        .onMessage(Tick.class, notUsed -> onTick())\n        .onMessage(GetValue.class, this::onGetValue)\n        .build();\n  }\n\n  private Behavior<Message> onIncrement() {\n    count++;\n    getContext().getLog().debug(\"[{}] Incremented counter to [{}]\", name, count);\n    return this;\n  }\n\n  private Behavior<Message> onTick() {\n    count++;\n    getContext()\n        .getLog()\n        .debug(\"[{}] Incremented counter by background tick to [{}]\", name, count);\n    return this;\n  }\n\n  private Behavior<Message> onGetValue(GetValue command) {\n    command.replyTo.tell(new Value(count));\n    return this;\n  }\n}\nprivate visibility can be defined for the PrivateCommand messages but it’s not strictly needed since they can’t be sent to an ActorRef[Command]ActorRef, which is the public message type of the actor.\nSingleton messages For messages without parameters the enum singleton pattern is recommended: Java copysourcepublic enum Increment implements Command {\n  INSTANCE\n} In the ReceiveBuilder it can be matched in same way as other messages: Java copysource.onMessage(Increment.class, notUsed -> onIncrement())\nLambdas versus method references It’s recommended to keep the message matching with the ReceiveBuilder as short and clean as possible and delegate to methods. This improves readability and ease of method navigation with an IDE. The delegation can be with lambdas or method references. Example of delegation using a lambda: Java copysource@Override\npublic Receive<Command> createReceive() {\n  return newReceiveBuilder()\n      .onMessage(Increment.class, notUsed -> onIncrement())\n      .build();\n}\n\nprivate Behavior<Command> onIncrement() {\n  count++;\n  getContext().getLog().debug(\"[{}] Incremented counter to [{}]\", name, count);\n  return this;\n} When possible it’s preferred to use method references instead of lambdas. The benefit is less verbosity and in some cases it can actually give better type inference. Java copysource@Override\npublic Receive<Command> createReceive() {\n  return newReceiveBuilder()\n      .onMessage(GetValue.class, this::onGetValue)\n      .build();\n}\n\nprivate Behavior<Command> onGetValue(GetValue command) {\n  command.replyTo.tell(new Value(count));\n  return this;\n} this::onGetValue is a method reference in above example. It corresponds to command -> onGetValue(command). If you are using IntelliJ IDEA it has support for converting lambdas to method references. More important than the choice between lambdas or method references is to avoid lambdas with a large block of code. An anti-pattern would be to inline all message handling inside the lambdas like this: Java copysource// this is an anti-pattern, don't use lambdas with a large block of code\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(\n            Increment.class,\n            notUsed -> {\n              count++;\n              getContext().getLog().debug(\"[{}] Incremented counter to [{}]\", name, count);\n              return this;\n            })\n        .onMessage(\n            Tick.class,\n            notUsed -> {\n              count++;\n              getContext()\n                  .getLog()\n                  .debug(\"[{}] Incremented counter by background tick to [{}]\", name, count);\n              return this;\n            })\n        .onMessage(\n            GetValue.class,\n            command -> {\n              command.replyTo.tell(new Value(count));\n              return this;\n            })\n        .build();\n  } In a real application it would often be more than 3 lines for each message. It’s not only making it more difficult to get an overview of the message matching, but compiler errors related to lambdas can sometimes be difficult to understand. Ideally, lambdas should be written in one line of code. Two lines can be ok, but three is probably too much. Also, don’t use braces and return statements in one-line lambda bodies.\nPartial versus total Function It’s recommended to use a sealed trait as the super type of the commands (incoming messages) of an actor as the compiler will emit a warning if a message type is forgotten in the pattern match. Scala copysourcesealed trait Command\ncase object Down extends Command\nfinal case class GetValue(replyTo: ActorRef[Value]) extends Command\nfinal case class Value(n: Int) That is the main reason for Behaviors.receive, Behaviors.receiveMessage taking a Function rather than a PartialFunction. The compiler warning if GetValue is not handled would be: [warn] ... Counter.scala:45:34: match may not be exhaustive.\n[warn] It would fail on the following input: GetValue(_)\n[warn]         Behaviors.receiveMessage {\n[warn]                                  ^\n Note that a MatchError will be thrown at runtime if a message is not handled, so it’s important to pay attention to those. If a Behavior should not handle certain messages you can still include them in the pattern match and return Behaviors.unhandled. Scala copysourceval zero: Behavior[Command] = {\n  Behaviors.receiveMessage {\n    case GetValue(replyTo) =>\n      replyTo ! Value(0)\n      Behaviors.same\n    case Down =>\n      Behaviors.unhandled\n  }\n} It’s recommended to use the sealed trait and total functions with exhaustiveness check to detect mistakes of forgetting to handle some messages. Sometimes, that can be inconvenient and then you can use a PartialFunction with Behaviors.receivePartial or Behaviors.receiveMessagePartial Scala copysourceval zero: Behavior[Command] = {\n  Behaviors.receiveMessagePartial {\n    case GetValue(replyTo) =>\n      replyTo ! Value(0)\n      Behaviors.same\n  }\n}\nHow to compose Partial Functions Following up from previous section, there are times when one might want to combine different PartialFunctions into one Behavior. A good use case for composing two or more PartialFunctions is when there is a bit of behavior that repeats across different states of the Actor. Below, you can find a simplified example for this use case. The Command definition is still highly recommended be kept within a sealed Trait: Scala copysourcesealed trait Command\ncase object Down extends Command\nfinal case class GetValue(replyTo: ActorRef[Value]) extends Command\nfinal case class Value(n: Int) In this particular case, the Behavior that is repeating over is the one in charge to handle the GetValue Command, as it behaves the same regardless of the Actor’s internal state. Instead of defining the specific handlers as a Behavior, we can define them as a PartialFunction: Scala copysourcedef getHandler(value: Int): PartialFunction[Command, Behavior[Command]] = {\n  case GetValue(replyTo) =>\n    replyTo ! Value(value)\n    Behaviors.same\n}\ndef setHandlerNotZero(value: Int): PartialFunction[Command, Behavior[Command]] = {\n  case Down =>\n    if (value == 1)\n      zero\n    else\n      nonZero(value - 1)\n}\ndef setHandlerZero(log: Logger): PartialFunction[Command, Behavior[Command]] = {\n  case Down =>\n    log.error(\"Counter is already at zero!\")\n    Behaviors.same\n} Finally, we can go on defining the two different behaviors for this specific actor. For each Behavior we would go and concatenate all needed PartialFunction instances with orElse to finally apply the command to the resulting one: Scala copysourceval zero: Behavior[Command] = Behaviors.setup { context =>\n  Behaviors.receiveMessagePartial(getHandler(0).orElse(setHandlerZero(context.log)))\n}\n\ndef nonZero(capacity: Int): Behavior[Command] =\n  Behaviors.receiveMessagePartial(getHandler(capacity).orElse(setHandlerNotZero(capacity)))\n\n// Default Initial Behavior for this actor\ndef apply(initialCapacity: Int): Behavior[Command] = nonZero(initialCapacity) Even though in this particular example we could use receiveMessage as we cover all cases, we use receiveMessagePartial instead to cover potential future unhandled message cases.\nask versus ? When using the AskPattern it’s recommended to use the ask method rather than the infix ? operator, like so: Scala copysourceimport org.apache.pekko\nimport pekko.actor.typed.scaladsl.AskPattern._\nimport pekko.util.Timeout\n\nimplicit val timeout: Timeout = Timeout(3.seconds)\nval counter: ActorRef[Command] = ???\n\nval result: Future[OperationResult] = counter.ask(replyTo => Increment(delta = 2, replyTo)) You may also use the more terse placeholder syntax _ instead of replyTo: Scala copysourceval result2: Future[OperationResult] = counter.ask(Increment(delta = 2, _)) However, using the infix operator ? with the placeholder syntax _, like is done in the following example, won’t typecheck because of the binding scope rules for wildcard parameters: Scala copysource// doesn't compile\nval result3: Future[OperationResult] = counter ? Increment(delta = 2, _) Adding the necessary parentheses (as shown below) makes it typecheck, but, subjectively, it’s rather ugly so the recommendation is to use ask. Scala copysourceval result3: Future[OperationResult] = counter ? (Increment(delta = 2, _)) Note that AskPattern is only intended for request-response interaction from outside an actor. If the requester is inside an actor, prefer ActorContext.ask as it provides better thread-safety by not requiring the use of a FutureCompletionStage inside the actor.\nReceiveBuilder Using the ReceiveBuilder is the typical, and recommended, way of defining message handlers, but it can be good to know that it’s optional in case you would prefer a different approach. Alternatives could be like: direct processing because there is only one message type if or switch statements annotation processor Vavr Pattern Matching DSL pattern matching since JDK 14 (JEP 305) In Behaviors there are receive, receiveMessage and receiveSignal factory methods that takes functions instead of using the ReceiveBuilder, which is the receive with the class parameter. In AbstractBehavior you can return your own org.apache.pekko.actor.typed.javadsl.Receive from createReceive instead of using newReceiveBuilder. Implement the receiveMessage and receiveSignal in the Receive subclass.","title":"Public versus private messages"},{"location":"/typed/style-guide.html#nesting-setup","text":"When an actor behavior needs more than one of setup, withTimers and withStash the methods can be nested to access the needed dependencies:\nScala copysourcedef apply(): Behavior[Command] =\n  Behaviors.setup[Command](context =>\n    Behaviors.withStash(100)(stash =>\n      Behaviors.withTimers { timers =>\n        context.log.debug(\"Starting up\")\n\n        // behavior using context, stash and timers ...\n      })) Java copysourcepublic static Behavior<Command> apply() {\n  return Behaviors.setup(\n      context ->\n          Behaviors.withStash(\n              100,\n              stash ->\n                  Behaviors.withTimers(\n                      timers -> {\n                        context.getLog().debug(\"Starting up\");\n\n                        // behavior using context, stash and timers ...\n                      })));\n}\nThe order of the nesting does not change the behavior as long as there is no additional logic in any other function than the innermost one. It can be nice to default to put setup outermost as that is the least likely block that will be removed if the actor logic changes.\nNote that adding supervise to the mix is different as it will restart the behavior it wraps, but not the behavior around itself:\nScala copysourcedef apply(): Behavior[Command] =\n  Behaviors.setup { context =>\n    // only run on initial actor start, not on crash-restart\n    context.log.info(\"Starting\")\n\n    Behaviors\n      .supervise(Behaviors.withStash[Command](100) { stash =>\n        // every time the actor crashes and restarts a new stash is created (previous stash is lost)\n        context.log.debug(\"Starting up with stash\")\n        // Behaviors.receiveMessage { ... }\n      })\n      .onFailure[RuntimeException](SupervisorStrategy.restart)\n  } Java copysourcepublic static Behavior<Command> create() {\n  return Behaviors.setup(\n      context -> {\n        // only run on initial actor start, not on crash-restart\n        context.getLog().info(\"Starting\");\n\n        return Behaviors.<Command>supervise(\n                Behaviors.withStash(\n                    100,\n                    stash -> {\n                      // every time the actor crashes and restarts a new stash is created\n                      // (previous stash is lost)\n                      context.getLog().debug(\"Starting up with stash\");\n                      // Behaviors.receiveMessage { ... }\n                    }))\n            .onFailure(RuntimeException.class, SupervisorStrategy.restart());\n      });\n}","title":"Nesting setup"},{"location":"/typed/style-guide.html#additional-naming-conventions","text":"Some naming conventions have already been mentioned in the context of other recommendations, but here is a list of additional conventions:\nreplyTo is the typical name for the ActorRef[Reply]ActorRef<Reply> parameter in messages to which a reply or acknowledgement should be sent. Incoming messages to an actor are typically called commands, and therefore the super type of all messages that an actor can handle is typically sealed trait Commandinterface Command {}. Use past tense for the events persisted by an EventSourcedBehavior since those represent facts that have happened, for example Incremented.","title":"Additional naming conventions"},{"location":"/typed/from-classic.html","text":"","title":"Learning Pekko Typed from Classic"},{"location":"/typed/from-classic.html#learning-pekko-typed-from-classic","text":"Pekko Classic is the original Actor APIs, which have been improved by more type safe and guided Actor APIs, known as Pekko Typed.\nIf you already know the classic Actor APIs and would like to learn Pekko Typed, this reference is a good resource. Many concepts are the same and this page tries to highlight differences and how to do certain things in Typed compared to classic.\nYou should probably learn some of the basics of Pekko Typed to see how it looks like before diving into the differences and details described here. A good starting point for that is the IoT example in the Getting Started Guide or the examples shown in Introduction to Actors.\nNote that Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use Pekko Typed together with classic actors within the same ActorSystem, see coexistence. For new projects we recommend using the new Actor APIs.","title":"Learning Pekko Typed from Classic"},{"location":"/typed/from-classic.html#dependencies","text":"The dependencies of the Typed modules are named by adding -typed suffix of the corresponding classic module, with a few exceptions.\nFor example pekko-cluster-typed:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-typed_${versions.ScalaBinary}\"\n}\nArtifact names:\nClassic Typed pekko-actor pekko-actor-typed pekko-cluster pekko-cluster-typed pekko-cluster-sharding pekko-cluster-sharding-typed pekko-cluster-tools pekko-cluster-typed pekko-distributed-data pekko-cluster-typed pekko-persistence pekko-persistence-typed pekko-stream pekko-stream-typed pekko-testkit akka-actor-testkit-typed\nCluster Singleton and Distributed Data are included in akka-cluster-typed.\nArtifacts not listed in above table don’t have a specific API for Pekko Typed.","title":"Dependencies"},{"location":"/typed/from-classic.html#package-names","text":"The convention of the package names in Pekko Typed is to add typed.scaladsl and typed.javadsl to the corresponding Pekko classic package name. scaladsl and javadsl is the convention to separate Scala and Java APIs, which is familiar from Pekko Streams.\nExamples of a few package names:\nClassic Typed for Scala Typed for Java org.apache.pekko.actor org.apache.pekko.actor.typed.scaladsl org.apache.pekko.actor.typed.javadsl org.apache.pekko.cluster org.apache.pekko.cluster.typed org.apache.pekko.cluster.typed org.apache.pekko.cluster.sharding org.apache.pekko.cluster.sharding.typed.scaladsl org.apache.pekko.cluster.sharding.typed.javadsl org.apache.pekko.persistence org.apache.pekko.persistence.typed.scaladsl org.apache.pekko.persistence.typed.javadsl","title":"Package names"},{"location":"/typed/from-classic.html#actor-definition","text":"A classic actor is defined by a class extending org.apache.pekko.actor.Actororg.apache.pekko.actor.AbstractActor.\nAn actor in Typed is defined by a class extending org.apache.pekko.actor.typed.scaladsl.AbstractBehaviororg.apache.pekko.actor.typed.javadsl.AbstractBehavior.\nIt’s also possible to define an actor in Typed from functions instead of extending a class. This is called the functional style.\nClassic HelloWorld actor:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.Actor\nimport pekko.actor.ActorLogging\nimport pekko.actor.Props\n\nobject HelloWorld {\n  final case class Greet(whom: String)\n  final case class Greeted(whom: String)\n\n  def props(): Props =\n    Props(new HelloWorld)\n}\n\nclass HelloWorld extends Actor with ActorLogging {\n  import HelloWorld._\n\n  override def receive: Receive = {\n    case Greet(whom) =>\n      log.info(\"Hello {}!\", whom)\n      sender() ! Greeted(whom)\n  }\n} Java copysourceimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.actor.Props;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\n\npublic class HelloWorld extends AbstractActor {\n\n  public static final class Greet {\n    public final String whom;\n\n    public Greet(String whom) {\n      this.whom = whom;\n    }\n  }\n\n  public static final class Greeted {\n    public final String whom;\n\n    public Greeted(String whom) {\n      this.whom = whom;\n    }\n  }\n\n  public static Props props() {\n    return Props.create(HelloWorld.class, HelloWorld::new);\n  }\n\n  private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(Greet.class, this::onGreet).build();\n  }\n\n  private void onGreet(Greet command) {\n    log.info(\"Hello {}!\", command.whom);\n    getSender().tell(new Greeted(command.whom), getSelf());\n  }\n}\nTyped HelloWorld actor:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.AbstractBehavior\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject HelloWorld {\n  final case class Greet(whom: String, replyTo: ActorRef[Greeted])\n  final case class Greeted(whom: String, from: ActorRef[Greet])\n\n  def apply(): Behavior[HelloWorld.Greet] =\n    Behaviors.setup(context => new HelloWorld(context))\n}\n\nclass HelloWorld(context: ActorContext[HelloWorld.Greet]) extends AbstractBehavior[HelloWorld.Greet](context) {\n  import HelloWorld._\n\n  override def onMessage(message: Greet): Behavior[Greet] = {\n    context.log.info(\"Hello {}!\", message.whom)\n    message.replyTo ! Greeted(message.whom, context.self)\n    this\n  }\n} Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class HelloWorld extends AbstractBehavior<HelloWorld.Greet> {\n\n  public static final class Greet {\n    public final String whom;\n    public final ActorRef<Greeted> replyTo;\n\n    public Greet(String whom, ActorRef<Greeted> replyTo) {\n      this.whom = whom;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class Greeted {\n    public final String whom;\n    public final ActorRef<Greet> from;\n\n    public Greeted(String whom, ActorRef<Greet> from) {\n      this.whom = whom;\n      this.from = from;\n    }\n  }\n\n  public static Behavior<Greet> create() {\n    return Behaviors.setup(HelloWorld::new);\n  }\n\n  private HelloWorld(ActorContext<Greet> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Greet> createReceive() {\n    return newReceiveBuilder().onMessage(Greet.class, this::onGreet).build();\n  }\n\n  private Behavior<Greet> onGreet(Greet command) {\n    getContext().getLog().info(\"Hello {}!\", command.whom);\n    command.replyTo.tell(new Greeted(command.whom, getContext().getSelf()));\n    return this;\n  }\n}\nWhy is it called Behavior and not Actor?\nIn Typed, the Behavior defines how to handle incoming messages. After processing a message, a different Behavior may be returned for processing the next message. This means that an actor is started with an initial Behavior and may change Behavior over its lifecycle. This is described more in the section about become.\nNote that the Behavior has a type parameter describing the type of messages that it can handle. This information is not defined explicitly for a classic actor.\nLinks to reference documentation:\nClassic Typed","title":"Actor definition"},{"location":"/typed/from-classic.html#actorof-and-props","text":"A classic actor is started with the actorOf method of the ActorContext or ActorSystem.\nCorresponding method in Typed is called spawn in the org.apache.pekko.actor.typed.scaladsl.ActorContextorg.apache.pekko.actor.typed.javadsl.ActorContext.\nThere is no spawn method in the org.apache.pekko.actor.typed.scaladsl.ActorSystemorg.apache.pekko.actor.typed.javadsl.ActorSystem for creating top level actors. Instead, there is a single top level actor defined by a user guardian Behavior that is given when starting the ActorSystem. Other actors are started as children of that user guardian actor or children of other actors in the actor hierarchy. This is explained more in ActorSystem.\nNote that when mixing classic and typed and have a classic system, spawning top level actors from the side is possible, see Coexistence.\nThe actorOf method takes an org.apache.pekko.actor.Props parameter, which is like a factory for creating the actor instance, and it’s also used when creating a new instance when the actor is restarted. The Props may also define additional properties such as which dispatcher to use for the actor.\nIn typed, the spawn method creates an actor directly from a given Behavior without using a Props factory. It does however accept an optional org.apache.pekko.actor.typed.Props for specifying Actor metadata. The factory aspect is instead defined via Behaviors.setup when using the object-oriented style with a class extending AbstractBehavior. For the function style there is typically no need for the factory.\nAdditional properties such as which dispatcher to use for the actor can still be given via an optional org.apache.pekko.actor.typed.Props parameter of the spawn method.\nThe name parameter of actorOf is optional and if not defined the actor will have a generated name. Corresponding in Typed is achieved with the spawnAnonymous method.\nLinks to reference documentation:\nClassic Typed","title":"actorOf and Props"},{"location":"/typed/from-classic.html#actorref","text":"org.apache.pekko.actor.ActorRef has its correspondence in org.apache.pekko.actor.typed.ActorRef. The difference being that the latter has a type parameter describing which messages the actor can handle. This information is not defined for a classic actor and you can send any type of message to a classic ActorRef even though the actor may not understand it.","title":"ActorRef"},{"location":"/typed/from-classic.html#actorsystem","text":"org.apache.pekko.actor.ActorSystem has its correspondence in org.apache.pekko.actor.typed.ActorSystem. One difference is that when creating an ActorSystem in Typed you give it a Behavior that will be used as the top level actor, also known as the user guardian.\nAdditional actors for an application are created from the user guardian alongside performing the initialization of Pekko components such as Cluster Sharding. In contrast, in a classic ActorSystem, such initialization is typically performed from the “outside”.\nThe actorOf method of the classic ActorSystem is typically used to create a few (or many) top level actors. The ActorSystem in Typed doesn’t have that capability. Instead, such actors are started as children of the user guardian actor or children of other actors in the actor hierarchy. The rationale for this is partly about consistency. In a typed system you can’t create children to an arbitrary actor from anywhere in your app without messaging it, so this will also hold true for the user guardian actor. That noted, in cases where you do need to spawn outside of this guardian then you can use the SpawnProtocol to spawn as needed.","title":"ActorSystem"},{"location":"/typed/from-classic.html#become","text":"A classic actor can change its message processing behavior by using become in ActorContext. In Typed this is done by returning a new Behavior after processing a message. The returned Behavior will be used for the next received message.\nThere is no correspondence to unbecome in Typed. Instead you must explicitly keep track of and return the “previous” Behavior.\nLinks to reference documentation:\nClassic","title":"become"},{"location":"/typed/from-classic.html#sender","text":"There is no sender()getSender() in Typed. Instead you have to explicitly include an ActorRef representing the sender—or rather representing where to send a reply to—in the messages.\nThe reason for not having an implicit sender in Typed is that it wouldn’t be possible to know the type for the sender ActorRef[T]ActorRef<T> at compile time. It’s also much better to define this explicitly in the messages as it becomes more clear what the message protocol expects.\nLinks to reference documentation:\nClassic Typed","title":"sender"},{"location":"/typed/from-classic.html#parent","text":"There is no parentgetParent in Typed. Instead you have to explicitly include the ActorRef of the parent as a parameter when constructing the Behavior.\nThe reason for not having a parent in Typed is that it wouldn’t be possible to know the type for the parent ActorRef[T]ActorRef<T> at compile time without having an additional type parameter in the Behavior. For testing purposes it’s also better to pass in the parent since it can be replaced by a probe or being stubbed out in tests.","title":"parent"},{"location":"/typed/from-classic.html#supervision","text":"An important difference between classic and typed is that in typed, actors are stopped by default if an exception is thrown and no supervision strategy is defined. In contrast, in classic, by default, actors are restarted.\nIn classic actors the supervision strategy for child actors are defined by overriding the supervisorStrategy method in the parent actor.\nIn Typed the supervisor strategy is defined by wrapping the Behavior of the child actor with Behaviors.supervise.\nThe classic BackoffSupervisor is supported by SupervisorStrategy.restartWithBackoff as an ordinary SupervisorStrategy in Typed.\nSupervisorStrategy.Escalate isn’t supported in Typed, but similar can be achieved as described in Bubble failures up through the hierarchy.\nLinks to reference documentation:\nClassic Typed","title":"Supervision"},{"location":"/typed/from-classic.html#lifecycle-hooks","text":"Classic actors have methods preStart, preRestart, postRestart and postStop that can be overridden to act on changes to the actor’s lifecycle.\nThis is supported with corresponding PreRestart and PostStop signal messages in Typed. There are no PreStart and PostRestart signals because such action can be done from Behaviors.setup or the constructor of the AbstractBehavior class.\nNote that in classic, the postStop lifecycle hook is also called when the actor is restarted. That is not the case in Typed, only the PreRestart signal is emitted. If you need to do resource cleanup on both restart and stop, you have to do that for both PreRestart and PostStop.\nLinks to reference documentation:\nClassic Typed","title":"Lifecycle hooks"},{"location":"/typed/from-classic.html#watch","text":"watch and the Terminated message are pretty much the same, with some additional capabilities in Typed.\nTerminated is a signal in Typed since it is a different type than the declared message type of the Behavior.\nThe watchWith method of the ActorContext in Typed can be used to send a message instead of the Terminated signal.\nWhen watching child actors it’s possible to see if the child terminated voluntarily or due to a failure via the ChildFailed signal, which is a subclass of Terminated.\nLinks to reference documentation:\nClassic Typed","title":"watch"},{"location":"/typed/from-classic.html#stopping","text":"Classic actors can be stopped with the stop method of ActorContext or ActorSystem. In Typed an actor stops itself by returning Behaviors.stopped. There is also a stop method in the ActorContext but it can only be used for stopping direct child actors and not any arbitrary actor.\nPoisonPill is not supported in Typed. Instead, if you need to request an actor to stop you should define a message that the actor understands and let it return Behaviors.stopped when receiving that message.\nLinks to reference documentation:\nClassic Typed","title":"Stopping"},{"location":"/typed/from-classic.html#actorselection","text":"ActorSelection isn’t supported in Typed. Instead the Receptionist is supposed to be used for finding actors by a registered key.\nActorSelection can be used for sending messages to a path without having an ActorRef of the destination. Note that a Group Router can be used for that.\nLinks to reference documentation:\nClassic Typed","title":"ActorSelection"},{"location":"/typed/from-classic.html#ask","text":"The classic ask pattern returns a FutureCompletionStage for the response.\nCorresponding ask exists in Typed and is good when the requester itself isn’t an actor. It is located in org.apache.pekko.actor.typed.scaladsl.AskPatternorg.apache.pekko.actor.typed.javadsl.AskPattern.\nWhen the requester is an actor it is better to use the ask method of the ActorContext in Typed. It has the advantage of not having to mix FutureCompletionStage callbacks that are running on different threads with actor code.\nLinks to reference documentation:\nClassic Typed","title":"ask"},{"location":"/typed/from-classic.html#pipeto","text":"pipeTo is typically used together with ask in an actor. The ask method of the ActorContext in Typed removes the need for pipeTo. However, for interactions with other APIs that return FutureCompletionStage it is still useful to send the result as a message to the actor. For this purpose there is a pipeToSelf method in the ActorContext in Typed.","title":"pipeTo"},{"location":"/typed/from-classic.html#actorcontext-children","text":"The ActorContext has methods children and childgetChildren and getChild to retrieve the ActorRef of started child actors in both Typed and Classic.\nThe type of the returned ActorRef is unknown, since different types can be used for different children. Therefore, this is not a useful way to lookup children when the purpose is to send messages to them.\nInstead of finding children via the ActorContext, it is recommended to use an application specific collection for bookkeeping of children, such as a Map[String, ActorRef[Child.Command]] Map<String, ActorRef<Child.Command>>. It can look like this:\nScala copysourceobject Parent {\n  sealed trait Command\n  case class DelegateToChild(name: String, message: Child.Command) extends Command\n  private case class ChildTerminated(name: String) extends Command\n\n  def apply(): Behavior[Command] = {\n    def updated(children: Map[String, ActorRef[Child.Command]]): Behavior[Command] = {\n      Behaviors.receive { (context, command) =>\n        command match {\n          case DelegateToChild(name, childCommand) =>\n            children.get(name) match {\n              case Some(ref) =>\n                ref ! childCommand\n                Behaviors.same\n              case None =>\n                val ref = context.spawn(Child(), name)\n                context.watchWith(ref, ChildTerminated(name))\n                ref ! childCommand\n                updated(children + (name -> ref))\n            }\n\n          case ChildTerminated(name) =>\n            updated(children - name)\n        }\n      }\n    }\n\n    updated(Map.empty)\n  }\n} Java copysourcepublic class Parent extends AbstractBehavior<Parent.Command> {\n\n  public interface Command {}\n\n  public static class DelegateToChild implements Command {\n    public final String name;\n    public final Child.Command message;\n\n    public DelegateToChild(String name, Child.Command message) {\n      this.name = name;\n      this.message = message;\n    }\n  }\n\n  private static class ChildTerminated implements Command {\n    final String name;\n\n    ChildTerminated(String name) {\n      this.name = name;\n    }\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(Parent::new);\n  }\n\n  private Map<String, ActorRef<Child.Command>> children = new HashMap<>();\n\n  private Parent(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(DelegateToChild.class, this::onDelegateToChild)\n        .onMessage(ChildTerminated.class, this::onChildTerminated)\n        .build();\n  }\n\n  private Behavior<Command> onDelegateToChild(DelegateToChild command) {\n    ActorRef<Child.Command> ref = children.get(command.name);\n    if (ref == null) {\n      ref = getContext().spawn(Child.create(), command.name);\n      getContext().watchWith(ref, new ChildTerminated(command.name));\n      children.put(command.name, ref);\n    }\n    ref.tell(command.message);\n    return this;\n  }\n\n  private Behavior<Command> onChildTerminated(ChildTerminated command) {\n    children.remove(command.name);\n    return this;\n  }\n}\nRemember to remove entries from the Map when the children are terminated. For that purpose it’s convenient to use watchWith, as illustrated in the example above, because then you can include the key to the Map in the termination message. In that way the name of the actor doesn’t have to be the same as identifier used for bookkeeping.\nRetrieving the children from the ActorContext can still be useful for some use cases, such as:\nsee if a child name is in use stopping children the type of the child is well known and unsafeUpcast of the ActorRef is considered “safe enough”","title":"ActorContext.children"},{"location":"/typed/from-classic.html#remote-deployment","text":"Starting an actor on a remote node—so called remote deployment—isn’t supported in Typed.\nThis feature would be discouraged because it often results in tight coupling between nodes and undesirable failure handling. For example if the node of the parent actor crashes, all remote deployed child actors are brought down with it. Sometimes, that can be desired but many times it is used without realizing. This can be achieved by other means, such as using watch.","title":"Remote deployment"},{"location":"/typed/from-classic.html#routers","text":"Routers are provided in Typed, but in a much simplified form compared to the classic routers.\nDestinations of group routers are registered in the Receptionist, which makes them Cluster aware and also more dynamic than classic group routers.\nPool routers are only for local actor destinations in Typed, since remote deployment isn’t supported.\nLinks to reference documentation:\nClassic Typed","title":"Routers"},{"location":"/typed/from-classic.html#fsm","text":"With classic actors there is explicit support for building Finite State Machines. No support is needed in Pekko Typed as it is straightforward to represent FSMs with behaviors.\nLinks to reference documentation:\nClassic Typed","title":"FSM"},{"location":"/typed/from-classic.html#timers","text":"In classic actors you mixin with Timersextend AbstractActorWithTimers to gain access to delayed and periodic scheduling of messages. In Typed you have access to similar capabilities via Behaviors.withTimers.\nLinks to reference documentation:\nClassic Typed","title":"Timers"},{"location":"/typed/from-classic.html#stash","text":"In classic actors you mixin with Stashextend AbstractActorWithStash to gain access to stashing of messages. In Typed you have access to similar capabilities via Behaviors.withStash.\nLinks to reference documentation:\nClassic Typed","title":"Stash"},{"location":"/typed/from-classic.html#persistentactor","text":"The correspondence of the classic PersistentActor is org.apache.pekko.persistence.typed.scaladsl.EventSourcedBehaviororg.apache.pekko.persistence.typed.javadsl.EventSourcedBehavior.\nThe Typed API is much more guided to facilitate Event Sourcing best practices. It also has tighter integration with Cluster Sharding.\nLinks to reference documentation:\nClassic Typed","title":"PersistentActor"},{"location":"/typed/from-classic.html#asynchronous-testing","text":"The Test Kits for asynchronous testing are rather similar.\nLinks to reference documentation:\nClassic Typed","title":"Asynchronous Testing"},{"location":"/typed/from-classic.html#synchronous-testing","text":"Classic and typed have different Test Kits for synchronous testing.\nBehaviors in Typed can be tested in isolation without having to be packaged into an actor. As a consequence, tests can run fully synchronously without having to worry about timeouts and spurious failures.\nThe BehaviorTestKit provides a nice way of unit testing a Behavior in a deterministic way, but it has some limitations to be aware of. Similar limitations exists for synchronous testing of classic actors.\nLinks to reference documentation:\nClassic Typed","title":"Synchronous Testing"},{"location":"/typed/index-cluster.html","text":"","title":"Cluster"},{"location":"/typed/index-cluster.html#cluster","text":"Cluster Usage Module info Cluster API Extension Cluster Membership API Node Roles Failure Detector How to test Configuration Higher level Cluster tools Example project Cluster Specification Introduction Terms Cluster Membership Service Introduction Member States Member Events Membership Lifecycle Leader WeaklyUp Members Full cluster shutdown State Diagrams Phi Accrual Failure Detector Introduction Failure Detector Heartbeats Logging Failure Detector Threshold Distributed Data Module info Introduction Using the Replicator Replicated data types Durable Storage Limitations Learn More about CRDTs Configuration Example project Cluster Singleton Module info Introduction Potential problems to be aware of Example Supervision Application specific stop message Lease Accessing singleton of another data centre Configuration Cluster Sharding Module info Introduction Basic example Persistence example Shard allocation How it works Passivation Automatic Passivation Sharding State Remembering Entities Startup after minimum number of members Health check Inspecting cluster sharding state Lease Removal of internal Cluster Sharding data Configuration Example project Cluster Sharding concepts Scenarios Shard location Shard rebalancing ShardCoordinator state Message ordering Reliable delivery Overhead Sharded Daemon Process Module info Introduction Basic example Addressing the actors Scalability Multi-DC Cluster Dependency Motivation Defining the data centers Membership Failure Detection Cluster Singleton Cluster Sharding Distributed Publish Subscribe in Cluster Module info The Topic Actor Pub Sub Scalability Delivery Guarantee Reliable delivery Module info Introduction Point-to-point Work pulling Sharding Durable producer Ask from the producer Only flow control Chunk large messages Configuration Serialization Dependency Introduction Usage Customization Serialization of Pekko’s messages Java serialization Rolling updates Serialization with Jackson Dependency Introduction Usage Security Annotations Schema Evolution Rolling updates Jackson Modules Using Pekko Serialization for embedded types Additional configuration Additional features Multi JVM Testing Setup Running tests Creating application tests Changing Defaults Configuration of the JVM instances ScalaTest Multi Node Additions Example project Multi Node Testing Module info Multi Node Testing Concepts The Test Conductor The Multi Node Spec The SbtMultiJvm Plugin A Multi Node Testing Example Things to Keep in Mind Configuration Artery Remoting Dependency Configuration Introduction Selecting a transport Canonical address Acquiring references to remote actors Remote Security Quarantine Serialization Routers with Remote Destinations What is new in Artery Performance tuning Remote Configuration Classic Remoting (Deprecated) Module info Configuration Introduction Types of Remote Interaction Looking up Remote Actors Creating Actors Remotely Lifecycle and Failure Recovery Model Watching Remote Actors Serialization Routers with Remote Destinations Remote Security Remote Configuration Split Brain Resolver Module info Enable the Split Brain Resolver The Problem Strategies Indirectly connected nodes Down all when unstable Multiple data centers Cluster Singleton and Cluster Sharding Coordination Module info Lease Using a lease Usages in other Pekko modules Lease implementations Implementing a lease Choosing Pekko Cluster Microservices Traditional distributed application Distributed monolith","title":"Cluster"},{"location":"/typed/cluster.html","text":"","title":"Cluster Usage"},{"location":"/typed/cluster.html#cluster-usage","text":"This document describes how to use Apache Pekko Cluster and the Cluster APIs.\nFor specific documentation topics see:\nWhen and where to use Pekko Cluster Cluster Specification Cluster Membership Service Higher level Cluster tools Rolling Updates Operating, Managing, Observability\nYou are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Cluster.\nYou have to enable serialization to send messages between ActorSystems (nodes) in the Cluster. Serialization with Jackson is a good choice in many cases, and our recommendation if you don’t have other preferences or constraints.","title":"Cluster Usage"},{"location":"/typed/cluster.html#module-info","text":"To use Pekko Cluster add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-typed_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster (typed) Artifact org.apache.pekko pekko-cluster-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/cluster.html#cluster-api-extension","text":"The Cluster extension gives you access to management tasks such as Joining, Leaving and Downing and subscription of cluster membership events such as MemberUp, MemberRemoved and UnreachableMember, which are exposed as event APIs.\nIt does this through these references on the ClusterCluster extension:\nmanager: An ActorRefActorRef[cluster.typed.ClusterCommandcluster.typed.ClusterCommand]ActorRefActorRef<cluster.typed.ClusterCommandcluster.typed.ClusterCommand> where a ClusterCommand is a command such as: JoinJoin, LeaveLeave and DownDown subscriptions: An ActorRefActorRef[cluster.typed.ClusterStateSubscriptioncluster.typed.ClusterStateSubscription]ActorRefActorRef<cluster.typed.ClusterStateSubscriptioncluster.typed.ClusterStateSubscription> where a ClusterStateSubscription is one of GetCurrentStateGetCurrentState or SubscribeSubscribe and UnsubscribeUnsubscribe to cluster events like MemberRemovedMemberRemoved state: The current CurrentClusterStateCurrentClusterState\nAll of the examples below assume the following imports:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed._\nimport pekko.actor.typed.scaladsl._\nimport pekko.cluster.ClusterEvent._\nimport pekko.cluster.MemberStatus\nimport pekko.cluster.typed._ Java copysourceimport org.apache.pekko.actor.typed.*;\nimport org.apache.pekko.actor.typed.javadsl.*;\nimport org.apache.pekko.cluster.ClusterEvent;\nimport org.apache.pekko.cluster.typed.*;\nThe minimum configuration required is to set a host/port for remoting and the pekko.actor.provider = \"cluster\".\ncopysourcepekko {\n  actor {\n    provider = \"cluster\"\n  }\n  remote.artery {\n    canonical {\n      hostname = \"127.0.0.1\"\n      port = 2551\n    }\n  }\n\n  cluster {\n    seed-nodes = [\n      \"akka://ClusterSystem@127.0.0.1:2551\",\n      \"akka://ClusterSystem@127.0.0.1:2552\"]\n    \n    downing-provider-class = \"org.apache.pekko.cluster.sbr.SplitBrainResolverProvider\"\n  }\n}\nAccessing the ClusterCluster extension on each node:\nScala copysourceval cluster = Cluster(system) Java copysourceCluster cluster = Cluster.get(system);\nNote The name of the cluster’s ActorSystemActorSystem must be the same for all members, which is passed in when you start the ActorSystem.","title":"Cluster API Extension"},{"location":"/typed/cluster.html#joining-and-leaving-a-cluster","text":"If not using configuration to specify seed nodes to join, joining the cluster can be done programmatically via the managermanager().\nScala copysourcecluster.manager ! Join(cluster.selfMember.address) Java copysourcecluster.manager().tell(Join.create(cluster.selfMember().address()));\nLeaving the cluster and downing a node are similar:\nScala copysourcecluster2.manager ! Leave(cluster2.selfMember.address) Java copysourcecluster2.manager().tell(Leave.create(cluster2.selfMember().address()));","title":"Joining and Leaving a Cluster"},{"location":"/typed/cluster.html#cluster-subscriptions","text":"Cluster subscriptionssubscriptions() can be used to receive messages when cluster state changes. For example, registering for all MemberEventMemberEvent’s, then using the manager to have a node leave the cluster will result in events for the node going through the Membership Lifecycle.\nThis example subscribes to a subscriber: ActorRef[MemberEvent]ActorRef<MemberEvent> subscriber:\nScala copysourcecluster.subscriptions ! Subscribe(subscriber, classOf[MemberEvent]) Java copysourcecluster.subscriptions().tell(Subscribe.create(subscriber, ClusterEvent.MemberEvent.class));\nThen asking a node to leave:\nScala copysourcecluster.manager ! Leave(anotherMemberAddress)\n// subscriber will receive events MemberLeft, MemberExited and MemberRemoved Java copysourcecluster.manager().tell(Leave.create(anotherMemberAddress));\n// subscriber will receive events MemberLeft, MemberExited and MemberRemoved","title":"Cluster Subscriptions"},{"location":"/typed/cluster.html#cluster-state","text":"Instead of subscribing to cluster events it can sometimes be convenient to only get the full membership state with Cluster(system).stateCluster.get(system).state(). Note that this state is not necessarily in sync with the events published to a cluster subscription.\nSee Cluster Membership more information on member events specifically. There are more types of change events, consult the API documentation of classes that extends cluster.ClusterEvent.ClusterDomainEventcluster.ClusterEvent.ClusterDomainEvent for details about the events.","title":"Cluster State"},{"location":"/typed/cluster.html#cluster-membership-api","text":"","title":"Cluster Membership API"},{"location":"/typed/cluster.html#joining","text":"The seed nodes are initial contact points for joining a cluster, which can be done in different ways:\nautomatically with Cluster Bootstrap with configuration of seed-nodes programatically\nAfter the joining process the seed nodes are not special and they participate in the cluster in exactly the same way as other nodes.","title":"Joining"},{"location":"/typed/cluster.html#joining-automatically-to-seed-nodes-with-cluster-bootstrap","text":"Automatic discovery of nodes for the joining process is available using the open source Pekko Management project’s module, Cluster Bootstrap. Please refer to its documentation for more details.","title":"Joining automatically to seed nodes with Cluster Bootstrap"},{"location":"/typed/cluster.html#joining-configured-seed-nodes","text":"When a new node is started it sends a message to all seed nodes and then sends a join command to the one that answers first. If none of the seed nodes replies (might not be started yet) it retries this procedure until success or shutdown.\nYou can define the seed nodes in the configuration file (application.conf):\npekko.cluster.seed-nodes = [\n  \"pekko://ClusterSystem@host1:2552\",\n  \"pekko://ClusterSystem@host2:2552\"]\nThis can also be defined as Java system properties when starting the JVM using the following syntax:\n-Dpekko.cluster.seed-nodes.0=pekko://ClusterSystem@host1:2552\n-Dpekko.cluster.seed-nodes.1=pekko://ClusterSystem@host2:2552\nWhen a new node is started it sends a message to all configured seed-nodes and then sends a join command to the one that answers first. If none of the seed nodes replies (might not be started yet) it retries this procedure until successful or shutdown.\nThe seed nodes can be started in any order. It is not necessary to have all seed nodes running, but the node configured as the first element in the seed-nodes list must be started when initially starting a cluster. If it is not, the other seed-nodes will not become initialized, and no other node can join the cluster. The reason for the special first seed node is to avoid forming separated islands when starting from an empty cluster. It is quickest to start all configured seed nodes at the same time (order doesn’t matter), otherwise it can take up to the configured seed-node-timeout until the nodes can join.\nAs soon as more than two seed nodes have been started, it is no problem to shut down the first seed node. If the first seed node is restarted, it will first try to join the other seed nodes in the existing cluster. Note that if you stop all seed nodes at the same time and restart them with the same seed-nodes configuration they will join themselves and form a new cluster, instead of joining remaining nodes of the existing cluster. That is likely not desired and can be avoided by listing several nodes as seed nodes for redundancy, and don’t stop all of them at the same time.\nIf you are going to start the nodes on different machines you need to specify the ip-addresses or host names of the machines in application.conf instead of 127.0.0.1","title":"Joining configured seed nodes"},{"location":"/typed/cluster.html#joining-programmatically-to-seed-nodes","text":"Joining programmatically is useful when dynamically discovering other nodes at startup through an external tool or API.\nScala copysourceimport pekko.actor.Address\nimport pekko.actor.AddressFromURIString\nimport pekko.cluster.typed.JoinSeedNodes\n\nval seedNodes: List[Address] =\n  List(\"akka://ClusterSystem@127.0.0.1:2551\", \"akka://ClusterSystem@127.0.0.1:2552\").map(AddressFromURIString.parse)\nCluster(system).manager ! JoinSeedNodes(seedNodes) Java copysourceimport org.apache.pekko.actor.Address;\nimport org.apache.pekko.actor.AddressFromURIString;\nimport org.apache.pekko.cluster.Member;\nimport org.apache.pekko.cluster.typed.JoinSeedNodes;\n\nList<Address> seedNodes = new ArrayList<>();\nseedNodes.add(AddressFromURIString.parse(\"akka://ClusterSystem@127.0.0.1:2551\"));\nseedNodes.add(AddressFromURIString.parse(\"akka://ClusterSystem@127.0.0.1:2552\"));\n\nCluster.get(system).manager().tell(new JoinSeedNodes(seedNodes));\nThe seed node address list has the same semantics as the configured seed-nodes, and the the underlying implementation of the process is the same, see Joining configured seed nodes.\nWhen joining to seed nodes you should not include the node itself, except for the node that is supposed to be the first seed node bootstrapping the cluster. The desired initial seed node address should be placed first in the parameter to the programmatic join.","title":"Joining programmatically to seed nodes"},{"location":"/typed/cluster.html#tuning-joins","text":"Unsuccessful attempts to contact seed nodes are automatically retried after the time period defined in configuration property seed-node-timeout. Unsuccessful attempts to join a specific seed node are automatically retried after the configured retry-unsuccessful-join-after. Retrying means that it tries to contact all seed nodes, then joins the node that answers first. The first node in the list of seed nodes will join itself if it cannot contact any of the other seed nodes within the configured seed-node-timeout.\nThe joining of given seed nodes will, by default, be retried indefinitely until a successful join. That process can be aborted if unsuccessful by configuring a timeout. When aborted it will run Coordinated Shutdown, which will terminate the ActorSystem by default. CoordinatedShutdown can also be configured to exit the JVM. If the seed-nodes are assembled dynamically, it is useful to define this timeout, and a restart with new seed-nodes should be tried after unsuccessful attempts.\npekko.cluster.shutdown-after-unsuccessful-join-seed-nodes = 20s\npekko.coordinated-shutdown.exit-jvm = on\nIf you don’t configure seed nodes or use one of the join seed node functions, you need to join the cluster manually by using JMX or HTTP.\nYou can join to any node in the cluster. It does not have to be configured as a seed node. Note that you can only join to an existing cluster member, which for bootstrapping means a node must join itself and subsequent nodes could join them to make up a cluster.\nAn actor system can only join a cluster once, additional attempts will be ignored. Once an actor system has successfully joined a cluster, it would have to be restarted to join the same cluster again. It can use the same host name and port after the restart. When it come up as a new incarnation of an existing member in the cluster and attempts to join, the existing member will be removed and its new incarnation allowed to join.","title":"Tuning joins"},{"location":"/typed/cluster.html#leaving","text":"There are a few ways to remove a member from the cluster.\nThe recommended way to leave a cluster is a graceful exit, informing the cluster that a node shall leave. This is performed by Coordinated Shutdown when the ActorSystemActorSystem is terminated and also when a SIGTERM is sent from the environment to stop the JVM process. Graceful exit can also be performed using HTTP or JMX. When a graceful exit is not possible, for example in case of abrupt termination of the the JVM process, the node will be detected as unreachable by other nodes and removed after Downing.\nGraceful leaving offers faster hand off to peer nodes during node shutdown than abrupt termination and downing.\nThe Coordinated Shutdown will also run when the cluster node sees itself as Exiting, i.e. leaving from another node will trigger the shutdown process on the leaving node. Tasks for graceful leaving of cluster, including graceful shutdown of Cluster Singletons and Cluster Sharding, are added automatically when Pekko Cluster is used. For example, running the shutdown process will also trigger the graceful leaving if not already in progress.\nNormally this is handled automatically, but in case of network failures during this process it may still be necessary to set the node’s status to Down in order to complete the removal, see Downing.","title":"Leaving"},{"location":"/typed/cluster.html#downing","text":"In many cases a member can gracefully exit from the cluster, as described in Leaving, but there are scenarios when an explicit downing decision is needed before it can be removed. For example in case of abrupt termination of the the JVM process, system overload that doesn’t recover, or network partitions that don’t heal. In such cases, the node(s) will be detected as unreachable by other nodes, but they must also be marked as Down before they are removed.\nWhen a member is considered by the failure detector to be unreachable the leader is not allowed to perform its duties, such as changing status of new joining members to ‘Up’. The node must first become reachable again, or the status of the unreachable member must be changed to Down. Changing status to Down can be performed automatically or manually.\nWe recommend that you enable the Split Brain Resolver that is part of the Pekko Cluster module. You enable it with configuration:\npekko.cluster.downing-provider-class = \"org.apache.pekko.cluster.sbr.SplitBrainResolverProvider\"\nYou should also consider the different available downing strategies.\nIf a downing provider is not configured downing must be performed manually using HTTP or JMX.\nNote that Cluster Singleton or Cluster Sharding entities that are running on a crashed (unreachable) node will not be started on another node until the previous node has been removed from the Cluster. Removal of crashed (unreachable) nodes is performed after a downing decision.\nDowning can also be performed programmatically with Cluster(system).manager ! Down(address)Cluster.get(system).manager().tell(Down(address)), but that is mostly useful from tests and when implementing a DowningProviderDowningProvider.\nIf a crashed node is restarted and joining the cluster again with the same hostname and port, the previous incarnation of that member will first be downed and removed. The new join attempt with same hostname and port is used as evidence that the previous is no longer alive.\nIf a node is still running and sees its self as Down it will shutdown. Coordinated Shutdown will automatically run if run-coordinated-shutdown-when-down is set to on (the default) however the node will not try and leave the cluster gracefully.","title":"Downing"},{"location":"/typed/cluster.html#node-roles","text":"Not all nodes of a cluster need to perform the same function. For example, there might be one sub-set which runs the web front-end, one which runs the data access layer and one for the number-crunching. Choosing which actors to start on each node, for example cluster-aware routers, can take node roles into account to achieve this distribution of responsibilities.\nThe node roles are defined in the configuration property named pekko.cluster.roles and typically defined in the start script as a system property or environment variable.\nThe roles are part of the membership information in MemberEventMemberEvent that you can subscribe to. The roles of the own node are available from the selfMemberselfMember() and that can be used for conditionally starting certain actors:\nScala copysourceval selfMember = Cluster(context.system).selfMember\nif (selfMember.hasRole(\"backend\")) {\n  context.spawn(Backend(), \"back\")\n} else if (selfMember.hasRole(\"frontend\")) {\n  context.spawn(Frontend(), \"front\")\n} Java copysourceMember selfMember = Cluster.get(context.getSystem()).selfMember();\nif (selfMember.hasRole(\"backend\")) {\n  context.spawn(Backend.create(), \"back\");\n} else if (selfMember.hasRole(\"front\")) {\n  context.spawn(Frontend.create(), \"front\");\n}","title":"Node Roles"},{"location":"/typed/cluster.html#failure-detector","text":"The nodes in the cluster monitor each other by sending heartbeats to detect if a node is unreachable from the rest of the cluster. Please see:\nFailure Detector specification Phi Accrual Failure Detector implementation Using the Failure Detector","title":"Failure Detector"},{"location":"/typed/cluster.html#using-the-failure-detector","text":"Cluster uses the remote.PhiAccrualFailureDetectorremote.PhiAccrualFailureDetector failure detector by default, or you can provide your by implementing the remote.FailureDetectorremote.FailureDetector and configuring it:\npekko.cluster.implementation-class = \"com.example.CustomFailureDetector\"\nIn the Cluster Configuration you may want to adjust these depending on you environment:\nWhen a phi value is considered to be a failure pekko.cluster.failure-detector.threshold Margin of error for sudden abnormalities pekko.cluster.failure-detector.acceptable-heartbeat-pause","title":"Using the Failure Detector"},{"location":"/typed/cluster.html#how-to-test","text":"Pekko comes with and uses several types of testing strategies:\nTesting Multi Node Testing Multi JVM Testing","title":"How to test"},{"location":"/typed/cluster.html#configuration","text":"There are several configuration properties for the cluster. Refer to the reference configuration for full configuration descriptions, default values and options.","title":"Configuration"},{"location":"/typed/cluster.html#how-to-startup-when-a-cluster-size-is-reached","text":"A common use case is to start actors after the cluster has been initialized, members have joined, and the cluster has reached a certain size.\nWith a configuration option you can define the required number of members before the leader changes member status of ‘Joining’ members to ‘Up’.:\npekko.cluster.min-nr-of-members = 3\nIn a similar way you can define the required number of members of a certain role before the leader changes member status of ‘Joining’ members to ‘Up’.:\npekko.cluster.role {\n  frontend.min-nr-of-members = 1\n  backend.min-nr-of-members = 2\n}","title":"How To Startup when a Cluster size is reached"},{"location":"/typed/cluster.html#cluster-info-logging","text":"You can silence the logging of cluster events at info level with configuration property:\npekko.cluster.log-info = off\nYou can enable verbose logging of cluster events at info level, e.g. for temporary troubleshooting, with configuration property:\npekko.cluster.log-info-verbose = on","title":"Cluster Info Logging"},{"location":"/typed/cluster.html#cluster-dispatcher","text":"The Cluster extension is implemented with actors. To protect them against disturbance from user actors they are by default run on the internal dispatcher configured under pekko.actor.internal-dispatcher. The cluster actors can potentially be isolated even further, onto their own dispatcher using the setting pekko.cluster.use-dispatcher or made run on the same dispatcher to keep the number of threads down.","title":"Cluster Dispatcher"},{"location":"/typed/cluster.html#configuration-compatibility-check","text":"Creating a cluster is about deploying two or more nodes and having them behave as if they were a single application. Therefore it’s extremely important that all nodes in a cluster are configured with compatible settings.\nThe Configuration Compatibility Check feature ensures that all nodes in a cluster have a compatible configuration. Whenever a new node is joining an existing cluster, a subset of its configuration settings (only those that are required to be checked) is sent to the nodes in the cluster for verification. Once the configuration is checked on the cluster side, the cluster sends back its own set of required configuration settings. The joining node will then verify if it’s compliant with the cluster configuration. The joining node will only proceed if all checks pass, on both sides.\nNew custom checkers can be added by extending cluster.JoinConfigCompatCheckercluster.JoinConfigCompatChecker and including them in the configuration. Each checker must be associated with a unique key:\npekko.cluster.configuration-compatibility-check.checkers {\n  my-custom-config = \"com.company.MyCustomJoinConfigCompatChecker\"\n}\nNote Configuration Compatibility Check is enabled by default, but can be disabled by setting pekko.cluster.configuration-compatibility-check.enforce-on-join = off. This is specially useful when performing rolling updates. Obviously this should only be done if a complete cluster shutdown isn’t an option. A cluster with nodes with different configuration settings may lead to data loss or data corruption. This setting should only be disabled on the joining nodes. The checks are always performed on both sides, and warnings are logged. In case of incompatibilities, it is the responsibility of the joining node to decide if the process should be interrupted or not.","title":"Configuration Compatibility Check"},{"location":"/typed/cluster.html#higher-level-cluster-tools","text":"","title":"Higher level Cluster tools"},{"location":"/typed/cluster.html#cluster-singleton","text":"For some use cases it is convenient or necessary to ensure only one actor of a certain type is running somewhere in the cluster. This can be implemented by subscribing to member events, but there are several corner cases to consider. Therefore, this specific use case is covered by the Cluster Singleton.\nSee Cluster Singleton.","title":"Cluster Singleton"},{"location":"/typed/cluster.html#cluster-sharding","text":"Distributes actors across several nodes in the cluster and supports interaction with the actors using their logical identifier, but without having to care about their physical location in the cluster.\nSee Cluster Sharding.","title":"Cluster Sharding"},{"location":"/typed/cluster.html#distributed-data","text":"Distributed Data is useful when you need to share data between nodes in an Pekko Cluster. The data is accessed with an actor providing a key-value store like API.\nSee Distributed Data.","title":"Distributed Data"},{"location":"/typed/cluster.html#distributed-publish-subscribe","text":"Publish-subscribe messaging between actors in the cluster based on a topic, i.e. the sender does not have to know on which node the destination actor is running.\nSee Distributed Publish Subscribe.","title":"Distributed Publish Subscribe"},{"location":"/typed/cluster.html#cluster-aware-routers","text":"Distribute messages to actors on different nodes in the cluster with routing strategies like round-robin and consistent hashing.\nSee Group Routers.","title":"Cluster aware routers"},{"location":"/typed/cluster.html#cluster-across-multiple-data-centers","text":"Pekko Cluster can be used across multiple data centers, availability zones or regions, so that one Cluster can span multiple data centers and still be tolerant to network partitions.\nSee Cluster Multi-DC.","title":"Cluster across multiple data centers"},{"location":"/typed/cluster.html#reliable-delivery","text":"Reliable delivery and flow control of messages between actors in the Cluster.\nSee Reliable Delivery","title":"Reliable Delivery"},{"location":"/typed/cluster.html#example-project","text":"Cluster example project Cluster example project is an example project that can be downloaded, and with instructions of how to run.\nThis project contains samples illustrating different Cluster features, such as subscribing to cluster membership events, and sending messages to actors running on nodes in the cluster with Cluster aware routers.","title":"Example project"},{"location":"/typed/cluster-concepts.html","text":"","title":"Cluster Specification"},{"location":"/typed/cluster-concepts.html#cluster-specification","text":"This document describes the design concepts of Pekko Cluster. For the guide on using Pekko Cluster please see either\nCluster Usage Cluster Usage with classic Pekko APIs Cluster Membership Service","title":"Cluster Specification"},{"location":"/typed/cluster-concepts.html#introduction","text":"Pekko Cluster provides a fault-tolerant decentralized peer-to-peer based Cluster Membership Service with no single point of failure or single point of bottleneck. It does this using gossip protocols and an automatic failure detector.\nPekko Cluster allows for building distributed applications, where one application or service spans multiple nodes (in practice multiple ActorSystemActorSystems).","title":"Introduction"},{"location":"/typed/cluster-concepts.html#terms","text":"node A logical member of a cluster. There could be multiple nodes on a physical machine. Defined by a hostname:port:uid tuple. cluster A set of nodes joined together through the Cluster Membership Service. leader A single node in the cluster that acts as the leader. Managing cluster convergence and membership state transitions.","title":"Terms"},{"location":"/typed/cluster-concepts.html#gossip","text":"The cluster membership used in Pekko is based on Amazon’s Dynamo system and particularly the approach taken in Basho’s’ Riak distributed database. Cluster membership is communicated using a Gossip Protocol, where the current state of the cluster is gossiped randomly through the cluster, with preference to members that have not seen the latest version.","title":"Gossip"},{"location":"/typed/cluster-concepts.html#vector-clocks","text":"Vector clocks are a type of data structure and algorithm for generating a partial ordering of events in a distributed system and detecting causality violations.\nWe use vector clocks to reconcile and merge differences in cluster state during gossiping. A vector clock is a set of (node, counter) pairs. Each update to the cluster state has an accompanying update to the vector clock.","title":"Vector Clocks"},{"location":"/typed/cluster-concepts.html#gossip-convergence","text":"Information about the cluster converges locally at a node at certain points in time. This is when a node can prove that the cluster state it is observing has been observed by all other nodes in the cluster. Convergence is implemented by passing a set of nodes that have seen current state version during gossip. This information is referred to as the seen set in the gossip overview. When all nodes are included in the seen set there is convergence.\nGossip convergence cannot occur while any nodes are unreachable. The nodes need to become reachable again, or moved to the down and removed states (see the Cluster Membership Lifecycle section). This only blocks the leader from performing its cluster membership management and does not influence the application running on top of the cluster. For example this means that during a network partition it is not possible to add more nodes to the cluster. The nodes can join, but they will not be moved to the up state until the partition has healed or the unreachable nodes have been downed.","title":"Gossip Convergence"},{"location":"/typed/cluster-concepts.html#failure-detector","text":"The failure detector in Pekko Cluster is responsible for trying to detect if a node is unreachable from the rest of the cluster. For this we are using the Phi Accrual Failure Detector implementation. To be able to survive sudden abnormalities, such as garbage collection pauses and transient network failures the failure detector is easily configurable for tuning to your environments and needs.\nIn a cluster each node is monitored by a few (default maximum 5) other nodes. The nodes to monitor are selected from neighbors in a hashed ordered node ring. This is to increase the likelihood to monitor across racks and data centers, but the order is the same on all nodes, which ensures full coverage.\nWhen any node is detected to be unreachable this data is spread to the rest of the cluster through the gossip. In other words, only one node needs to mark a node unreachable to have the rest of the cluster mark that node unreachable.\nThe failure detector will also detect if the node becomes reachable again. When all nodes that monitored the unreachable node detect it as reachable again the cluster, after gossip dissemination, will consider it as reachable.\nIf system messages cannot be delivered to a node it will be quarantined and then it cannot come back from unreachable. This can happen if the there are too many unacknowledged system messages (e.g. watch, Terminated, remote actor deployment, failures of actors supervised by remote parent). Then the node needs to be moved to the down or removed states (see Cluster Membership Lifecycle) and the actor system of the quarantined node must be restarted before it can join the cluster again.\nSee the following for more details:\nPhi Accrual Failure Detector implementation Using the Failure Detector","title":"Failure Detector"},{"location":"/typed/cluster-concepts.html#leader","text":"After gossip convergence a leader for the cluster can be determined. There is no leader election process, the leader can always be recognised deterministically by any node whenever there is gossip convergence. The leader is only a role, any node can be the leader and it can change between convergence rounds. The leader is the first node in sorted order that is able to take the leadership role, where the preferred member states for a leader are up and leaving (see the Cluster Membership Lifecycle for more information about member states).\nThe role of the leader is to shift members in and out of the cluster, changing joining members to the up state or exiting members to the removed state. Currently leader actions are only triggered by receiving a new cluster state with gossip convergence.","title":"Leader"},{"location":"/typed/cluster-concepts.html#seed-nodes","text":"The seed nodes are contact points for new nodes joining the cluster. When a new node is started it sends a message to all seed nodes and then sends a join command to the seed node that answers first.\nThe seed nodes configuration value does not have any influence on the running cluster itself, it is only relevant for new nodes joining the cluster as it helps them to find contact points to send the join command to; a new member can send this command to any current member of the cluster, not only to the seed nodes.","title":"Seed Nodes"},{"location":"/typed/cluster-concepts.html#gossip-protocol","text":"A variation of push-pull gossip is used to reduce the amount of gossip information sent around the cluster. In push-pull gossip a digest is sent representing current versions but not actual values; the recipient of the gossip can then send back any values for which it has newer versions and also request values for which it has outdated versions. Pekko uses a single shared state with a vector clock for versioning, so the variant of push-pull gossip used in Pekko makes use of this version to only push the actual state as needed.\nPeriodically, the default is every 1 second, each node chooses another random node to initiate a round of gossip with. If less than ½ of the nodes resides in the seen set (have seen the new state) then the cluster gossips 3 times instead of once every second. This adjusted gossip interval is a way to speed up the convergence process in the early dissemination phase after a state change.\nThe choice of node to gossip with is random but biased towards nodes that might not have seen the current state version. During each round of gossip exchange, when convergence is not yet reached, a node uses a very high probability (which is configurable) to gossip with another node which is not part of the seen set, i.e. which is likely to have an older version of the state. Otherwise it gossips with any random live node.\nThis biased selection is a way to speed up the convergence process in the late dissemination phase after a state change.\nFor clusters larger than 400 nodes (configurable, and suggested by empirical evidence) the 0.8 probability is gradually reduced to avoid overwhelming single stragglers with too many concurrent gossip requests. The gossip receiver also has a mechanism to protect itself from too many simultaneous gossip messages by dropping messages that have been enqueued in the mailbox for too long of a time.\nWhile the cluster is in a converged state the gossiper only sends a small gossip status message containing the gossip version to the chosen node. As soon as there is a change to the cluster (meaning non-convergence) then it goes back to biased gossip again.\nThe recipient of the gossip state or the gossip status can use the gossip version (vector clock) to determine whether:\nit has a newer version of the gossip state, in which case it sends that back to the gossiper it has an outdated version of the state, in which case the recipient requests the current state from the gossiper by sending back its version of the gossip state it has conflicting gossip versions, in which case the different versions are merged and sent back\nIf the recipient and the gossip have the same version then the gossip state is not sent or requested.\nThe periodic nature of the gossip has a nice batching effect of state changes, e.g. joining several nodes quickly after each other to one node will result in only one state change to be spread to other members in the cluster.\nThe gossip messages are serialized with protobuf and also gzipped to reduce payload size.","title":"Gossip Protocol"},{"location":"/typed/cluster-membership.html","text":"","title":"Cluster Membership Service"},{"location":"/typed/cluster-membership.html#cluster-membership-service","text":"The core of Apache Pekko Cluster is the cluster membership, to keep track of what nodes are part of the cluster and their health. Cluster membership is communicated using gossip and failure detection.\nThere are several Higher level Cluster tools that are built on top of the cluster membership service.","title":"Cluster Membership Service"},{"location":"/typed/cluster-membership.html#introduction","text":"A cluster is made up of a set of member nodes. The identifier for each node is a hostname:port:uid tuple. A Pekko application can be distributed over a cluster with each node hosting some part of the application. Cluster membership and the actors running on that node of the application are decoupled. A node could be a member of a cluster without hosting any actors. Joining a cluster is initiated by issuing a Join command to one of the nodes in the cluster to join.\nThe node identifier internally also contains a UID that uniquely identifies this actor system instance at that hostname:port. Pekko uses the UID to be able to reliably trigger remote death watch. This means that the same actor system can never join a cluster again once it’s been removed from that cluster. To re-join an actor system with the same hostname:port to a cluster you have to stop the actor system and start a new one with the same hostname:port which will then receive a different UID.","title":"Introduction"},{"location":"/typed/cluster-membership.html#member-states","text":"The cluster membership state is a specialized CRDT, which means that it has a monotonic merge function. When concurrent changes occur on different nodes the updates can always be merged and converge to the same end result.\njoining - transient state when joining a cluster weakly up - transient state while network split (only if pekko.cluster.allow-weakly-up-members=on) up - normal operating state preparing for shutdown / ready for shutdown - an optional state that can be moved to before doing a full cluster shut down leaving / exiting - states during graceful removal down - marked as down (no longer part of cluster decisions) removed - tombstone state (no longer a member)","title":"Member States"},{"location":"/typed/cluster-membership.html#member-events","text":"The events to track the life-cycle of members are:\nClusterEvent.MemberJoined - A new member has joined the cluster and its status has been changed to Joining ClusterEvent.MemberUp - A new member has joined the cluster and its status has been changed to Up ClusterEvent.MemberExited - A member is leaving the cluster and its status has been changed to Exiting Note that the node might already have been shutdown when this event is published on another node. ClusterEvent.MemberRemoved - Member completely removed from the cluster. ClusterEvent.UnreachableMember - A member is considered as unreachable, detected by the failure detector of at least one other node. ClusterEvent.ReachableMember - A member is considered as reachable again, after having been unreachable. All nodes that previously detected it as unreachable has detected it as reachable again. ClusterEvent.MemberPreparingForShutdown - A member is preparing for a full cluster shutdown ClusterEvent.MemberReadyForShutdown - A member is ready for a full cluster shutdown","title":"Member Events"},{"location":"/typed/cluster-membership.html#membership-lifecycle","text":"A node is introduced to the cluster by invoking the join action which puts the node in the joining state. Once all nodes have seen that the new node is joining (through gossip convergence) the leader will set the member state to up.\nIf a node is leaving the cluster in a safe, expected manner, for example through coordinated shutdown, it invokes the leave action which switches it to the leaving state. Once the leader sees the convergence on the node in the leaving state, the leader will then move it to exiting. Once all nodes have seen the exiting state (convergence) the leader will remove the node from the cluster, marking it as removed.\nIf a node is unreachable then gossip convergence is not possible and therefore most leader actions are impossible (for instance, allowing a node to become a part of the cluster). To be able to move forward, the node must become reachable again or the node must be explicitly “downed”. This is required because the state of an unreachable node is unknown and the cluster cannot know if the node has crashed or is only temporarily unreachable because of network issues or GC pauses. See the section about User Actions below for ways a node can be downed.\nThe actor system on a node that exited or was downed cannot join the cluster again. In particular, a node that was downed while being unreachable and then regains connectivity cannot rejoin the cluster. Instead, the process has to be restarted on the node, creating a new actor system that can go through the joining process again.\nA special case is a node that was restarted without going through the leaving or downing process e.g. because the machine hosting the node was unexpectedly restarted. When the new instance of the node tries to rejoin the cluster, the cluster might still track the old instance as unreachable. In this case, however, it is clear that the old node is gone because the new instance will have the same address (host and port) as its old instance. In this case, the previous instance will be automatically marked as down and the new instance can rejoin the cluster without manual intervention.","title":"Membership Lifecycle"},{"location":"/typed/cluster-membership.html#leader","text":"The purpose of the leader is to confirm state changes when convergence is reached. The leader can be determined by each node unambiguously after gossip convergence. Any node might be required to take the role of the leader depending on the current cluster composition.\nWithout convergence, different nodes might have different views about which node is the leader. Therefore, most leader actions are only allowed if there is convergence to ensure that all nodes agree about the current state of the cluster and state changes are originated from a single node. Most regular state changes like changing a node from joining to up are of that kind.\nOther situations require that an action is taken even if convergence cannot be reached currently. Notably, convergence cannot be reached if one or more nodes in the cluster are currently unreachable as determined by the failure detector. In such a case, the cluster might be partitioned (a split brain scenario) and each partition might have its own view about which nodes are reachable and which are not. In this case, a node on each side of the partition might view itself as the leader of the reachable nodes. Any action that the leader performs in such a case must be designed in a way that all concurrent leaders would come to the same conclusion (which might be impossible in general and only feasible under additional constraints). The most important case of that kind is a split brain scenario where nodes need to be downed, either manually or automatically, to bring the cluster back to convergence.\nThe Split Brain Resolver is the built-in implementation of that.\nAnother transition that is possible without convergence is marking members as WeaklyUp as described in the next section.","title":"Leader"},{"location":"/typed/cluster-membership.html#weaklyup-members","text":"If a node is unreachable then gossip convergence is not possible and therefore most leader actions are impossible. By enabling pekko.cluster.allow-weakly-up-members (which is enabled by default), joining nodes can be promoted to WeaklyUp even while convergence is not yet reached. Once gossip convergence can be established again, the leader will move WeaklyUp members to Up.\nYou can subscribe to the WeaklyUp membership event to make use of the members that are in this state, but you should be aware of that members on the other side of a network partition have no knowledge about the existence of the new members. You should for example not count WeaklyUp members in quorum decisions.","title":"WeaklyUp Members"},{"location":"/typed/cluster-membership.html#full-cluster-shutdown","text":"In some rare cases it may be desirable to do a full cluster shutdown rather than a rolling deploy. For example, a protocol change where it is simpler to restart the cluster than to make the protocol change backward compatible.\nAs of Pekko 2.6.13 it can be signalled that a full cluster shutdown is about to happen and any expensive actions such as:\nCluster sharding rebalances Moving of Cluster singletons\nWon’t happen. That way the shutdown will be as quick as possible and a new version can be started up without delay.\nIf a cluster isn’t to be restarted right away then there is no need to prepare it for shutdown.\nTo use this feature use Cluster(system).prepareForFullClusterShutdown() in classic or PrepareForFullClusterShutdownPrepareForFullClusterShutdown in typed.\nWait for all Up members to become ReadyForShutdown and then all nodes can be shutdown and restarted. Members that aren’t Up yet will remain in the Joining or WeaklyUp states. Any node that is already leaving the cluster i.e. in the Leaving or Exiting states will continue to leave the cluster via the normal path.","title":"Full cluster shutdown"},{"location":"/typed/cluster-membership.html#state-diagrams","text":"","title":"State Diagrams"},{"location":"/typed/cluster-membership.html#state-diagram-for-the-member-states","text":"","title":"State Diagram for the Member States"},{"location":"/typed/cluster-membership.html#user-actions","text":"join - join a single node to a cluster - can be explicit or automatic on startup if a node to join have been specified in the configuration leave - tell a node to leave the cluster gracefully, normally triggered by ActorSystem or JVM shutdown through coordinated shutdown down - mark a node as down. This action is required to remove crashed nodes (that did not ‘leave’) from the cluster. It can be triggered manually, through Cluster HTTP Management, or automatically by a downing provider like Split Brain Resolver","title":"User Actions"},{"location":"/typed/cluster-membership.html#leader-actions","text":"The leader has the duty of confirming user actions to shift members in and out of the cluster:\njoining ⭢ up joining ⭢ weakly up (no convergence is needed for this leader action to be performed which works even if there are unreachable nodes) weakly up ⭢ up (after full convergence is reached again) leaving ⭢ exiting exiting ⭢ removed down ⭢ removed","title":"Leader Actions"},{"location":"/typed/cluster-membership.html#failure-detection-and-unreachability","text":"Being unreachable is not a separate member state but rather a flag in addition to the state. A failure detector on each node that monitors another node can mark the monitored node as unreachable independent of its state. Afterwards the failure detector continues monitoring the node until it detects it as reachable again and removes the flag. A node is considered reachable again only after all monitoring nodes see it as reachable again.","title":"Failure Detection and Unreachability"},{"location":"/typed/failure-detector.html","text":"","title":"Phi Accrual Failure Detector"},{"location":"/typed/failure-detector.html#phi-accrual-failure-detector","text":"","title":"Phi Accrual Failure Detector"},{"location":"/typed/failure-detector.html#introduction","text":"Remote DeathWatch uses heartbeat messages and the failure detector to detect network failures and JVM crashes.\nThe heartbeat arrival times are interpreted by an implementation of The Phi Accrual Failure Detector by Hayashibara et al.","title":"Introduction"},{"location":"/typed/failure-detector.html#failure-detector-heartbeats","text":"Heartbeats are sent every second by default, which is configurable. They are performed in a request/reply handshake, and the replies are input to the failure detector.\nThe suspicion level of failure is represented by a value called phi. The basic idea of the phi failure detector is to express the value of phi on a scale that is dynamically adjusted to reflect current network conditions.\nThe value of phi is calculated as:\nphi = -log10(1 - F(timeSinceLastHeartbeat))\nwhere F is the cumulative distribution function of a normal distribution with mean and standard deviation estimated from historical heartbeat inter-arrival times.\nAn accrual failure detector decouples monitoring and interpretation. That makes them applicable to a wider area of scenarios and more adequate to build generic failure detection services. The idea is that it is keeping a history of failure statistics, calculated from heartbeats received from other nodes, and is trying to do educated guesses by taking multiple factors, and how they accumulate over time, into account in order to come up with a better guess if a specific node is up or down. Rather than only answering “yes” or “no” to the question “is the node down?” it returns a phi value representing the likelihood that the node is down.\nThe following chart illustrates how phi increase with increasing time since the previous heartbeat.\nPhi is calculated from the mean and standard deviation of historical inter arrival times. The previous chart is an example for standard deviation of 200 ms. If the heartbeats arrive with less deviation the curve becomes steeper, i.e. it is possible to determine failure more quickly. The curve looks like this for a standard deviation of 100 ms.\nTo be able to survive sudden abnormalities, such as garbage collection pauses and transient network failures the failure detector is configured with a margin, which you may want to adjust depending on you environment. This is how the curve looks like for failure-detector.acceptable-heartbeat-pause configured to 3 seconds.","title":"Failure Detector Heartbeats"},{"location":"/typed/failure-detector.html#logging","text":"When the Cluster failure detector observes another node as unreachable it will log:\nMarking node(s) as UNREACHABLE\nand if it becomes reachable again:\nMarking node(s) as REACHABLE\nThere is also a warning when the heartbeat arrival interval exceeds 2/3 of the acceptable-heartbeat-pause\nheartbeat interval is growing too large\nIf you see false positives, as indicated by frequent UNREACHABLE followed by REACHABLE logging, you can increase the acceptable-heartbeat-pause if you suspect that your environment is more unstable than what is tolerated by the default value. However, it can be good to investigate the reason so that it is not caused by long (unexpected) garbage collection pauses, overloading the system, too restrictive CPU quotas settings, and similar.\npekko.cluster.failure-detector.acceptable-heartbeat-pause = 7s\nAnother log message to watch out for that typically requires investigation of the root cause:\nScheduled sending of heartbeat was delayed","title":"Logging"},{"location":"/typed/failure-detector.html#failure-detector-threshold","text":"The threshold that is the basis for the calculation is configurable by the user, but typically it’s enough to configure the acceptable-heartbeat-pause as described above.\nA low threshold is prone to generate many false positives but ensures a quick detection in the event of a real crash. Conversely, a high threshold generates fewer mistakes but needs more time to detect actual crashes. The default threshold is 8 and is appropriate for most situations. However in cloud environments, such as Amazon EC2, the value could be increased to 12 in order to account for network issues that sometimes occur on such platforms.","title":"Failure Detector Threshold"},{"location":"/typed/distributed-data.html","text":"","title":"Distributed Data"},{"location":"/typed/distributed-data.html#distributed-data","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Distributed Data.","title":"Distributed Data"},{"location":"/typed/distributed-data.html#module-info","text":"To use Pekko Cluster Distributed Data, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-typed_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster (typed) Artifact org.apache.pekko pekko-cluster-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/distributed-data.html#introduction","text":"Pekko Distributed Data is useful when you need to share data between nodes in an Pekko Cluster. The data is accessed with an actor providing a key-value store like API. The keys are unique identifiers with type information of the data values. The values are Conflict Free Replicated Data Types (CRDTs).\nAll data entries are spread to all nodes, or nodes with a certain role, in the cluster via direct replication and gossip based dissemination. You have fine grained control of the consistency level for reads and writes.\nThe nature of CRDTs makes it possible to perform updates from any node without coordination. Concurrent updates from different nodes will automatically be resolved by the monotonic merge function, which all data types must provide. The state changes always converge. Several useful data types for counters, sets, maps and registers are provided and you can also implement your own custom data types.\nIt is eventually consistent and geared toward providing high read and write availability (partition tolerance), with low latency. Note that in an eventually consistent system a read may return an out-of-date value.","title":"Introduction"},{"location":"/typed/distributed-data.html#using-the-replicator","text":"You can interact with the data through the replicator actor which can be accessed through the DistributedDataDistributedData extension.\nThe messages for the replicator, such as Replicator.UpdateReplicator.Update are defined as subclasses of Replicator.CommandReplicator.Command and the actual CRDTs are defined in the pekko.cluster.ddata package, for example GCounterGCounter. It requires a implicit org.apache.pekko.cluster.ddata.SelfUniqueAddress, available from:\nScala copysourceimplicit val node: SelfUniqueAddress = DistributedData(context.system).selfUniqueAddress Java copysourcefinal SelfUniqueAddress node = DistributedData.get(context.getSystem()).selfUniqueAddress();\nThe replicator can contain multiple entries each containing a replicated data type, we therefore need to create a key identifying the entry and helping us know what type it has, and then use that key for every interaction with the replicator. Each replicated data type contains a factory for defining such a key.\nCluster members with status WeaklyUp, will participate in Distributed Data. This means that the data will be replicated to the WeaklyUp nodes with the background gossip protocol. Note that it will not participate in any actions where the consistency mode is to read/write from all nodes or the majority of nodes. The WeaklyUp node is not counted as part of the cluster. So 3 nodes + 5 WeaklyUp is essentially a 3 node cluster as far as consistent actions are concerned.\nThis sample uses the replicated data type GCounter to implement a counter that can be written to on any node of the cluster:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.cluster.ddata.GCounter\nimport pekko.cluster.ddata.GCounterKey\nimport pekko.cluster.ddata.typed.scaladsl.Replicator._\n\nobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Int]) extends Command\n  final case class GetCachedValue(replyTo: ActorRef[Int]) extends Command\n  case object Unsubscribe extends Command\n  private sealed trait InternalCommand extends Command\n  private case class InternalUpdateResponse(rsp: Replicator.UpdateResponse[GCounter]) extends InternalCommand\n  private case class InternalGetResponse(rsp: Replicator.GetResponse[GCounter], replyTo: ActorRef[Int])\n      extends InternalCommand\n  private case class InternalSubscribeResponse(chg: Replicator.SubscribeResponse[GCounter]) extends InternalCommand\n\n  def apply(key: GCounterKey): Behavior[Command] =\n    Behaviors.setup[Command] { context =>\n      implicit val node: SelfUniqueAddress = DistributedData(context.system).selfUniqueAddress\n\n      // adapter that turns the response messages from the replicator into our own protocol\n      DistributedData.withReplicatorMessageAdapter[Command, GCounter] { replicatorAdapter =>\n        // Subscribe to changes of the given `key`.\n        replicatorAdapter.subscribe(key, InternalSubscribeResponse.apply)\n\n        def updated(cachedValue: Int): Behavior[Command] = {\n          Behaviors.receiveMessage[Command] {\n            case Increment =>\n              replicatorAdapter.askUpdate(\n                askReplyTo => Replicator.Update(key, GCounter.empty, Replicator.WriteLocal, askReplyTo)(_ :+ 1),\n                InternalUpdateResponse.apply)\n\n              Behaviors.same\n\n            case GetValue(replyTo) =>\n              replicatorAdapter.askGet(\n                askReplyTo => Replicator.Get(key, Replicator.ReadLocal, askReplyTo),\n                value => InternalGetResponse(value, replyTo))\n\n              Behaviors.same\n\n            case GetCachedValue(replyTo) =>\n              replyTo ! cachedValue\n              Behaviors.same\n\n            case Unsubscribe =>\n              replicatorAdapter.unsubscribe(key)\n              Behaviors.same\n\n            case internal: InternalCommand =>\n              internal match {\n                case InternalUpdateResponse(_) => Behaviors.same // ok\n\n                case InternalGetResponse(rsp @ Replicator.GetSuccess(`key`), replyTo) =>\n                  val value = rsp.get(key).value.toInt\n                  replyTo ! value\n                  Behaviors.same\n\n                case InternalGetResponse(_, _) =>\n                  Behaviors.unhandled // not dealing with failures\n                case InternalSubscribeResponse(chg @ Replicator.Changed(`key`)) =>\n                  val value = chg.get(key).value.intValue\n                  updated(value)\n\n                case InternalSubscribeResponse(Replicator.Deleted(_)) =>\n                  Behaviors.unhandled // no deletes\n\n                case InternalSubscribeResponse(_) => // changed but wrong key\n                  Behaviors.unhandled\n\n              }\n          }\n        }\n\n        updated(cachedValue = 0)\n      }\n    }\n} Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\nimport org.apache.pekko.cluster.ddata.GCounter;\nimport org.apache.pekko.cluster.ddata.Key;\nimport org.apache.pekko.cluster.ddata.SelfUniqueAddress;\nimport org.apache.pekko.cluster.ddata.typed.javadsl.DistributedData;\nimport org.apache.pekko.cluster.ddata.typed.javadsl.Replicator;\nimport org.apache.pekko.cluster.ddata.typed.javadsl.ReplicatorMessageAdapter;\n\n  public class Counter extends AbstractBehavior<Counter.Command> {\n    interface Command {}\n\n    enum Increment implements Command {\n      INSTANCE\n    }\n\n    public static class GetValue implements Command {\n      public final ActorRef<Integer> replyTo;\n\n      public GetValue(ActorRef<Integer> replyTo) {\n        this.replyTo = replyTo;\n      }\n    }\n\n    public static class GetCachedValue implements Command {\n      public final ActorRef<Integer> replyTo;\n\n      public GetCachedValue(ActorRef<Integer> replyTo) {\n        this.replyTo = replyTo;\n      }\n    }\n\n    enum Unsubscribe implements Command {\n      INSTANCE\n    }\n\n    private interface InternalCommand extends Command {}\n\n    private static class InternalUpdateResponse implements InternalCommand {\n      final Replicator.UpdateResponse<GCounter> rsp;\n\n      InternalUpdateResponse(Replicator.UpdateResponse<GCounter> rsp) {\n        this.rsp = rsp;\n      }\n    }\n\n    private static class InternalGetResponse implements InternalCommand {\n      final Replicator.GetResponse<GCounter> rsp;\n      final ActorRef<Integer> replyTo;\n\n      InternalGetResponse(Replicator.GetResponse<GCounter> rsp, ActorRef<Integer> replyTo) {\n        this.rsp = rsp;\n        this.replyTo = replyTo;\n      }\n    }\n\n    private static final class InternalSubscribeResponse implements InternalCommand {\n      final Replicator.SubscribeResponse<GCounter> rsp;\n\n      InternalSubscribeResponse(Replicator.SubscribeResponse<GCounter> rsp) {\n        this.rsp = rsp;\n      }\n    }\n\n    public static Behavior<Command> create(Key<GCounter> key) {\n      return Behaviors.setup(\n          ctx ->\n              DistributedData.withReplicatorMessageAdapter(\n                  (ReplicatorMessageAdapter<Command, GCounter> replicatorAdapter) ->\n                      new Counter(ctx, replicatorAdapter, key)));\n    }\n\n    // adapter that turns the response messages from the replicator into our own protocol\n    private final ReplicatorMessageAdapter<Command, GCounter> replicatorAdapter;\n    private final SelfUniqueAddress node;\n    private final Key<GCounter> key;\n\n    private int cachedValue = 0;\n\n    private Counter(\n        ActorContext<Command> context,\n        ReplicatorMessageAdapter<Command, GCounter> replicatorAdapter,\n        Key<GCounter> key) {\n      super(context);\n\n      this.replicatorAdapter = replicatorAdapter;\n      this.key = key;\n\n      final SelfUniqueAddress node = DistributedData.get(context.getSystem()).selfUniqueAddress();\n\n      this.node = DistributedData.get(context.getSystem()).selfUniqueAddress();\n\n      this.replicatorAdapter.subscribe(this.key, InternalSubscribeResponse::new);\n    }\n\n    @Override\n    public Receive<Command> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(Increment.class, this::onIncrement)\n          .onMessage(InternalUpdateResponse.class, msg -> Behaviors.same())\n          .onMessage(GetValue.class, this::onGetValue)\n          .onMessage(GetCachedValue.class, this::onGetCachedValue)\n          .onMessage(Unsubscribe.class, this::onUnsubscribe)\n          .onMessage(InternalGetResponse.class, this::onInternalGetResponse)\n          .onMessage(InternalSubscribeResponse.class, this::onInternalSubscribeResponse)\n          .build();\n    }\n\n    private Behavior<Command> onIncrement(Increment cmd) {\n      replicatorAdapter.askUpdate(\n          askReplyTo ->\n              new Replicator.Update<>(\n                  key,\n                  GCounter.empty(),\n                  Replicator.writeLocal(),\n                  askReplyTo,\n                  curr -> curr.increment(node, 1)),\n          InternalUpdateResponse::new);\n\n      return this;\n    }\n\n    private Behavior<Command> onGetValue(GetValue cmd) {\n      replicatorAdapter.askGet(\n          askReplyTo -> new Replicator.Get<>(key, Replicator.readLocal(), askReplyTo),\n          rsp -> new InternalGetResponse(rsp, cmd.replyTo));\n\n      return this;\n    }\n\n    private Behavior<Command> onGetCachedValue(GetCachedValue cmd) {\n      cmd.replyTo.tell(cachedValue);\n      return this;\n    }\n\n    private Behavior<Command> onUnsubscribe(Unsubscribe cmd) {\n      replicatorAdapter.unsubscribe(key);\n      return this;\n    }\n\n    private Behavior<Command> onInternalGetResponse(InternalGetResponse msg) {\n      if (msg.rsp instanceof Replicator.GetSuccess) {\n        int value = ((Replicator.GetSuccess<?>) msg.rsp).get(key).getValue().intValue();\n        msg.replyTo.tell(value);\n        return this;\n      } else {\n        // not dealing with failures\n        return Behaviors.unhandled();\n      }\n    }\n\n    private Behavior<Command> onInternalSubscribeResponse(InternalSubscribeResponse msg) {\n      if (msg.rsp instanceof Replicator.Changed) {\n        GCounter counter = ((Replicator.Changed<?>) msg.rsp).get(key);\n        cachedValue = counter.getValue().intValue();\n        return this;\n      } else {\n        // no deletes\n        return Behaviors.unhandled();\n      }\n    }\n  }\n}\nAlthough you can interact with the Replicator using the ActorRef[Replicator.Command]ActorRef<Replicator.Command> from DistributedData(ctx.system).replicatorDistributedData(ctx.getSystem()).replicator() it’s often more convenient to use the ReplicatorMessageAdapter as in the above example.","title":"Using the Replicator"},{"location":"/typed/distributed-data.html#update","text":"To modify and replicate a data value you send a Replicator.Update message to the local Replicator.\nIn the above example, for an incoming Increment command, we send the replicator a Replicator.Update request, it contains five values:\nthe KeyKEY we want to update the data to use as the empty state if the replicator has not seen the key before the write consistency level we want for the update an ActorRef[Replicator.UpdateResponse[GCounter]]ActorRef<Replicator.UpdateResponse<GCounter>> to respond to when the update is completed a modify function that takes a previous state and updates it, in our case by incrementing it with 1\nThere is alternative way of constructing the function for the Update message: Scala copysource// alternative way to define the `createRequest` function\n// Replicator.Update instance has a curried `apply` method\nreplicatorAdapter.askUpdate(\n  Replicator.Update(key, GCounter.empty, Replicator.WriteLocal)(_ :+ 1),\n  InternalUpdateResponse.apply)\n\n// that is the same as\nreplicatorAdapter.askUpdate(\n  askReplyTo => Replicator.Update(key, GCounter.empty, Replicator.WriteLocal, askReplyTo)(_ :+ 1),\n  InternalUpdateResponse.apply)\nThe current data value for the key of the Update is passed as parameter to the modify function of the Update. The function is supposed to return the new value of the data, which will then be replicated according to the given write consistency level.\nThe modify function is called by the Replicator actor and must therefore be a pure function that only uses the data parameter and stable fields from enclosing scope. It must for example not access the ActorContext or mutable state of an enclosing actor. Update is intended to only be sent from an actor running in same local ActorSystem as the Replicator, because the modify function is typically not serializable.\nYou will always see your own writes. For example if you send two Update messages changing the value of the same key, the modify function of the second message will see the change that was performed by the first Update message.\nAs reply of the Update a Replicator.UpdateSuccess is sent to the replyTo of the Update if the value was successfully replicated according to the supplied consistency level within the supplied timeout. Otherwise a Replicator.UpdateFailure subclass is sent back. Note that a Replicator.UpdateTimeout reply does not mean that the update completely failed or was rolled back. It may still have been replicated to some nodes, and will eventually be replicated to all nodes with the gossip protocol.\nIt is possible to abort the Update when inspecting the state parameter that is passed in to the modify function by throwing an exception. That happens before the update is performed and a Replicator.ModifyFailure is sent back as reply.","title":"Update"},{"location":"/typed/distributed-data.html#get","text":"To retrieve the current value of a data you send Replicator.Get message to the Replicator.\nThe example has the GetValue command, which is asking the replicator for current value. Note how the replyTo from the incoming message can be used when the GetSuccess response from the replicator is received.\nAlternative way of constructing the function for the Get and Delete: Scala copysource// alternative way to define the `createRequest` function\n// Replicator.Get instance has a curried `apply` method\nreplicatorAdapter.askGet(Replicator.Get(key, Replicator.ReadLocal), value => InternalGetResponse(value, replyTo))\n\n// that is the same as\nreplicatorAdapter.askGet(\n  askReplyTo => Replicator.Get(key, Replicator.ReadLocal, askReplyTo),\n  value => InternalGetResponse(value, replyTo))\nFor a Get you supply a read consistency level.\nYou will always read your own writes. For example if you send a Update message followed by a Get of the same key the Get will retrieve the change that was performed by the preceding Update message. However, the order of the reply messages are not defined, i.e. in the previous example you may receive the GetSuccess before the UpdateSuccess.\nAs reply of the Get a Replicator.GetSuccess is sent to the replyTo of the Get if the value was successfully retrieved according to the supplied consistency level within the supplied timeout. Otherwise a Replicator.GetFailure is sent. If the key does not exist the reply will be Replicator.NotFound.","title":"Get"},{"location":"/typed/distributed-data.html#subscribe","text":"Whenever the distributed counter in the example is updated, we cache the value so that we can answer requests about the value without the extra interaction with the replicator using the GetCachedValue command.\nWhen we start up the actor we subscribe it to changes for our key, meaning whenever the replicator observes a change for the counter our actor will receive a Replicator.Changed[GCounter]Replicator.Changed<GCounter>. Since this is not a message in our protocol, we use a message transformation function to wrap it in the internal InternalSubscribeResponse message, which is then handled in the regular message handling of the behavior, as shown in the above example. Subscribers will be notified of changes, if there are any, based on the configurable pekko.cluster.distributed-data.notify-subscribers-interval.\nThe subscriber is automatically unsubscribed if the subscriber is terminated. A subscriber can also be de-registered with the replicatorAdapter.unsubscribe(key) function.","title":"Subscribe"},{"location":"/typed/distributed-data.html#delete","text":"A data entry can be deleted by sending a Replicator.Delete message to the local Replicator. As reply of the Delete a Replicator.DeleteSuccess is sent to the replyTo of the Delete if the value was successfully deleted according to the supplied consistency level within the supplied timeout. Otherwise a Replicator.ReplicationDeleteFailure is sent. Note that ReplicationDeleteFailure does not mean that the delete completely failed or was rolled back. It may still have been replicated to some nodes, and may eventually be replicated to all nodes.\nA deleted key cannot be reused again, but it is still recommended to delete unused data entries because that reduces the replication overhead when new nodes join the cluster. Subsequent Delete, Update and Get requests will be replied with Replicator.DataDeleted. Subscribers will receive Replicator.Deleted.\nWarning As deleted keys continue to be included in the stored data on each node as well as in gossip messages, a continuous series of updates and deletes of top-level entities will result in growing memory usage until an ActorSystem runs out of memory. To use Pekko Distributed Data where frequent adds and removes are required, you should use a fixed number of top-level data types that support both updates and removals, for example ORMap or ORSet.","title":"Delete"},{"location":"/typed/distributed-data.html#consistency","text":"The consistency level that is supplied in the Update and Get specifies per request how many replicas that must respond successfully to a write and read request.\nWriteAll and ReadAll is the strongest consistency level, but also the slowest and with lowest availability. For example, it is enough that one node is unavailable for a Get request and you will not receive the value.\nFor low latency reads you use ReadLocalreadLocal with the risk of retrieving stale data, i.e. updates from other nodes might not be visible yet.","title":"Consistency"},{"location":"/typed/distributed-data.html#write-consistency","text":"When using WriteLocalwriteLocal the Update is only written to the local replica and then disseminated in the background with the gossip protocol, which can take few seconds to spread to all nodes.\nFor an update you supply a write consistency level which has the following meaning:\nWriteLocalwriteLocal the value will immediately only be written to the local replica, and later disseminated with gossip WriteTo(n) the value will immediately be written to at least n replicas, including the local replica WriteMajority the value will immediately be written to a majority of replicas, i.e. at least N/2 + 1 replicas, where N is the number of nodes in the cluster (or cluster role group) WriteMajorityPlus is like WriteMajority but with the given number of additional nodes added to the majority count. At most all nodes. This gives better tolerance for membership changes between writes and reads. Exiting nodes are excluded using WriteMajorityPlus because those are typically about to be removed and will not be able to respond. WriteAll the value will immediately be written to all nodes in the cluster (or all nodes in the cluster role group). Exiting nodes are excluded using WriteAll because those are typically about to be removed and will not be able to respond.\nWhen you specify to write to n out of x nodes, the update will first replicate to n nodes. If there are not enough Acks after a 1/5th of the timeout, the update will be replicated to n other nodes. If there are less than n nodes left all of the remaining nodes are used. Reachable nodes are preferred over unreachable nodes.\nNote that WriteMajority and WriteMajorityPlus have a minCap parameter that is useful to specify to achieve better safety for small clusters.","title":"Write consistency"},{"location":"/typed/distributed-data.html#read-consistency","text":"If consistency is a priority, you can ensure that a read always reflects the most recent write by using the following formula:\n(nodes_written + nodes_read) > N\nwhere N is the total number of nodes in the cluster, or the number of nodes with the role that is used for the Replicator.\nYou supply a consistency level which has the following meaning:\nReadLocalreadLocal the value will only be read from the local replica ReadFrom(n) the value will be read and merged from n replicas, including the local replica ReadMajority the value will be read and merged from a majority of replicas, i.e. at least N/2 + 1 replicas, where N is the number of nodes in the cluster (or cluster role group) ReadMajorityPlus is like ReadMajority but with the given number of additional nodes added to the majority count. At most all nodes. This gives better tolerance for membership changes between writes and reads. Exiting nodes are excluded using ReadMajorityPlus because those are typically about to be removed and will not be able to respond. ReadAll the value will be read and merged from all nodes in the cluster (or all nodes in the cluster role group). Exiting nodes are excluded using ReadAll because those are typically about to be removed and will not be able to respond.\nNote that ReadMajority and ReadMajorityPlus have a minCap parameter that is useful to specify to achieve better safety for small clusters.","title":"Read consistency"},{"location":"/typed/distributed-data.html#consistency-and-response-types","text":"When using ReadLocal, you will never receive a GetFailure response, since the local replica is always available to local readers. WriteLocal however may still reply with UpdateFailure messages if the modify function throws an exception, or if it fails to persist to durable storage.","title":"Consistency and response types"},{"location":"/typed/distributed-data.html#examples","text":"In a 7 node cluster these consistency properties are achieved by writing to 4 nodes and reading from 4 nodes, or writing to 5 nodes and reading from 3 nodes.\nBy combining WriteMajority and ReadMajority levels a read always reflects the most recent write. The Replicator writes and reads to a majority of replicas, i.e. N / 2 + 1. For example, in a 5 node cluster it writes to 3 nodes and reads from 3 nodes. In a 6 node cluster it writes to 4 nodes and reads from 4 nodes.\nYou can define a minimum number of nodes for WriteMajority and ReadMajority, this will minimize the risk of reading stale data. Minimum cap is provided by minCap property of WriteMajority and ReadMajority and defines the required majority. If the minCap is higher then N / 2 + 1 the minCap will be used.\nFor example if the minCap is 5 the WriteMajority and ReadMajority for cluster of 3 nodes will be 3, for cluster of 6 nodes will be 5 and for cluster of 12 nodes will be 7 ( N / 2 + 1 ).\nFor small clusters (<7) the risk of membership changes between a WriteMajority and ReadMajority is rather high and then the nice properties of combining majority write and reads are not guaranteed. Therefore the ReadMajority and WriteMajority have a minCap parameter that is useful to specify to achieve better safety for small clusters. It means that if the cluster size is smaller than the majority size it will use the minCap number of nodes but at most the total size of the cluster.\nIn some rare cases, when performing an Update it is needed to first try to fetch latest data from other nodes. That can be done by first sending a Get with ReadMajority and then continue with the Update when the GetSuccess, GetFailure or NotFound reply is received. This might be needed when you need to base a decision on latest information or when removing entries from an ORSet or ORMap. If an entry is added to an ORSet or ORMap from one node and removed from another node the entry will only be removed if the added entry is visible on the node where the removal is performed (hence the name observed-removed set).\nWarning Caveat: Even if you use WriteMajority and ReadMajority there is small risk that you may read stale data if the cluster membership has changed between the Update and the Get. For example, in cluster of 5 nodes when you Update and that change is written to 3 nodes: n1, n2, n3. Then 2 more nodes are added and a Get request is reading from 4 nodes, which happens to be n4, n5, n6, n7, i.e. the value on n1, n2, n3 is not seen in the response of the Get request. For additional tolerance of membership changes between writes and reads you can use WriteMajorityPlus and ReadMajorityPlus.","title":"Examples"},{"location":"/typed/distributed-data.html#running-separate-instances-of-the-replicator","text":"For some use cases, for example when limiting the replicator to certain roles, or using different subsets on different roles, it makes sense to start separate replicators, this needs to be done on all nodes, or the group of nodes tagged with a specific role. To do this with Distributed Data you will first have to start a classic Replicator and pass it to the Replicator.behavior method that takes a classic actor ref. All such Replicators must run on the same path in the classic actor hierarchy.\nA standalone ReplicatorMessageAdapter can also be created for a given Replicator instead of creating one via the DistributedData extension.","title":"Running separate instances of the replicator"},{"location":"/typed/distributed-data.html#replicated-data-types","text":"Pekko contains a set of useful replicated data types and it is fully possible to implement custom replicated data types.\nThe data types must be convergent (stateful) CRDTs and implement the ReplicatedData traitAbstractReplicatedData interface, i.e. they provide a monotonic merge function and the state changes always converge.\nYou can use your own custom ReplicatedData or DeltaReplicatedDataAbstractReplicatedData or AbstractDeltaReplicatedData types, and several types are provided by this package, such as:\nCounters: GCounter, PNCounter Sets: GSet, ORSet Maps: ORMap, ORMultiMap, LWWMap, PNCounterMap Registers: LWWRegister, Flag","title":"Replicated data types"},{"location":"/typed/distributed-data.html#counters","text":"GCounter is a “grow only counter”. It only supports increments, no decrements.\nIt works in a similar way as a vector clock. It keeps track of one counter per node and the total value is the sum of these counters. The merge is implemented by taking the maximum count for each node.\nIf you need both increments and decrements you can use the PNCounter (positive/negative counter).\nIt is tracking the increments (P) separate from the decrements (N). Both P and N are represented as two internal GCounters. Merge is handled by merging the internal P and N counters. The value of the counter is the value of the P counter minus the value of the N counter.\nScala copysourceimplicit val node = DistributedData(system).selfUniqueAddress\n\nval c0 = PNCounter.empty\nval c1 = c0 :+ 1\nval c2 = c1 :+ 7\nval c3: PNCounter = c2.decrement(2)\nprintln(c3.value) // 6 Java copysourcefinal SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();\nfinal PNCounter c0 = PNCounter.create();\nfinal PNCounter c1 = c0.increment(node, 1);\nfinal PNCounter c2 = c1.increment(node, 7);\nfinal PNCounter c3 = c2.decrement(node, 2);\nSystem.out.println(c3.value()); // 6\nGCounter and PNCounter have support for delta-CRDT and don’t need causal delivery of deltas.\nSeveral related counters can be managed in a map with the PNCounterMap data type. When the counters are placed in a PNCounterMap as opposed to placing them as separate top level values they are guaranteed to be replicated together as one unit, which is sometimes necessary for related data.\nScala copysourceimplicit val node = DistributedData(system).selfUniqueAddress\nval m0 = PNCounterMap.empty[String]\nval m1 = m0.increment(node, \"a\", 7)\nval m2 = m1.decrement(node, \"a\", 2)\nval m3 = m2.increment(node, \"b\", 1)\nprintln(m3.get(\"a\")) // 5\nm3.entries.foreach { case (key, value) => println(s\"$key -> $value\") } Java copysourcefinal SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();\nfinal PNCounterMap<String> m0 = PNCounterMap.create();\nfinal PNCounterMap<String> m1 = m0.increment(node, \"a\", 7);\nfinal PNCounterMap<String> m2 = m1.decrement(node, \"a\", 2);\nfinal PNCounterMap<String> m3 = m2.increment(node, \"b\", 1);\nSystem.out.println(m3.get(\"a\")); // 5\nSystem.out.println(m3.getEntries());","title":"Counters"},{"location":"/typed/distributed-data.html#sets","text":"If you only need to add elements to a set and not remove elements the GSet (grow-only set) is the data type to use. The elements can be any type of values that can be serialized. Merge is the union of the two sets.\nScala copysourceval s0 = GSet.empty[String]\nval s1 = s0 + \"a\"\nval s2 = s1 + \"b\" + \"c\"\nif (s2.contains(\"a\"))\n  println(s2.elements) // a, b, c Java copysourcefinal GSet<String> s0 = GSet.create();\nfinal GSet<String> s1 = s0.add(\"a\");\nfinal GSet<String> s2 = s1.add(\"b\").add(\"c\");\nif (s2.contains(\"a\")) System.out.println(s2.getElements()); // a, b, c\nGSet has support for delta-CRDT and it doesn’t require causal delivery of deltas.\nIf you need add and remove operations you should use the ORSet (observed-remove set). Elements can be added and removed any number of times. If an element is concurrently added and removed, the add will win. You cannot remove an element that you have not seen.\nThe ORSet has a version vector that is incremented when an element is added to the set. The version for the node that added the element is also tracked for each element in a so called “birth dot”. The version vector and the dots are used by the merge function to track causality of the operations and resolve concurrent updates.\nScala copysourceimplicit val node = DistributedData(system).selfUniqueAddress\nval s0 = ORSet.empty[String]\nval s1 = s0 :+ \"a\"\nval s2 = s1 :+ \"b\"\nval s3 = s2.remove(\"a\")\nprintln(s3.elements) // b Java copysourcefinal SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();\nfinal ORSet<String> s0 = ORSet.create();\nfinal ORSet<String> s1 = s0.add(node, \"a\");\nfinal ORSet<String> s2 = s1.add(node, \"b\");\nfinal ORSet<String> s3 = s2.remove(node, \"a\");\nSystem.out.println(s3.getElements()); // b\nORSet has support for delta-CRDT and it requires causal delivery of deltas.","title":"Sets"},{"location":"/typed/distributed-data.html#maps","text":"ORMap (observed-remove map) is a map with keys of Any type and the values are ReplicatedData types themselves. It supports add, update and remove any number of times for a map entry.\nIf an entry is concurrently added and removed, the add will win. You cannot remove an entry that you have not seen. This is the same semantics as for the ORSet.\nIf an entry is concurrently updated to different values the values will be merged, hence the requirement that the values must be ReplicatedData types.\nWhile the ORMap supports removing and re-adding keys any number of times, the impact that this has on the values can be non-deterministic. A merge will always attempt to merge two values for the same key, regardless of whether that key has been removed and re-added in the meantime, an attempt to replace a value with a new one may not have the intended effect. This means that old values can effectively be resurrected if a node, that has seen both the remove and the update,gossips with a node that has seen neither. One consequence of this is that changing the value type of the CRDT, for example, from a GCounter to a GSet, could result in the merge function for the CRDT always failing. This could be an unrecoverable state for the node, hence, the types of ORMap values must never change for a given key.\nIt is rather inconvenient to use the ORMap directly since it does not expose specific types of the values. The ORMap is intended as a low level tool for building more specific maps, such as the following specialized maps.\nORMultiMap (observed-remove multi-map) is a multi-map implementation that wraps an ORMap with an ORSet for the map’s value.\nPNCounterMap (positive negative counter map) is a map of named counters (where the name can be of any type). It is a specialized ORMap with PNCounter values.\nLWWMap (last writer wins map) is a specialized ORMap with LWWRegister (last writer wins register) values.\nORMap, ORMultiMap, PNCounterMap and LWWMap have support for delta-CRDT and they require causal delivery of deltas. Support for deltas here means that the ORSet being underlying key type for all those maps uses delta propagation to deliver updates. Effectively, the update for map is then a pair, consisting of delta for the ORSet being the key and full update for the respective value (ORSet, PNCounter or LWWRegister) kept in the map.\nScala copysourceimplicit val node = DistributedData(system).selfUniqueAddress\nval m0 = ORMultiMap.empty[String, Int]\nval m1 = m0 :+ (\"a\" -> Set(1, 2, 3))\nval m2 = m1.addBinding(node, \"a\", 4)\nval m3 = m2.removeBinding(node, \"a\", 2)\nval m4 = m3.addBinding(node, \"b\", 1)\nprintln(m4.entries) Java copysourcefinal SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();\nfinal ORMultiMap<String, Integer> m0 = ORMultiMap.create();\nfinal ORMultiMap<String, Integer> m1 = m0.put(node, \"a\", new HashSet<>(Arrays.asList(1, 2, 3)));\nfinal ORMultiMap<String, Integer> m2 = m1.addBinding(node, \"a\", 4);\nfinal ORMultiMap<String, Integer> m3 = m2.removeBinding(node, \"a\", 2);\nfinal ORMultiMap<String, Integer> m4 = m3.addBinding(node, \"b\", 1);\nSystem.out.println(m4.getEntries());\nWhen a data entry is changed the full state of that entry is replicated to other nodes, i.e. when you update a map, the whole map is replicated. Therefore, instead of using one ORMap with 1000 elements it is more efficient to split that up in 10 top level ORMap entries with 100 elements each. Top level entries are replicated individually, which has the trade-off that different entries may not be replicated at the same time and you may see inconsistencies between related entries. Separate top level entries cannot be updated atomically together.\nThere is a special version of ORMultiMap, created by using separate constructor ORMultiMap.emptyWithValueDeltas[A, B], that also propagates the updates to its values (of ORSet type) as deltas. This means that the ORMultiMap initiated with ORMultiMap.emptyWithValueDeltas propagates its updates as pairs consisting of delta of the key and delta of the value. It is much more efficient in terms of network bandwidth consumed.\nHowever, this behavior has not been made default for ORMultiMap and if you wish to use it in your code, you need to replace invocations of ORMultiMap.empty[A, B] (or ORMultiMap()) with ORMultiMap.emptyWithValueDeltas[A, B] where A and B are types respectively of keys and values in the map.\nPlease also note, that despite having the same Scala type, ORMultiMap.emptyWithValueDeltas is not compatible with ‘vanilla’ ORMultiMap, because of different replication mechanism. One needs to be extra careful not to mix the two, as they have the same type, so compiler will not hint the error. Nonetheless ORMultiMap.emptyWithValueDeltas uses the same ORMultiMapKey type as the ‘vanilla’ ORMultiMap for referencing.\nNote that LWWRegister and therefore LWWMap relies on synchronized clocks and should only be used when the choice of value is not important for concurrent updates occurring within the clock skew. Read more in the below section about LWWRegister.","title":"Maps"},{"location":"/typed/distributed-data.html#flags-and-registers","text":"Flag is a data type for a boolean value that is initialized to false and can be switched to true. Thereafter it cannot be changed. true wins over false in merge.\nScala copysourceval f0 = Flag.Disabled\nval f1 = f0.switchOn\nprintln(f1.enabled) Java copysourcefinal Flag f0 = Flag.create();\nfinal Flag f1 = f0.switchOn();\nSystem.out.println(f1.enabled());\nLWWRegister (last writer wins register) can hold any (serializable) value.\nMerge of a LWWRegister takes the register with highest timestamp. Note that this relies on synchronized clocks. LWWRegister should only be used when the choice of value is not important for concurrent updates occurring within the clock skew.\nMerge takes the register updated by the node with lowest address (UniqueAddress is ordered) if the timestamps are exactly the same.\nScala copysourceimplicit val node = DistributedData(system).selfUniqueAddress\nval r1 = LWWRegister.create(\"Hello\")\nval r2 = r1.withValueOf(\"Hi\")\nprintln(s\"${r1.value} by ${r1.updatedBy} at ${r1.timestamp}\") Java copysourcefinal SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();\nfinal LWWRegister<String> r1 = LWWRegister.create(node, \"Hello\");\nfinal LWWRegister<String> r2 = r1.withValue(node, \"Hi\");\nSystem.out.println(r1.value() + \" by \" + r1.updatedBy() + \" at \" + r1.timestamp());\nInstead of using timestamps based on System.currentTimeMillis() time it is possible to use a timestamp value based on something else, for example an increasing version number from a database record that is used for optimistic concurrency control.\nScala copysourcecase class Record(version: Int, name: String, address: String)\n\nimplicit val node = DistributedData(system).selfUniqueAddress\nimplicit val recordClock: LWWRegister.Clock[Record] = new LWWRegister.Clock[Record] {\n  override def apply(currentTimestamp: Long, value: Record): Long =\n    value.version\n}\n\nval record1 = Record(version = 1, \"Alice\", \"Union Square\")\nval r1 = LWWRegister(node, record1, recordClock)\n\nval record2 = Record(version = 2, \"Alice\", \"Madison Square\")\nval r2 = LWWRegister(node, record2, recordClock)\n\nval r3 = r1.merge(r2)\nprintln(r3.value) Java copysourceclass Record {\n  public final int version;\n  public final String name;\n  public final String address;\n\n  public Record(int version, String name, String address) {\n    this.version = version;\n    this.name = name;\n    this.address = address;\n  }\n}\n\n\n  final SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();\n  final LWWRegister.Clock<Record> recordClock =\n      new LWWRegister.Clock<Record>() {\n        @Override\n        public long apply(long currentTimestamp, Record value) {\n          return value.version;\n        }\n      };\n\n  final Record record1 = new Record(1, \"Alice\", \"Union Square\");\n  final LWWRegister<Record> r1 = LWWRegister.create(node, record1);\n\n  final Record record2 = new Record(2, \"Alice\", \"Madison Square\");\n  final LWWRegister<Record> r2 = LWWRegister.create(node, record2);\n\n  final LWWRegister<Record> r3 = r1.merge(r2);\n  System.out.println(r3.value());\nFor first-write-wins semantics you can use the LWWRegister#reverseClock instead of the LWWRegister#defaultClock.\nThe defaultClock is using max value of System.currentTimeMillis() and currentTimestamp + 1. This means that the timestamp is increased for changes on the same node that occurs within the same millisecond. It also means that it is safe to use the LWWRegister without synchronized clocks when there is only one active writer, e.g. a Cluster Singleton. Such a single writer should then first read current value with ReadMajority (or more) before changing and writing the value with WriteMajority (or more). When using LWWRegister with Cluster Singleton it’s also recommended to enable:\n# Update and Get operations are sent to oldest nodes first.\npekko.cluster.distributed-data.prefer-oldest = on","title":"Flags and Registers"},{"location":"/typed/distributed-data.html#delta-crdt","text":"Delta State Replicated Data Types are supported. A delta-CRDT is a way to reduce the need for sending the full state for updates. For example adding element 'c' and 'd' to set {'a', 'b'} would result in sending the delta {'c', 'd'} and merge that with the state on the receiving side, resulting in set {'a', 'b', 'c', 'd'}.\nThe protocol for replicating the deltas supports causal consistency if the data type is marked with RequiresCausalDeliveryOfDeltas. Otherwise it is only eventually consistent. Without causal consistency it means that if elements 'c' and 'd' are added in two separate Update operations these deltas may occasionally be propagated to nodes in a different order to the causal order of the updates. For this example it can result in that set {'a', 'b', 'd'} can be seen before element ‘c’ is seen. Eventually it will be {'a', 'b', 'c', 'd'}.\nNote that the full state is occasionally also replicated for delta-CRDTs, for example when new nodes are added to the cluster or when deltas could not be propagated because of network partitions or similar problems.\nThe the delta propagation can be disabled with configuration property:\npekko.cluster.distributed-data.delta-crdt.enabled=off","title":"Delta-CRDT"},{"location":"/typed/distributed-data.html#custom-data-type","text":"You can implement your own data types. The only requirement is that it implements the mergemergeData function of the ReplicatedDataAbstractReplicatedData trait.\nA nice property of stateful CRDTs is that they typically compose nicely, i.e. you can combine several smaller data types to build richer data structures. For example, the PNCounter is composed of two internal GCounter instances to keep track of increments and decrements separately.\nHere is s simple implementation of a custom TwoPhaseSet that is using two internal GSet types to keep track of addition and removals. A TwoPhaseSet is a set where an element may be added and removed, but never added again thereafter.\nScala copysourcecase class TwoPhaseSet(adds: GSet[String] = GSet.empty, removals: GSet[String] = GSet.empty) extends ReplicatedData {\n  type T = TwoPhaseSet\n\n  def add(element: String): TwoPhaseSet =\n    copy(adds = adds.add(element))\n\n  def remove(element: String): TwoPhaseSet =\n    copy(removals = removals.add(element))\n\n  def elements: Set[String] = adds.elements.diff(removals.elements)\n\n  override def merge(that: TwoPhaseSet): TwoPhaseSet =\n    copy(adds = this.adds.merge(that.adds), removals = this.removals.merge(that.removals))\n} Java copysourcepublic class TwoPhaseSet extends AbstractReplicatedData<TwoPhaseSet> {\n\n  public final GSet<String> adds;\n  public final GSet<String> removals;\n\n  public TwoPhaseSet(GSet<String> adds, GSet<String> removals) {\n    this.adds = adds;\n    this.removals = removals;\n  }\n\n  public static TwoPhaseSet create() {\n    return new TwoPhaseSet(GSet.create(), GSet.create());\n  }\n\n  public TwoPhaseSet add(String element) {\n    return new TwoPhaseSet(adds.add(element), removals);\n  }\n\n  public TwoPhaseSet remove(String element) {\n    return new TwoPhaseSet(adds, removals.add(element));\n  }\n\n  public Set<String> getElements() {\n    Set<String> result = new HashSet<>(adds.getElements());\n    result.removeAll(removals.getElements());\n    return result;\n  }\n\n  @Override\n  public TwoPhaseSet mergeData(TwoPhaseSet that) {\n    return new TwoPhaseSet(this.adds.merge(that.adds), this.removals.merge(that.removals));\n  }\n}\nData types should be immutable, i.e. “modifying” methods should return a new instance.\nImplement the additional methods of DeltaReplicatedDataAbstractDeltaReplicatedData if it has support for delta-CRDT replication.","title":"Custom Data Type"},{"location":"/typed/distributed-data.html#serialization","text":"The data types must be serializable with an Pekko Serializer. It is highly recommended that you implement efficient serialization with Protobuf or similar for your custom data types. The built in data types are marked with ReplicatedDataSerialization and serialized with org.apache.pekko.cluster.ddata.protobuf.ReplicatedDataSerializer.\nSerialization of the data types are used in remote messages and also for creating message digests (SHA-1) to detect changes. Therefore it is important that the serialization is efficient and produce the same bytes for the same content. For example sets and maps should be sorted deterministically in the serialization.\nThis is a protobuf representation of the above TwoPhaseSet:\ncopysourceoption java_package = \"docs.ddata.protobuf.msg\";\noption optimize_for = SPEED;\n\nmessage TwoPhaseSet {\n  repeated string adds = 1;\n  repeated string removals = 2;\n}\nThe serializer for the TwoPhaseSet:\nScala copysourceimport java.util.ArrayList\nimport java.util.Collections\nimport org.apache.pekko\nimport pekko.util.ccompat.JavaConverters._\nimport pekko.actor.ExtendedActorSystem\nimport pekko.cluster.ddata.GSet\nimport pekko.cluster.ddata.protobuf.SerializationSupport\nimport pekko.serialization.Serializer\nimport docs.ddata.TwoPhaseSet\nimport docs.ddata.protobuf.msg.TwoPhaseSetMessages\n\nclass TwoPhaseSetSerializer(val system: ExtendedActorSystem) extends Serializer with SerializationSupport {\n\n  override def includeManifest: Boolean = false\n\n  override def identifier = 99999\n\n  override def toBinary(obj: AnyRef): Array[Byte] = obj match {\n    case m: TwoPhaseSet => twoPhaseSetToProto(m).toByteArray\n    case _              => throw new IllegalArgumentException(s\"Can't serialize object of type ${obj.getClass}\")\n  }\n\n  override def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {\n    twoPhaseSetFromBinary(bytes)\n  }\n\n  def twoPhaseSetToProto(twoPhaseSet: TwoPhaseSet): TwoPhaseSetMessages.TwoPhaseSet = {\n    val b = TwoPhaseSetMessages.TwoPhaseSet.newBuilder()\n    // using java collections and sorting for performance (avoid conversions)\n    val adds = new ArrayList[String]\n    twoPhaseSet.adds.elements.foreach(adds.add)\n    if (!adds.isEmpty) {\n      Collections.sort(adds)\n      b.addAllAdds(adds)\n    }\n    val removals = new ArrayList[String]\n    twoPhaseSet.removals.elements.foreach(removals.add)\n    if (!removals.isEmpty) {\n      Collections.sort(removals)\n      b.addAllRemovals(removals)\n    }\n    b.build()\n  }\n\n  def twoPhaseSetFromBinary(bytes: Array[Byte]): TwoPhaseSet = {\n    val msg = TwoPhaseSetMessages.TwoPhaseSet.parseFrom(bytes)\n    val addsSet = msg.getAddsList.iterator.asScala.toSet\n    val removalsSet = msg.getRemovalsList.iterator.asScala.toSet\n    val adds = addsSet.foldLeft(GSet.empty[String])((acc, el) => acc.add(el))\n    val removals = removalsSet.foldLeft(GSet.empty[String])((acc, el) => acc.add(el))\n    // GSet will accumulate deltas when adding elements,\n    // but those are not of interest in the result of the deserialization\n    TwoPhaseSet(adds.resetDelta, removals.resetDelta)\n  }\n} Java copysourceimport jdocs.ddata.TwoPhaseSet;\nimport docs.ddata.protobuf.msg.TwoPhaseSetMessages;\nimport docs.ddata.protobuf.msg.TwoPhaseSetMessages.TwoPhaseSet.Builder;\nimport java.util.ArrayList;\nimport java.util.Collections;\n\nimport org.apache.pekko.actor.ExtendedActorSystem;\nimport org.apache.pekko.cluster.ddata.GSet;\nimport org.apache.pekko.cluster.ddata.protobuf.AbstractSerializationSupport;\n\npublic class TwoPhaseSetSerializer extends AbstractSerializationSupport {\n\n  private final ExtendedActorSystem system;\n\n  public TwoPhaseSetSerializer(ExtendedActorSystem system) {\n    this.system = system;\n  }\n\n  @Override\n  public ExtendedActorSystem system() {\n    return this.system;\n  }\n\n  @Override\n  public boolean includeManifest() {\n    return false;\n  }\n\n  @Override\n  public int identifier() {\n    return 99998;\n  }\n\n  @Override\n  public byte[] toBinary(Object obj) {\n    if (obj instanceof TwoPhaseSet) {\n      return twoPhaseSetToProto((TwoPhaseSet) obj).toByteArray();\n    } else {\n      throw new IllegalArgumentException(\"Can't serialize object of type \" + obj.getClass());\n    }\n  }\n\n  @Override\n  public Object fromBinaryJava(byte[] bytes, Class<?> manifest) {\n    return twoPhaseSetFromBinary(bytes);\n  }\n\n  protected TwoPhaseSetMessages.TwoPhaseSet twoPhaseSetToProto(TwoPhaseSet twoPhaseSet) {\n    Builder b = TwoPhaseSetMessages.TwoPhaseSet.newBuilder();\n    ArrayList<String> adds = new ArrayList<>(twoPhaseSet.adds.getElements());\n    if (!adds.isEmpty()) {\n      Collections.sort(adds);\n      b.addAllAdds(adds);\n    }\n    ArrayList<String> removals = new ArrayList<>(twoPhaseSet.removals.getElements());\n    if (!removals.isEmpty()) {\n      Collections.sort(removals);\n      b.addAllRemovals(removals);\n    }\n    return b.build();\n  }\n\n  protected TwoPhaseSet twoPhaseSetFromBinary(byte[] bytes) {\n    try {\n      TwoPhaseSetMessages.TwoPhaseSet msg = TwoPhaseSetMessages.TwoPhaseSet.parseFrom(bytes);\n      GSet<String> adds = GSet.create();\n      for (String elem : msg.getAddsList()) {\n        adds = adds.add(elem);\n      }\n      GSet<String> removals = GSet.create();\n      for (String elem : msg.getRemovalsList()) {\n        removals = removals.add(elem);\n      }\n      // GSet will accumulate deltas when adding elements,\n      // but those are not of interest in the result of the deserialization\n      return new TwoPhaseSet(adds.resetDelta(), removals.resetDelta());\n    } catch (Exception e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n  }\n}\nNote that the elements of the sets are sorted so the SHA-1 digests are the same for the same elements.\nYou register the serializer in configuration:\nScala copysourcepekko.actor {\n  serializers {\n    two-phase-set = \"docs.ddata.protobuf.TwoPhaseSetSerializer\"\n  }\n  serialization-bindings {\n    \"docs.ddata.TwoPhaseSet\" = two-phase-set\n  }\n} Java copysourcepekko.actor {\n  serializers {\n    twophaseset = \"jdocs.ddata.protobuf.TwoPhaseSetSerializer\"\n  }\n  serialization-bindings {\n    \"jdocs.ddata.TwoPhaseSet\" = twophaseset\n  }\n}\nUsing compression can sometimes be a good idea to reduce the data size. Gzip compression is provided by the org.apache.pekko.cluster.ddata.protobuf.SerializationSupport traitorg.apache.pekko.cluster.ddata.protobuf.AbstractSerializationSupport interface:\nScala copysourceoverride def toBinary(obj: AnyRef): Array[Byte] = obj match {\n  case m: TwoPhaseSet => compress(twoPhaseSetToProto(m))\n  case _              => throw new IllegalArgumentException(s\"Can't serialize object of type ${obj.getClass}\")\n}\n\noverride def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {\n  twoPhaseSetFromBinary(decompress(bytes))\n} Java copysource@Override\npublic byte[] toBinary(Object obj) {\n  if (obj instanceof TwoPhaseSet) {\n    return compress(twoPhaseSetToProto((TwoPhaseSet) obj));\n  } else {\n    throw new IllegalArgumentException(\"Can't serialize object of type \" + obj.getClass());\n  }\n}\n\n@Override\npublic Object fromBinaryJava(byte[] bytes, Class<?> manifest) {\n  return twoPhaseSetFromBinary(decompress(bytes));\n}\nThe two embedded GSet can be serialized as illustrated above, but in general when composing new data types from the existing built in types it is better to make use of the existing serializer for those types. This can be done by declaring those as bytes fields in protobuf:\ncopysourcemessage TwoPhaseSet2 {\n  optional bytes adds = 1;\n  optional bytes removals = 2;\n}\nand use the methods otherMessageToProto and otherMessageFromBinary that are provided by the SerializationSupport trait to serialize and deserialize the GSet instances. This works with any type that has a registered Pekko serializer. This is how such an serializer would look like for the TwoPhaseSet:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.ExtendedActorSystem\nimport pekko.cluster.ddata.GSet\nimport pekko.cluster.ddata.protobuf.SerializationSupport\nimport pekko.serialization.Serializer\nimport docs.ddata.TwoPhaseSet\nimport docs.ddata.protobuf.msg.TwoPhaseSetMessages\n\nclass TwoPhaseSetSerializer2(val system: ExtendedActorSystem) extends Serializer with SerializationSupport {\n\n  override def includeManifest: Boolean = false\n\n  override def identifier = 99999\n\n  override def toBinary(obj: AnyRef): Array[Byte] = obj match {\n    case m: TwoPhaseSet => twoPhaseSetToProto(m).toByteArray\n    case _              => throw new IllegalArgumentException(s\"Can't serialize object of type ${obj.getClass}\")\n  }\n\n  override def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {\n    twoPhaseSetFromBinary(bytes)\n  }\n\n  def twoPhaseSetToProto(twoPhaseSet: TwoPhaseSet): TwoPhaseSetMessages.TwoPhaseSet2 = {\n    val b = TwoPhaseSetMessages.TwoPhaseSet2.newBuilder()\n    if (!twoPhaseSet.adds.isEmpty)\n      b.setAdds(otherMessageToProto(twoPhaseSet.adds).toByteString())\n    if (!twoPhaseSet.removals.isEmpty)\n      b.setRemovals(otherMessageToProto(twoPhaseSet.removals).toByteString())\n    b.build()\n  }\n\n  def twoPhaseSetFromBinary(bytes: Array[Byte]): TwoPhaseSet = {\n    val msg = TwoPhaseSetMessages.TwoPhaseSet2.parseFrom(bytes)\n    val adds =\n      if (msg.hasAdds)\n        otherMessageFromBinary(msg.getAdds.toByteArray).asInstanceOf[GSet[String]]\n      else\n        GSet.empty[String]\n    val removals =\n      if (msg.hasRemovals)\n        otherMessageFromBinary(msg.getRemovals.toByteArray).asInstanceOf[GSet[String]]\n      else\n        GSet.empty[String]\n    TwoPhaseSet(adds, removals)\n  }\n} Java copysourceimport jdocs.ddata.TwoPhaseSet;\nimport docs.ddata.protobuf.msg.TwoPhaseSetMessages;\nimport docs.ddata.protobuf.msg.TwoPhaseSetMessages.TwoPhaseSet2.Builder;\n\nimport org.apache.pekko.actor.ExtendedActorSystem;\nimport org.apache.pekko.cluster.ddata.GSet;\nimport org.apache.pekko.cluster.ddata.protobuf.AbstractSerializationSupport;\nimport org.apache.pekko.cluster.ddata.protobuf.ReplicatedDataSerializer;\n\npublic class TwoPhaseSetSerializer2 extends AbstractSerializationSupport {\n\n  private final ExtendedActorSystem system;\n  private final ReplicatedDataSerializer replicatedDataSerializer;\n\n  public TwoPhaseSetSerializer2(ExtendedActorSystem system) {\n    this.system = system;\n    this.replicatedDataSerializer = new ReplicatedDataSerializer(system);\n  }\n\n  @Override\n  public ExtendedActorSystem system() {\n    return this.system;\n  }\n\n  @Override\n  public boolean includeManifest() {\n    return false;\n  }\n\n  @Override\n  public int identifier() {\n    return 99998;\n  }\n\n  @Override\n  public byte[] toBinary(Object obj) {\n    if (obj instanceof TwoPhaseSet) {\n      return twoPhaseSetToProto((TwoPhaseSet) obj).toByteArray();\n    } else {\n      throw new IllegalArgumentException(\"Can't serialize object of type \" + obj.getClass());\n    }\n  }\n\n  @Override\n  public Object fromBinaryJava(byte[] bytes, Class<?> manifest) {\n    return twoPhaseSetFromBinary(bytes);\n  }\n\n  protected TwoPhaseSetMessages.TwoPhaseSet2 twoPhaseSetToProto(TwoPhaseSet twoPhaseSet) {\n    Builder b = TwoPhaseSetMessages.TwoPhaseSet2.newBuilder();\n    if (!twoPhaseSet.adds.isEmpty())\n      b.setAdds(otherMessageToProto(twoPhaseSet.adds).toByteString());\n    if (!twoPhaseSet.removals.isEmpty())\n      b.setRemovals(otherMessageToProto(twoPhaseSet.removals).toByteString());\n    return b.build();\n  }\n\n  @SuppressWarnings(\"unchecked\")\n  protected TwoPhaseSet twoPhaseSetFromBinary(byte[] bytes) {\n    try {\n      TwoPhaseSetMessages.TwoPhaseSet2 msg = TwoPhaseSetMessages.TwoPhaseSet2.parseFrom(bytes);\n\n      GSet<String> adds = GSet.create();\n      if (msg.hasAdds()) adds = (GSet<String>) otherMessageFromBinary(msg.getAdds().toByteArray());\n\n      GSet<String> removals = GSet.create();\n      if (msg.hasRemovals())\n        adds = (GSet<String>) otherMessageFromBinary(msg.getRemovals().toByteArray());\n\n      return new TwoPhaseSet(adds, removals);\n    } catch (Exception e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n  }\n}","title":"Serialization"},{"location":"/typed/distributed-data.html#durable-storage","text":"By default the data is only kept in memory. It is redundant since it is replicated to other nodes in the cluster, but if you stop all nodes the data is lost, unless you have saved it elsewhere.\nEntries can be configured to be durable, i.e. stored on local disk on each node. The stored data will be loaded next time the replicator is started, i.e. when actor system is restarted. This means data will survive as long as at least one node from the old cluster takes part in a new cluster. The keys of the durable entries are configured with:\npekko.cluster.distributed-data.durable.keys = [\"a\", \"b\", \"durable*\"]\nPrefix matching is supported by using * at the end of a key.\nAll entries can be made durable by specifying:\npekko.cluster.distributed-data.durable.keys = [\"*\"]\nLMDBLMDB is the default storage implementation. It is possible to replace that with another implementation by implementing the actor protocol described in org.apache.pekko.cluster.ddata.DurableStore and defining the pekko.cluster.distributed-data.durable.store-actor-class property for the new implementation.\nThe location of the files for the data is configured with:\nScala # Directory of LMDB file. There are two options:\n# 1. A relative or absolute path to a directory that ends with 'ddata'\n#    the full name of the directory will contain name of the ActorSystem\n#    and its remote port.\n# 2. Otherwise the path is used as is, as a relative or absolute path to\n#    a directory.\npekko.cluster.distributed-data.durable.lmdb.dir = \"ddata\"\n Java # Directory of LMDB file. There are two options:\n# 1. A relative or absolute path to a directory that ends with 'ddata'\n#    the full name of the directory will contain name of the ActorSystem\n#    and its remote port.\n# 2. Otherwise the path is used as is, as a relative or absolute path to\n#    a directory.\npekko.cluster.distributed-data.durable.lmdb.dir = \"ddata\"\nWhen running in production you may want to configure the directory to a specific path (alt 2), since the default directory contains the remote port of the actor system to make the name unique. If using a dynamically assigned port (0) it will be different each time and the previously stored data will not be loaded.\nMaking the data durable has a performance cost. By default, each update is flushed to disk before the UpdateSuccess reply is sent. For better performance, but with the risk of losing the last writes if the JVM crashes, you can enable write behind mode. Changes are then accumulated during a time period before it is written to LMDB and flushed to disk. Enabling write behind is especially efficient when performing many writes to the same key, because it is only the last value for each key that will be serialized and stored. The risk of losing writes if the JVM crashes is small since the data is typically replicated to other nodes immediately according to the given WriteConsistency.\npekko.cluster.distributed-data.durable.lmdb.write-behind-interval = 200 ms\nNote that you should be prepared to receive WriteFailure as reply to an Update of a durable entry if the data could not be stored for some reason. When enabling write-behind-interval such errors will only be logged and UpdateSuccess will still be the reply to the Update.\nThere is one important caveat when it comes pruning of CRDT Garbage for durable data. If an old data entry that was never pruned is injected and merged with existing data after that the pruning markers have been removed the value will not be correct. The time-to-live of the markers is defined by configuration pekko.cluster.distributed-data.durable.remove-pruning-marker-after and is in the magnitude of days. This would be possible if a node with durable data didn’t participate in the pruning (e.g. it was shutdown) and later started after this time. A node with durable data should not be stopped for longer time than this duration and if it is joining again after this duration its data should first be manually removed (from the lmdb directory).","title":"Durable Storage"},{"location":"/typed/distributed-data.html#limitations","text":"There are some limitations that you should be aware of.\nCRDTs cannot be used for all types of problems, and eventual consistency does not fit all domains. Sometimes, you need strong consistency.\nIt is not intended for Big Data. The number of top level entries should not exceed 100000. When a new node is added to the cluster all these entries are transferred (gossiped) to the new node. The entries are split up in chunks and all existing nodes collaborate in the gossip, but it will take a while (tens of seconds) to transfer all entries and this means that you cannot have too many top level entries. The current recommended limit is 100000. We will be able to improve this if needed, but the design is still not intended for billions of entries.\nAll data is held in memory, which is another reason why it is not intended for Big Data.\nWhen a data entry is changed the full state of that entry may be replicated to other nodes if it doesn’t support delta-CRDT. The full state is also replicated for delta-CRDTs, for example when new nodes are added to the cluster or when deltas could not be propagated because of network partitions or similar problems. This means that you cannot have too large data entries, because then the remote message size will be too large.","title":"Limitations"},{"location":"/typed/distributed-data.html#crdt-garbage","text":"One thing that can be problematic with CRDTs is that some data types accumulate history (garbage). For example a GCounter keeps track of one counter per node. If a GCounter has been updated from one node it will associate the identifier of that node forever. That can become a problem for long running systems with many cluster nodes being added and removed. To solve this problem the Replicator performs pruning of data associated with nodes that have been removed from the cluster. Data types that need pruning have to implement the RemovedNodePruning trait. See the API documentation of the Replicator for details.","title":"CRDT Garbage"},{"location":"/typed/distributed-data.html#learn-more-about-crdts","text":"Strong Eventual Consistency and Conflict-free Replicated Data Types (video) talk by Mark Shapiro A comprehensive study of Convergent and Commutative Replicated Data Types paper by Mark Shapiro et. al.","title":"Learn More about CRDTs"},{"location":"/typed/distributed-data.html#configuration","text":"The DistributedData extension can be configured with the following properties:\ncopysource# Settings for the DistributedData extension\npekko.cluster.distributed-data {\n  # Actor name of the Replicator actor, /system/ddataReplicator\n  name = ddataReplicator\n\n  # Replicas are running on members tagged with this role.\n  # All members are used if undefined or empty.\n  role = \"\"\n\n  # How often the Replicator should send out gossip information\n  gossip-interval = 2 s\n  \n  # How often the subscribers will be notified of changes, if any\n  notify-subscribers-interval = 500 ms\n\n  # Logging of data with payload size in bytes larger than\n  # this value. Maximum detected size per key is logged once,\n  # with an increase threshold of 10%.\n  # It can be disabled by setting the property to off.\n  log-data-size-exceeding = 10 KiB\n\n  # Maximum number of entries to transfer in one round of gossip exchange when\n  # synchronizing the replicas. Next chunk will be transferred in next round of gossip.\n  # The actual number of data entries in each Gossip message is dynamically\n  # adjusted to not exceed the maximum remote message size (maximum-frame-size).\n  max-delta-elements = 500\n  \n  # The id of the dispatcher to use for Replicator actors.\n  # If specified you need to define the settings of the actual dispatcher.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n  # How often the Replicator checks for pruning of data associated with\n  # removed cluster nodes. If this is set to 'off' the pruning feature will\n  # be completely disabled.\n  pruning-interval = 120 s\n  \n  # How long time it takes to spread the data to all other replica nodes.\n  # This is used when initiating and completing the pruning process of data associated\n  # with removed cluster nodes. The time measurement is stopped when any replica is \n  # unreachable, but it's still recommended to configure this with certain margin.\n  # It should be in the magnitude of minutes even though typical dissemination time\n  # is shorter (grows logarithmic with number of nodes). There is no advantage of \n  # setting this too low. Setting it to large value will delay the pruning process.\n  max-pruning-dissemination = 300 s\n  \n  # The markers of that pruning has been performed for a removed node are kept for this\n  # time and thereafter removed. If and old data entry that was never pruned is somehow\n  # injected and merged with existing data after this time the value will not be correct.\n  # This would be possible (although unlikely) in the case of a long network partition.\n  # It should be in the magnitude of hours. For durable data it is configured by \n  # 'pekko.cluster.distributed-data.durable.pruning-marker-time-to-live'.\n pruning-marker-time-to-live = 6 h\n  \n  # Serialized Write and Read messages are cached when they are sent to \n  # several nodes. If no further activity they are removed from the cache\n  # after this duration.\n  serializer-cache-time-to-live = 10s\n\n  # Update and Get operations are sent to oldest nodes first.\n  # This is useful together with Cluster Singleton, which is running on oldest nodes.\n  prefer-oldest = off\n  \n  # Settings for delta-CRDT\n  delta-crdt {\n    # enable or disable delta-CRDT replication\n    enabled = on\n    \n    # Some complex deltas grow in size for each update and above this\n    # threshold such deltas are discarded and sent as full state instead.\n    # This is number of elements or similar size hint, not size in bytes.\n    max-delta-size = 50\n  }\n  \n  durable {\n    # List of keys that are durable. Prefix matching is supported by using * at the\n    # end of a key.  \n    keys = []\n    \n    # The markers of that pruning has been performed for a removed node are kept for this\n    # time and thereafter removed. If and old data entry that was never pruned is\n    # injected and merged with existing data after this time the value will not be correct.\n    # This would be possible if replica with durable data didn't participate in the pruning\n    # (e.g. it was shutdown) and later started after this time. A durable replica should not \n    # be stopped for longer time than this duration and if it is joining again after this\n    # duration its data should first be manually removed (from the lmdb directory).\n    # It should be in the magnitude of days. Note that there is a corresponding setting\n    # for non-durable data: 'pekko.cluster.distributed-data.pruning-marker-time-to-live'.\n    pruning-marker-time-to-live = 10 d\n    \n    # Fully qualified class name of the durable store actor. It must be a subclass\n    # of pekko.actor.Actor and handle the protocol defined in \n    # org.apache.pekko.cluster.ddata.DurableStore. The class must have a constructor with\n    # com.typesafe.config.Config parameter.\n    store-actor-class = org.apache.pekko.cluster.ddata.LmdbDurableStore\n    \n    use-dispatcher = pekko.cluster.distributed-data.durable.pinned-store\n    \n    pinned-store {\n      executor = thread-pool-executor\n      type = PinnedDispatcher\n    }\n    \n    # Config for the LmdbDurableStore\n    lmdb {\n      # Directory of LMDB file. There are two options:\n      # 1. A relative or absolute path to a directory that ends with 'ddata'\n      #    the full name of the directory will contain name of the ActorSystem\n      #    and its remote port.\n      # 2. Otherwise the path is used as is, as a relative or absolute path to\n      #    a directory.\n      #\n      # When running in production you may want to configure this to a specific\n      # path (alt 2), since the default directory contains the remote port of the\n      # actor system to make the name unique. If using a dynamically assigned \n      # port (0) it will be different each time and the previously stored data \n      # will not be loaded.\n      dir = \"ddata\"\n      \n      # Size in bytes of the memory mapped file.\n      map-size = 100 MiB\n      \n      # Accumulate changes before storing improves performance with the\n      # risk of losing the last writes if the JVM crashes.\n      # The interval is by default set to 'off' to write each update immediately.\n      # Enabling write behind by specifying a duration, e.g. 200ms, is especially \n      # efficient when performing many writes to the same key, because it is only \n      # the last value for each key that will be serialized and stored.  \n      # write-behind-interval = 200 ms\n      write-behind-interval = off\n    }\n  }\n  \n}","title":"Configuration"},{"location":"/typed/distributed-data.html#example-project","text":"Distributed Data example project Distributed Data example project is an example project that can be downloaded, and with instructions of how to run.\nThis project contains several samples illustrating how to use Distributed Data.","title":"Example project"},{"location":"/typed/cluster-singleton.html","text":"","title":"Cluster Singleton"},{"location":"/typed/cluster-singleton.html#cluster-singleton","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Cluster Singleton.","title":"Cluster Singleton"},{"location":"/typed/cluster-singleton.html#module-info","text":"To use Cluster Singleton, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-typed_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster (typed) Artifact org.apache.pekko pekko-cluster-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/cluster-singleton.html#introduction","text":"For some use cases it is convenient and sometimes also mandatory to ensure that you have exactly one actor of a certain type running somewhere in the cluster.\nSome examples:\nsingle point of responsibility for certain cluster-wide consistent decisions, or coordination of actions across the cluster system single entry point to an external system single master, many workers centralized naming service, or routing logic\nUsing a singleton should not be the first design choice. It has several drawbacks, such as single-point of bottleneck. Single-point of failure is also a relevant concern, but for some cases this feature takes care of that by making sure that another singleton instance will eventually be started.\nWarning Make sure to not use a Cluster downing strategy that may split the cluster into several separate clusters in case of network problems or system overload (long GC pauses), since that will result in in multiple Singletons being started, one in each separate cluster! See Downing.","title":"Introduction"},{"location":"/typed/cluster-singleton.html#singleton-manager","text":"The cluster singleton pattern manages one singleton actor instance among all cluster nodes or a group of nodes tagged with a specific role. The singleton manager is an actor that is supposed to be started with ClusterSingleton.initClusterSingleton.init as early as possible on all nodes, or all nodes with specified role, in the cluster.\nThe actual singleton actor is\nStarted on the oldest node by creating a child actor from supplied BehaviorBehavior. It makes sure that at most one singleton instance is running at any point in time. Always running on the oldest member with specified role.\nThe oldest member is determined by cluster.Member#isOlderThancluster.Member#isOlderThan This can change when removing that member from the cluster. Be aware that there is a short time period when there is no active singleton during the hand-over process.\nWhen the oldest node is Leaving the cluster there is an exchange from the oldest and the new oldest before a new singleton is started up.\nThe cluster failure detector will notice when oldest node becomes unreachable due to things like JVM crash, hard shut down, or network failure. After Downing and removing that node the a new oldest node will take over and a new singleton actor is created. For these failure scenarios there will not be a graceful hand-over, but more than one active singletons is prevented by all reasonable means. Some corner cases are eventually resolved by configurable timeouts. Additional safety can be added by using a Lease.","title":"Singleton manager"},{"location":"/typed/cluster-singleton.html#singleton-proxy","text":"To communicate with a given named singleton in the cluster you can access it though a proxy ActorRefActorRef. When calling ClusterSingleton.initClusterSingleton.init for a given singletonName on a node an ActorRef is returned. It is to this ActorRef that you can send messages to the singleton instance, independent of which node the singleton instance is active. ClusterSingleton.init can be called multiple times, if there already is a singleton manager running on this node, no additional manager is started, and if there is one running an ActorRef to the proxy is returned.\nThe proxy will route all messages to the current instance of the singleton, and keep track of the oldest node in the cluster and discover the singleton’s ActorRef. There might be periods of time during which the singleton is unavailable, e.g., when a node leaves the cluster. In these cases, the proxy will buffer the messages sent to the singleton and then deliver them when the singleton is finally available. If the buffer is full the proxy will drop old messages when new messages are sent via the proxy. The size of the buffer is configurable and it can be disabled by using a buffer size of 0.\nIt’s worth noting that messages can always be lost because of the distributed nature of these actors. As always, additional logic should be implemented in the singleton (acknowledgement) and in the client (retry) actors to ensure at-least-once message delivery.\nThe singleton instance will not run on members with status WeaklyUp.","title":"Singleton proxy"},{"location":"/typed/cluster-singleton.html#potential-problems-to-be-aware-of","text":"This pattern may seem to be very tempting to use at first, but it has several drawbacks, some of them are listed below:\nThe cluster singleton may quickly become a performance bottleneck. You can not rely on the cluster singleton to be non-stop available — e.g. when the node on which the singleton has been running dies, it will take a few seconds for this to be noticed and the singleton be migrated to another node. If many singletons are used be aware of that all will run on the oldest node (or oldest with configured role). Cluster Sharding combined with keeping the “singleton” entities alive can be a better alternative.\nWarning Make sure to not use a Cluster downing strategy that may split the cluster into several separate clusters in case of network problems or system overload (long GC pauses), since that will result in in multiple Singletons being started, one in each separate cluster! See Downing.","title":"Potential problems to be aware of"},{"location":"/typed/cluster-singleton.html#example","text":"Any BehaviorBehavior can be run as a singleton. E.g. a basic counter:\nScala copysourceobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Int]) extends Command\n  case object GoodByeCounter extends Command\n\n  def apply(): Behavior[Command] = {\n    def updated(value: Int): Behavior[Command] = {\n      Behaviors.receiveMessage[Command] {\n        case Increment =>\n          updated(value + 1)\n        case GetValue(replyTo) =>\n          replyTo ! value\n          Behaviors.same\n        case GoodByeCounter =>\n          // Possible async action then stop\n          Behaviors.stopped\n      }\n    }\n\n    updated(0)\n  }\n} Java copysourcepublic class Counter extends AbstractBehavior<Counter.Command> {\n\n  public interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    private final ActorRef<Integer> replyTo;\n\n    public GetValue(ActorRef<Integer> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public enum GoodByeCounter implements Command {\n    INSTANCE\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(Counter::new);\n  }\n\n  private int value = 0;\n\n  private Counter(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Increment.class, msg -> onIncrement())\n        .onMessage(GetValue.class, this::onGetValue)\n        .onMessage(GoodByeCounter.class, msg -> onGoodByCounter())\n        .build();\n  }\n\n  private Behavior<Command> onIncrement() {\n    value++;\n    return this;\n  }\n\n  private Behavior<Command> onGetValue(GetValue msg) {\n    msg.replyTo.tell(value);\n    return this;\n  }\n\n  private Behavior<Command> onGoodByCounter() {\n    // Possible async action then stop\n    return this;\n  }\n}\nThen on every node in the cluster, or every node with a given role, use the ClusterSingletonClusterSingleton extension to spawn the singleton. An instance will per data centre of the cluster:\nScala copysourceimport org.apache.pekko\nimport pekko.cluster.typed.ClusterSingleton\nimport pekko.cluster.typed.SingletonActor\n\nval singletonManager = ClusterSingleton(system)\n// Start if needed and provide a proxy to a named singleton\nval proxy: ActorRef[Counter.Command] = singletonManager.init(\n  SingletonActor(Behaviors.supervise(Counter()).onFailure[Exception](SupervisorStrategy.restart), \"GlobalCounter\"))\n\nproxy ! Counter.Increment Java copysourceimport org.apache.pekko.cluster.typed.ClusterSingleton;\nimport org.apache.pekko.cluster.typed.ClusterSingletonSettings;\nimport org.apache.pekko.cluster.typed.SingletonActor;\n\nClusterSingleton singleton = ClusterSingleton.get(system);\n// Start if needed and provide a proxy to a named singleton\nActorRef<Counter.Command> proxy =\n    singleton.init(SingletonActor.of(Counter.create(), \"GlobalCounter\"));\n\nproxy.tell(Counter.Increment.INSTANCE);","title":"Example"},{"location":"/typed/cluster-singleton.html#supervision","text":"The default supervision strategy when an exception is thrown is for an actor to be stopped. The above example overrides this to restart to ensure it is always running. Another option would be to restart with a backoff:\nScala copysourceval proxyBackOff: ActorRef[Counter.Command] = singletonManager.init(\n  SingletonActor(\n    Behaviors\n      .supervise(Counter())\n      .onFailure[Exception](SupervisorStrategy.restartWithBackoff(1.second, 10.seconds, 0.2)),\n    \"GlobalCounter\")) Java copysourceClusterSingleton singleton = ClusterSingleton.get(system);\nActorRef<Counter.Command> proxy =\n    singleton.init(\n        SingletonActor.of(\n            Behaviors.supervise(Counter.create())\n                .onFailure(\n                    SupervisorStrategy.restartWithBackoff(\n                        Duration.ofSeconds(1), Duration.ofSeconds(10), 0.2)),\n            \"GlobalCounter\"));\nBe aware that this means there will be times when the singleton won’t be running as restart is delayed. See Fault Tolerance for a full list of supervision options.","title":"Supervision"},{"location":"/typed/cluster-singleton.html#application-specific-stop-message","text":"An application specific stopMessage can be used to close the resources before actually stopping the singleton actor. This stopMessage is sent to the singleton actor to tell it to finish its work, close resources, and stop. The hand-over to the new oldest node is completed when the singleton actor is terminated. If the shutdown logic does not include any asynchronous actions it can be executed in the PostStopPostStop signal handler.\nScala copysourceval singletonActor = SingletonActor(Counter(), \"GlobalCounter\").withStopMessage(Counter.GoodByeCounter)\nsingletonManager.init(singletonActor) Java copysourceSingletonActor<Counter.Command> counterSingleton =\n    SingletonActor.of(Counter.create(), \"GlobalCounter\")\n        .withStopMessage(Counter.GoodByeCounter.INSTANCE);\nActorRef<Counter.Command> proxy = singleton.init(counterSingleton);","title":"Application specific stop message"},{"location":"/typed/cluster-singleton.html#lease","text":"A lease can be used as an additional safety measure to ensure that two singletons don’t run at the same time. Reasons for how this can happen:\nNetwork partitions without an appropriate downing provider Mistakes in the deployment process leading to two separate Pekko Clusters Timing issues between removing members from the Cluster on one side of a network partition and shutting them down on the other side\nA lease can be a final backup that means that the singleton actor won’t be created unless the lease can be acquired.\nTo use a lease for singleton set pekko.cluster.singleton.use-lease to the configuration location of the lease to use. A lease with with the name <actor system name>-singleton-<singleton actor path> is used and the owner is set to the Cluster(system).selfAddress.hostPortCluster.get(system).selfAddress().hostPort().\nIf the cluster singleton manager can’t acquire the lease it will keep retrying while it is the oldest node in the cluster. If the lease is lost then the singleton actor will be terminated then the lease will be re-tried.","title":"Lease"},{"location":"/typed/cluster-singleton.html#accessing-singleton-of-another-data-centre","text":"TODO #27705","title":"Accessing singleton of another data centre"},{"location":"/typed/cluster-singleton.html#configuration","text":"The following configuration properties are read by the ClusterSingletonManagerSettingsClusterSingletonManagerSettings when created with a ActorSystemActorSystem parameter. It is also possible to amend the ClusterSingletonManagerSettings or create it from another config section with the same layout as below. ClusterSingletonManagerSettings is a parameter to the ClusterSingletonManager.propsClusterSingletonManager.props factory method, i.e. each singleton can be configured with different settings if needed.\ncopysourcepekko.cluster.singleton {\n  # The actor name of the child singleton actor.\n  singleton-name = \"singleton\"\n  \n  # Singleton among the nodes tagged with specified role.\n  # If the role is not specified it's a singleton among all nodes in the cluster.\n  role = \"\"\n  \n  # When a node is becoming oldest it sends hand-over request to previous oldest, \n  # that might be leaving the cluster. This is retried with this interval until \n  # the previous oldest confirms that the hand over has started or the previous \n  # oldest member is removed from the cluster (+ pekko.cluster.down-removal-margin).\n  hand-over-retry-interval = 1s\n  \n  # The number of retries are derived from hand-over-retry-interval and\n  # pekko.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),\n  # but it will never be less than this property.\n  # After the hand over retries and it's still not able to exchange the hand over messages\n  # with the previous oldest it will restart itself by throwing ClusterSingletonManagerIsStuck,\n  # to start from a clean state. After that it will still not start the singleton instance\n  # until the previous oldest node has been removed from the cluster.\n  # On the other side, on the previous oldest node, the same number of retries - 3 are used\n  # and after that the singleton instance is stopped.\n  # For large clusters it might be necessary to increase this to avoid too early timeouts while\n  # gossip dissemination of the Leaving to Exiting phase occurs. For normal leaving scenarios\n  # it will not be a quicker hand over by reducing this value, but in extreme failure scenarios\n  # the recovery might be faster.\n  min-number-of-hand-over-retries = 15\n\n  # Config path of the lease to be taken before creating the singleton actor\n  # if the lease is lost then the actor is restarted and it will need to re-acquire the lease\n  # the default is no lease\n  use-lease = \"\"\n\n  # The interval between retries for acquiring the lease\n  lease-retry-interval = 5s\n}\nThe following configuration properties are read by the ClusterSingletonSettingsClusterSingletonSettings when created with a ActorSystemActorSystem parameter. ClusterSingletonSettings is an optional parameter in ClusterSingleton.initClusterSingleton.init. It is also possible to amend the ClusterSingletonProxySettingsClusterSingletonProxySettings or create it from another config section with the same layout as below.\ncopysourcepekko.cluster.singleton-proxy {\n  # The actor name of the singleton actor that is started by the ClusterSingletonManager\n  singleton-name = ${pekko.cluster.singleton.singleton-name}\n  \n  # The role of the cluster nodes where the singleton can be deployed.\n  # Corresponding to the role used by the `ClusterSingletonManager`. If the role is not\n  # specified it's a singleton among all nodes in the cluster, and the `ClusterSingletonManager`\n  # must then also be configured in same way.\n  role = \"\"\n  \n  # Interval at which the proxy will try to resolve the singleton instance.\n  singleton-identification-interval = 1s\n  \n  # If the location of the singleton is unknown the proxy will buffer this\n  # number of messages and deliver them when the singleton is identified. \n  # When the buffer is full old messages will be dropped when new messages are\n  # sent via the proxy.\n  # Use 0 to disable buffering, i.e. messages will be dropped immediately if\n  # the location of the singleton is unknown.\n  # Maximum allowed buffer size is 10000.\n  buffer-size = 1000 \n}","title":"Configuration"},{"location":"/typed/cluster-sharding.html","text":"","title":"Cluster Sharding"},{"location":"/typed/cluster-sharding.html#cluster-sharding","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Cluster Sharding","title":"Cluster Sharding"},{"location":"/typed/cluster-sharding.html#module-info","text":"To use Pekko Cluster Sharding, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-sharding-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-sharding-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-sharding-typed_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster Sharding (typed) Artifact org.apache.pekko pekko-cluster-sharding-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.sharding.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/cluster-sharding.html#introduction","text":"Cluster sharding is useful when you need to distribute actors across several nodes in the cluster and want to be able to interact with them using their logical identifier, but without having to care about their physical location in the cluster, which might also change over time.\nIt could for example be actors representing Aggregate Roots in Domain-Driven Design terminology. Here we call these actors “entities”. These actors typically have persistent (durable) state, but this feature is not limited to actors with persistent state.\nCluster sharding is typically used when you have many stateful actors that together consume more resources (e.g. memory) than fit on one machine. If you only have a few stateful actors it might be easier to run them on a Cluster Singleton node.\nIn this context sharding means that actors with an identifier, so called entities, can be automatically distributed across multiple nodes in the cluster. Each entity actor runs only at one place, and messages can be sent to the entity without requiring the sender to know the location of the destination actor. This is achieved by sending the messages via a ShardRegion actor provided by this extension, which knows how to route the message with the entity id to the final destination.\nCluster sharding will not be active on members with status WeaklyUp if that feature is enabled.\nWarning Make sure to not use a Cluster downing strategy that may split the cluster into several separate clusters in case of network problems or system overload (long GC pauses), since that will result in multiple shards and entities being started, one in each separate cluster! See Downing.","title":"Introduction"},{"location":"/typed/cluster-sharding.html#basic-example","text":"Sharding is accessed via the ClusterShardingClusterSharding extension\nScala copysourceimport org.apache.pekko\nimport pekko.cluster.sharding.typed.ShardingEnvelope\nimport pekko.cluster.sharding.typed.scaladsl.ClusterSharding\nimport pekko.cluster.sharding.typed.scaladsl.EntityTypeKey\nimport pekko.cluster.sharding.typed.scaladsl.EntityRef\n\nval sharding = ClusterSharding(system) Java copysourceimport org.apache.pekko.cluster.sharding.typed.ShardingEnvelope;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.ClusterSharding;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.EntityTypeKey;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.EntityRef;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.Entity;\nimport org.apache.pekko.persistence.typed.PersistenceId;\n\nClusterSharding sharding = ClusterSharding.get(system);\nIt is common for sharding to be used with persistence however any BehaviorBehavior can be used with sharding e.g. a basic counter:\nScala copysourceobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Int]) extends Command\n\n  def apply(entityId: String): Behavior[Command] = {\n    def updated(value: Int): Behavior[Command] = {\n      Behaviors.receiveMessage[Command] {\n        case Increment =>\n          updated(value + 1)\n        case GetValue(replyTo) =>\n          replyTo ! value\n          Behaviors.same\n      }\n    }\n\n    updated(0)\n\n  }\n} Java copysourcepublic class Counter extends AbstractBehavior<Counter.Command> {\n\n  public interface Command {}\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    private final ActorRef<Integer> replyTo;\n\n    public GetValue(ActorRef<Integer> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static Behavior<Command> create(String entityId) {\n    return Behaviors.setup(context -> new Counter(context, entityId));\n  }\n\n  private final String entityId;\n  private int value = 0;\n\n  private Counter(ActorContext<Command> context, String entityId) {\n    super(context);\n    this.entityId = entityId;\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Increment.class, msg -> onIncrement())\n        .onMessage(GetValue.class, this::onGetValue)\n        .build();\n  }\n\n  private Behavior<Command> onIncrement() {\n    value++;\n    return this;\n  }\n\n  private Behavior<Command> onGetValue(GetValue msg) {\n    msg.replyTo.tell(value);\n    return this;\n  }\n}\nEach Entity type has a key that is then used to retrieve an EntityRef for a given entity identifier. Note in the sample’s Counter.applyCounter.create function that the entityId parameter is not called, it is included to demonstrate how one can pass it to an entity. Another way to do this is by sending the entityId as part of the message if needed.\nScala copysourceval TypeKey = EntityTypeKey[Counter.Command](\"Counter\")\n\nval shardRegion: ActorRef[ShardingEnvelope[Counter.Command]] =\n  sharding.init(Entity(TypeKey)(createBehavior = entityContext => Counter(entityContext.entityId))) Java copysourceEntityTypeKey<Counter.Command> typeKey = EntityTypeKey.create(Counter.Command.class, \"Counter\");\n\nActorRef<ShardingEnvelope<Counter.Command>> shardRegion =\n    sharding.init(Entity.of(typeKey, ctx -> Counter.create(ctx.getEntityId())));\nMessages to a specific entity are then sent via an EntityRefEntityRef. The entityId and the name of the Entity’s key can be retrieved from the EntityRef. It is also possible to wrap methods in a ShardingEnvelopeShardingEnvelope or define extractor functions and send messages directly to the shard region.\nScala copysource// With an EntityRef\nval counterOne: EntityRef[Counter.Command] = sharding.entityRefFor(TypeKey, \"counter-1\")\ncounterOne ! Counter.Increment\n\n// Entity id is specified via an `ShardingEnvelope`\nshardRegion ! ShardingEnvelope(\"counter-1\", Counter.Increment) Java copysourceEntityRef<Counter.Command> counterOne = sharding.entityRefFor(typeKey, \"counter-1\");\ncounterOne.tell(Counter.Increment.INSTANCE);\n\nshardRegion.tell(new ShardingEnvelope<>(\"counter-1\", Counter.Increment.INSTANCE));\nCluster sharding initinit should be called on every node for each entity type. Which nodes entity actors are created on can be controlled with roles. init will create a ShardRegion or a proxy depending on whether the node’s role matches the entity’s role.\nThe behavior factory lambda passed to the init method is defined on each node and only used locally, this means it is safe to use it for injecting for example a node local ActorRefActorRef that each sharded actor should have access to or some object that is not possible to serialize.\nSpecifying the role:\nScala copysourcesharding.init(\n  Entity(TypeKey)(createBehavior = entityContext => Counter(entityContext.entityId)).withRole(\"backend\")) Java copysourceEntityTypeKey<Counter.Command> typeKey = EntityTypeKey.create(Counter.Command.class, \"Counter\");\n\nActorRef<ShardingEnvelope<Counter.Command>> shardRegionOrProxy =\n    sharding.init(\n        Entity.of(typeKey, ctx -> Counter.create(ctx.getEntityId())).withRole(\"backend\"));","title":"Basic example"},{"location":"/typed/cluster-sharding.html#a-note-about-entityref-and-serialization","text":"If including EntityRefEntityRef’s in messages or the State/Events of an EventSourcedBehaviorEventSourcedBehavior, those EntityRefs will need to be serialized. The entityId, typeKey, and (in multi-DC use-cases) dataCenter of an EntityRefgetEntityId, getTypeKey, and (in multi-DC use-cases) getDataCenter methods of an EntityRef provide exactly the information needed upon deserialization to regenerate an EntityRef equivalent to the one serialized, given an expected type of messages to send to the entity.\nAt this time, serialization of EntityRefs requires a custom serializer, as the specific EntityTypeKeyEntityTypeKey (including the type of message which the desired entity type accepts) should not simply be encoded in the serialized representation but looked up on the deserializing side.","title":"A note about EntityRef and serialization"},{"location":"/typed/cluster-sharding.html#persistence-example","text":"When using sharding, entities can be moved to different nodes in the cluster. Persistence can be used to recover the state of an actor after it has moved.\nPekko Persistence is based on the single-writer principle, for a particular PersistenceIdPersistenceId only one persistent actor instance should be active. If multiple instances were to persist events at the same time, the events would be interleaved and might not be interpreted correctly on replay. Cluster Sharding is typically used together with persistence to ensure that there is only one active entity for each PersistenceId (entityId).\nHere is an example of a persistent actor that is used as a sharded entity:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.Behavior\nimport pekko.cluster.sharding.typed.scaladsl.EntityTypeKey\nimport pekko.persistence.typed.scaladsl.Effect\n\nobject HelloWorld {\n\n  // Command\n  sealed trait Command extends CborSerializable\n  final case class Greet(whom: String)(val replyTo: ActorRef[Greeting]) extends Command\n  // Response\n  final case class Greeting(whom: String, numberOfPeople: Int) extends CborSerializable\n\n  // Event\n  final case class Greeted(whom: String) extends CborSerializable\n\n  // State\n  final case class KnownPeople(names: Set[String]) extends CborSerializable {\n    def add(name: String): KnownPeople = copy(names = names + name)\n\n    def numberOfPeople: Int = names.size\n  }\n\n  private val commandHandler: (KnownPeople, Command) => Effect[Greeted, KnownPeople] = { (_, cmd) =>\n    cmd match {\n      case cmd: Greet => greet(cmd)\n    }\n  }\n\n  private def greet(cmd: Greet): Effect[Greeted, KnownPeople] =\n    Effect.persist(Greeted(cmd.whom)).thenRun(state => cmd.replyTo ! Greeting(cmd.whom, state.numberOfPeople))\n\n  private val eventHandler: (KnownPeople, Greeted) => KnownPeople = { (state, evt) =>\n    state.add(evt.whom)\n  }\n\n  val TypeKey: EntityTypeKey[Command] =\n    EntityTypeKey[Command](\"HelloWorld\")\n\n  def apply(entityId: String, persistenceId: PersistenceId): Behavior[Command] = {\n    Behaviors.setup { context =>\n      context.log.info(\"Starting HelloWorld {}\", entityId)\n      EventSourcedBehavior(persistenceId, emptyState = KnownPeople(Set.empty), commandHandler, eventHandler)\n    }\n  }\n\n} Java copysourceimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.EntityTypeKey;\nimport org.apache.pekko.persistence.typed.PersistenceId;\nimport org.apache.pekko.persistence.typed.javadsl.CommandHandler;\nimport org.apache.pekko.persistence.typed.javadsl.Effect;\nimport org.apache.pekko.persistence.typed.javadsl.EventHandler;\n\npublic static class HelloWorld\n    extends EventSourcedBehavior<HelloWorld.Command, HelloWorld.Greeted, HelloWorld.KnownPeople> {\n\n  // Command\n  public interface Command extends CborSerializable {}\n\n  public static final class Greet implements Command {\n    public final String whom;\n    public final ActorRef<Greeting> replyTo;\n\n    public Greet(String whom, ActorRef<Greeting> replyTo) {\n      this.whom = whom;\n      this.replyTo = replyTo;\n    }\n  }\n\n  // Response\n  public static final class Greeting implements CborSerializable {\n    public final String whom;\n    public final int numberOfPeople;\n\n    public Greeting(String whom, int numberOfPeople) {\n      this.whom = whom;\n      this.numberOfPeople = numberOfPeople;\n    }\n  }\n\n  // Event\n  public static final class Greeted implements CborSerializable {\n    public final String whom;\n\n    @JsonCreator\n    public Greeted(String whom) {\n      this.whom = whom;\n    }\n  }\n\n  // State\n  static final class KnownPeople implements CborSerializable {\n    private Set<String> names = Collections.emptySet();\n\n    KnownPeople() {}\n\n    private KnownPeople(Set<String> names) {\n      this.names = names;\n    }\n\n    KnownPeople add(String name) {\n      Set<String> newNames = new HashSet<>(names);\n      newNames.add(name);\n      return new KnownPeople(newNames);\n    }\n\n    int numberOfPeople() {\n      return names.size();\n    }\n  }\n\n  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =\n      EntityTypeKey.create(Command.class, \"HelloWorld\");\n\n  public static Behavior<Command> create(String entityId, PersistenceId persistenceId) {\n    return Behaviors.setup(context -> new HelloWorld(context, entityId, persistenceId));\n  }\n\n  private HelloWorld(\n      ActorContext<Command> context, String entityId, PersistenceId persistenceId) {\n    super(persistenceId);\n    context.getLog().info(\"Starting HelloWorld {}\", entityId);\n  }\n\n  @Override\n  public KnownPeople emptyState() {\n    return new KnownPeople();\n  }\n\n  @Override\n  public CommandHandler<Command, Greeted, KnownPeople> commandHandler() {\n    return newCommandHandlerBuilder().forAnyState().onCommand(Greet.class, this::greet).build();\n  }\n\n  private Effect<Greeted, KnownPeople> greet(KnownPeople state, Greet cmd) {\n    return Effect()\n        .persist(new Greeted(cmd.whom))\n        .thenRun(newState -> cmd.replyTo.tell(new Greeting(cmd.whom, newState.numberOfPeople())));\n  }\n\n  @Override\n  public EventHandler<KnownPeople, Greeted> eventHandler() {\n    return (state, evt) -> state.add(evt.whom);\n  }\n}\nTo initialize and use the entity:\nScala copysourceimport org.apache.pekko\nimport pekko.cluster.sharding.typed.scaladsl.ClusterSharding\nimport pekko.cluster.sharding.typed.scaladsl.Entity\nimport pekko.util.Timeout\n\nclass HelloWorldService(system: ActorSystem[_]) {\n  import system.executionContext\n\n  private val sharding = ClusterSharding(system)\n\n  // registration at startup\n  sharding.init(Entity(typeKey = HelloWorld.TypeKey) { entityContext =>\n    HelloWorld(entityContext.entityId, PersistenceId(entityContext.entityTypeKey.name, entityContext.entityId))\n  })\n\n  private implicit val askTimeout: Timeout = Timeout(5.seconds)\n\n  def greet(worldId: String, whom: String): Future[Int] = {\n    val entityRef = sharding.entityRefFor(HelloWorld.TypeKey, worldId)\n    val greeting = entityRef ? HelloWorld.Greet(whom)\n    greeting.map(_.numberOfPeople)\n  }\n\n} Java copysourceimport org.apache.pekko.cluster.sharding.typed.javadsl.ClusterSharding;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.EntityRef;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.Entity;\nimport org.apache.pekko.persistence.typed.javadsl.EventSourcedBehavior;\nimport org.apache.pekko.serialization.jackson.CborSerializable;\nimport org.apache.pekko.util.Timeout;\nimport com.fasterxml.jackson.annotation.JsonCreator;\n\npublic static class HelloWorldService {\n  private final ActorSystem<?> system;\n  private final ClusterSharding sharding;\n  private final Duration askTimeout = Duration.ofSeconds(5);\n\n  // registration at startup\n  public HelloWorldService(ActorSystem<?> system) {\n    this.system = system;\n    sharding = ClusterSharding.get(system);\n\n    // registration at startup\n    sharding.init(\n        Entity.of(\n            HelloWorld.ENTITY_TYPE_KEY,\n            entityContext ->\n                HelloWorld.create(\n                    entityContext.getEntityId(),\n                    PersistenceId.of(\n                        entityContext.getEntityTypeKey().name(), entityContext.getEntityId()))));\n  }\n\n  // usage example\n  public CompletionStage<Integer> sayHello(String worldId, String whom) {\n    EntityRef<HelloWorld.Command> entityRef =\n        sharding.entityRefFor(HelloWorld.ENTITY_TYPE_KEY, worldId);\n    CompletionStage<HelloWorld.Greeting> result =\n        entityRef.ask(replyTo -> new HelloWorld.Greet(whom, replyTo), askTimeout);\n    return result.thenApply(greeting -> greeting.numberOfPeople);\n  }\n}\nNote how an unique PersistenceIdPersistenceId can be constructed from the EntityTypeKeyEntityTypeKey and the entityId provided by the EntityContextEntityContext in the factory function for the BehaviorBehavior. This is a typical way of defining the PersistenceId but other formats are possible, as described in the PersistenceId section.\nSending messages to persistent entities is the same as if the entity wasn’t persistent. The only difference is when an entity is moved the state will be restored. In the above example ask is used but tell or any of the other Interaction Patterns can be used.\nSee persistence for more details.","title":"Persistence example"},{"location":"/typed/cluster-sharding.html#shard-allocation","text":"A shard is a group of entities that will be managed together. The grouping is typically defined by a hashing function of the entityId. For a specific entity identifier the shard identifier must always be the same. Otherwise the entity actor might accidentally be started in several places at the same time.\nBy default the shard identifier is the absolute value of the hashCode of the entity identifier modulo the total number of shards. The number of shards is configured by:\ncopysourcepekko.cluster.sharding {\n  # Number of shards used by the default HashCodeMessageExtractor\n  # when no other message extractor is defined. This value must be\n  # the same for all nodes in the cluster and that is verified by\n  # configuration check when joining. Changing the value requires\n  # stopping all nodes in the cluster.\n  number-of-shards = 1000\n}\nAs a rule of thumb, the number of shards should be a factor ten greater than the planned maximum number of cluster nodes. It doesn’t have to be exact. Fewer shards than number of nodes will result in that some nodes will not host any shards. Too many shards will result in less efficient management of the shards, e.g. rebalancing overhead, and increased latency because the coordinator is involved in the routing of the first message for each shard.\nThe number-of-shards configuration value must be the same for all nodes in the cluster and that is verified by configuration check when joining. Changing the value requires stopping all nodes in the cluster.\nThe shards are allocated to the nodes in the cluster. The decision of where to allocate a shard is done by a shard allocation strategy.\nThe default implementation LeastShardAllocationStrategy allocates new shards to the ShardRegion (node) with least number of previously allocated shards. This strategy can be replaced by an application specific implementation.\nWhen a node is added to the cluster the shards on the existing nodes will be rebalanced to the new node. The LeastShardAllocationStrategy picks shards for rebalancing from the ShardRegions with most number of previously allocated shards. They will then be allocated to the ShardRegion with least number of previously allocated shards, i.e. new members in the cluster. The amount of shards to rebalance in each round can be limited to make it progress slower since rebalancing too many shards at the same time could result in additional load on the system. For example, causing many Event Sourced entites to be started at the same time.\nA new rebalance algorithm is included in Pekko. It can reach optimal balance in a few rebalance rounds (typically 1 or 2 rounds). For backwards compatibility the new algorithm is not enabled by default. The new algorithm is recommended and will become the default in future versions of Pekko. You enable the new algorithm by setting rebalance-absolute-limit > 0, for example:\npekko.cluster.sharding.least-shard-allocation-strategy.rebalance-absolute-limit = 20\nThe rebalance-absolute-limit is the maximum number of shards that will be rebalanced in one rebalance round.\nYou may also want to tune the pekko.cluster.sharding.least-shard-allocation-strategy.rebalance-relative-limit. The rebalance-relative-limit is a fraction (< 1.0) of total number of (known) shards that will be rebalanced in one rebalance round. The lower result of rebalance-relative-limit and rebalance-absolute-limit will be used.","title":"Shard allocation"},{"location":"/typed/cluster-sharding.html#external-shard-allocation","text":"An alternative allocation strategy is the ExternalShardAllocationStrategyExternalShardAllocationStrategy which allows explicit control over where shards are allocated via the ExternalShardAllocationExternalShardAllocation extension.\nThis can be used, for example, to match up Kafka Partition consumption with shard locations. Pekko Connector Kafka provides an extension for Pekko Cluster Sharding.\nTo use it set it as the allocation strategy on your EntityEntity:\nScala copysourceval TypeKey = EntityTypeKey[Counter.Command](\"Counter\")\n\nval entity = Entity(TypeKey)(createBehavior = entityContext => Counter(entityContext.entityId))\n  .withAllocationStrategy(new ExternalShardAllocationStrategy(system, TypeKey.name)) Java copysourceEntityTypeKey<Counter.Command> typeKey = EntityTypeKey.create(Counter.Command.class, \"Counter\");\n\nActorRef<ShardingEnvelope<Counter.Command>> shardRegion =\n    sharding.init(Entity.of(typeKey, ctx -> Counter.create(ctx.getEntityId())));\nFor any shardId that has not been allocated it will be allocated to the requesting node. To make explicit allocations:\nScala copysourceval client: ExternalShardAllocationClient = ExternalShardAllocation(system).clientFor(TypeKey.name)\nval done: Future[Done] = client.updateShardLocation(\"shard-id-1\", Address(\"akka\", \"system\", \"127.0.0.1\", 2552)) Java copysourceExternalShardAllocationClient client =\n    ExternalShardAllocation.get(system).getClient(typeKey.name());\nCompletionStage<Done> done =\n    client.setShardLocation(\"shard-id-1\", new Address(\"akka\", \"system\", \"127.0.0.1\", 2552));\nAny new or moved shard allocations will be moved on the next rebalance.\nThe communication from the client to the shard allocation strategy is via Distributed Data. It uses a single LWWMapLWWMap that can support 10s of thousands of shards. Later versions could use multiple keys to support a greater number of shards.","title":"External shard allocation"},{"location":"/typed/cluster-sharding.html#example-project-for-external-allocation-strategy","text":"Kafka to Cluster Sharding is an example project that can be downloaded, and with instructions of how to run, that demonstrates how to use external sharding to co-locate Kafka partition consumption with shards.","title":"Example project for external allocation strategy"},{"location":"/typed/cluster-sharding.html#custom-shard-allocation","text":"An optional custom shard allocation strategy can be passed into the optional parameter when initializing an entity type or explicitly using the withAllocationStrategywithAllocationStrategy function. See the API documentation of ShardAllocationStrategyAbstractShardAllocationStrategy for details of how to implement a custom ShardAllocationStrategy.","title":"Custom shard allocation"},{"location":"/typed/cluster-sharding.html#how-it-works","text":"See Cluster Sharding concepts.","title":"How it works"},{"location":"/typed/cluster-sharding.html#passivation","text":"If the state of the entities are persistent you may stop entities that are not used to reduce memory consumption. This is done by the application specific implementation of the entity actors for example by defining receive timeout (context.setReceiveTimeoutcontext.setReceiveTimeout). If a message is already enqueued to the entity when it stops itself the enqueued message in the mailbox will be dropped. To support graceful passivation without losing such messages the entity actor can send ClusterSharding.PassivateClusterSharding.Passivate to the ActorRefActorRef[ShardCommand]<ShardCommand> that was passed in to the factory method when creating the entity. The optional stopMessage message will be sent back to the entity, which is then supposed to stop itself, otherwise it will be stopped automatically. Incoming messages will be buffered by the Shard between reception of Passivate and termination of the entity. Such buffered messages are thereafter delivered to a new incarnation of the entity.\nScala copysourceobject Counter {\n  sealed trait Command\n  case object Increment extends Command\n  final case class GetValue(replyTo: ActorRef[Int]) extends Command\n  private case object Idle extends Command\n  case object GoodByeCounter extends Command\n\n  def apply(shard: ActorRef[ClusterSharding.ShardCommand], entityId: String): Behavior[Command] = {\n    Behaviors.setup { ctx =>\n      def updated(value: Int): Behavior[Command] =\n        Behaviors.receiveMessage[Command] {\n          case Increment =>\n            updated(value + 1)\n          case GetValue(replyTo) =>\n            replyTo ! value\n            Behaviors.same\n          case Idle =>\n            // after receive timeout\n            shard ! ClusterSharding.Passivate(ctx.self)\n            Behaviors.same\n          case GoodByeCounter =>\n            // the stopMessage, used for rebalance and passivate\n            Behaviors.stopped\n        }\n\n      ctx.setReceiveTimeout(30.seconds, Idle)\n      updated(0)\n    }\n  }\n} Java copysourcepublic class Counter2 extends AbstractBehavior<Counter2.Command> {\n\n  public interface Command {}\n\n  private enum Idle implements Command {\n    INSTANCE\n  }\n\n  public enum GoodByeCounter implements Command {\n    INSTANCE\n  }\n\n  public enum Increment implements Command {\n    INSTANCE\n  }\n\n  public static class GetValue implements Command {\n    private final ActorRef<Integer> replyTo;\n\n    public GetValue(ActorRef<Integer> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static Behavior<Command> create(\n      ActorRef<ClusterSharding.ShardCommand> shard, String entityId) {\n    return Behaviors.setup(\n        ctx -> {\n          ctx.setReceiveTimeout(Duration.ofSeconds(30), Idle.INSTANCE);\n          return new Counter2(ctx, shard, entityId);\n        });\n  }\n\n  private final ActorRef<ClusterSharding.ShardCommand> shard;\n  private final String entityId;\n  private int value = 0;\n\n  private Counter2(\n      ActorContext<Command> context,\n      ActorRef<ClusterSharding.ShardCommand> shard,\n      String entityId) {\n    super(context);\n    this.shard = shard;\n    this.entityId = entityId;\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(Increment.class, msg -> onIncrement())\n        .onMessage(GetValue.class, this::onGetValue)\n        .onMessage(Idle.class, msg -> onIdle())\n        .onMessage(GoodByeCounter.class, msg -> onGoodByeCounter())\n        .build();\n  }\n\n  private Behavior<Command> onIncrement() {\n    value++;\n    return this;\n  }\n\n  private Behavior<Command> onGetValue(GetValue msg) {\n    msg.replyTo.tell(value);\n    return this;\n  }\n\n  private Behavior<Command> onIdle() {\n    // after receive timeout\n    shard.tell(new ClusterSharding.Passivate<>(getContext().getSelf()));\n    return this;\n  }\n\n  private Behavior<Command> onGoodByeCounter() {\n    // the stopMessage, used for rebalance and passivate\n    return Behaviors.stopped();\n  }\n}\nand then initialized with:\nScala copysourceval TypeKey = EntityTypeKey[Counter.Command](\"Counter\")\n\nClusterSharding(system).init(Entity(TypeKey)(createBehavior = entityContext =>\n  Counter(entityContext.shard, entityContext.entityId)).withStopMessage(Counter.GoodByeCounter)) Java copysource EntityTypeKey<Counter2.Command> typeKey =\n    EntityTypeKey.create(Counter2.Command.class, \"Counter\");\n\nsharding.init(\n    Entity.of(typeKey, ctx -> Counter2.create(ctx.getShard(), ctx.getEntityId()))\n        .withStopMessage(Counter2.GoodByeCounter.INSTANCE));\nNote that in the above example the stopMessage is specified as GoodByeCounter. That message will be sent to the entity when it’s supposed to stop itself due to rebalance or passivation. If the stopMessage is not defined it will be stopped automatically without receiving a specific message. It can be useful to define a custom stop message if the entity needs to perform some asynchronous cleanup or interactions before stopping.\nThe stop message is only sent locally, from the shard to the entity so does not require an entity id to end up in the right actor. When using a custom ShardingMessageExtractorShardingMessageExtractor without envelopes, the extractor will still have to handle the stop message type to please the compiler, even though it will never actually be passed to the extractor.","title":"Passivation"},{"location":"/typed/cluster-sharding.html#automatic-passivation","text":"Entities are automatically passivated based on a passivation strategy. The default passivation strategy is to passivate idle entities when they haven’t received a message within a specified interval, and this is the current default strategy to maintain compatibility with earlier versions. It’s recommended to switch to a passivation strategy with an active entity limit and a pre-configured default strategy is provided. Active entity limits and idle entity timeouts can also be used together.\nNote The automatic passivation strategies, except passivate idle entities are marked as may change in the sense of being the subject of final development. This means that the configuration or semantics can change without warning or deprecation period. The passivation strategies can be used in production, but we reserve the right to adjust the configuration after additional testing and feedback.\nAutomatic passivation can be disabled by setting pekko.cluster.sharding.passivation.strategy = none. It is disabled automatically if Remembering Entities is enabled.\nNote Only messages sent through Cluster Sharding are counted as entity activity for automatic passivation. Messages sent directly to the ActorRefActorRef, including messages that the actor sends to itself, are not counted as entity activity.","title":"Automatic Passivation"},{"location":"/typed/cluster-sharding.html#idle-entity-passivation","text":"Idle entities can be automatically passivated when they have not received a message for a specified length of time. This is currently the default strategy, for compatibility, and is enabled automatically with a timeout of 2 minutes. Specify a different idle timeout with configuration:\ncopysourcepekko.cluster.sharding.passivation {\n  default-idle-strategy.idle-entity.timeout = 3 minutes\n}\nOr specify the idle timeout as a duration using the withPassivationStrategywithPassivationStrategy method on ClusterShardingSettings.\nIdle entity timeouts can be enabled and configured for any passivation strategy.","title":"Idle entity passivation"},{"location":"/typed/cluster-sharding.html#active-entity-limits","text":"Automatic passivation strategies can limit the number of active entities. Limit-based passivation strategies use a replacement policy to determine which active entities should be passivated when the active entity limit is exceeded. The configurable limit is for a whole shard region and is divided evenly among the active shards in each region.\nA recommended passivation strategy, which will become the new default passivation strategy in future versions of Pekko Cluster Sharding, can be enabled with configuration:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = default-strategy\n}\nThis default strategy uses a composite passivation strategy which combines recency-based and frequency-based tracking: the main area is configured with a segmented least recently used policy with a frequency-biased admission filter, fronted by a recency-biased admission window with adaptive sizing enabled.\nThe active entity limit for the default strategy can be configured:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = default-strategy\n  default-strategy {\n    active-entity-limit = 1000000\n  }\n}\nOr using the withActiveEntityLimitwithActiveEntityLimit method on ClusterShardingSettings.PassivationStrategySettings.\nAn idle entity timeout can also be enabled and configured for this strategy:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = default-strategy\n  default-strategy {\n    idle-entity.timeout = 30.minutes\n  }\n}\nOr using the withIdleEntityPassivationwithIdleEntityPassivation method on ClusterShardingSettings.PassivationStrategySettings.\nIf the default strategy is not appropriate for particular workloads and access patterns, a custom passivation strategy can be created with configurable replacement policies, active entity limits, and idle entity timeouts.","title":"Active entity limits"},{"location":"/typed/cluster-sharding.html#custom-passivation-strategies","text":"To configure a custom passivation strategy, create a configuration section for the strategy under pekko.cluster.sharding.passivation and select this strategy using the strategy setting. The strategy needs a replacement policy to be chosen, an active entity limit to be set, and can optionally passivate idle entities. For example, a custom strategy can be configured to use the least recently used policy:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-lru-strategy\n  custom-lru-strategy {\n    active-entity-limit = 1000000\n    replacement.policy = least-recently-used\n  }\n}\nThe active entity limit and replacement policy can also be configured using the withPassivationStrategy method on ClusterShardingSettings, passing custom ClusterShardingSettings.PassivationStrategySettings.","title":"Custom passivation strategies"},{"location":"/typed/cluster-sharding.html#least-recently-used-policy","text":"The least recently used policy passivates those entities that have the least recent activity when the number of active entities passes the specified limit.\nWhen to use: the least recently used policy should be used when access patterns are recency biased, where entities that were recently accessed are likely to be accessed again. See the segmented least recently used policy for a variation that also distinguishes frequency of access.\nConfigure a passivation strategy to use the least recently used policy:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-lru-strategy\n  custom-lru-strategy {\n    active-entity-limit = 1000000\n    replacement.policy = least-recently-used\n  }\n}\nOr using the withLeastRecentlyUsedReplacementwithLeastRecentlyUsedReplacement method on ClusterShardingSettings.PassivationStrategySettings.","title":"Least recently used policy"},{"location":"/typed/cluster-sharding.html#segmented-least-recently-used-policy","text":"A variation of the least recently used policy can be enabled that divides the active entity space into multiple segments to introduce frequency information into the passivation strategy. Higher-level segments contain entities that have been accessed more often. The first segment is for entities that have only been accessed once, the second segment for entities that have been accessed at least twice, and so on. When an entity is accessed again, it will be promoted to the most recent position of the next-level or highest-level segment. The higher-level segments are limited, where the total limit is either evenly divided among segments, or proportions of the segments can be configured. When a higher-level segment exceeds its limit, the least recently used active entity tracked in that segment will be demoted to the level below. Only the least recently used entities in the lowest level will be candidates for passivation. The higher levels are considered “protected”, where entities will have additional opportunities to be accessed before being considered for passivation.\nWhen to use: the segmented least recently used policy can be used for workloads where some entities are more popular than others, to prioritize those entities that are accessed more frequently.\nTo configure a segmented least recently used (SLRU) policy, with two levels and a protected segment limited to 80% of the total limit:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-slru-strategy\n  custom-slru-strategy {\n    active-entity-limit = 1000000\n    replacement {\n      policy = least-recently-used\n      least-recently-used {\n        segmented {\n          levels = 2\n          proportions = [0.2, 0.8]\n        }\n      }\n    }\n  }\n}\nOr to configure a 4-level segmented least recently used (S4LRU) policy, with 4 evenly divided levels:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-s4lru-strategy\n  custom-s4lru-strategy {\n    active-entity-limit = 1000000\n    replacement {\n      policy = least-recently-used\n      least-recently-used {\n        segmented.levels = 4\n      }\n    }\n  }\n}\nOr using custom ClusterShardingSettings.PassivationStrategySettings.LeastRecentlyUsedSettings.","title":"Segmented least recently used policy"},{"location":"/typed/cluster-sharding.html#most-recently-used-policy","text":"The most recently used policy passivates those entities that have the most recent activity when the number of active entities passes the specified limit.\nWhen to use: the most recently used policy is most useful when the older an entity is, the more likely that entity will be accessed again; as seen in cyclic access patterns.\nConfigure a passivation strategy to use the most recently used policy:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-mru-strategy\n  custom-mru-strategy {\n    active-entity-limit = 1000000\n    replacement.policy = most-recently-used\n  }\n}\nOr using the withMostRecentlyUsedReplacementwithMostRecentlyUsedReplacement method on ClusterShardingSettings.PassivationStrategySettings.","title":"Most recently used policy"},{"location":"/typed/cluster-sharding.html#least-frequently-used-policy","text":"The least frequently used policy passivates those entities that have the least frequent activity when the number of active entities passes the specified limit.\nWhen to use: the least frequently used policy should be used when access patterns are frequency biased, where some entities are much more popular than others and should be prioritized. See the least frequently used with dynamic aging policy for a variation that also handles shifts in popularity.\nConfigure automatic passivation to use the least frequently used policy:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-lfu-strategy\n  custom-lfu-strategy {\n    active-entity-limit = 1000000\n    replacement.policy = least-frequently-used\n  }\n}\nOr using the withLeastFrequentlyUsedReplacementwithLeastFrequentlyUsedReplacement method on ClusterShardingSettings.PassivationStrategySettings.","title":"Least frequently used policy"},{"location":"/typed/cluster-sharding.html#least-frequently-used-with-dynamic-aging-policy","text":"A variation of the least frequently used policy can be enabled that uses “dynamic aging” to adapt to shifts in the set of popular entities, which is useful for smaller active entity limits and when shifts in popularity are common. If entities were frequently accessed in the past but then become unpopular, they can still remain active for a long time given their high frequency counts. Dynamic aging effectively increases the frequencies for recently accessed entities so they can more easily become higher priority over entities that are no longer accessed.\nWhen to use: the least frequently used with dynamic aging policy can be used when workloads are frequency biased (there are some entities that are much more popular), but which entities are most popular changes over time. Shifts in popularity can have more impact on a least frequently used policy if the active entity limit is small.\nConfigure dynamic aging with the least frequently used policy:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-lfu-with-dynamic-aging\n  custom-lfu-with-dynamic-aging {\n    active-entity-limit = 1000\n    replacement {\n      policy = least-frequently-used\n      least-frequently-used {\n        dynamic-aging = on\n      }\n    }\n  }\n}\nOr using custom ClusterShardingSettings.PassivationStrategySettings.LeastFrequentlyUsedSettings.","title":"Least frequently used with dynamic aging policy"},{"location":"/typed/cluster-sharding.html#composite-passivation-strategies","text":"Passivation strategies can be combined using an admission window and admission filter. The admission window tracks newly activated entities. Entities are replaced in the admission window using one of the replacement policies, such as the least recently used replacement policy. When an entity is replaced in the window area it has an opportunity to enter the main entity tracking area, based on the admission filter. The admission filter determines whether an entity that has left the window area should be admitted into the main area, or otherwise be passivated. A frequency sketch is the default admission filter and estimates the access frequency of entities over the lifespan of the cluster sharding node, selecting the entity that is estimated to be accessed more frequently. Composite passivation strategies with an admission window and admission filter are implementing the Window-TinyLFU caching algorithm.","title":"Composite passivation strategies"},{"location":"/typed/cluster-sharding.html#admission-window-policy","text":"The admission window tracks newly activated entities. When an entity is replaced in the window area, it has an opportunity to enter the main entity tracking area, based on the admission filter. The admission window can be enabled by selecting a policy (while the regular replacement policy is for the main area):\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-strategy-with-admission-window\n  custom-strategy-with-admission-window {\n    active-entity-limit = 1000000\n    admission.window.policy = least-recently-used\n    replacement.policy = least-frequently-used\n  }\n}\nThe proportion of the active entity limit used for the admission window can be configured (the default is 1%):\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-strategy-with-admission-window\n  custom-strategy-with-admission-window {\n    active-entity-limit = 1000000\n    admission.window {\n      policy = least-recently-used\n      proportion = 0.1 # 10%\n    }\n    replacement.policy = least-frequently-used\n  }\n}\nThe proportion for the admission window can also be adapted and optimized dynamically, by enabling an admission window optimizer.","title":"Admission window policy"},{"location":"/typed/cluster-sharding.html#admission-window-optimizer","text":"The proportion of the active entity limit used for the admission window can be adapted dynamically using an optimizer. The window area will usually retain entities that are accessed again in a short time (recency-biased), while the main area can track entities that are accessed more frequently over longer times (frequency-biased). If access patterns for entities are changeable, then the adaptive sizing of the window allows the passivation strategy to adapt between recency-biased and frequency-biased workloads.\nThe optimizer currently available uses a simple hill-climbing algorithm, which searches for a window proportion that provides an optimal active rate (where entities are already active when accessed, the cache hit rate). Enable adaptive window sizing by configuring the hill-climbing window optimizer:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-strategy-with-admission-window\n  custom-strategy-with-admission-window {\n    active-entity-limit = 1000000\n    admission.window {\n      policy = least-recently-used\n      optimizer = hill-climbing\n    }\n    replacement.policy = least-frequently-used\n  }\n}\nSee the reference.conf for parameters that can be tuned for the hill climbing admission window optimizer.","title":"Admission window optimizer"},{"location":"/typed/cluster-sharding.html#admission-filter","text":"An admission filter can be enabled, which determines whether an entity that has left the window area (or a newly activated entity if there is no admission window) should be admitted into the main entity tracking area, or otherwise be passivated. If no admission filter is configured, then entities will always be admitted into the main area.\nA frequency sketch is the default admission filter and estimates the access frequency of entities over the lifespan of the cluster sharding node, selecting the entity that is estimated to be accessed more frequently. The frequency sketch automatically ages entries, using the approach from the TinyLFU cache admission algorithm. Enable an admission filter by configuring the frequency-sketch admission filter:\ncopysourcepekko.cluster.sharding.passivation {\n  strategy = custom-strategy-with-admission\n  custom-strategy-with-admission {\n    active-entity-limit = 1000000\n    admission {\n      window {\n        policy = least-recently-used\n        optimizer = hill-climbing\n      }\n      filter = frequency-sketch\n    }\n    replacement {\n      policy = least-recently-used\n      least-recently-used {\n        segmented {\n          levels = 2\n          proportions = [0.2, 0.8]\n        }\n      }\n    }\n  }\n}\nSee the reference.conf for parameters that can be tuned for the frequency sketch admission filter.","title":"Admission filter"},{"location":"/typed/cluster-sharding.html#sharding-state","text":"There are two types of state managed:\nShardCoordinator State - the Shard locations. This is stored in the State Store. Remembering Entities - the active shards and the entities in each Shard, which is optional, and disabled by default. This is stored in the Remember Entities Store.","title":"Sharding State"},{"location":"/typed/cluster-sharding.html#state-store","text":"A state store is mandatory for sharding, it contains the location of shards. The ShardCoordinator needs to load this state after it moves between nodes.\nThere are two options for the state store:\nDistributed Data Mode - uses Pekko Distributed Data (CRDTs) (the default) Persistence Mode - (deprecated) uses Pekko Persistence (Event Sourcing)\nWarning Persistence for state store mode is deprecated. It is recommended to migrate to ddata for the coordinator state and if using replicated entities migrate to eventsourced for the replicated entities state. The data written by the deprecated persistence state store mode for remembered entities can be read by the new remember entities eventsourced mode. Once you’ve migrated you can not go back to persistence mode.","title":"State Store"},{"location":"/typed/cluster-sharding.html#distributed-data-mode","text":"To enable distributed data store mode (the default):\npekko.cluster.sharding.state-store-mode = ddata\nThe state of the ShardCoordinator is replicated across the cluster but is not stored to disk. Distributed Data handles the ShardCoordinator’s state with WriteMajorityPlusWriteMajorityPlus / ReadMajorityPlusReadMajorityPlus consistency. When all nodes in the cluster have been stopped, the state is no longer needed and dropped.\nCluster Sharding uses its own Distributed Data ReplicatorReplicator per node. If using roles with sharding there is one Replicator per role, which enables a subset of all nodes for some entity types and another subset for other entity types. Each replicator has a name that contains the node role and therefore the role configuration must be the same on all nodes in the cluster, for example you can’t change the roles when performing a rolling update. Changing roles requires a full cluster restart.\nThe pekko.cluster.sharding.distributed-data config section configures the settings for Distributed Data. It’s not possible to have different distributed-data settings for different sharding entity types.","title":"Distributed Data Mode"},{"location":"/typed/cluster-sharding.html#persistence-mode","text":"To enable persistence store mode:\npekko.cluster.sharding.state-store-mode = persistence\nSince it is running in a cluster Persistence must be configured with a distributed journal.\nWarning Persistence mode for Remembering Entities has been replaced by a remember entities state mode. It should not be used for new projects and existing projects should migrate as soon as possible.","title":"Persistence mode"},{"location":"/typed/cluster-sharding.html#remembering-entities","text":"Remembering entities automatically restarts entities after a rebalance or entity crash. Without remembered entities restarts happen on the arrival of a message.\nEnabling remembered entities disables Automatic Passivation.\nThe state of the entities themselves is not restored unless they have been made persistent, for example with Event Sourcing.\nTo enable remember entities set rememberEntities flag to true in ClusterShardingSettingsClusterShardingSettings when starting a shard region (or its proxy) for a given entity type or configure pekko.cluster.sharding.remember-entities = on.\nStarting and stopping entities has an overhead but this is limited by batching operations to the underlying remember entities store.","title":"Remembering Entities"},{"location":"/typed/cluster-sharding.html#behavior-when-enabled","text":"When rememberEntities is enabled, whenever a Shard is rebalanced onto another node or recovers after a crash, it will recreate all the entities which were previously running in that Shard.\nTo permanently stop entities send a ClusterSharding.PassivateClusterSharding.Passivate to the ActorRefActorRef[ShardCommand]<ShardCommand> that was passed in to the factory method when creating the entity. Otherwise, the entity will be automatically restarted after the entity restart backoff specified in the configuration.","title":"Behavior When Enabled"},{"location":"/typed/cluster-sharding.html#remember-entities-store","text":"There are two options for the remember entities store:\nddata eventsourced","title":"Remember entities store"},{"location":"/typed/cluster-sharding.html#remember-entities-distributed-data-mode","text":"Enable ddata mode with (enabled by default):\npekko.cluster.sharding.remember-entities-store = ddata\nTo support restarting entities after a full cluster restart (non-rolling) the remember entities store is persisted to disk by distributed data. This can be disabled if not needed:\npekko.cluster.sharding.distributed-data.durable.keys = []\nReasons for disabling:\nNo requirement for remembering entities after a full cluster shutdown Running in an environment without access to disk between restarts e.g. Kubernetes without persistent volumes\nFor supporting remembered entities in an environment without disk storage use eventsourced mode instead.","title":"Remember entities distributed data mode"},{"location":"/typed/cluster-sharding.html#event-sourced-mode","text":"Enable eventsourced mode with:\npekko.cluster.sharding.remember-entities-store = eventsourced\nThis mode uses Event Sourcing to store the active shards and active entities for each shard so a persistence and snapshot plugin must be configured.\npekko.cluster.sharding.journal-plugin-id = <plugin>\npekko.cluster.sharding.snapshot-plugin-id = <plugin>","title":"Event sourced mode"},{"location":"/typed/cluster-sharding.html#migrating-from-deprecated-persistence-mode","text":"If not using remembered entities you can migrate to ddata with a full cluster restart.\nIf using remembered entities there are two migration options:\nddata for the state store and ddata for remembering entities. All remembered entities will be lost after a full cluster restart. ddata for the state store and eventsourced for remembering entities. The new eventsourced remembering entities store reads the data written by the old persistence mode. Your remembered entities will be remembered after a full cluster restart.\nFor migrating existing remembered entities an event adapter needs to be configured in the config for the journal you use in your application.conf. In this example cassandra is the used journal:\npekko.persistence.cassandra.journal {\n  event-adapters {\n    coordinator-migration = \"org.apache.pekko.cluster.sharding.OldCoordinatorStateMigrationEventAdapter\"\n  }\n\n  event-adapter-bindings {\n    \"org.apache.pekko.cluster.sharding.ShardCoordinator$Internal$DomainEvent\" = coordinator-migration\n  }\n}\nOnce you have migrated you cannot go back to the old persistence store, a rolling update is therefore not possible.\nWhen Distributed Data mode is used the identifiers of the entities are stored in Durable Storage of Distributed Data. You may want to change the configuration of the pekko.cluster.sharding.distributed-data.durable.lmdb.dir, since the default directory contains the remote port of the actor system. If using a dynamically assigned port (0) it will be different each time and the previously stored data will not be loaded.\nThe reason for storing the identifiers of the active entities in durable storage, i.e. stored to disk, is that the same entities should be started also after a complete cluster restart. If this is not needed you can disable durable storage and benefit from better performance by using the following configuration:\npekko.cluster.sharding.distributed-data.durable.keys = []","title":"Migrating from deprecated persistence mode"},{"location":"/typed/cluster-sharding.html#startup-after-minimum-number-of-members","text":"It’s recommended to use Cluster Sharding with the Cluster setting pekko.cluster.min-nr-of-members or pekko.cluster.role.<role-name>.min-nr-of-members. min-nr-of-members will defer the allocation of the shards until at least that number of regions have been started and registered to the coordinator. This avoids that many shards are allocated to the first region that registers and only later are rebalanced to other nodes.\nSee How To Startup when Cluster Size Reached for more information about min-nr-of-members.","title":"Startup after minimum number of members"},{"location":"/typed/cluster-sharding.html#health-check","text":"An Pekko Management compatible health check is included that returns healthy once the local shard region has registered with the coordinator. This health check should be used in cases where you don’t want to receive production traffic until the local shard region is ready to retrieve locations for shards. For shard regions that aren’t critical and therefore should not block this node becoming ready do not include them.\nThe health check does not fail after an initial successful check. Once a shard region is registered and is operational it stays available for incoming message.\nCluster sharding enables the health check automatically. To disable:\npekko.management.health-checks.readiness-checks {\n  sharding = \"\"\n}\nMonitoring of each shard region is off by default. Add them by defining the entity type names (EntityTypeKey.name):\npekko.cluster.sharding.healthcheck.names = [\"counter-1\", \"HelloWorld\"]\nSee also additional information about how to make smooth rolling updates.","title":"Health check"},{"location":"/typed/cluster-sharding.html#inspecting-cluster-sharding-state","text":"Two requests to inspect the cluster state are available:\nGetShardRegionStateGetShardRegionState which will reply with a ShardRegion.CurrentShardRegionStateShardRegion.CurrentShardRegionState that contains the identifiers of the shards running in a Region and what entities are alive for each of them.\nScala copysourceimport org.apache.pekko\nimport pekko.cluster.sharding.typed.GetShardRegionState\nimport pekko.cluster.sharding.ShardRegion.CurrentShardRegionState\n\nval replyTo: ActorRef[CurrentShardRegionState] = replyMessageAdapter\n\nClusterSharding(system).shardState ! GetShardRegionState(Counter.TypeKey, replyTo) Java copysourceimport org.apache.pekko.cluster.sharding.typed.GetShardRegionState;\nimport org.apache.pekko.cluster.sharding.ShardRegion.CurrentShardRegionState;\n\nActorRef<CurrentShardRegionState> replyTo = replyMessageAdapter;\n\nClusterSharding.get(system).shardState().tell(new GetShardRegionState(typeKey, replyTo));\nGetClusterShardingStatsGetClusterShardingStats which will query all the regions in the cluster and reply with a ShardRegion.ClusterShardingStatsShardRegion.ClusterShardingStats containing the identifiers of the shards running in each region and a count of entities that are alive in each shard.\nScala copysourceimport org.apache.pekko\nimport pekko.cluster.sharding.typed.GetClusterShardingStats\nimport pekko.cluster.sharding.ShardRegion.ClusterShardingStats\nimport scala.concurrent.duration._\n\nval replyTo: ActorRef[ClusterShardingStats] = replyMessageAdapter\nval timeout: FiniteDuration = 5.seconds\n\nClusterSharding(system).shardState ! GetClusterShardingStats(Counter.TypeKey, timeout, replyTo) Java copysourceimport org.apache.pekko.cluster.sharding.typed.GetClusterShardingStats;\nimport org.apache.pekko.cluster.sharding.ShardRegion.ClusterShardingStats;\n\nActorRef<ClusterShardingStats> replyTo = replyMessageAdapter;\nDuration timeout = Duration.ofSeconds(5);\n\nClusterSharding.get(system)\n    .shardState()\n    .tell(new GetClusterShardingStats(typeKey, timeout, replyTo));\nIf any shard queries failed, for example due to timeout if a shard was too busy to reply within the configured pekko.cluster.sharding.shard-region-query-timeout, ShardRegion.CurrentShardRegionState and ShardRegion.ClusterShardingStats will also include the set of shard identifiers by region that failed.\nThe purpose of these messages is testing and monitoring, they are not provided to give access to directly sending messages to the individual entities.","title":"Inspecting cluster sharding state"},{"location":"/typed/cluster-sharding.html#lease","text":"A lease can be used as an additional safety measure to ensure a shard does not run on two nodes.\nReasons for how this can happen:\nNetwork partitions without an appropriate downing provider Mistakes in the deployment process leading to two separate Pekko Clusters Timing issues between removing members from the Cluster on one side of a network partition and shutting them down on the other side\nA lease can be a final backup that means that each shard won’t create child entity actors unless it has the lease.\nTo use a lease for sharding set pekko.cluster.sharding.use-lease to the configuration location of the lease to use. Each shard will try and acquire a lease with with the name <actor system name>-shard-<type name>-<shard id> and the owner is set to the Cluster(system).selfAddress.hostPort.\nIf a shard can’t acquire a lease it will remain uninitialized so messages for entities it owns will be buffered in the ShardRegion. If the lease is lost after initialization the Shard will be terminated.","title":"Lease"},{"location":"/typed/cluster-sharding.html#removal-of-internal-cluster-sharding-data","text":"Removal of internal Cluster Sharding data is only relevant for “Persistent Mode”. The Cluster Sharding ShardCoordinator stores locations of the shards. This data is safely be removed when restarting the whole Pekko Cluster. Note that this does not include application data.\nThere is a utility program RemoveInternalClusterShardingDataRemoveInternalClusterShardingData that removes this data.\nWarning Never use this program while there are running Pekko Cluster nodes that are using Cluster Sharding. Stop all Cluster nodes before using this program.\nIt can be needed to remove the data if the Cluster Sharding coordinator cannot startup because of corrupt data, which may happen if accidentally two clusters were running at the same time, e.g. caused by an invalid downing provider when there was a network partition.\nUse this program as a standalone Java main program:\njava -classpath <jar files, including pekko-cluster-sharding>\n  org.apache.pekko.cluster.sharding.RemoveInternalClusterShardingData\n    -2.3 entityType1 entityType2 entityType3\nThe program is included in the pekko-cluster-sharding jar file. It is easiest to run it with same classpath and configuration as your ordinary application. It can be run from sbt or Maven in similar way.\nSpecify the entity type names (same as you use in the init method of ClusterSharding) as program arguments.","title":"Removal of internal Cluster Sharding data"},{"location":"/typed/cluster-sharding.html#configuration","text":"The ClusterShardingClusterSharding extension can be configured with the following properties. These configuration properties are read by the ClusterShardingSettingsClusterShardingSettings when created with an ActorSystem parameter. It is also possible to amend the ClusterShardingSettings or create it from another config section with the same layout as below.\nOne important configuration property is number-of-shards as described in Shard allocation.\nYou may also need to tune the configuration properties is rebalance-absolute-limit and rebalance-relative-limit as described in Shard allocation.\ncopysource# Settings for the ClusterShardingExtension\npekko.cluster.sharding {\n\n  # The extension creates a top level actor with this name in top level system scope,\n  # e.g. '/system/sharding'\n  guardian-name = sharding\n\n  # Specifies that entities run on cluster nodes with a specific role.\n  # If the role is not specified (or empty) all nodes in the cluster are used.\n  role = \"\"\n\n  # When this is set to 'on' the active entity actors will automatically be restarted\n  # upon Shard restart. i.e. if the Shard is started on a different ShardRegion\n  # due to rebalance or crash.\n  remember-entities = off\n\n  # When 'remember-entities' is enabled and the state store mode is ddata this controls\n  # how the remembered entities and shards are stored. Possible values are \"eventsourced\" and \"ddata\"\n  # Default is ddata for backwards compatibility.\n  remember-entities-store = \"ddata\"\n\n  # Deprecated: use the `passivation.default-idle-strategy.idle-entity.timeout` setting instead.\n  # Set this to a time duration to have sharding passivate entities when they have not\n  # received any message in this length of time. Set to 'off' to disable.\n  # It is always disabled if `remember-entities` is enabled.\n  passivate-idle-entity-after = null\n\n  # Automatic entity passivation settings.\n  passivation {\n\n    # Automatic passivation strategy to use.\n    # Set to \"none\" or \"off\" to disable automatic passivation.\n    # Set to \"default-strategy\" to switch to the recommended default strategy with an active entity limit.\n    # See the strategy-defaults section for possible passivation strategy settings and default values.\n    # Passivation strategies are always disabled if `remember-entities` is enabled.\n    #\n    # API MAY CHANGE: Configuration for passivation strategies, except default-idle-strategy,\n    # may change after additional testing and feedback.\n    strategy = \"default-idle-strategy\"\n\n    # Default passivation strategy without active entity limit; time out idle entities after 2 minutes.\n    default-idle-strategy {\n      idle-entity.timeout = 120s\n    }\n\n    # Recommended default strategy for automatic passivation with an active entity limit.\n    # Configured with an adaptive recency-based admission window, a frequency-based admission filter, and\n    # a segmented least recently used (SLRU) replacement policy for the main active entity tracking.\n    default-strategy {\n      # Default limit of 100k active entities in a shard region (in a cluster node).\n      active-entity-limit = 100000\n\n      # Admisson window with LRU policy and adaptive sizing, and a frequency sketch admission filter to the main area.\n      admission {\n        window {\n          policy = least-recently-used\n          optimizer = hill-climbing\n        }\n        filter = frequency-sketch\n      }\n\n      # Main area with segmented LRU replacement policy with an 80% \"protected\" level by default.\n      replacement {\n        policy = least-recently-used\n        least-recently-used {\n          segmented {\n            levels = 2\n            proportions = [0.2, 0.8]\n          }\n        }\n      }\n    }\n\n    strategy-defaults {\n      # Passivate entities when they have not received a message for a specified length of time.\n      idle-entity {\n        # Passivate idle entities after the timeout. Set to \"none\" or \"off\" to disable.\n        timeout = none\n\n        # Check idle entities every interval. Set to \"default\" to use half the timeout by default.\n        interval = default\n      }\n\n      # Limit of active entities in a shard region.\n      # Passivate entities when the number of active entities in a shard region reaches this limit.\n      # The per-region limit is divided evenly among the active shards in a region.\n      # Set to \"none\" or \"off\" to disable limit-based automatic passivation, to only use idle entity timeouts.\n      active-entity-limit = none\n\n      # Entity replacement settings, for when the active entity limit is reached.\n      replacement {\n        # Entity replacement policy to use when the active entity limit is reached. Possible values are:\n        #   - \"least-recently-used\"\n        #   - \"most-recently-used\"\n        #   - \"least-frequently-used\"\n        # Set to \"none\" or \"off\" to disable the replacement policy and ignore the active entity limit.\n        policy = none\n\n        # Least recently used entity replacement policy.\n        least-recently-used {\n          # Optionally use a \"segmented\" least recently used strategy.\n          # Disabled when segmented.levels are set to \"none\" or \"off\".\n          segmented {\n            # Number of segmented levels.\n            levels = none\n\n            # Fractional proportions for the segmented levels.\n            # If empty then segments are divided evenly by the number of levels.\n            proportions = []\n          }\n        }\n\n        # Most recently used entity replacement policy.\n        most-recently-used {}\n\n        # Least frequently used entity replacement policy.\n        least-frequently-used {\n          # New frequency counts will be \"dynamically aged\" when enabled.\n          dynamic-aging = off\n        }\n      }\n\n      # An optional admission area, with a window for newly and recently activated entities, and an admission filter\n      # to determine whether a candidate should be admitted to the main area of the passivation strategy.\n      admission {\n        # An optional window area, where newly created entities will be admitted initially, and when evicted\n        # from the window area have an opportunity to move to the main area based on the admission filter.\n        window {\n          # The initial sizing for the window area (if enabled), as a fraction of the total active entity limit.\n          proportion = 0.01\n\n          # The minimum adaptive sizing for the window area, as a fraction of the total active entity limit.\n          # Only applies when an adaptive window optimizer is enabled.\n          minimum-proportion = 0.01\n\n          # The maximum adaptive sizing for the window area, as a fraction of the total active entity limit.\n          # Only applies when an adaptive window optimizer is enabled.\n          maximum-proportion = 1.0\n\n          # Adaptive optimizer to use for dynamically resizing the window area. Possible values are:\n          #   - \"hill-climbing\"\n          # Set to \"none\" or \"off\" to disable adaptive sizing of the window area.\n          optimizer = off\n\n          # A window proportion optimizer using a simple hill-climbing algorithm.\n          hill-climbing {\n            # Multiplier of the active entity limit for how often (in accesses) to adjust the window proportion.\n            adjust-multiplier = 10.0\n\n            # The size of the initial step to take (also used when the climbing restarts).\n            initial-step = 0.0625\n\n            # A threshold for the change in active rate (hit rate) to restart climbing.\n            restart-threshold = 0.05\n\n            # The decay ratio applied on each climbing step.\n            step-decay = 0.98\n          }\n\n          # Replacement policy to use for the window area.\n          # Entities that are evicted from the window area may move to the main area, based on the admission filter.\n          # Possible values are the same as for the main replacement policy.\n          # Set to \"none\" or \"off\" to disable the window area.\n          policy = none\n\n          least-recently-used {\n            segmented {\n              levels = none\n              proportions = []\n            }\n          }\n\n          most-recently-used {}\n\n          least-frequently-used {\n            dynamic-aging = off\n          }\n        }\n\n        # The admission filter for the main area of the passivation strategy. Possible values are:\n        #   - \"frequency-sketch\"\n        # Set to \"none\" or \"off\" to disable the admission filter and always admit to the main area.\n        filter = none\n\n        # An admission filter based on a frequency sketch (a variation of a count-min sketch).\n        frequency-sketch {\n          # The depth of the frequency sketch (the number of hash functions).\n          depth = 4\n\n          # The size of the frequency counters in bits: 2, 4, 8, 16, 32, or 64 bits.\n          counter-bits = 4\n\n          # Multiplier of the active entity limit for the width of the frequency sketch.\n          width-multiplier = 4\n\n          # Multiplier of the active entity limit for how often the reset operation of the frequency sketch is applied.\n          reset-multiplier = 10.0\n        }\n      }\n    }\n  }\n\n  # If the coordinator can't store state changes it will be stopped\n  # and started again after this duration, with an exponential back-off\n  # of up to 5 times this duration.\n  coordinator-failure-backoff = 5 s\n\n  # The ShardRegion retries registration and shard location requests to the\n  # ShardCoordinator with this interval if it does not reply.\n  retry-interval = 2 s\n\n  # Maximum number of messages that are buffered by a ShardRegion actor.\n  buffer-size = 100000\n\n  # Timeout of the shard rebalancing process.\n  # Additionally, if an entity doesn't handle the stopMessage\n  # after (handoff-timeout - 5.seconds).max(1.second) it will be stopped forcefully\n  handoff-timeout = 60 s\n\n  # Time given to a region to acknowledge it's hosting a shard.\n  shard-start-timeout = 10 s\n\n  # If the shard is remembering entities and can't store state changes, it\n  # will be stopped and then started again after this duration. Any messages\n  # sent to an affected entity may be lost in this process.\n  shard-failure-backoff = 10 s\n\n  # If the shard is remembering entities and an entity stops itself without\n  # using passivate, the entity will be restarted after this duration or when\n  # the next message for it is received, whichever occurs first.\n  entity-restart-backoff = 10 s\n\n  # Rebalance check is performed periodically with this interval.\n  rebalance-interval = 10 s\n\n  # Absolute path to the journal plugin configuration entity that is to be\n  # used for the internal persistence of ClusterSharding. If not defined,\n  # the default journal plugin is used. Note that this is not related to\n  # persistence used by the entity actors.\n  # Only used when state-store-mode=persistence\n  journal-plugin-id = \"\"\n\n  # Absolute path to the snapshot plugin configuration entity that is to be\n  # used for the internal persistence of ClusterSharding. If not defined,\n  # the default snapshot plugin is used. Note that this is not related to\n  # persistence used by the entity actors.\n  # Only used when state-store-mode=persistence\n  snapshot-plugin-id = \"\"\n\n  # Defines how the coordinator stores its state. Same is also used by the\n  # shards for rememberEntities.\n  # Valid values are \"ddata\" or \"persistence\".\n  # \"persistence\" mode is deprecated\n  state-store-mode = \"ddata\"\n\n  # The shard saves persistent snapshots after this number of persistent\n  # events. Snapshots are used to reduce recovery times. A snapshot trigger might be delayed if a batch of updates is processed.\n  # Only used when state-store-mode=persistence\n  snapshot-after = 1000\n\n  # The shard deletes persistent events (messages and snapshots) after doing snapshot\n  # keeping this number of old persistent batches.\n  # Batch is of size `snapshot-after`.\n  # When set to 0, after snapshot is successfully done, all events with equal or lower sequence number will be deleted.\n  # Default value of 2 leaves last maximum 2*`snapshot-after` events and 3 snapshots (2 old ones + latest snapshot).\n  # If larger than 0, one additional batch of journal messages is kept when state-store-mode=persistence to include messages from delayed snapshots.\n  keep-nr-of-batches = 2\n\n  # Settings for LeastShardAllocationStrategy.\n  #\n  # A new rebalance algorithm was included in Pekko 2.6.10. It can reach optimal balance in\n  # less rebalance rounds (typically 1 or 2 rounds). The amount of shards to rebalance in each\n  # round can still be limited to make it progress slower. For backwards compatibility,\n  # the new algorithm is not enabled by default. Enable the new algorithm by setting\n  # `rebalance-absolute-limit` > 0, for example:\n  # pekko.cluster.sharding.least-shard-allocation-strategy.rebalance-absolute-limit=20\n  # The new algorithm is recommended and will become the default in future versions of Pekko.\n  least-shard-allocation-strategy {\n    # Maximum number of shards that will be rebalanced in one rebalance round.\n    # The lower of this and `rebalance-relative-limit` will be used.\n    rebalance-absolute-limit = 0\n\n    # Maximum number of shards that will be rebalanced in one rebalance round.\n    # Fraction of total number of (known) shards.\n    # The lower of this and `rebalance-absolute-limit` will be used.\n    rebalance-relative-limit = 0.1\n\n    # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used\n    # when rebalance-absolute-limit > 0.\n    #\n    # Threshold of how large the difference between most and least number of\n    # allocated shards must be to begin the rebalancing.\n    # The difference between number of shards in the region with most shards and\n    # the region with least shards must be greater than (>) the `rebalanceThreshold`\n    # for the rebalance to occur.\n    # It is also the maximum number of shards that will start rebalancing per rebalance-interval\n    # 1 gives the best distribution and therefore typically the best choice.\n    # Increasing the threshold can result in quicker rebalance but has the\n    # drawback of increased difference between number of shards (and therefore load)\n    # on different nodes before rebalance will occur.\n    rebalance-threshold = 1\n\n    # Deprecated: Use rebalance-absolute-limit and rebalance-relative-limit instead. This property is not used\n    # when rebalance-absolute-limit > 0.\n    #\n    # The number of ongoing rebalancing processes is limited to this number.\n    max-simultaneous-rebalance = 3\n  }\n\n  external-shard-allocation-strategy {\n    # How long to wait for the client to persist an allocation to ddata or get all shard locations\n    client-timeout = 5s\n  }\n\n  # Timeout of waiting the initial distributed state for the shard coordinator (an initial state will be queried again if the timeout happened)\n  # and for a shard to get its state when remembered entities is enabled\n  # The read from ddata is a ReadMajority, for small clusters (< majority-min-cap) every node needs to respond\n  # so is more likely to time out if there are nodes restarting e.g. when there is a rolling re-deploy happening\n  waiting-for-state-timeout = 2 s\n\n  # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)\n  # Also used as timeout for writes of remember entities when that is enabled\n  updating-state-timeout = 5 s\n\n  # Timeout to wait for querying all shards for a given `ShardRegion`.\n  shard-region-query-timeout = 3 s\n\n  # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used\n  # by the persistent shard when rebalancing or restarting and is applied per remembered shard starting up (not for\n  # entire shard region). The value can either be \"all\" or \"constant\". The \"all\"\n  # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying\n  # entity actors at a fix rate. The default strategy \"all\".\n  entity-recovery-strategy = \"all\"\n\n  # Default settings for the constant rate entity recovery strategy\n  entity-recovery-constant-rate-strategy {\n    # Sets the frequency at which a batch of entity actors is started.\n    frequency = 100 ms\n    # Sets the number of entity actors to be restart at a particular interval\n    number-of-entities = 5\n  }\n\n  event-sourced-remember-entities-store {\n    # When using remember entities and the event sourced remember entities store the batches\n    # written to the store are limited by this number to avoid getting a too large event for\n    # the journal to handle. If using long persistence ids you may have to increase this.\n    max-updates-per-write = 100\n  }\n\n  # Settings for the coordinator singleton. Same layout as pekko.cluster.singleton.\n  # The \"role\" of the singleton configuration is not used. The singleton role will\n  # be the same as \"pekko.cluster.sharding.role\" if\n  # \"pekko.cluster.sharding.coordinator-singleton-role-override\" is enabled. Disabling it will allow to\n  # use separate nodes for the shard coordinator and the shards themselves.\n  # A lease can be configured in these settings for the coordinator singleton\n  coordinator-singleton = ${pekko.cluster.singleton}\n\n\n  # Copies the role for the coordinator singleton from the shards role instead of using the one provided in the\n  # \"pekko.cluster.sharding.coordinator-singleton.role\"\n  coordinator-singleton-role-override = on\n\n  coordinator-state {\n    # State updates are required to be written to a majority of nodes plus this\n    # number of additional nodes. Can also be set to \"all\" to require\n    # writes to all nodes. The reason for write/read to more than majority\n    # is to have more tolerance for membership changes between write and read.\n    # The tradeoff of increasing this is that updates will be slower.\n    # It is more important to increase the `read-majority-plus`.\n    write-majority-plus = 3\n    # State retrieval when ShardCoordinator is started is required to be read\n    # from a majority of nodes plus this number of additional nodes. Can also\n    # be set to \"all\" to require reads from all nodes. The reason for write/read\n    # to more than majority is to have more tolerance for membership changes between\n    # write and read.\n    # The tradeoff of increasing this is that coordinator startup will be slower.\n    read-majority-plus = 5\n  }\n  \n  # Settings for the Distributed Data replicator. \n  # Same layout as pekko.cluster.distributed-data.\n  # The \"role\" of the distributed-data configuration is not used. The distributed-data\n  # role will be the same as \"pekko.cluster.sharding.role\".\n  # Note that there is one Replicator per role and it's not possible\n  # to have different distributed-data settings for different sharding entity types.\n  # Only used when state-store-mode=ddata\n  distributed-data = ${pekko.cluster.distributed-data}\n  distributed-data {\n    # minCap parameter to MajorityWrite and MajorityRead consistency level.\n    majority-min-cap = 5\n    durable.keys = [\"shard-*\"]\n    \n    # When using many entities with \"remember entities\" the Gossip message\n    # can become too large if including too many in same message. Limit to\n    # the same number as the number of ORSet per shard.\n    max-delta-elements = 5\n\n    # ShardCoordinator is singleton running on oldest\n    prefer-oldest = on\n  }\n\n  # The id of the dispatcher to use for ClusterSharding actors.\n  # If specified, you need to define the settings of the actual dispatcher.\n  # This dispatcher for the entity actors is defined by the user provided\n  # Props, i.e. this dispatcher is not used for the entity actors.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n  # Config path of the lease that each shard must acquire before starting entity actors\n  # default is no lease\n  # A lease can also be used for the singleton coordinator by settings it in the coordinator-singleton properties\n  use-lease = \"\"\n\n  # The interval between retries for acquiring the lease\n  lease-retry-interval = 5s\n\n  # Provide a higher level of details in the debug logs, often per routed message. Be careful about enabling\n  # in production systems.\n  verbose-debug-logging = off\n\n  # Throw an exception if the internal state machine in the Shard actor does an invalid state transition.\n  # Mostly for the Pekko test suite. If off, the invalid transition is logged as a warning instead of throwing and\n  # crashing the shard.\n  fail-on-invalid-entity-state-transition = off\n\n  # Healthcheck that can be used with Pekko management health checks: https://pekko.apache.org/docs/pekko-management/current/healthchecks.html\n  healthcheck {\n    # sharding names to check have registered with the coordinator for the health check to pass\n    # once initial registration has taken place the health check always returns true to prevent the coordinator\n    # moving making the health check of all nodes fail\n    # by default no sharding instances are monitored\n    names = []\n\n    # Timeout for the local shard region to respond. This should be lower than your monitoring system's\n    # timeout for health checks\n    timeout = 5s\n  }\n}\ncopysourcepekko.cluster.sharding {\n  # Number of shards used by the default HashCodeMessageExtractor\n  # when no other message extractor is defined. This value must be\n  # the same for all nodes in the cluster and that is verified by\n  # configuration check when joining. Changing the value requires\n  # stopping all nodes in the cluster.\n  number-of-shards = 1000\n}","title":"Configuration"},{"location":"/typed/cluster-sharding.html#example-project","text":"Sharding example project Sharding example project is an example project that can be downloaded, and with instructions of how to run.\nThis project contains a KillrWeather sample illustrating how to use Cluster Sharding.","title":"Example project"},{"location":"/typed/cluster-sharding-concepts.html","text":"","title":"Cluster Sharding concepts"},{"location":"/typed/cluster-sharding-concepts.html#cluster-sharding-concepts","text":"The ShardRegion actor is started on each node in the cluster, or group of nodes tagged with a specific role. The ShardRegion is created with two application specific functions to extract the entity identifier and the shard identifier from incoming messages. A Shard is a group of entities that will be managed together. For the first message in a specific shard the ShardRegion requests the location of the shard from a central coordinator, the ShardCoordinator.\nThe ShardCoordinator decides which ShardRegion shall own the Shard and informs that ShardRegion. The region will confirm this request and create the Shard supervisor as a child actor. The individual Entities will then be created when needed by the Shard actor. Incoming messages thus travel via the ShardRegion and the Shard to the target Entity.\nIf the shard home is another ShardRegion instance messages will be forwarded to that ShardRegion instance instead. While resolving the location of a shard incoming messages for that shard are buffered and later delivered when the shard home is known. Subsequent messages to the resolved shard can be delivered to the target destination immediately without involving the ShardCoordinator.","title":"Cluster Sharding concepts"},{"location":"/typed/cluster-sharding-concepts.html#scenarios","text":"Once a Shard location is known ShardRegions send messages directly. Here are the scenarios for getting to this state. In the scenarios the following notation is used:\nSC - ShardCoordinator M# - Message 1, 2, 3, etc SR# - ShardRegion 1, 2 3, etc S# - Shard 1 2 3, etc E# - Entity 1 2 3, etc. An entity refers to an Actor managed by Cluster Sharding.\nWhere # is a number to distinguish between instances as there are multiple in the Cluster.","title":"Scenarios"},{"location":"/typed/cluster-sharding-concepts.html#scenario-1-message-to-an-unknown-shard-that-belongs-to-the-local-shardregion","text":"Incoming message M1 to ShardRegion instance SR1. M1 is mapped to shard S1. SR1 doesn’t know about S1, so it asks the SC for the location of S1. SC answers that the home of S1 is SR1. SR1 creates child actor shard S1 and forwards the message to it. S1 creates child actor for E1 and forwards the message to it. All incoming messages for S1 which arrive at SR1 can be handled by SR1 without SC.","title":"Scenario 1: Message to an unknown shard that belongs to the local ShardRegion"},{"location":"/typed/cluster-sharding-concepts.html#scenario-2-message-to-an-unknown-shard-that-belongs-to-a-remote-shardregion","text":"Incoming message M2 to ShardRegion instance SR1. M2 is mapped to S2. SR1 doesn’t know about S2, so it asks SC for the location of S2. SC answers that the home of S2 is SR2. SR1 sends buffered messages for S2 to SR2. All incoming messages for S2 which arrive at SR1 can be handled by SR1 without SC. It forwards messages to SR2. SR2 receives message for S2, ask SC, which answers that the home of S2 is SR2, and we are in Scenario 1 (but for SR2).","title":"Scenario 2: Message to an unknown shard that belongs to a remote ShardRegion"},{"location":"/typed/cluster-sharding-concepts.html#shard-location","text":"To make sure that at most one instance of a specific entity actor is running somewhere in the cluster it is important that all nodes have the same view of where the shards are located. Therefore the shard allocation decisions are taken by the central ShardCoordinator, which is running as a cluster singleton, i.e. one instance on the oldest member among all cluster nodes or a group of nodes tagged with a specific role.\nThe logic that decides where a shard is to be located is defined in a pluggable shard allocation strategy.","title":"Shard location"},{"location":"/typed/cluster-sharding-concepts.html#shard-rebalancing","text":"To be able to use newly added members in the cluster the coordinator facilitates rebalancing of shards, i.e. migrate entities from one node to another. In the rebalance process the coordinator first notifies all ShardRegion actors that a handoff for a shard has started. That means they will start buffering incoming messages for that shard, in the same way as if the shard location is unknown. During the rebalance process the coordinator will not answer any requests for the location of shards that are being rebalanced, i.e. local buffering will continue until the handoff is completed. The ShardRegion responsible for the rebalanced shard will stop all entities in that shard by sending the specified stopMessage (default PoisonPill) to them. When all entities have been terminated the ShardRegion owning the entities will acknowledge the handoff as completed to the coordinator. Thereafter the coordinator will reply to requests for the location of the shard, thereby allocating a new home for the shard, and then buffered messages in the ShardRegion actors are delivered to the new location. This means that the state of the entities are not transferred or migrated. If the state of the entities are of importance it should be persistent (durable), e.g. with Persistence (or see Classic Persistence), so that it can be recovered at the new location.\nThe logic that decides which shards to rebalance is defined in a pluggable shard allocation strategy. The default implementation LeastShardAllocationStrategy allocates new shards to the ShardRegion (node) with least number of previously allocated shards.\nSee also Shard allocation.","title":"Shard rebalancing"},{"location":"/typed/cluster-sharding-concepts.html#shardcoordinator-state","text":"The state of shard locations in the ShardCoordinator is persistent (durable) with Distributed Data (or see Classic Distributed Data) to survive failures.\nWhen a crashed or unreachable coordinator node has been removed (via down) from the cluster a new ShardCoordinator singleton actor will take over and the state is recovered. During such a failure period shards with a known location are still available, while messages for new (unknown) shards are buffered until the new ShardCoordinator becomes available.","title":"ShardCoordinator state"},{"location":"/typed/cluster-sharding-concepts.html#message-ordering","text":"As long as a sender uses the same ShardRegion actor to deliver messages to an entity actor the order of the messages is preserved. As long as the buffer limit is not reached messages are delivered on a best effort basis, with at-most once delivery semantics, in the same way as ordinary message sending.","title":"Message ordering"},{"location":"/typed/cluster-sharding-concepts.html#reliable-delivery","text":"Reliable end-to-end messaging, with at-least-once semantics can be added by using the Reliable Delivery feature.","title":"Reliable delivery"},{"location":"/typed/cluster-sharding-concepts.html#overhead","text":"Some additional latency is introduced for messages targeted to new or previously unused shards due to the round-trip to the coordinator. Rebalancing of shards may also add latency. This should be considered when designing the application specific shard resolution, e.g. to avoid too fine grained shards. Once a shard’s location is known the only overhead is sending a message via the ShardRegion rather than directly.","title":"Overhead"},{"location":"/typed/cluster-sharded-daemon-process.html","text":"","title":"Sharded Daemon Process"},{"location":"/typed/cluster-sharded-daemon-process.html#sharded-daemon-process","text":"","title":"Sharded Daemon Process"},{"location":"/typed/cluster-sharded-daemon-process.html#module-info","text":"To use Pekko Sharded Daemon Process, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-sharding-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-sharding-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-sharding-typed_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster Sharding (typed) Artifact org.apache.pekko pekko-cluster-sharding-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.sharding.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/cluster-sharded-daemon-process.html#introduction","text":"Sharded Daemon Process provides a way to run N actors, each given a numeric id starting from 0 that are then kept alive and balanced across the cluster. When a rebalance is needed the actor is stopped and, triggered by a keep alive running on all nodes, started on a new node (the keep alive should be seen as an implementation detail and may change in future versions).\nThe intended use case is for splitting data processing workloads across a set number of workers that each get to work on a subset of the data that needs to be processed. This is commonly needed to create projections based on the event streams available from all the EventSourcedBehaviors in a CQRS application. Events are tagged with one out of N tags used to split the workload of consuming and updating a projection between N workers.\nFor cases where a single actor needs to be kept alive see Cluster Singleton","title":"Introduction"},{"location":"/typed/cluster-sharded-daemon-process.html#basic-example","text":"To set up a set of actors running with Sharded Daemon process each node in the cluster needs to run the same initialization when starting up:\nScala copysourceval tags = Vector(\"tag-1\", \"tag-2\", \"tag-3\")\nShardedDaemonProcess(system).init(\"TagProcessors\", tags.size, id => TagProcessor(tags(id))) Java copysourceList<String> tags = Arrays.asList(\"tag-1\", \"tag-2\", \"tag-3\");\nShardedDaemonProcess.get(system)\n    .init(\n        TagProcessor.Command.class,\n        \"TagProcessors\",\n        tags.size(),\n        id -> TagProcessor.create(tags.get(id)));\nAn additional factory method is provided for further configurability and providing a graceful stop message for the actor.","title":"Basic example"},{"location":"/typed/cluster-sharded-daemon-process.html#addressing-the-actors","text":"In use cases where you need to send messages to the daemon process actors it is recommended to use the system receptionist either with a single ServiceKey which all daemon process actors register themeselves to for broadcasts or individual keys if more fine grained messaging is needed.","title":"Addressing the actors"},{"location":"/typed/cluster-sharded-daemon-process.html#scalability","text":"This cluster tool is intended for small numbers of consumers and will not scale well to a large set. In large clusters it is recommended to limit the nodes the sharded daemon process will run on using a role.","title":"Scalability"},{"location":"/typed/cluster-dc.html","text":"","title":"Multi-DC Cluster"},{"location":"/typed/cluster-dc.html#multi-dc-cluster","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Multi-DC Cluster\nThis chapter describes how Pekko Cluster can be used across multiple data centers, availability zones or regions.\nThe reason for making the Pekko Cluster aware of data center boundaries is that communication across data centers typically has much higher latency and higher failure rate than communication between nodes in the same data center.\nHowever, the grouping of nodes is not limited to the physical boundaries of data centers, even though that is the primary use case. It could also be used as a logical grouping for other reasons, such as isolation of certain nodes to improve stability or splitting up a large cluster into smaller groups of nodes for better scalability.","title":"Multi-DC Cluster"},{"location":"/typed/cluster-dc.html#dependency","text":"To use Pekko Cluster add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/cluster-dc.html#motivation","text":"There can be many reasons for using more than one data center, such as:\nRedundancy to tolerate failures in one location and still be operational. Serve requests from a location near the user to provide better responsiveness. Balance the load over many servers.\nIt’s possible to run an ordinary Pekko Cluster with default settings that spans multiple data centers but that may result in problems like:\nManagement of Cluster membership is stalled during network partitions as described in a separate section below. This means that nodes would not be able to be added and removed during network partitions between data centers. More frequent false positive failure detection for network connections across data centers. It’s not possible to have different settings for the failure detection within vs. across data centers. Downing/removal of nodes in the case of network partitions should typically be treated differently for failures within vs. across data centers. For network partitions between data centers the system should typically not down the unreachable nodes, but instead wait until it heals or a decision is made by a human or external monitoring system. For failures within same data center automatic, more aggressive, downing mechanisms can be employed for quick fail over. Quick fail over of Cluster Singleton and Cluster Sharding from one data center to another is difficult to do in a safe way. There is a risk that singletons or sharded entities become active on both sides of a network partition. Lack of location information makes it difficult to optimize communication to prefer nodes that are close over distant nodes. E.g. a cluster aware router would be more efficient if it would prefer routing messages to nodes in the own data center.\nTo avoid some of these problems one can run a separate Pekko Cluster per data center and use another communication channel between the data centers, such as HTTP, an external message broker. However, many of the nice tools that are built on top of the Cluster membership information are lost. For example, it wouldn’t be possible to use Distributed Data across the separate clusters.\nWe often recommend implementing a micro-service as one Pekko Cluster. The external API of the service would be HTTP, gRPC or a message broker, and not Pekko Remoting or Cluster (see additional discussion in When and where to use Pekko Cluster).\nThe internal communication within the service that is running on several nodes would use ordinary actor messaging or the tools based on Pekko Cluster. When deploying this service to multiple data centers it would be inconvenient if the internal communication could not use ordinary actor messaging because it was separated into several Pekko Clusters. The benefit of using Pekko messaging internally is performance as well as ease of development and reasoning about your domain in terms of Actors.\nTherefore, it’s possible to make the Pekko Cluster aware of data centers so that one Pekko Cluster can span multiple data centers and still be tolerant to network partitions.","title":"Motivation"},{"location":"/typed/cluster-dc.html#defining-the-data-centers","text":"The features are based on the idea that nodes can be assigned to a group of nodes by setting the pekko.cluster.multi-data-center.self-data-center configuration property. A node can only belong to one data center and if nothing is specified a node will belong to the default data center.\nThe grouping of nodes is not limited to the physical boundaries of data centers, even though that is the primary use case. It could also be used as a logical grouping for other reasons, such as isolation of certain nodes to improve stability or splitting up a large cluster into smaller groups of nodes for better scalability.","title":"Defining the data centers"},{"location":"/typed/cluster-dc.html#membership","text":"Some membership transitions are managed by one node called the leader. There is one leader per data center and it is responsible for these transitions for the members within the same data center. Members of other data centers are managed independently by the leader of the respective data center. These actions cannot be performed while there are any unreachability observations among the nodes in the data center, but unreachability across different data centers don’t influence the progress of membership management within a data center. Nodes can be added and removed also when there are network partitions between data centers, which is impossible if nodes are not grouped into data centers.\nUser actions like joining, leaving, and downing can be sent to any node in the cluster, not only to the nodes in the data center of the node. Seed nodes are also global.\nThe data center membership is implemented by adding the data center name prefixed with \"dc-\" to the roles of the member and thereby this information is known by all other members in the cluster. This is an implementation detail, but it can be good to know if you see this in log messages.\nYou can retrieve information about what data center a member belongs to:\nScala copysourceval cluster = Cluster(system)\n// this node's data center\nval dc = cluster.selfMember.dataCenter\n// all known data centers\nval allDc = cluster.state.allDataCenters\n// a specific member's data center\nval aMember = cluster.state.members.head\nval aDc = aMember.dataCenter Java copysourcefinal Cluster cluster = Cluster.get(system);\n// this node's data center\nString dc = cluster.selfMember().dataCenter();\n// all known data centers\nSet<String> allDc = cluster.state().getAllDataCenters();\n// a specific member's data center\nMember aMember = cluster.state().getMembers().iterator().next();\nString aDc = aMember.dataCenter();","title":"Membership"},{"location":"/typed/cluster-dc.html#failure-detection","text":"Failure detection is performed by sending heartbeat messages to detect if a node is unreachable. This is done more frequently and with more certainty among the nodes in the same data center than across data centers. The failure detection across different data centers should be interpreted as an indication of problem with the network link between the data centers.\nTwo different failure detectors can be configured for these two purposes:\npekko.cluster.failure-detector for failure detection within own data center pekko.cluster.multi-data-center.failure-detector for failure detection across different data centers\nWhen subscribing to cluster events the UnreachableMember and ReachableMember events are for observations within the own data center. The same data center as where the subscription was registered.\nFor cross data center unreachability notifications you can subscribe to UnreachableDataCenter and ReachableDataCenter events.\nHeartbeat messages for failure detection across data centers are only performed between a number of the oldest nodes on each side. The number of nodes is configured with pekko.cluster.multi-data-center.cross-data-center-connections. The reason for only using a limited number of nodes is to keep the number of connections across data centers low. The same nodes are also used for the gossip protocol when disseminating the membership information across data centers. Within a data center all nodes are involved in gossip and failure detection.\nThis influences how rolling updates should be performed. Don’t stop all of the oldest that are used for gossip at the same time. Stop one or a few at a time so that new nodes can take over the responsibility. It’s best to leave the oldest nodes until last.\nSee the failure detector for more details.","title":"Failure Detection"},{"location":"/typed/cluster-dc.html#cluster-singleton","text":"The Cluster Singleton is a singleton per data center. If you start the ClusterSingletonManager on all nodes and you have defined 3 different data centers there will be 3 active singleton instances in the cluster, one in each data center. This is taken care of automatically, but is important to be aware of. Designing the system for one singleton per data center makes it possible for the system to be available also during network partitions between data centers.\nThe reason why the singleton is per data center and not global is that membership information is not guaranteed to be consistent across data centers when using one leader per data center and that makes it difficult to select a single global singleton.\nIf you need a global singleton you have to pick one data center to host that singleton and only start the ClusterSingletonManager on nodes of that data center. If the data center is unreachable from another data center the singleton is inaccessible, which is a reasonable trade-off when selecting consistency over availability.\nThe singleton proxy is by default routing messages to the singleton in the own data center, but it can be started with a dataCenter parameter in the ClusterSingletonProxySettings to define that it should route messages to a singleton located in another data center. That is useful for example when having a global singleton in one data center and accessing it from other data centers.\nThis is how to create a singleton proxy for a specific data center:\nScala copysourceval singletonProxy: ActorRef[Counter.Command] = ClusterSingleton(system).init(\n  SingletonActor(Counter(), \"GlobalCounter\").withSettings(ClusterSingletonSettings(system).withDataCenter(\"dc2\"))) Java copysourceActorRef<Counter.Command> singletonProxy =\n    ClusterSingleton.get(system)\n        .init(\n            SingletonActor.of(Counter.create(), \"GlobalCounter\")\n                .withSettings(ClusterSingletonSettings.create(system).withDataCenter(\"B\")));\nIf using the own data center as the withDataCenter parameter that would be a proxy for the singleton in the own data center, which is also the default if withDataCenter is not given.","title":"Cluster Singleton"},{"location":"/typed/cluster-dc.html#cluster-sharding","text":"The coordinator in Cluster Sharding is a Cluster Singleton and therefore, as explained above, Cluster Sharding is also per data center. Each data center will have its own coordinator and regions, isolated from other data centers. If you start an entity type with the same name on all nodes and you have defined 3 different data centers and then send messages to the same entity id to sharding regions in all data centers you will end up with 3 active entity instances for that entity id, one in each data center. This is because the region/coordinator is only aware of its own data center and will activate the entity there. It’s unaware of the existence of corresponding entities in the other data centers.\nEspecially when used together with Pekko Persistence that is based on the single-writer principle it is important to avoid running the same entity at multiple locations at the same time with a shared data store. That would result in corrupt data since the events stored by different instances may be interleaved and would be interpreted differently in a later replay. For replicated persistent entities see Replicated Event Sourcing.\nIf you need global entities you have to pick one data center to host that entity type and only start ClusterSharding on nodes of that data center. If the data center is unreachable from another data center the entities are inaccessible, which is a reasonable trade-off when selecting consistency over availability.\nThe Cluster Sharding proxy is by default routing messages to the shard regions in their own data center, but it can be started with a data-center parameter to define that it should route messages to a shard region located in another data center. That is useful for example when having global entities in one data center and accessing them from other data centers.\nThis is how to create a sharding proxy for a specific data center:\nScala copysourceval proxy: ActorRef[ShardingEnvelope[Command]] =\n  ClusterSharding(system).init(Entity(typeKey)(_ => MultiDcPinger()).withDataCenter(\"dc2\")) Java copysourceActorRef<ShardingEnvelope<Counter.Command>> proxy =\n    ClusterSharding.get(system)\n        .init(\n            Entity.of(typeKey, ctx -> Counter.create(ctx.getEntityId())).withDataCenter(\"dc2\"));\nand it can also be used with an EntityRef:\nScala copysource// it must still be started before usage\nClusterSharding(system).init(Entity(typeKey)(_ => MultiDcPinger()).withDataCenter(\"dc2\"))\n\nval entityRef = ClusterSharding(system).entityRefFor(typeKey, entityId, \"dc2\") Java copysource// it must still be started before usage\nClusterSharding.get(system)\n    .init(Entity.of(typeKey, ctx -> Counter.create(ctx.getEntityId())).withDataCenter(\"dc2\"));\n\nEntityRef<Counter.Command> entityRef =\n    ClusterSharding.get(system).entityRefFor(typeKey, entityId, \"dc2\");\nAnother way to manage global entities is to make sure that certain entity ids are located in only one data center by routing the messages to the right region. For example, the routing function could be that odd entity ids are routed to data center A and even entity ids to data center B. Before sending the message to the local region actor you make the decision of which data center it should be routed to. Messages for another data center can be sent with a sharding proxy as explained above and messages for the own data center are sent to the local region.","title":"Cluster Sharding"},{"location":"/typed/distributed-pub-sub.html","text":"","title":"Distributed Publish Subscribe in Cluster"},{"location":"/typed/distributed-pub-sub.html#distributed-publish-subscribe-in-cluster","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Distributed Publish Subscribe.","title":"Distributed Publish Subscribe in Cluster"},{"location":"/typed/distributed-pub-sub.html#module-info","text":"The distributed publish subscribe topic API is available and usable with the core (/actor-typed module, however it will only be distributed when used in a clustered application:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"(/cluster-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>(/bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>(/cluster-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:(/bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:(/cluster-typed_${versions.ScalaBinary}\"\n}","title":"Module info"},{"location":"/typed/distributed-pub-sub.html#the-topic-actor","text":"Distributed publish subscribe is achieved by representing each pub sub topic with an actor, actor.typed.pubsub.Topicactor.typed.pubsub.Topic.\nThe topic actor needs to run on each node where subscribers will live or that wants to publish messages to the topic.\nThe identity of the topic is a tuple of the type of messages that can be published and a string topic name but it is recommended to not define multiple topics with different types and the same topic name.\nScala @@snip [PubSubExample.scala](/(/actor-typed-tests/src/test/scala/docs/org/apache/pekko/typed/pubsub/PubSubExample.scala) { #start-topic } Java @@snip [PubSubExample.java](/(/actor-typed-tests/src/test/java/jdocs/org/apache/pekko/typed/pubsub/PubSubExample.java) { #start-topic }\nLocal actors can then subscribe to the topic (and unsubscribe from it):\nScala @@snip [PubSubExample.scala](/(/actor-typed-tests/src/test/scala/docs/org/apache/pekko/typed/pubsub/PubSubExample.scala) { #subscribe } Java @@snip [PubSubExample.java](/(/actor-typed-tests/src/test/java/jdocs/org/apache/pekko/typed/pubsub/PubSubExample.java) { #subscribe }\nAnd publish messages to the topic:\nScala @@snip [PubSubExample.scala](/(/actor-typed-tests/src/test/scala/docs/org/apache/pekko/typed/pubsub/PubSubExample.scala) { #publish } Java @@snip [PubSubExample.java](/(/actor-typed-tests/src/test/java/jdocs/org/apache/pekko/typed/pubsub/PubSubExample.java) { #publish }","title":"The Topic Actor"},{"location":"/typed/distributed-pub-sub.html#pub-sub-scalability","text":"Each topic is represented by one Receptionist service key meaning that the number of topics will scale to thousands or tens of thousands but for higher numbers of topics will require custom solutions. It also means that a very high turnaround of unique topics will not work well and for such use cases a custom solution is advised.\nThe topic actor acts as a proxy and delegates to the local subscribers handling deduplication so that a published message is only sent once to a node regardless of how many subscribers there are to the topic on that node.\nWhen a topic actor has no subscribers for a topic it will deregister itself from the receptionist meaning published messages for the topic will not be sent to it.","title":"Pub Sub Scalability"},{"location":"/typed/distributed-pub-sub.html#delivery-guarantee","text":"As in Message Delivery Reliability of Pekko, message delivery guarantee in distributed pub sub modes is at-most-once delivery. In other words, messages can be lost over the wire. In addition to that the registry of nodes which have subscribers is eventually consistent meaning that subscribing an actor on one node will have a short delay before it is known on other nodes and published to.\nIf you are looking for at-least-once delivery guarantee, we recommend [Pekko Connector Kafka](https://pekko.apache.org/docs/alp(/kafka/current/).","title":"Delivery Guarantee"},{"location":"/typed/reliable-delivery.html","text":"","title":"Reliable delivery"},{"location":"/typed/reliable-delivery.html#reliable-delivery","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic At-Least-Once Delivery.\nWarning This module is currently marked as may change because it is a new feature that needs feedback from real usage before finalizing the API. This means that API or semantics can change without warning or deprecation period. It is also not recommended to use this module in production just yet.","title":"Reliable delivery"},{"location":"/typed/reliable-delivery.html#module-info","text":"To use reliable delivery, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Module info"},{"location":"/typed/reliable-delivery.html#introduction","text":"Normal message delivery reliability is at-most-once delivery, which means that messages may be lost. That should be rare, but still possible.\nFor interactions between some actors, that is not acceptable and at-least-once delivery or effectively-once processing is needed. The tools for reliable delivery described here help with implementing that. It can’t be achieved automatically under the hood without collaboration from the application. This is because confirming when a message has been fully processed is a business level concern. Only ensuring that it was transferred over the network or delivered to the mailbox of the actor would not be enough, since the actor may crash right before being able to process the message.\nLost messages are detected, resent and deduplicated as needed. In addition, it also includes flow control for the sending of messages to avoid that a fast producer overwhelms a slower consumer or sends messages at a higher rate than what can be transferred over the network. This can be a common problem in interactions between actors, resulting in fatal errors like OutOfMemoryError because too many messages are queued in the mailboxes of the actors. The detection of lost messages and the flow control is driven by the consumer side, which means that the producer side will not send faster than the demand requested by the consumer side. The producer side will not push resends unless requested by the consumer side.\nThere are 3 supported patterns, which are described in the following sections:\nPoint-to-point Work pulling Sharding\nThe Point-to-Point pattern has support for automatically splitting up large messages and assemble them again on the consumer side. This feature is useful for avoiding head of line blocking from serialization and transfer of large messages.","title":"Introduction"},{"location":"/typed/reliable-delivery.html#point-to-point","text":"This pattern implements point-to-point reliable delivery between a single producer actor sending messages and a single consumer actor receiving the messages.\nMessages are sent from the producer to ProducerControllerProducerController and via ConsumerControllerConsumerController actors, which handle the delivery and confirmation of the processing in the destination consumer actor.\nThe producer actor will start the flow by sending a ProducerController.Start message to the ProducerController.\nThe ProducerController sends RequestNext to the producer, which is then allowed to send one message to the ProducerController. Thereafter the producer will receive a new RequestNext when it’s allowed to send one more message.\nThe producer and ProducerController actors are required to be local so that message delivery is both fast and guaranteed. This requirement is enforced by a runtime check.\nSimilarly, on the consumer side the destination consumer actor will start the flow by sending an initial ConsumerController.Start message to the ConsumerController.\nFor the ProducerController to know where to send the messages, it must be connected with the ConsumerController. This can be done with the ProducerController.RegisterConsumer or ConsumerController.RegisterToProducerController messages. When using the point-to-point pattern, it is the application’s responsibility to connect them together. For example, this can be done by sending the ActorRef in an ordinary message to the other side, or by registering the ActorRef in the Receptionist so it can be found on the other side.\nYou must also take measures to reconnect them if any of the sides crashes, for example by watching it for termination.\nMessages sent by the producer are wrapped in ConsumerController.Delivery when received by a consumer and the consumer should reply with ConsumerController.Confirmed when it has processed the message.\nThe next message is not delivered until the previous one is confirmed. Any messages from the producer that arrive while waiting for the confirmation are stashed by the ConsumerController and delivered when the previous message is confirmed.\nSimilar to the producer side, the consumer and the ConsumerController actors are required to be local so that message delivery is both fast and guaranteed. This requirement is enforced by a runtime check.\nMany unconfirmed messages can be in flight between the ProducerController and ConsumerController, but their number is limited by a flow control window. The flow control is driven by the consumer side, which means that the ProducerController will not send faster than the demand requested by the ConsumerController.","title":"Point-to-point"},{"location":"/typed/reliable-delivery.html#point-to-point-example","text":"An example of a fibonacci number generator (producer):\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.delivery.ProducerController\nimport pekko.actor.typed.scaladsl.Behaviors\n\nobject FibonacciProducer {\n  sealed trait Command\n\n  private case class WrappedRequestNext(r: ProducerController.RequestNext[FibonacciConsumer.Command]) extends Command\n\n  def apply(\n      producerController: ActorRef[ProducerController.Command[FibonacciConsumer.Command]]): Behavior[Command] = {\n    Behaviors.setup { context =>\n      val requestNextAdapter =\n        context.messageAdapter[ProducerController.RequestNext[FibonacciConsumer.Command]](WrappedRequestNext(_))\n      producerController ! ProducerController.Start(requestNextAdapter)\n\n      fibonacci(0, 1, 0)\n    }\n  }\n\n  private def fibonacci(n: Long, b: BigInt, a: BigInt): Behavior[Command] = {\n    Behaviors.receive {\n      case (context, WrappedRequestNext(next)) =>\n        context.log.info(\"Generated fibonacci {}: {}\", n, a)\n        next.sendNextTo ! FibonacciConsumer.FibonacciNumber(n, a)\n\n        if (n == 1000)\n          Behaviors.stopped\n        else\n          fibonacci(n + 1, a + b, b)\n    }\n  }\n} Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.delivery.ProducerController;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\nimport java.math.BigInteger;\nimport java.util.Optional;\n\npublic class FibonacciProducer extends AbstractBehavior<FibonacciProducer.Command> {\n\n  private long n = 0;\n  private BigInteger b = BigInteger.ONE;\n  private BigInteger a = BigInteger.ZERO;\n\n  interface Command {}\n\n  private static class WrappedRequestNext implements Command {\n    final ProducerController.RequestNext<FibonacciConsumer.Command> next;\n\n    private WrappedRequestNext(ProducerController.RequestNext<FibonacciConsumer.Command> next) {\n      this.next = next;\n    }\n  }\n\n  private FibonacciProducer(ActorContext<Command> context) {\n    super(context);\n  }\n\n  public static Behavior<Command> create(\n      ActorRef<ProducerController.Command<FibonacciConsumer.Command>> producerController) {\n    return Behaviors.setup(\n        context -> {\n          ActorRef<ProducerController.RequestNext<FibonacciConsumer.Command>> requestNextAdapter =\n              context.messageAdapter(\n                  ProducerController.requestNextClass(), WrappedRequestNext::new);\n          producerController.tell(new ProducerController.Start<>(requestNextAdapter));\n\n          return new FibonacciProducer(context);\n        });\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder()\n        .onMessage(WrappedRequestNext.class, w -> onWrappedRequestNext(w))\n        .build();\n  }\n\n  private Behavior<Command> onWrappedRequestNext(WrappedRequestNext w) {\n    getContext().getLog().info(\"Generated fibonacci {}: {}\", n, a);\n    w.next.sendNextTo().tell(new FibonacciConsumer.FibonacciNumber(n, a));\n\n    if (n == 1000) {\n      return Behaviors.stopped();\n    } else {\n      n = n + 1;\n      b = a.add(b);\n      a = b;\n      return this;\n    }\n  }\n}\nand consumer of the fibonacci numbers:\nScala copysourceimport pekko.actor.typed.delivery.ConsumerController\n\nobject FibonacciConsumer {\n  sealed trait Command\n\n  final case class FibonacciNumber(n: Long, value: BigInt) extends Command\n\n  private case class WrappedDelivery(d: ConsumerController.Delivery[Command]) extends Command\n\n  def apply(\n      consumerController: ActorRef[ConsumerController.Command[FibonacciConsumer.Command]]): Behavior[Command] = {\n    Behaviors.setup { context =>\n      val deliveryAdapter =\n        context.messageAdapter[ConsumerController.Delivery[FibonacciConsumer.Command]](WrappedDelivery(_))\n      consumerController ! ConsumerController.Start(deliveryAdapter)\n\n      Behaviors.receiveMessagePartial {\n        case WrappedDelivery(ConsumerController.Delivery(FibonacciNumber(n, value), confirmTo)) =>\n          context.log.info(\"Processed fibonacci {}: {}\", n, value)\n          confirmTo ! ConsumerController.Confirmed\n          Behaviors.same\n      }\n    }\n  }\n} Java copysourceimport org.apache.pekko.actor.typed.delivery.ConsumerController;\n\npublic class FibonacciConsumer extends AbstractBehavior<FibonacciConsumer.Command> {\n\n  interface Command {}\n\n  public static class FibonacciNumber implements Command {\n    public final long n;\n    public final BigInteger value;\n\n    public FibonacciNumber(long n, BigInteger value) {\n      this.n = n;\n      this.value = value;\n    }\n  }\n\n  private static class WrappedDelivery implements Command {\n    final ConsumerController.Delivery<Command> delivery;\n\n    private WrappedDelivery(ConsumerController.Delivery<Command> delivery) {\n      this.delivery = delivery;\n    }\n  }\n\n  public static Behavior<Command> create(\n      ActorRef<ConsumerController.Command<FibonacciConsumer.Command>> consumerController) {\n    return Behaviors.setup(\n        context -> {\n          ActorRef<ConsumerController.Delivery<FibonacciConsumer.Command>> deliveryAdapter =\n              context.messageAdapter(ConsumerController.deliveryClass(), WrappedDelivery::new);\n          consumerController.tell(new ConsumerController.Start<>(deliveryAdapter));\n\n          return new FibonacciConsumer(context);\n        });\n  }\n\n  private FibonacciConsumer(ActorContext<Command> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<Command> createReceive() {\n    return newReceiveBuilder().onMessage(WrappedDelivery.class, this::onDelivery).build();\n  }\n\n  private Behavior<Command> onDelivery(WrappedDelivery w) {\n    FibonacciNumber number = (FibonacciNumber) w.delivery.message();\n    getContext().getLog().info(\"Processed fibonacci {}: {}\", number.n, number.value);\n    w.delivery.confirmTo().tell(ConsumerController.confirmed());\n    return this;\n  }\n}\nThe FibonacciProducer sends the messages to a ProducerController. The FibonacciConsumer receives the messages from a ConsumerController. Note how the ActorRef in the Start messages are constructed as message adapters to map the RequestNext and Delivery to the protocol of the producer and consumer actors respectively.\nThe ConsumerController and ProducerController are connected via the ConsumerController.RegisterToProducerController message. The ActorRef of the ProducerController can be shared between producer and consumer sides with ordinary messages, or by using the Receptionist. Alternatively, they can be connected in the other direction by sending ProducerController.RegisterConsumer to the ProducerController.\nScala copysourceval consumerController = context.spawn(ConsumerController[FibonacciConsumer.Command](), \"consumerController\")\ncontext.spawn(FibonacciConsumer(consumerController), \"consumer\")\n\nval producerId = s\"fibonacci-${UUID.randomUUID()}\"\nval producerController = context.spawn(\n  ProducerController[FibonacciConsumer.Command](producerId, durableQueueBehavior = None),\n  \"producerController\")\ncontext.spawn(FibonacciProducer(producerController), \"producer\")\n\nconsumerController ! ConsumerController.RegisterToProducerController(producerController) Java copysourceActorRef<ConsumerController.Command<FibonacciConsumer.Command>> consumerController =\n    context.spawn(ConsumerController.create(), \"consumerController\");\ncontext.spawn(FibonacciConsumer.create(consumerController), \"consumer\");\n\nString producerId = \"fibonacci-\" + UUID.randomUUID();\nActorRef<ProducerController.Command<FibonacciConsumer.Command>> producerController =\n    context.spawn(\n        ProducerController.create(\n            FibonacciConsumer.Command.class, producerId, Optional.empty()),\n        \"producerController\");\ncontext.spawn(FibonacciProducer.create(producerController), \"producer\");\n\nconsumerController.tell(\n    new ConsumerController.RegisterToProducerController<>(producerController));","title":"Point-to-point example"},{"location":"/typed/reliable-delivery.html#point-to-point-delivery-semantics","text":"As long as neither producer nor consumer crash, the messages are delivered to the consumer actor in the same order as they were sent to the ProducerController, without loss or duplicates. This means effectively-once processing without any business level deduplication.\nUnconfirmed messages may be lost if the producer crashes. To avoid that, you need to enable the durable queue on the producer side. By doing so, any stored unconfirmed messages will be redelivered when the corresponding producer is started again. Even if the same ConsumerController instance is used, there may be delivery of messages that had already been processed but the fact that they were confirmed had not been stored yet. This means that we have at-least-once delivery.\nIf the consumer crashes, a new ConsumerController can be connected to the original ProducerConsumer without restarting it. The ProducerConsumer will then redeliver all unconfirmed messages. In that case the unconfirmed messages will be delivered to the new consumer and some of these may already have been processed by the previous consumer. Again, this means that we have at-least-once delivery.","title":"Point-to-point delivery semantics"},{"location":"/typed/reliable-delivery.html#work-pulling","text":"Work pulling is a pattern where several worker actors pull tasks at their own pace from a shared work manager instead of that the manager pushes work to the workers blindly without knowing their individual capacity and current availability.\nOne important property is that the order of the messages should not matter, because each message is routed randomly to one of the workers with demand. In other words, two subsequent messages may be routed to two different workers and processed independent of each other.\nMessages are sent from the producer to WorkPullingProducerControllerWorkPullingProducerController and via ConsumerControllerConsumerController actors, which handle the delivery and confirmation of the processing in the destination worker (consumer) actor.\nand adding another worker\nA worker actor (consumer) and its ConsumerController is dynamically registered to the WorkPullingProducerController via a ServiceKey. It will register itself to the Receptionist, and the WorkPullingProducerController subscribes to the same key to find active workers. In this way workers can be dynamically added or removed from any node in the cluster.\nThe work manager (producer) actor will start the flow by sending a WorkPullingProducerController.Start message to the WorkPullingProducerController.\nThe WorkPullingProducerController sends RequestNext to the producer, which is then allowed to send one message to the WorkPullingProducerController. Thereafter the producer will receive a new RequestNext when it’s allowed to send one more message. WorkPullingProducerController will send a new RequestNext when there is a demand from any worker. It’s possible that all workers with demand are deregistered after the RequestNext is sent and before the actual messages is sent to the WorkPullingProducerController. In that case the message is buffered and will be delivered when a new worker is registered or when there is a new demand.\nThe producer and WorkPullingProducerController actors are supposed to be local so that these messages are fast and not lost. This is enforced by a runtime check.\nSimilarly, on the consumer side the destination consumer actor will start the flow by sending an initial ConsumerController.Start message to the ConsumerController.\nReceived messages from the producer are wrapped in ConsumerController.Delivery when sent to the consumer, which is supposed to reply with ConsumerController.Confirmed when it has processed the message. Next message is not delivered until the previous is confirmed. More messages from the producer that arrive while waiting for the confirmation are stashed by the ConsumerController and delivered when the previous message is confirmed.\nThe consumer and the ConsumerController actors are supposed to be local so that these messages are fast and not lost. This is enforced by a runtime check.\nMany unconfirmed messages can be in flight between the WorkPullingProducerController and each ConsumerController, but it is limited by a flow control window. The flow control is driven by the consumer side, which means that the WorkPullingProducerController will not send faster than the demand requested by the workers.","title":"Work pulling"},{"location":"/typed/reliable-delivery.html#work-pulling-example","text":"Example of image converter worker (consumer):\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.delivery.ConsumerController\nimport pekko.actor.typed.receptionist.ServiceKey\n\nobject ImageConverter {\n  sealed trait Command\n  final case class ConversionJob(resultId: UUID, fromFormat: String, toFormat: String, image: Array[Byte])\n  private case class WrappedDelivery(d: ConsumerController.Delivery[ConversionJob]) extends Command\n\n  val serviceKey = ServiceKey[ConsumerController.Command[ConversionJob]](\"ImageConverter\")\n\n  def apply(): Behavior[Command] = {\n    Behaviors.setup { context =>\n      val deliveryAdapter =\n        context.messageAdapter[ConsumerController.Delivery[ConversionJob]](WrappedDelivery(_))\n      val consumerController =\n        context.spawn(ConsumerController(serviceKey), \"consumerController\")\n      consumerController ! ConsumerController.Start(deliveryAdapter)\n\n      Behaviors.receiveMessage {\n        case WrappedDelivery(delivery) =>\n          val image = delivery.message.image\n          val fromFormat = delivery.message.fromFormat\n          val toFormat = delivery.message.toFormat\n          // convert image...\n          // store result with resultId key for later retrieval\n\n          // and when completed confirm\n          delivery.confirmTo ! ConsumerController.Confirmed\n\n          Behaviors.same\n      }\n\n    }\n  }\n\n} Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.delivery.ConsumerController;\nimport org.apache.pekko.actor.typed.delivery.DurableProducerQueue;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\n\nimport java.time.Duration;\nimport java.util.Optional;\nimport java.util.UUID;\n\npublic class ImageConverter {\n  interface Command {}\n\n  public static class ConversionJob {\n    public final UUID resultId;\n    public final String fromFormat;\n    public final String toFormat;\n    public final byte[] image;\n\n    public ConversionJob(UUID resultId, String fromFormat, String toFormat, byte[] image) {\n      this.resultId = resultId;\n      this.fromFormat = fromFormat;\n      this.toFormat = toFormat;\n      this.image = image;\n    }\n  }\n\n  private static class WrappedDelivery implements Command {\n    final ConsumerController.Delivery<ConversionJob> delivery;\n\n    private WrappedDelivery(ConsumerController.Delivery<ConversionJob> delivery) {\n      this.delivery = delivery;\n    }\n  }\n\n  public static ServiceKey<ConsumerController.Command<ConversionJob>> serviceKey =\n      ServiceKey.create(ConsumerController.serviceKeyClass(), \"ImageConverter\");\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(\n        context -> {\n          ActorRef<ConsumerController.Delivery<ConversionJob>> deliveryAdapter =\n              context.messageAdapter(ConsumerController.deliveryClass(), WrappedDelivery::new);\n          ActorRef<ConsumerController.Command<ConversionJob>> consumerController =\n              context.spawn(ConsumerController.create(serviceKey), \"consumerController\");\n          consumerController.tell(new ConsumerController.Start<>(deliveryAdapter));\n\n          return Behaviors.receive(Command.class)\n              .onMessage(WrappedDelivery.class, ImageConverter::onDelivery)\n              .build();\n        });\n  }\n\n  private static Behavior<Command> onDelivery(WrappedDelivery w) {\n    byte[] image = w.delivery.message().image;\n    String fromFormat = w.delivery.message().fromFormat;\n    String toFormat = w.delivery.message().toFormat;\n    // convert image...\n    // store result with resultId key for later retrieval\n\n    // and when completed confirm\n    w.delivery.confirmTo().tell(ConsumerController.confirmed());\n\n    return Behaviors.same();\n  }\n}\nand image converter job manager (producer):\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.delivery.WorkPullingProducerController\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.StashBuffer\n\nobject ImageWorkManager {\n  sealed trait Command\n  final case class Convert(fromFormat: String, toFormat: String, image: Array[Byte]) extends Command\n  private case class WrappedRequestNext(r: WorkPullingProducerController.RequestNext[ImageConverter.ConversionJob])\n      extends Command\n\n  final case class GetResult(resultId: UUID, replyTo: ActorRef[Option[Array[Byte]]]) extends Command\n\n  def apply(): Behavior[Command] = {\n    Behaviors.setup { context =>\n      val requestNextAdapter =\n        context.messageAdapter[WorkPullingProducerController.RequestNext[ImageConverter.ConversionJob]](\n          WrappedRequestNext(_))\n      val producerController = context.spawn(\n        WorkPullingProducerController(\n          producerId = \"workManager\",\n          workerServiceKey = ImageConverter.serviceKey,\n          durableQueueBehavior = None),\n        \"producerController\")\n      producerController ! WorkPullingProducerController.Start(requestNextAdapter)\n\n      Behaviors.withStash(1000) { stashBuffer =>\n        new ImageWorkManager(context, stashBuffer).waitForNext()\n      }\n    }\n  }\n\n}\n\nfinal class ImageWorkManager(\n    context: ActorContext[ImageWorkManager.Command],\n    stashBuffer: StashBuffer[ImageWorkManager.Command]) {\n\n  import ImageWorkManager._\n\n  private def waitForNext(): Behavior[Command] = {\n    Behaviors.receiveMessagePartial {\n      case WrappedRequestNext(next) =>\n        stashBuffer.unstashAll(active(next))\n      case c: Convert =>\n        if (stashBuffer.isFull) {\n          context.log.warn(\"Too many Convert requests.\")\n          Behaviors.same\n        } else {\n          stashBuffer.stash(c)\n          Behaviors.same\n        }\n      case GetResult(resultId, replyTo) =>\n        // TODO retrieve the stored result and reply\n        Behaviors.same\n    }\n  }\n\n  private def active(\n      next: WorkPullingProducerController.RequestNext[ImageConverter.ConversionJob]): Behavior[Command] = {\n    Behaviors.receiveMessagePartial {\n      case Convert(from, to, image) =>\n        val resultId = UUID.randomUUID()\n        next.sendNextTo ! ImageConverter.ConversionJob(resultId, from, to, image)\n        waitForNext()\n      case GetResult(resultId, replyTo) =>\n        // TODO retrieve the stored result and reply\n        Behaviors.same\n      case _: WrappedRequestNext =>\n        throw new IllegalStateException(\"Unexpected RequestNext\")\n    }\n  }\n} Java copysourceimport org.apache.pekko.actor.typed.delivery.WorkPullingProducerController;\nimport org.apache.pekko.Done;\n\npublic class ImageWorkManager {\n\n  interface Command {}\n\n  public static class Convert implements Command {\n    public final String fromFormat;\n    public final String toFormat;\n    public final byte[] image;\n\n    public Convert(String fromFormat, String toFormat, byte[] image) {\n      this.fromFormat = fromFormat;\n      this.toFormat = toFormat;\n      this.image = image;\n    }\n  }\n\n  public static class GetResult implements Command {\n    public final UUID resultId;\n    public final ActorRef<Optional<byte[]>> replyTo;\n\n    public GetResult(UUID resultId, ActorRef<Optional<byte[]>> replyTo) {\n      this.resultId = resultId;\n      this.replyTo = replyTo;\n    }\n  }\n\n  private static class WrappedRequestNext implements Command {\n    final WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next;\n\n    private WrappedRequestNext(\n        WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {\n      this.next = next;\n    }\n  }\n\n\n  private final ActorContext<Command> context;\n  private final StashBuffer<Command> stashBuffer;\n\n  private ImageWorkManager(ActorContext<Command> context, StashBuffer<Command> stashBuffer) {\n    this.context = context;\n    this.stashBuffer = stashBuffer;\n  }\n\n  public static Behavior<Command> create() {\n    return Behaviors.setup(\n        context -> {\n          ActorRef<WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob>>\n              requestNextAdapter =\n                  context.messageAdapter(\n                      WorkPullingProducerController.requestNextClass(), WrappedRequestNext::new);\n          ActorRef<WorkPullingProducerController.Command<ImageConverter.ConversionJob>>\n              producerController =\n                  context.spawn(\n                      WorkPullingProducerController.create(\n                          ImageConverter.ConversionJob.class,\n                          \"workManager\",\n                          ImageConverter.serviceKey,\n                          Optional.empty()),\n                      \"producerController\");\n          producerController.tell(new WorkPullingProducerController.Start<>(requestNextAdapter));\n\n          return Behaviors.withStash(\n              1000, stashBuffer -> new ImageWorkManager(context, stashBuffer).waitForNext());\n        });\n  }\n\n  private Behavior<Command> waitForNext() {\n    return Behaviors.receive(Command.class)\n        .onMessage(WrappedRequestNext.class, this::onWrappedRequestNext)\n        .onMessage(Convert.class, this::onConvertWait)\n        .onMessage(GetResult.class, this::onGetResult)\n        .build();\n  }\n\n  private Behavior<Command> onWrappedRequestNext(WrappedRequestNext w) {\n    return stashBuffer.unstashAll(active(w.next));\n  }\n\n  private Behavior<Command> onConvertWait(Convert convert) {\n    if (stashBuffer.isFull()) {\n      context.getLog().warn(\"Too many Convert requests.\");\n      return Behaviors.same();\n    } else {\n      stashBuffer.stash(convert);\n      return Behaviors.same();\n    }\n  }\n\n  private Behavior<Command> onGetResult(GetResult get) {\n    // TODO retrieve the stored result and reply\n    return Behaviors.same();\n  }\n\n  private Behavior<Command> active(\n      WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {\n    return Behaviors.receive(Command.class)\n        .onMessage(Convert.class, c -> onConvert(c, next))\n        .onMessage(GetResult.class, this::onGetResult)\n        .onMessage(WrappedRequestNext.class, this::onUnexpectedWrappedRequestNext)\n        .build();\n  }\n\n  private Behavior<Command> onUnexpectedWrappedRequestNext(WrappedRequestNext w) {\n    throw new IllegalStateException(\"Unexpected RequestNext\");\n  }\n\n  private Behavior<Command> onConvert(\n      Convert convert,\n      WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {\n    UUID resultId = UUID.randomUUID();\n    next.sendNextTo()\n        .tell(\n            new ImageConverter.ConversionJob(\n                resultId, convert.fromFormat, convert.toFormat, convert.image));\n    return waitForNext();\n  }\n}\nNote how the ActorRef in the Start messages are constructed as message adapters to map the RequestNext and Delivery to the protocol of the producer and consumer actors respectively.\nSee also the corresponding example that is using ask from the producer.","title":"Work pulling example"},{"location":"/typed/reliable-delivery.html#work-pulling-delivery-semantics","text":"For work pulling the order of the messages should not matter, because each message is routed randomly to one of the workers with demand and can therefore be processed in any order.\nAs long as neither producers nor workers crash (or workers being removed for other reasons) the messages are delivered to the workers without loss or duplicates. Meaning effectively-once processing without any business level deduplication.\nUnconfirmed messages may be lost if the producer crashes. To avoid that you need to enable the durable queue on the producer side. The stored unconfirmed messages will be redelivered when the corresponding producer is started again. Those messages may be routed to different workers than before and some of them may have already been processed but the fact that they were confirmed had not been stored yet. Meaning at-least-once delivery.\nIf a worker crashes or is stopped gracefully the unconfirmed messages will be redelivered to other workers. In that case some of these may already have been processed by the previous worker. Meaning at-least-once delivery.","title":"Work pulling delivery semantics"},{"location":"/typed/reliable-delivery.html#sharding","text":"To use reliable delivery with Cluster Sharding, add the following module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-sharding-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-sharding-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-sharding-typed_${versions.ScalaBinary}\"\n}\nReliable delivery between a producer actor sending messages to sharded consumer actor receiving the messages.\nand sending to another entity\nand sending from another producer (different node)\nThe ShardingProducerControllerShardingProducerController should be used together with ShardingConsumerControllerShardingConsumerController.\nA producer can send messages via a ShardingProducerController to any ShardingConsumerController identified by an entityId. A single ShardingProducerController per ActorSystem (node) can be shared for sending to all entities of a certain entity type. No explicit registration is needed between the ShardingConsumerController and ShardingProducerController.\nThe producer actor will start the flow by sending a ShardingProducerController.Start message to the ShardingProducerController.\nThe ShardingProducerController sends RequestNext to the producer, which is then allowed to send one message to the ShardingProducerController. Thereafter the producer will receive a new RequestNext when it’s allowed to send one more message.\nIn the ShardingProducerController.RequestNextShardingProducerController.RequestNext message there is information about which entities that have demand. It is allowed to send to a new entityId that is not included in the RequestNext.entitiesWithDemand. If sending to an entity that doesn’t have demand the message will be buffered. This support for buffering means that it is even allowed to send several messages in response to one RequestNext but it’s recommended to only send one message and wait for next RequestNext before sending more messages.\nThe producer and ShardingProducerController actors are supposed to be local so that these messages are fast and not lost. This is enforced by a runtime check.\nSimilarly, on the consumer side the destination consumer actor will start the flow by sending an initial ConsumerController.Start message to the ConsumerController.\nThere will be one ShardingConsumerController for each entity. Many unconfirmed messages can be in flight between the ShardingProducerController and each ShardingConsumerController, but it is limited by a flow control window. The flow control is driven by the consumer side, which means that the ShardingProducerController will not send faster than the demand requested by the consumers.","title":"Sharding"},{"location":"/typed/reliable-delivery.html#sharding-example","text":"The sharded entity is a todo list which uses an async database call to store its entire state on each change, and first when that completes replies to reliable delivery that the message was consumed.\nExample of TodoList entity (consumer):\nScala copysourceimport scala.concurrent.Future\nimport scala.concurrent.duration._\nimport scala.util.Failure\nimport scala.util.Success\n\nimport org.apache.pekko\nimport pekko.Done\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.Behavior\nimport pekko.actor.typed.delivery.ConsumerController\nimport pekko.actor.typed.scaladsl.ActorContext\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.cluster.sharding.typed.delivery.ShardingConsumerController\nimport pekko.util.Timeout\n\ntrait DB {\n  def save(id: String, value: TodoList.State): Future[Done]\n  def load(id: String): Future[TodoList.State]\n}\n\nobject TodoList {\n\n  sealed trait Command\n\n  final case class AddTask(item: String) extends Command\n  final case class CompleteTask(item: String) extends Command\n\n  private final case class InitialState(state: State) extends Command\n  private final case class SaveSuccess(confirmTo: ActorRef[ConsumerController.Confirmed]) extends Command\n  private final case class DBError(cause: Throwable) extends Command\n\n  private final case class CommandDelivery(command: Command, confirmTo: ActorRef[ConsumerController.Confirmed])\n      extends Command\n\n  final case class State(tasks: Vector[String])\n\n  def apply(\n      id: String,\n      db: DB,\n      consumerController: ActorRef[ConsumerController.Start[Command]]): Behavior[Command] = {\n    Behaviors.setup[Command] { context =>\n      new TodoList(context, id, db).start(consumerController)\n    }\n  }\n\n}\n\nclass TodoList(context: ActorContext[TodoList.Command], id: String, db: DB) {\n  import TodoList._\n\n  private def start(consumerController: ActorRef[ConsumerController.Start[Command]]): Behavior[Command] = {\n    context.pipeToSelf(db.load(id)) {\n      case Success(value) => InitialState(value)\n      case Failure(cause) => DBError(cause)\n    }\n\n    Behaviors.receiveMessagePartial {\n      case InitialState(state) =>\n        val deliveryAdapter: ActorRef[ConsumerController.Delivery[Command]] = context.messageAdapter { delivery =>\n          CommandDelivery(delivery.message, delivery.confirmTo)\n        }\n        consumerController ! ConsumerController.Start(deliveryAdapter)\n        active(state)\n      case DBError(cause) =>\n        throw cause\n    }\n  }\n\n  private def active(state: State): Behavior[Command] = {\n    Behaviors.receiveMessagePartial {\n      case CommandDelivery(AddTask(item), confirmTo) =>\n        val newState = state.copy(tasks = state.tasks :+ item)\n        save(newState, confirmTo)\n        active(newState)\n      case CommandDelivery(CompleteTask(item), confirmTo) =>\n        val newState = state.copy(tasks = state.tasks.filterNot(_ == item))\n        save(newState, confirmTo)\n        active(newState)\n      case SaveSuccess(confirmTo) =>\n        confirmTo ! ConsumerController.Confirmed\n        Behaviors.same\n      case DBError(cause) =>\n        throw cause\n    }\n  }\n\n  private def save(newState: State, confirmTo: ActorRef[ConsumerController.Confirmed]): Unit = {\n    context.pipeToSelf(db.save(id, newState)) {\n      case Success(_)     => SaveSuccess(confirmTo)\n      case Failure(cause) => DBError(cause)\n    }\n  }\n} Java copysourceimport org.apache.pekko.Done;\nimport org.apache.pekko.actor.Address;\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.delivery.ConsumerController;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\n\nimport java.time.Duration;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.concurrent.CompletionStage;\n\n\ninterface DB {\n  CompletionStage<Done> save(String id, TodoList.State state);\n\n  CompletionStage<TodoList.State> load(String id);\n}\n\npublic class TodoList {\n  interface Command {}\n\n  public static class AddTask implements Command {\n    public final String item;\n\n    public AddTask(String item) {\n      this.item = item;\n    }\n  }\n\n  public static class CompleteTask implements Command {\n    public final String item;\n\n    public CompleteTask(String item) {\n      this.item = item;\n    }\n  }\n\n  private static class InitialState implements Command {\n    final State state;\n\n    private InitialState(State state) {\n      this.state = state;\n    }\n  }\n\n  private static class SaveSuccess implements Command {\n    final ActorRef<ConsumerController.Confirmed> confirmTo;\n\n    private SaveSuccess(ActorRef<ConsumerController.Confirmed> confirmTo) {\n      this.confirmTo = confirmTo;\n    }\n  }\n\n  private static class DBError implements Command {\n    final Exception cause;\n\n    private DBError(Throwable cause) {\n      if (cause instanceof Exception) this.cause = (Exception) cause;\n      else this.cause = new RuntimeException(cause.getMessage(), cause);\n    }\n  }\n\n  private static class CommandDelivery implements Command {\n    final Command command;\n    final ActorRef<ConsumerController.Confirmed> confirmTo;\n\n    private CommandDelivery(Command command, ActorRef<ConsumerController.Confirmed> confirmTo) {\n      this.command = command;\n      this.confirmTo = confirmTo;\n    }\n  }\n\n  public static class State {\n    public final List<String> tasks;\n\n    public State(List<String> tasks) {\n      this.tasks = Collections.unmodifiableList(tasks);\n    }\n\n    public State add(String task) {\n      ArrayList<String> copy = new ArrayList<>(tasks);\n      copy.add(task);\n      return new State(copy);\n    }\n\n    public State remove(String task) {\n      ArrayList<String> copy = new ArrayList<>(tasks);\n      copy.remove(task);\n      return new State(copy);\n    }\n  }\n\n  public static Behavior<Command> create(\n      String id, DB db, ActorRef<ConsumerController.Start<Command>> consumerController) {\n    return Init.create(id, db, consumerController);\n  }\n\n  private static Behavior<Command> onDBError(DBError error) throws Exception {\n    throw error.cause;\n  }\n\n  static class Init extends AbstractBehavior<Command> {\n\n    private final String id;\n    private final DB db;\n    private final ActorRef<ConsumerController.Start<Command>> consumerController;\n\n    private Init(\n        ActorContext<Command> context,\n        String id,\n        DB db,\n        ActorRef<ConsumerController.Start<Command>> consumerController) {\n      super(context);\n      this.id = id;\n      this.db = db;\n      this.consumerController = consumerController;\n    }\n\n    static Behavior<Command> create(\n        String id, DB db, ActorRef<ConsumerController.Start<Command>> consumerController) {\n      return Behaviors.setup(\n          context -> {\n            context.pipeToSelf(\n                db.load(id),\n                (state, exc) -> {\n                  if (exc == null) return new InitialState(state);\n                  else return new DBError(exc);\n                });\n\n            return new Init(context, id, db, consumerController);\n          });\n    }\n\n    @Override\n    public Receive<Command> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(InitialState.class, this::onInitialState)\n          .onMessage(DBError.class, TodoList::onDBError)\n          .build();\n    }\n\n    private Behavior<Command> onInitialState(InitialState initial) {\n      ActorRef<ConsumerController.Delivery<Command>> deliveryAdapter =\n          getContext()\n              .messageAdapter(\n                  ConsumerController.deliveryClass(),\n                  d -> new CommandDelivery(d.message(), d.confirmTo()));\n      consumerController.tell(new ConsumerController.Start<>(deliveryAdapter));\n\n      return Active.create(id, db, initial.state);\n    }\n  }\n\n  static class Active extends AbstractBehavior<Command> {\n\n    private final String id;\n    private final DB db;\n    private State state;\n\n    private Active(ActorContext<Command> context, String id, DB db, State state) {\n      super(context);\n      this.id = id;\n      this.db = db;\n      this.state = state;\n    }\n\n    static Behavior<Command> create(String id, DB db, State state) {\n      return Behaviors.setup(context -> new Active(context, id, db, state));\n    }\n\n    @Override\n    public Receive<Command> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(CommandDelivery.class, this::onDelivery)\n          .onMessage(SaveSuccess.class, this::onSaveSuccess)\n          .onMessage(DBError.class, TodoList::onDBError)\n          .build();\n    }\n\n    private Behavior<Command> onDelivery(CommandDelivery delivery) {\n      if (delivery.command instanceof AddTask) {\n        AddTask addTask = (AddTask) delivery.command;\n        state = state.add(addTask.item);\n        save(state, delivery.confirmTo);\n        return this;\n      } else if (delivery.command instanceof CompleteTask) {\n        CompleteTask completeTask = (CompleteTask) delivery.command;\n        state = state.remove(completeTask.item);\n        save(state, delivery.confirmTo);\n        return this;\n      } else {\n        return Behaviors.unhandled();\n      }\n    }\n\n    private void save(State newState, ActorRef<ConsumerController.Confirmed> confirmTo) {\n      getContext()\n          .pipeToSelf(\n              db.save(id, newState),\n              (state, exc) -> {\n                if (exc == null) return new SaveSuccess(confirmTo);\n                else return new DBError(exc);\n              });\n    }\n\n    private Behavior<Command> onSaveSuccess(SaveSuccess success) {\n      success.confirmTo.tell(ConsumerController.confirmed());\n      return this;\n    }\n  }\n}\nand TodoService (producer):\nScala copysourceimport pekko.cluster.sharding.typed.delivery.ShardingProducerController\n\nobject TodoService {\n  sealed trait Command\n\n  final case class UpdateTodo(listId: String, item: String, completed: Boolean, replyTo: ActorRef[Response])\n      extends Command\n\n  sealed trait Response\n  case object Accepted extends Response\n  case object Rejected extends Response\n  case object MaybeAccepted extends Response\n\n  private final case class WrappedRequestNext(requestNext: ShardingProducerController.RequestNext[TodoList.Command])\n      extends Command\n  private final case class Confirmed(originalReplyTo: ActorRef[Response]) extends Command\n  private final case class TimedOut(originalReplyTo: ActorRef[Response]) extends Command\n\n  def apply(producerController: ActorRef[ShardingProducerController.Command[TodoList.Command]]): Behavior[Command] = {\n    Behaviors.setup { context =>\n      new TodoService(context).start(producerController)\n    }\n  }\n\n}\n\nclass TodoService(context: ActorContext[TodoService.Command]) {\n  import TodoService._\n\n  private implicit val askTimeout: Timeout = 5.seconds\n\n  private def start(\n      producerController: ActorRef[ShardingProducerController.Start[TodoList.Command]]): Behavior[Command] = {\n    val requestNextAdapter: ActorRef[ShardingProducerController.RequestNext[TodoList.Command]] =\n      context.messageAdapter(WrappedRequestNext.apply)\n    producerController ! ShardingProducerController.Start(requestNextAdapter)\n\n    Behaviors.receiveMessagePartial {\n      case WrappedRequestNext(next) =>\n        active(next)\n      case UpdateTodo(_, _, _, replyTo) =>\n        // not hooked up with shardingProducerController yet\n        replyTo ! Rejected\n        Behaviors.same\n    }\n  }\n\n  private def active(requestNext: ShardingProducerController.RequestNext[TodoList.Command]): Behavior[Command] = {\n    Behaviors.receiveMessage {\n      case WrappedRequestNext(next) =>\n        active(next)\n\n      case UpdateTodo(listId, item, completed, replyTo) =>\n        if (requestNext.bufferedForEntitiesWithoutDemand.getOrElse(listId, 0) >= 100)\n          replyTo ! Rejected\n        else {\n          val requestMsg = if (completed) TodoList.CompleteTask(item) else TodoList.AddTask(item)\n          context.ask[ShardingProducerController.MessageWithConfirmation[TodoList.Command], Done](\n            requestNext.askNextTo,\n            askReplyTo => ShardingProducerController.MessageWithConfirmation(listId, requestMsg, askReplyTo)) {\n            case Success(Done) => Confirmed(replyTo)\n            case Failure(_)    => TimedOut(replyTo)\n          }\n        }\n        Behaviors.same\n\n      case Confirmed(originalReplyTo) =>\n        originalReplyTo ! Accepted\n        Behaviors.same\n\n      case TimedOut(originalReplyTo) =>\n        originalReplyTo ! MaybeAccepted\n        Behaviors.same\n    }\n  }\n\n} Java copysourceimport org.apache.pekko.cluster.sharding.typed.delivery.ShardingProducerController;\n\npublic class TodoService {\n\n  interface Command {}\n\n  public static class UpdateTodo implements Command {\n    public final String listId;\n    public final String item;\n    public final boolean completed;\n    public final ActorRef<Response> replyTo;\n\n    public UpdateTodo(String listId, String item, boolean completed, ActorRef<Response> replyTo) {\n      this.listId = listId;\n      this.item = item;\n      this.completed = completed;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public enum Response {\n    ACCEPTED,\n    REJECTED,\n    MAYBE_ACCEPTED\n  }\n\n  private static class Confirmed implements Command {\n    final ActorRef<Response> originalReplyTo;\n\n    private Confirmed(ActorRef<Response> originalReplyTo) {\n      this.originalReplyTo = originalReplyTo;\n    }\n  }\n\n  private static class TimedOut implements Command {\n    final ActorRef<Response> originalReplyTo;\n\n    private TimedOut(ActorRef<Response> originalReplyTo) {\n      this.originalReplyTo = originalReplyTo;\n    }\n  }\n\n  private static class WrappedRequestNext implements Command {\n    final ShardingProducerController.RequestNext<TodoList.Command> next;\n\n    private WrappedRequestNext(ShardingProducerController.RequestNext<TodoList.Command> next) {\n      this.next = next;\n    }\n  }\n\n  public static Behavior<Command> create(\n      ActorRef<ShardingProducerController.Command<TodoList.Command>> producerController) {\n    return Init.create(producerController);\n  }\n\n  static class Init extends AbstractBehavior<TodoService.Command> {\n\n    static Behavior<Command> create(\n        ActorRef<ShardingProducerController.Command<TodoList.Command>> producerController) {\n      return Behaviors.setup(\n          context -> {\n            ActorRef<ShardingProducerController.RequestNext<TodoList.Command>>\n                requestNextAdapter =\n                    context.messageAdapter(\n                        ShardingProducerController.requestNextClass(), WrappedRequestNext::new);\n            producerController.tell(new ShardingProducerController.Start<>(requestNextAdapter));\n\n            return new Init(context);\n          });\n    }\n\n    private Init(ActorContext<Command> context) {\n      super(context);\n    }\n\n    @Override\n    public Receive<Command> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(WrappedRequestNext.class, w -> Active.create(w.next))\n          .onMessage(\n              UpdateTodo.class,\n              command -> {\n                // not hooked up with shardingProducerController yet\n                command.replyTo.tell(Response.REJECTED);\n                return this;\n              })\n          .build();\n    }\n  }\n\n  static class Active extends AbstractBehavior<TodoService.Command> {\n\n    private ShardingProducerController.RequestNext<TodoList.Command> requestNext;\n\n    static Behavior<Command> create(\n        ShardingProducerController.RequestNext<TodoList.Command> requestNext) {\n      return Behaviors.setup(context -> new Active(context, requestNext));\n    }\n\n    private Active(\n        ActorContext<Command> context,\n        ShardingProducerController.RequestNext<TodoList.Command> requestNext) {\n      super(context);\n      this.requestNext = requestNext;\n    }\n\n    @Override\n    public Receive<Command> createReceive() {\n      return newReceiveBuilder()\n          .onMessage(WrappedRequestNext.class, this::onRequestNext)\n          .onMessage(UpdateTodo.class, this::onUpdateTodo)\n          .onMessage(Confirmed.class, this::onConfirmed)\n          .onMessage(TimedOut.class, this::onTimedOut)\n          .build();\n    }\n\n    private Behavior<Command> onRequestNext(WrappedRequestNext w) {\n      requestNext = w.next;\n      return this;\n    }\n\n    private Behavior<Command> onUpdateTodo(UpdateTodo command) {\n      Integer buffered = requestNext.getBufferedForEntitiesWithoutDemand().get(command.listId);\n      if (buffered != null && buffered >= 100) {\n        command.replyTo.tell(Response.REJECTED);\n      } else {\n        TodoList.Command requestMsg;\n        if (command.completed) requestMsg = new TodoList.CompleteTask(command.item);\n        else requestMsg = new TodoList.AddTask(command.item);\n        getContext()\n            .ask(\n                Done.class,\n                requestNext.askNextTo(),\n                Duration.ofSeconds(5),\n                askReplyTo ->\n                    new ShardingProducerController.MessageWithConfirmation<>(\n                        command.listId, requestMsg, askReplyTo),\n                (done, exc) -> {\n                  if (exc == null) return new Confirmed(command.replyTo);\n                  else return new TimedOut(command.replyTo);\n                });\n      }\n      return this;\n    }\n\n    private Behavior<Command> onConfirmed(Confirmed confirmed) {\n      confirmed.originalReplyTo.tell(Response.ACCEPTED);\n      return this;\n    }\n\n    private Behavior<Command> onTimedOut(TimedOut timedOut) {\n      timedOut.originalReplyTo.tell(Response.MAYBE_ACCEPTED);\n      return this;\n    }\n  }\n}\nNote how the ActorRef in the Start messages are constructed as message adapters to map the RequestNext and Delivery to the protocol of the producer and consumer actors respectively.\nThose are initialized with sharding like this (from the guardian):\nJava copysourceimport org.apache.pekko.cluster.sharding.typed.delivery.ShardingConsumerController;\nimport org.apache.pekko.cluster.sharding.typed.ShardingEnvelope;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.ClusterSharding;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.Entity;\nimport org.apache.pekko.cluster.sharding.typed.javadsl.EntityTypeKey;\nimport org.apache.pekko.cluster.typed.Cluster;\nimport org.apache.pekko.actor.typed.ActorSystem;\n\nfinal DB db = theDatabaseImplementation();\n\nActorSystem<Void> system = context.getSystem();\n\nEntityTypeKey<ConsumerController.SequencedMessage<TodoList.Command>> entityTypeKey =\n    EntityTypeKey.create(ShardingConsumerController.entityTypeKeyClass(), \"todo\");\n\nActorRef<ShardingEnvelope<ConsumerController.SequencedMessage<TodoList.Command>>> region =\n    ClusterSharding.get(system)\n        .init(\n            Entity.of(\n                entityTypeKey,\n                entityContext ->\n                    ShardingConsumerController.create(\n                        start ->\n                            TodoList.create(entityContext.getEntityId(), db, start))));\n\nAddress selfAddress = Cluster.get(system).selfMember().address();\nString producerId = \"todo-producer-\" + selfAddress.hostPort();\n\nActorRef<ShardingProducerController.Command<TodoList.Command>> producerController =\n    context.spawn(\n        ShardingProducerController.create(\n            TodoList.Command.class, producerId, region, Optional.empty()),\n        \"producerController\");\n\ncontext.spawn(TodoService.create(producerController), \"producer\");","title":"Sharding example"},{"location":"/typed/reliable-delivery.html#sharding-delivery-semantics","text":"As long as neither producer nor consumer crash the messages are delivered to the consumer actor in the same order as they were sent to the ShardingProducerController, without loss or duplicates. Meaning effectively-once processing without any business level deduplication.\nUnconfirmed messages may be lost if the producer crashes. To avoid that you need to enable the durable queue on the producer side. The stored unconfirmed messages will be redelivered when the corresponding producer is started again. In that case there may be delivery of messages that had already been processed but the fact that they were confirmed had not been stored yet. Meaning at-least-once delivery.\nIf the consumer crashes or the shard is rebalanced the unconfirmed messages will be redelivered. In that case some of these may already have been processed by the previous consumer.","title":"Sharding delivery semantics"},{"location":"/typed/reliable-delivery.html#durable-producer","text":"Until sent messages have been confirmed the producer side keeps them in memory to be able to resend them. If the JVM of the producer side crashes those unconfirmed messages are lost. To make sure the messages can be delivered also in that scenario a DurableProducerQueueDurableProducerQueue can be used. Then the unconfirmed messages are stored in a durable way so that they can be redelivered when the producer is started again. An implementation of the DurableProducerQueue is provided by EventSourcedProducerQueueEventSourcedProducerQueue in pekko-persistence-typed.\nBe aware of that a DurableProducerQueue will add a substantial performance overhead.\nWhen using the EventSourcedProducerQueue the following dependency is needed:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-typed_${versions.ScalaBinary}\"\n}\nYou also have to select journal plugin and snapshot store plugin, see Persistence Plugins.\nExample of the image converter work manager from the Work pulling example with EventSourcedProducerQueue enabled:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.typed.delivery.EventSourcedProducerQueue\nimport pekko.persistence.typed.PersistenceId\n\nval durableQueue =\n  EventSourcedProducerQueue[ImageConverter.ConversionJob](PersistenceId.ofUniqueId(\"ImageWorkManager\"))\nval durableProducerController = context.spawn(\n  WorkPullingProducerController(\n    producerId = \"workManager\",\n    workerServiceKey = ImageConverter.serviceKey,\n    durableQueueBehavior = Some(durableQueue)),\n  \"producerController\") Java copysourceimport org.apache.pekko.persistence.typed.PersistenceId;\nimport org.apache.pekko.persistence.typed.delivery.EventSourcedProducerQueue;\n\nBehavior<DurableProducerQueue.Command<ImageConverter.ConversionJob>> durableQueue =\n    EventSourcedProducerQueue.create(PersistenceId.ofUniqueId(\"ImageWorkManager\"));\nActorRef<WorkPullingProducerController.Command<ImageConverter.ConversionJob>>\n    durableProducerController =\n        context.spawn(\n            WorkPullingProducerController.create(\n                ImageConverter.ConversionJob.class,\n                \"workManager\",\n                ImageConverter.serviceKey,\n                Optional.of(durableQueue)),\n            \"producerController\");\nIt’s important to note that the EventSourcedProducerQueue requires a PersistenceId, which must be unique. The same PersistenceId must not be used for different producers at the same time. A Cluster Singleton hosting the producer would satisfy that requirement, or one producer per node and a naming scheme to ensure that different nodes use different PersistenceId.\nTo deliver unconfirmed messages after a crash the producer must be started again with same PersistenceId as before the crash.","title":"Durable producer"},{"location":"/typed/reliable-delivery.html#ask-from-the-producer","text":"Instead of using tell with the sendNextTo in the RequestNext the producer can use context.ask with the askNextTo in the RequestNext. The difference is that a reply is sent back when the message has been handled. To include the replyTo ActorRef the message must be wrapped in a MessageWithConfirmation. If a DurableProducerQueue is used then the reply is sent when the message has been stored successfully, but it might not have been processed by the consumer yet. Otherwise the reply is sent after the consumer has processed and confirmed the message.\nExample of using ask in the image converter work manager from the Work pulling example:\nScala copysourcefinal case class ConvertRequest(\n    fromFormat: String,\n    toFormat: String,\n    image: Array[Byte],\n    replyTo: ActorRef[ConvertResponse])\n    extends Command\n\nsealed trait ConvertResponse\nfinal case class ConvertAccepted(resultId: UUID) extends ConvertResponse\ncase object ConvertRejected extends ConvertResponse\nfinal case class ConvertTimedOut(resultId: UUID) extends ConvertResponse\n\nprivate final case class AskReply(resultId: UUID, originalReplyTo: ActorRef[ConvertResponse], timeout: Boolean)\n    extends Command\n\n  import WorkPullingProducerController.MessageWithConfirmation\n  import pekko.util.Timeout\n\n  implicit val askTimeout: Timeout = 5.seconds\n\n  private def waitForNext(): Behavior[Command] = {\n    Behaviors.receiveMessagePartial {\n      case WrappedRequestNext(next) =>\n        stashBuffer.unstashAll(active(next))\n      case c: ConvertRequest =>\n        if (stashBuffer.isFull) {\n          c.replyTo ! ConvertRejected\n          Behaviors.same\n        } else {\n          stashBuffer.stash(c)\n          Behaviors.same\n        }\n      case AskReply(resultId, originalReplyTo, timeout) =>\n        val response = if (timeout) ConvertTimedOut(resultId) else ConvertAccepted(resultId)\n        originalReplyTo ! response\n        Behaviors.same\n      case GetResult(resultId, replyTo) =>\n        // TODO retrieve the stored result and reply\n        Behaviors.same\n    }\n  }\n\n  private def active(\n      next: WorkPullingProducerController.RequestNext[ImageConverter.ConversionJob]): Behavior[Command] = {\n    Behaviors.receiveMessagePartial {\n      case ConvertRequest(from, to, image, originalReplyTo) =>\n        val resultId = UUID.randomUUID()\n        context.ask[MessageWithConfirmation[ImageConverter.ConversionJob], Done](\n          next.askNextTo,\n          askReplyTo =>\n            MessageWithConfirmation(ImageConverter.ConversionJob(resultId, from, to, image), askReplyTo)) {\n          case Success(done) => AskReply(resultId, originalReplyTo, timeout = false)\n          case Failure(_)    => AskReply(resultId, originalReplyTo, timeout = true)\n        }\n        waitForNext()\n      case AskReply(resultId, originalReplyTo, timeout) =>\n        val response = if (timeout) ConvertTimedOut(resultId) else ConvertAccepted(resultId)\n        originalReplyTo ! response\n        Behaviors.same\n      case GetResult(resultId, replyTo) =>\n        // TODO retrieve the stored result and reply\n        Behaviors.same\n      case _: WrappedRequestNext =>\n        throw new IllegalStateException(\"Unexpected RequestNext\")\n    }\n  }\n Java copysourcepublic static class ConvertRequest implements Command {\n  public final String fromFormat;\n  public final String toFormat;\n  public final byte[] image;\n  public final ActorRef<ConvertResponse> replyTo;\n\n  public ConvertRequest(\n      String fromFormat, String toFormat, byte[] image, ActorRef<ConvertResponse> replyTo) {\n    this.fromFormat = fromFormat;\n    this.toFormat = toFormat;\n    this.image = image;\n    this.replyTo = replyTo;\n  }\n}\n\ninterface ConvertResponse {}\n\npublic static class ConvertAccepted implements ConvertResponse {\n  public final UUID resultId;\n\n  public ConvertAccepted(UUID resultId) {\n    this.resultId = resultId;\n  }\n}\n\nenum ConvertRejected implements ConvertResponse {\n  INSTANCE\n}\n\npublic static class ConvertTimedOut implements ConvertResponse {\n  public final UUID resultId;\n\n  public ConvertTimedOut(UUID resultId) {\n    this.resultId = resultId;\n  }\n}\n\nprivate static class AskReply implements Command {\n  final UUID resultId;\n  final ActorRef<ConvertResponse> originalReplyTo;\n  final boolean timeout;\n\n  private AskReply(UUID resultId, ActorRef<ConvertResponse> originalReplyTo, boolean timeout) {\n    this.resultId = resultId;\n    this.originalReplyTo = originalReplyTo;\n    this.timeout = timeout;\n  }\n}\n\n      private Behavior<Command> waitForNext() {\n        return Behaviors.receive(Command.class)\n            .onMessage(WrappedRequestNext.class, this::onWrappedRequestNext)\n            .onMessage(ConvertRequest.class, this::onConvertRequestWait)\n            .onMessage(AskReply.class, this::onAskReply)\n            .onMessage(GetResult.class, this::onGetResult)\n            .build();\n      }\n\n      private Behavior<Command> onConvertRequestWait(ConvertRequest convert) {\n        if (stashBuffer.isFull()) {\n          convert.replyTo.tell(ConvertRejected.INSTANCE);\n          return Behaviors.same();\n        } else {\n          stashBuffer.stash(convert);\n          return Behaviors.same();\n        }\n      }\n\n      private Behavior<Command> onAskReply(AskReply reply) {\n        if (reply.timeout) reply.originalReplyTo.tell(new ConvertTimedOut(reply.resultId));\n        else reply.originalReplyTo.tell(new ConvertAccepted(reply.resultId));\n        return Behaviors.same();\n      }\n\n      private Behavior<Command> onWrappedRequestNext(WrappedRequestNext w) {\n        return stashBuffer.unstashAll(active(w.next));\n      }\n\n      private Behavior<Command> onGetResult(GetResult get) {\n        // TODO retrieve the stored result and reply\n        return Behaviors.same();\n      }\n\n      private Behavior<Command> active(\n          WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {\n        return Behaviors.receive(Command.class)\n            .onMessage(ConvertRequest.class, c -> onConvertRequest(c, next))\n            .onMessage(AskReply.class, this::onAskReply)\n            .onMessage(GetResult.class, this::onGetResult)\n            .onMessage(WrappedRequestNext.class, this::onUnexpectedWrappedRequestNext)\n            .build();\n      }\n\n      private Behavior<Command> onConvertRequest(\n          ConvertRequest convert,\n          WorkPullingProducerController.RequestNext<ImageConverter.ConversionJob> next) {\n        UUID resultId = UUID.randomUUID();\n\n        context.ask(\n            Done.class,\n            next.askNextTo(),\n            Duration.ofSeconds(5),\n            askReplyTo ->\n                new WorkPullingProducerController.MessageWithConfirmation<>(\n                    new ImageConverter.ConversionJob(\n                        resultId, convert.fromFormat, convert.toFormat, convert.image),\n                    askReplyTo),\n            (done, exc) -> {\n              if (exc == null) return new AskReply(resultId, convert.replyTo, false);\n              else return new AskReply(resultId, convert.replyTo, true);\n            });\n\n        return waitForNext();\n      }\n\n      private Behavior<Command> onUnexpectedWrappedRequestNext(WrappedRequestNext w) {\n        throw new IllegalStateException(\"Unexpected RequestNext\");\n      }","title":"Ask from the producer"},{"location":"/typed/reliable-delivery.html#only-flow-control","text":"It’s possible to use this without resending lost messages, but the flow control is still used. This can for example be useful when both consumer and producer are know to be located in the same local ActorSystem. This can be more efficient since messages don’t have to be kept in memory in the ProducerController until they have been confirmed, but the drawback is that lost messages will not be delivered. See configuration only-flow-control of the ConsumerController.","title":"Only flow control"},{"location":"/typed/reliable-delivery.html#chunk-large-messages","text":"To avoid head of line blocking from serialization and transfer of large messages the Point-to-Point pattern has support for automatically splitting up large messages and assemble them again on the consumer side.\nSerialization and deserialization is performed by the ProducerController and ConsumerController respectively instead of in the remote transport layer.\nThis is enabled by configuration pekko.reliable-delivery.producer-controller.chunk-large-messages and defines the maximum size in bytes of the chunked pieces. Messages smaller than the configured size are not chunked, but serialization still takes place in the ProducerController and ConsumerController.\nAside from the configuration the API is the same as the Point-to-point pattern. If Durable producer is enabled the chunked pieces are stored rather than the full large message.\nThis feature is not implemented for Work pulling and Sharding yet.","title":"Chunk large messages"},{"location":"/typed/reliable-delivery.html#configuration","text":"There are several configuration properties, please refer to pekko.reliable-delivery config section in the reference configuration:\npekko-actor-typed reference configuration pekko-persistence-typed reference configuration pekko-cluster-sharding-typed reference configuration","title":"Configuration"},{"location":"/serialization.html","text":"","title":"Serialization"},{"location":"/serialization.html#serialization","text":"","title":"Serialization"},{"location":"/serialization.html#dependency","text":"To use Serialization, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/serialization.html#introduction","text":"The messages that Pekko actors send to each other are JVM objects (e.g. instances of Scala case classes). Message passing between actors that live on the same JVM is straightforward. It is done via reference passing. However, messages that have to escape the JVM to reach an actor running on a different host have to undergo some form of serialization (i.e. the objects have to be converted to and from byte arrays).\nThe serialization mechanism in Pekko allows you to write custom serializers and to define which serializer to use for what.\nSerialization with Jackson is a good choice in many cases and our recommendation if you don’t have other preference.\nGoogle Protocol Buffers is good if you want more control over the schema evolution of your messages, but it requires more work to develop and maintain the mapping between serialized representation and domain representation.\nPekko itself uses Protocol Buffers to serialize internal messages (for example cluster gossip messages).","title":"Introduction"},{"location":"/serialization.html#usage","text":"","title":"Usage"},{"location":"/serialization.html#configuration","text":"For Pekko to know which Serializer to use for what, you need to edit your configuration: in the pekko.actor.serializers-section, you bind names to implementations of the serialization.Serializerserialization.Serializer you wish to use, like this:\ncopysourcepekko {\n  actor {\n    serializers {\n      jackson-json = \"org.apache.pekko.serialization.jackson.JacksonJsonSerializer\"\n      jackson-cbor = \"org.apache.pekko.serialization.jackson.JacksonCborSerializer\"\n      proto = \"org.apache.pekko.remote.serialization.ProtobufSerializer\"\n      myown = \"docs.serialization.MyOwnSerializer\"\n    }\n  }\n}\nAfter you’ve bound names to different implementations of Serializer you need to wire which classes should be serialized using which Serializer, this is done in the pekko.actor.serialization-bindings-section:\ncopysourcepekko {\n  actor {\n    serializers {\n      jackson-json = \"org.apache.pekko.serialization.jackson.JacksonJsonSerializer\"\n      jackson-cbor = \"org.apache.pekko.serialization.jackson.JacksonCborSerializer\"\n      proto = \"org.apache.pekko.remote.serialization.ProtobufSerializer\"\n      myown = \"docs.serialization.MyOwnSerializer\"\n    }\n\n    serialization-bindings {\n      \"docs.serialization.JsonSerializable\" = jackson-json\n      \"docs.serialization.CborSerializable\" = jackson-cbor\n      \"com.google.protobuf.Message\" = proto\n      \"docs.serialization.MyOwnSerializable\" = myown\n    }\n  }\n}\nYou only need to specify the name of a traitan interface or abstract base class of the messages. In case of ambiguity, i.e. the message implements several of the configured classes, the most specific configured class will be used, i.e. the one of which all other candidates are superclasses. If this condition cannot be met, because e.g. two marker traitsinterfaces that have been configured for serialization both apply and neither is a subtype of the other, a warning will be issued.\nNote If you are using Scala for your message protocol and your messages are contained inside of a Scala object, then in order to reference those messages, you will need to use the fully qualified Java class name. For a message named Message contained inside the Scala object named Wrapper you would need to reference it as Wrapper$Message instead of Wrapper.Message.\nPekko provides serializers for several primitive types and protobuf com.google.protobuf.GeneratedMessage (protobuf2) and com.google.protobuf.GeneratedMessageV3 (protobuf3) by default (the latter only if depending on the pekko-remote module), so normally you don’t need to add configuration for that if you send raw protobuf messages as actor messages.","title":"Configuration"},{"location":"/serialization.html#programmatic","text":"If you want to programmatically serialize/deserialize using Pekko Serialization, here are some examples:\nScala copysourceimport org.apache.pekko\nimport pekko.actor._\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.cluster.Cluster\nimport pekko.serialization._\n Java copysourceimport org.apache.pekko.actor.*;\nimport org.apache.pekko.serialization.*;\n\nimport java.nio.charset.Charset;\nimport java.nio.charset.StandardCharsets;\nScala copysourceval system = ActorSystem(\"example\")\n\n// Get the Serialization Extension\nval serialization = SerializationExtension(system)\n\n// Have something to serialize\nval original = \"woohoo\"\n\n// Turn it into bytes, and retrieve the serializerId and manifest, which are needed for deserialization\nval bytes = serialization.serialize(original).get\nval serializerId = serialization.findSerializerFor(original).identifier\nval manifest = Serializers.manifestFor(serialization.findSerializerFor(original), original)\n\n// Turn it back into an object\nval back = serialization.deserialize(bytes, serializerId, manifest).get Java copysourceActorSystem system = ActorSystem.create(\"example\");\n\n// Get the Serialization Extension\nSerialization serialization = SerializationExtension.get(system);\n\n// Have something to serialize\nString original = \"woohoo\";\n\n// Turn it into bytes, and retrieve the serializerId and manifest, which are needed for\n// deserialization\nbyte[] bytes = serialization.serialize(original).get();\nint serializerId = serialization.findSerializerFor(original).identifier();\nString manifest = Serializers.manifestFor(serialization.findSerializerFor(original), original);\n\n// Turn it back into an object\nString back = (String) serialization.deserialize(bytes, serializerId, manifest).get();\nThe manifest is a type hint so that the same serializer can be used for different classes.\nNote that when deserializing from bytes the manifest and the identifier of the serializer are needed. It is important to use the serializer identifier in this way to support rolling updates, where the serialization-bindings for a class may have changed from one serializer to another. Therefore the three parts consisting of the bytes, the serializer id, and the manifest should always be transferred or stored together so that they can be deserialized with different serialization-bindings configuration.\nThe SerializationExtensionSerializationExtension is a Classic ExtensionExtension, but it can be used with an actor.typed.ActorSystemactor.typed.ActorSystem like this:\nScala copysourceimport org.apache.pekko.actor.typed.ActorSystem\n\nval system = ActorSystem(Behaviors.empty, \"example\")\n\n// Get the Serialization Extension\nval serialization = SerializationExtension(system) Java copysourceorg.apache.pekko.actor.typed.ActorSystem<Void> system =\n    org.apache.pekko.actor.typed.ActorSystem.create(Behaviors.empty(), \"example\");\n\n// Get the Serialization Extension\nSerialization serialization = SerializationExtension.get(system);","title":"Programmatic"},{"location":"/serialization.html#customization","text":"The first code snippet on this page contains a configuration file that references a custom serializer docs.serialization.MyOwnSerializer. How would we go about creating such a custom serializer?","title":"Customization"},{"location":"/serialization.html#creating-new-serializers","text":"A custom Serializer has to inherit from serialization.Serializerserialization.Serializerserialization.JSerializerserialization.JSerializer and can be defined like the following:\nScala copysourceimport org.apache.pekko\nimport pekko.actor._\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.cluster.Cluster\nimport pekko.serialization._\n Java copysourceimport org.apache.pekko.actor.*;\nimport org.apache.pekko.serialization.*;\n\nimport java.nio.charset.Charset;\nimport java.nio.charset.StandardCharsets;\nScala copysourceclass MyOwnSerializer extends Serializer {\n\n  // If you need logging here, introduce a constructor that takes an ExtendedActorSystem.\n  // class MyOwnSerializer(actorSystem: ExtendedActorSystem) extends Serializer\n  // Get a logger using:\n  // private val logger = Logging(actorSystem, this)\n\n  // This is whether \"fromBinary\" requires a \"clazz\" or not\n  def includeManifest: Boolean = true\n\n  // Pick a unique identifier for your Serializer,\n  // you've got a couple of billions to choose from,\n  // 0 - 40 is reserved by Pekko itself\n  def identifier = 1234567\n\n  // \"toBinary\" serializes the given object to an Array of Bytes\n  def toBinary(obj: AnyRef): Array[Byte] = {\n    // Put the code that serializes the object here\n    // #...\n    Array[Byte]()\n    // #...\n  }\n\n  // \"fromBinary\" deserializes the given array,\n  // using the type hint (if any, see \"includeManifest\" above)\n  def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {\n    // Put your code that deserializes here\n    // #...\n    null\n    // #...\n  }\n} Java copysourcestatic class MyOwnSerializer extends JSerializer {\n\n  // If you need logging here, introduce a constructor that takes an ExtendedActorSystem.\n  // public MyOwnSerializer(ExtendedActorSystem actorSystem)\n  // Get a logger using:\n  // private final LoggingAdapter logger = Logging.getLogger(actorSystem, this);\n\n  // This is whether \"fromBinary\" requires a \"clazz\" or not\n  @Override\n  public boolean includeManifest() {\n    return false;\n  }\n\n  // Pick a unique identifier for your Serializer,\n  // you've got a couple of billions to choose from,\n  // 0 - 40 is reserved by Pekko itself\n  @Override\n  public int identifier() {\n    return 1234567;\n  }\n\n  // \"toBinary\" serializes the given object to an Array of Bytes\n  @Override\n  public byte[] toBinary(Object obj) {\n    // Put the code that serializes the object here\n    // #...\n    return new byte[0];\n    // #...\n  }\n\n  // \"fromBinary\" deserializes the given array,\n  // using the type hint (if any, see \"includeManifest\" above)\n  @Override\n  public Object fromBinaryJava(byte[] bytes, Class<?> clazz) {\n    // Put your code that deserializes here\n    // #...\n    return null;\n    // #...\n  }\n}\nThe identifier must be unique. The identifier is used when selecting which serializer to use for deserialization. If you have accidentally configured several serializers with the same identifier that will be detected and prevent the ActorSystemActorSystem from being started. It can be a hardcoded value because it must remain the same value to support rolling updates.\nIf you prefer to define the identifier in cofiguration that is supported by the BaseSerializerBaseSerializer trait, which implements the def identifier by reading it from configuration based on the serializer’s class name: Scala copysourcepekko {\n  actor {\n    serialization-identifiers {\n      \"docs.serialization.MyOwnSerializer\" = 1234567\n    }\n  }\n}\nThe manifest is a type hint so that the same serializer can be used for different classes. The manifest parameter in fromBinaryfromBinaryJava is the class of the object that was serialized. In fromBinaryfromBinaryJava you can match on the class and deserialize the bytes to different objects.\nThen you only need to fill in the blanks, bind it to a name in your configuration and list which classes should be deserialized with it.\nThe serializers are initialized eagerly by the SerializationExtensionSerializationExtension when the ActorSystemActorSystem is started and therefore a serializer itself must not access the SerializationExtension from its constructor. Instead, it should access the SerializationExtension lazily.","title":"Creating new Serializers"},{"location":"/serialization.html#serializer-with-string-manifest","text":"The Serializer illustrated above supports a class based manifest (type hint). For serialization of data that need to evolve over time the SerializerWithStringManifestSerializerWithStringManifest is recommended instead of Serializer because the manifest (type hint) is a String instead of a Class. That means that the class can be moved/removed and the serializer can still deserialize old data by matching on the String. This is especially useful for Persistence.\nThe manifest string can also encode a version number that can be used in fromBinaryfromBinaryJava to deserialize in different ways to migrate old data to new domain objects.\nIf the data was originally serialized with Serializer and in a later version of the system you change to SerializerWithStringManifest then the manifest string will be the full class name if you used includeManifest=true, otherwise it will be the empty string.\nThis is how a SerializerWithStringManifest looks like:\nScala copysourceclass MyOwnSerializer2 extends SerializerWithStringManifest {\n\n  val CustomerManifest = \"customer\"\n  val UserManifest = \"user\"\n  val UTF_8 = StandardCharsets.UTF_8.name()\n\n  // Pick a unique identifier for your Serializer,\n  // you've got a couple of billions to choose from,\n  // 0 - 40 is reserved by Pekko itself\n  def identifier = 1234567\n\n  // The manifest (type hint) that will be provided in the fromBinary method\n  // Use `\"\"` if manifest is not needed.\n  def manifest(obj: AnyRef): String =\n    obj match {\n      case _: Customer => CustomerManifest\n      case _: User     => UserManifest\n    }\n\n  // \"toBinary\" serializes the given object to an Array of Bytes\n  def toBinary(obj: AnyRef): Array[Byte] = {\n    // Put the real code that serializes the object here\n    obj match {\n      case Customer(name) => name.getBytes(UTF_8)\n      case User(name)     => name.getBytes(UTF_8)\n    }\n  }\n\n  // \"fromBinary\" deserializes the given array,\n  // using the type hint\n  def fromBinary(bytes: Array[Byte], manifest: String): AnyRef = {\n    // Put the real code that deserializes here\n    manifest match {\n      case CustomerManifest =>\n        Customer(new String(bytes, UTF_8))\n      case UserManifest =>\n        User(new String(bytes, UTF_8))\n    }\n  }\n} Java copysourcestatic class MyOwnSerializer2 extends SerializerWithStringManifest {\n\n  private static final String CUSTOMER_MANIFEST = \"customer\";\n  private static final String USER_MANIFEST = \"user\";\n  private static final Charset UTF_8 = StandardCharsets.UTF_8;\n\n  // Pick a unique identifier for your Serializer,\n  // you've got a couple of billions to choose from,\n  // 0 - 40 is reserved by Pekko itself\n  @Override\n  public int identifier() {\n    return 1234567;\n  }\n\n  @Override\n  public String manifest(Object obj) {\n    if (obj instanceof Customer) return CUSTOMER_MANIFEST;\n    else if (obj instanceof User) return USER_MANIFEST;\n    else throw new IllegalArgumentException(\"Unknown type: \" + obj);\n  }\n\n  // \"toBinary\" serializes the given object to an Array of Bytes\n  @Override\n  public byte[] toBinary(Object obj) {\n    // Put the real code that serializes the object here\n    if (obj instanceof Customer) return ((Customer) obj).name.getBytes(UTF_8);\n    else if (obj instanceof User) return ((User) obj).name.getBytes(UTF_8);\n    else throw new IllegalArgumentException(\"Unknown type: \" + obj);\n  }\n\n  // \"fromBinary\" deserializes the given array,\n  // using the type hint\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) {\n    // Put the real code that deserializes here\n    if (manifest.equals(CUSTOMER_MANIFEST)) return new Customer(new String(bytes, UTF_8));\n    else if (manifest.equals(USER_MANIFEST)) return new User(new String(bytes, UTF_8));\n    else throw new IllegalArgumentException(\"Unknown manifest: \" + manifest);\n  }\n}\nYou must also bind it to a name in your configuration and then list which classes should be serialized by it.\nIt’s recommended to throw IllegalArgumentException or NotSerializableException in fromBinary if the manifest is unknown. This makes it possible to introduce new message types and send them to nodes that don’t know about them. This is typically needed when performing rolling updates, i.e. running a cluster with mixed versions for a while. Those exceptions are treated as a transient problem in the classic remoting layer. The problem will be logged and the message dropped. Other exceptions will tear down the TCP connection because it can be an indication of corrupt bytes from the underlying transport. Artery TCP handles all deserialization exceptions as transient problems.","title":"Serializer with String Manifest"},{"location":"/serialization.html#serializing-actorrefs","text":"Actor references are typically included in the messages. All ActorRefs are serializable when using Serialization with Jackson, but in case you are writing your own serializer, you might want to know how to serialize and deserialize them properly.\nTo serialize actor references to/from string representation you would use the ActorRefResolverActorRefResolver.\nFor example here’s how a serializer could look for Ping and Pong messages:\nScala copysourceclass PingSerializer(system: ExtendedActorSystem) extends SerializerWithStringManifest {\n  private val actorRefResolver = ActorRefResolver(system.toTyped)\n\n  private val PingManifest = \"a\"\n  private val PongManifest = \"b\"\n\n  override def identifier = 41\n\n  override def manifest(msg: AnyRef) = msg match {\n    case _: PingService.Ping => PingManifest\n    case PingService.Pong    => PongManifest\n    case _ =>\n      throw new IllegalArgumentException(s\"Can't serialize object of type ${msg.getClass} in [${getClass.getName}]\")\n  }\n\n  override def toBinary(msg: AnyRef) = msg match {\n    case PingService.Ping(who) =>\n      actorRefResolver.toSerializationFormat(who).getBytes(StandardCharsets.UTF_8)\n    case PingService.Pong =>\n      Array.emptyByteArray\n    case _ =>\n      throw new IllegalArgumentException(s\"Can't serialize object of type ${msg.getClass} in [${getClass.getName}]\")\n  }\n\n  override def fromBinary(bytes: Array[Byte], manifest: String) = {\n    manifest match {\n      case PingManifest =>\n        val str = new String(bytes, StandardCharsets.UTF_8)\n        val ref = actorRefResolver.resolveActorRef[PingService.Pong.type](str)\n        PingService.Ping(ref)\n      case PongManifest =>\n        PingService.Pong\n      case _ =>\n        throw new IllegalArgumentException(s\"Unknown manifest [$manifest]\")\n    }\n  }\n} Java copysourcepublic class PingSerializer extends SerializerWithStringManifest {\n\n  final ExtendedActorSystem system;\n  final ActorRefResolver actorRefResolver;\n\n  static final String PING_MANIFEST = \"a\";\n  static final String PONG_MANIFEST = \"b\";\n\n  PingSerializer(ExtendedActorSystem system) {\n    this.system = system;\n    actorRefResolver = ActorRefResolver.get(Adapter.toTyped(system));\n  }\n\n  @Override\n  public int identifier() {\n    return 97876;\n  }\n\n  @Override\n  public String manifest(Object obj) {\n    if (obj instanceof Ping) return PING_MANIFEST;\n    else if (obj instanceof Pong) return PONG_MANIFEST;\n    else\n      throw new IllegalArgumentException(\n          \"Can't serialize object of type \"\n              + obj.getClass()\n              + \" in [\"\n              + getClass().getName()\n              + \"]\");\n  }\n\n  @Override\n  public byte[] toBinary(Object obj) {\n    if (obj instanceof Ping)\n      return actorRefResolver\n          .toSerializationFormat(((Ping) obj).replyTo)\n          .getBytes(StandardCharsets.UTF_8);\n    else if (obj instanceof Pong) return new byte[0];\n    else\n      throw new IllegalArgumentException(\n          \"Can't serialize object of type \"\n              + obj.getClass()\n              + \" in [\"\n              + getClass().getName()\n              + \"]\");\n  }\n\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) {\n    if (PING_MANIFEST.equals(manifest)) {\n      String str = new String(bytes, StandardCharsets.UTF_8);\n      ActorRef<Pong> ref = actorRefResolver.resolveActorRef(str);\n      return new Ping(ref);\n    } else if (PONG_MANIFEST.equals(manifest)) {\n      return new Pong();\n    } else {\n      throw new IllegalArgumentException(\"Unable to handle manifest: \" + manifest);\n    }\n  }\n}\nSerialization of Classic ActorRefActorRef is described in Classic Serialization. Classic and Typed actor references have the same serialization format so they can be interchanged.","title":"Serializing ActorRefs"},{"location":"/serialization.html#deep-serialization-of-actors","text":"The recommended approach to do deep serialization of internal actor state is to use Pekko Persistence.","title":"Deep serialization of Actors"},{"location":"/serialization.html#serialization-of-pekkos-messages","text":"Pekko is using a Protobuf 3 for serialization of messages defined by Pekko. This dependency is shaded in the pekko-protobuf-v3 artifact so that applications can use another version of Protobuf.\nApplications should use standard Protobuf dependency and not pekko-protobuf-v3.","title":"Serialization of Pekko’s messages"},{"location":"/serialization.html#java-serialization","text":"Java serialization is known to be slow and prone to attacks of various kinds - it never was designed for high throughput messaging after all. One may think that network bandwidth and latency limit the performance of remote messaging, but serialization is a more typical bottleneck.\nNote Pekko serialization with Java serialization is disabled by default and Pekko itself doesn’t use Java serialization for any of its internal messages. It is highly discouraged to enable Java serialization in production. The log messages emitted by the disabled Java serializer in production SHOULD be treated as potential attacks which the serializer prevented, as they MAY indicate an external operator attempting to send malicious messages intending to use java serialization as attack vector. The attempts are logged with the SECURITY marker.\nHowever, for early prototyping it is very convenient to use. For that reason and for compatibility with older systems that rely on Java serialization it can be enabled with the following configuration:\npekko.actor.allow-java-serialization = on\nPekko will still log warning when Java serialization is used and to silent that you may add:\npekko.actor.warn-about-java-serializer-usage = off","title":"Java serialization"},{"location":"/serialization.html#java-serialization-compatibility","text":"It is not safe to mix major Scala versions when using the Java serialization as Scala does not guarantee compatibility and this could lead to very surprising errors.","title":"Java serialization compatibility"},{"location":"/serialization.html#rolling-updates","text":"A serialized remote message (or persistent event) consists of serializer-id, the manifest, and the binary payload. When deserializing it is only looking at the serializer-id to pick which SerializerJSerializer to use for fromBinaryfromBinaryJava. The message class (the bindings) is not used for deserialization. The manifest is only used within the SerializerJSerializer to decide how to deserialize the payload, so one SerializerJSerializer can handle many classes.\nThat means that it is possible to change serialization for a message by performing two rolling update steps to switch to the new serializer.\nAdd the SerializerJSerializer class and define it in pekko.actor.serializers config section, but not in pekko.actor.serialization-bindings. Perform a rolling update for this change. This means that the serializer class exists on all nodes and is registered, but it is still not used for serializing any messages. That is important because during the rolling update the old nodes still don’t know about the new serializer and would not be able to deserialize messages with that format. The second change is to register that the serializer is to be used for certain classes by defining those in the pekko.actor.serialization-bindings config section. Perform a rolling update for this change. This means that new nodes will use the new serializer when sending messages and old nodes will be able to deserialize the new format. Old nodes will continue to use the old serializer when sending messages and new nodes will be able to deserialize the old format.\nAs an optional third step the old serializer can be completely removed if it was not used for persistent events. It must still be possible to deserialize the events that were stored with the old serializer.","title":"Rolling updates"},{"location":"/serialization.html#verification","text":"Normally, messages sent between local actors (i.e. same JVM) do not undergo serialization. For testing, sometimes, it may be desirable to force serialization on all messages (both remote and local). If you want to do this in order to verify that your messages are serializable you can enable the following config option:\ncopysourcepekko {\n  actor {\n    serialize-messages = on\n  }\n}\nCertain messages can be excluded from verification by extending the marker traitinterface actor.NoSerializationVerificationNeededactor.NoSerializationVerificationNeeded or define a class name prefix in configuration pekko.actor.no-serialization-verification-needed-class-prefix.\nIf you want to verify that your PropsProps are serializable you can enable the following config option:\ncopysourcepekko {\n  actor {\n    serialize-creators = on\n  }\n}\nWarning We recommend having these config options turned on only when you’re running tests. Turning these options on in production is pointless, as it would negatively impact the performance of local message passing without giving any gain.","title":"Verification"},{"location":"/serialization-jackson.html","text":"","title":"Serialization with Jackson"},{"location":"/serialization-jackson.html#serialization-with-jackson","text":"","title":"Serialization with Jackson"},{"location":"/serialization-jackson.html#dependency","text":"To use Jackson Serialization, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-serialization-jackson\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-serialization-jackson_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-serialization-jackson_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/serialization-jackson.html#introduction","text":"You find general concepts for for Pekko serialization in the Serialization section. This section describes how to use the Jackson serializer for application specific messages and persistent events and snapshots.\nJackson has support for both text based JSON and binary formats.\nIn many cases ordinary classes can be serialized by Jackson without any additional hints, but sometimes annotations are needed to specify how to convert the objects to JSON/bytes.","title":"Introduction"},{"location":"/serialization-jackson.html#usage","text":"To enable Jackson serialization for a class you need to configure it or one of its super classes in serialization-bindings configuration. Typically you will create a marker traitinterface for that purpose and let the messages extendimplement that.\nScala copysource/**\n * Marker interface for messages, events and snapshots that are serialized with Jackson.\n */\ntrait MySerializable\n\nfinal case class Message(name: String, nr: Int) extends MySerializable Java copysource/** Marker interface for messages, events and snapshots that are serialized with Jackson. */\npublic interface MySerializable {}\n\nclass MyMessage implements MySerializable {\n  public final String name;\n  public final int nr;\n\n  public MyMessage(String name, int nr) {\n    this.name = name;\n    this.nr = nr;\n  }\n}\nThen you configure the class name of the marker traitinterface in serialization-bindings to one of the supported Jackson formats: jackson-json or jackson-cbor\ncopysourcepekko.actor {\n  serialization-bindings {\n    \"com.myservice.MySerializable\" = jackson-json\n  }\n}\nA good convention would be to name the marker interface CborSerializable or JsonSerializable. In this documentation we have used MySerializable to make it clear that the marker interface itself is not provided by Pekko.\nThat is all that is needed for basic classes where Jackson understands the structure. A few cases that requires annotations are described below.\nNote that it’s only the top level class or its marker traitinterface that must be defined in serialization-bindings, not nested classes that it references in member fields.\nNote Add the -parameters Java compiler option for usage by the ParameterNamesModule. It reduces the need for some annotations.","title":"Usage"},{"location":"/serialization-jackson.html#security","text":"For security reasons it is disallowed to bind the Jackson serializers to open ended types that might be a target for serialization gadgets, such as:\njava.lang.Object java.io.Serializable java.lang.Comparable.\nThe deny list of possible serialization gadget classes defined by Jackson databind are checked and disallowed for deserialization.\nWarning Don’t use @JsonTypeInfo(use = Id.CLASS) or ObjectMapper.enableDefaultTyping since that is a security risk when using polymorphic types.","title":"Security"},{"location":"/serialization-jackson.html#formats","text":"The following formats are supported, and you select which one to use in the serialization-bindings configuration as described above.\njackson-json - ordinary text based JSON jackson-cbor - binary CBOR data format\nThe binary format is more compact, with slightly better performance than the JSON format.","title":"Formats"},{"location":"/serialization-jackson.html#annotations","text":"Constructor with single parameter You might run into an exception like this: MismatchedInputException: Cannot construct instance of `...` (although at least one Creator exists): cannot deserialize from Object value (no delegate- or property-based Creator)\n That is probably because the class has a constructor with a single parameter, like: Java copysourcepublic class SimpleCommand implements MySerializable {\n  private final String name;\n\n  public SimpleCommand(String name) {\n    this.name = name;\n  }\n} That can be solved by adding @JsonCreator or @JsonProperty annotations: Java copysourcepublic class SimpleCommand implements MySerializable {\n  private final String name;\n\n  @JsonCreator\n  public SimpleCommand(String name) {\n    this.name = name;\n  }\n} or Java copysourcepublic class SimpleCommand implements MySerializable {\n  private final String name;\n\n  public SimpleCommand(@JsonProperty(\"name\") String name) {\n    this.name = name;\n  }\n} The ParameterNamesModule is configured with JsonCreator.Mode.PROPERTIES as described in the Jackson documentation","title":"Annotations"},{"location":"/serialization-jackson.html#polymorphic-types","text":"A polymorphic type is when a certain base type has multiple alternative implementations. When nested fields or collections are of polymorphic type the concrete implementations of the type must be listed with @JsonTypeInfo and @JsonSubTypes annotations.\nExample:\nScala copysourcefinal case class Zoo(primaryAttraction: Animal) extends MySerializable\n\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = \"type\")\n@JsonSubTypes(\n  Array(\n    new JsonSubTypes.Type(value = classOf[Lion], name = \"lion\"),\n    new JsonSubTypes.Type(value = classOf[Elephant], name = \"elephant\")))\nsealed trait Animal\n\nfinal case class Lion(name: String) extends Animal\n\nfinal case class Elephant(name: String, age: Int) extends Animal Java copysourcepublic class Zoo implements MySerializable {\n  public final Animal primaryAttraction;\n\n  @JsonCreator\n  public Zoo(Animal primaryAttraction) {\n    this.primaryAttraction = primaryAttraction;\n  }\n}\n\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = \"type\")\n@JsonSubTypes({\n  @JsonSubTypes.Type(value = Lion.class, name = \"lion\"),\n  @JsonSubTypes.Type(value = Elephant.class, name = \"elephant\")\n})\ninterface Animal {}\n\npublic final class Lion implements Animal {\n  public final String name;\n\n  @JsonCreator\n  public Lion(String name) {\n    this.name = name;\n  }\n}\n\npublic final class Elephant implements Animal {\n  public final String name;\n  public final int age;\n\n  public Elephant(String name, int age) {\n    this.name = name;\n    this.age = age;\n  }\n}\nIf you haven’t defined the annotations you will see an exception like this:\nInvalidDefinitionException: Cannot construct instance of `...` (no Creators, like default construct, exist): abstract types either need to be mapped to concrete types, have custom deserializer, or contain additional type information\nNote that this is not needed for a top level class, but for fields inside it. In this example Animal is used inside of Zoo, which is sent as a message or persisted. If Animal was sent or persisted standalone the annotations are not needed because then it is the concrete subclasses Lion or Elephant that are serialized.\nWhen specifying allowed subclasses with those annotations the class names will not be included in the serialized representation and that is important for preventing loading of malicious serialization gadgets when deserializing.\nWarning Don’t use @JsonTypeInfo(use = Id.CLASS) or ObjectMapper.enableDefaultTyping since that is a security risk when using polymorphic types.\nADT with trait and case object It’s common in Scala to use a sealed trait and case objects to represent enums. If the values are case classes the @JsonSubTypes annotation as described above works, but if the values are case objects it will not. The annotation requires a Class and there is no way to define that in an annotation for a case object. The easiest workaround is to define the case objects as case class without any field. Alternatively, you can define an intermediate trait for the case object and a custom deserializer for it. The example below builds on the previous Animal sample by adding a fictitious, single instance, new animal, an Unicorn. Scala copysourcefinal case class Zoo(primaryAttraction: Animal) extends MySerializable\n\n@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = \"type\")\n@JsonSubTypes(\n  Array(\n    new JsonSubTypes.Type(value = classOf[Lion], name = \"lion\"),\n    new JsonSubTypes.Type(value = classOf[Elephant], name = \"elephant\"),\n    new JsonSubTypes.Type(value = classOf[Unicorn], name = \"unicorn\")))\nsealed trait Animal\n\nfinal case class Lion(name: String) extends Animal\nfinal case class Elephant(name: String, age: Int) extends Animal\n\n@JsonDeserialize(`using` = classOf[UnicornDeserializer])\nsealed trait Unicorn extends Animal\n@JsonTypeName(\"unicorn\")\ncase object Unicorn extends Unicorn\n\nclass UnicornDeserializer extends StdDeserializer[Unicorn](Unicorn.getClass) {\n  // whenever we need to deserialize an instance of Unicorn trait, we return the object Unicorn\n  override def deserialize(p: JsonParser, ctxt: DeserializationContext): Unicorn = Unicorn\n} The case object Unicorn can’t be used in a @JsonSubTypes annotation, but its trait can. When serializing the case object we need to know which type tag to use, hence the @JsonTypeName annotation on the object. When deserializing, Jackson will only know about the trait variant therefore we need a custom deserializer that returns the case object. On the other hand, if the ADT only has case objects, you can solve it by implementing a custom serialization for the enums. Annotate the trait with @JsonSerialize and @JsonDeserialize and implement the serialization with StdSerializer and StdDeserializer. Scala copysourceimport com.fasterxml.jackson.core.JsonGenerator\nimport com.fasterxml.jackson.core.JsonParser\nimport com.fasterxml.jackson.databind.DeserializationContext\nimport com.fasterxml.jackson.databind.SerializerProvider\nimport com.fasterxml.jackson.databind.annotation.JsonDeserialize\nimport com.fasterxml.jackson.databind.annotation.JsonSerialize\nimport com.fasterxml.jackson.databind.deser.std.StdDeserializer\nimport com.fasterxml.jackson.databind.ser.std.StdSerializer\n\n@JsonSerialize(`using` = classOf[DirectionJsonSerializer])\n@JsonDeserialize(`using` = classOf[DirectionJsonDeserializer])\nsealed trait Direction\n\nobject Direction {\n  case object North extends Direction\n  case object East extends Direction\n  case object South extends Direction\n  case object West extends Direction\n}\n\nclass DirectionJsonSerializer extends StdSerializer[Direction](classOf[Direction]) {\n  import Direction._\n\n  override def serialize(value: Direction, gen: JsonGenerator, provider: SerializerProvider): Unit = {\n    val strValue = value match {\n      case North => \"N\"\n      case East  => \"E\"\n      case South => \"S\"\n      case West  => \"W\"\n    }\n    gen.writeString(strValue)\n  }\n}\n\nclass DirectionJsonDeserializer extends StdDeserializer[Direction](classOf[Direction]) {\n  import Direction._\n\n  override def deserialize(p: JsonParser, ctxt: DeserializationContext): Direction = {\n    p.getText match {\n      case \"N\" => North\n      case \"E\" => East\n      case \"S\" => South\n      case \"W\" => West\n    }\n  }\n}\n\nfinal case class Compass(currentDirection: Direction) extends MySerializable Enumerations Jackson support for Scala Enumerations defaults to serializing a Value as a JsonObject that includes a field with the \"value\" and a field with the \"type\" whose value is the FQCN of the enumeration. Jackson includes the @JsonScalaEnumeration to statically specify the type information to a field. When using the @JsonScalaEnumeration annotation the enumeration value is serialized as a JsonString. Scala copysourceobject Planet extends Enumeration {\n  type Planet = Value\n  val Mercury, Venus, Earth, Mars, Krypton = Value\n}\n\n// Uses default Jackson serialization format for Scala Enumerations\nfinal case class Alien(name: String, planet: Planet.Planet) extends TestMessage\n\n// Serializes planet values as a JsonString\nclass PlanetType extends TypeReference[Planet.type] {}\nfinal case class Superhero(name: String, @JsonScalaEnumeration(classOf[PlanetType]) planet: Planet.Planet)\n    extends TestMessage","title":"Polymorphic types"},{"location":"/serialization-jackson.html#schema-evolution","text":"When using Event Sourcing, but also for rolling updates, schema evolution becomes an important aspect of developing your application. The requirements as well as our own understanding of the business domain may (and will) change over time.\nThe Jackson serializer provides a way to perform transformations of the JSON tree model during deserialization. This is working in the same way for the textual and binary formats.\nWe will look at a few scenarios of how the classes may be evolved.","title":"Schema Evolution"},{"location":"/serialization-jackson.html#remove-field","text":"Removing a field can be done without any migration code. The Jackson serializer will ignore properties that does not exist in the class.","title":"Remove Field"},{"location":"/serialization-jackson.html#add-optional-field","text":"Adding an optional field can be done without any migration code. The default value will be NoneOptional.empty.\nOld class:\nScala copysourcecase class ItemAdded(shoppingCartId: String, productId: String, quantity: Int) extends MySerializable Java copysourcepublic class ItemAdded implements MySerializable {\n  public final String shoppingCartId;\n  public final String productId;\n  public final int quantity;\n\n  public ItemAdded(String shoppingCartId, String productId, int quantity) {\n    this.shoppingCartId = shoppingCartId;\n    this.productId = productId;\n    this.quantity = quantity;\n  }\n}\nNew class with a new optional discount property and a new note field with default value:\nScala copysourcecase class ItemAdded(shoppingCartId: String, productId: String, quantity: Int, discount: Option[Double], note: String)\n    extends MySerializable {\n\n  // alternative constructor because `note` should have default value \"\" when not defined in json\n  @JsonCreator\n  def this(shoppingCartId: String, productId: String, quantity: Int, discount: Option[Double], note: Option[String]) =\n    this(shoppingCartId, productId, quantity, discount, note.getOrElse(\"\"))\n} Java copysourcepublic class ItemAdded implements MySerializable {\n  public final String shoppingCartId;\n  public final String productId;\n  public final int quantity;\n  public final Optional<Double> discount;\n  public final String note;\n\n  @JsonCreator\n  public ItemAdded(\n      String shoppingCartId,\n      String productId,\n      int quantity,\n      Optional<Double> discount,\n      String note) {\n    this.shoppingCartId = shoppingCartId;\n    this.productId = productId;\n    this.quantity = quantity;\n    this.discount = discount;\n\n    // default for note is \"\" if not included in json\n    if (note == null) this.note = \"\";\n    else this.note = note;\n  }\n\n  public ItemAdded(\n      String shoppingCartId, String productId, int quantity, Optional<Double> discount) {\n    this(shoppingCartId, productId, quantity, discount, \"\");\n  }\n}","title":"Add Optional Field"},{"location":"/serialization-jackson.html#add-mandatory-field","text":"Let’s say we want to have a mandatory discount property without default value instead:\nScala copysourcecase class ItemAdded(shoppingCartId: String, productId: String, quantity: Int, discount: Double) extends MySerializable Java copysourcepublic class ItemAdded implements MySerializable {\n  public final String shoppingCartId;\n  public final String productId;\n  public final int quantity;\n  public final double discount;\n\n  public ItemAdded(String shoppingCartId, String productId, int quantity, double discount) {\n    this.shoppingCartId = shoppingCartId;\n    this.productId = productId;\n    this.quantity = quantity;\n    this.discount = discount;\n  }\n}\nTo add a new mandatory field we have to use a JacksonMigrationJacksonMigration class and set the default value in the migration code.\nThis is how a migration class would look like for adding a discount field:\nScala copysourceimport com.fasterxml.jackson.databind.JsonNode\nimport com.fasterxml.jackson.databind.node.DoubleNode\nimport com.fasterxml.jackson.databind.node.ObjectNode\nimport org.apache.pekko.serialization.jackson.JacksonMigration\n\nclass ItemAddedMigration extends JacksonMigration {\n\n  override def currentVersion: Int = 2\n\n  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {\n    val root = json.asInstanceOf[ObjectNode]\n    if (fromVersion <= 1) {\n      root.set(\"discount\", DoubleNode.valueOf(0.0))\n    }\n    root\n  }\n} Java copysourceimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.DoubleNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\nimport org.apache.pekko.serialization.jackson.JacksonMigration;\n\npublic class ItemAddedMigration extends JacksonMigration {\n\n  @Override\n  public int currentVersion() {\n    return 2;\n  }\n\n  @Override\n  public JsonNode transform(int fromVersion, JsonNode json) {\n    ObjectNode root = (ObjectNode) json;\n    if (fromVersion <= 1) {\n      root.set(\"discount\", DoubleNode.valueOf(0.0));\n    }\n    return root;\n  }\n}\nOverride the currentVersioncurrentVersion() method to define the version number of the current (latest) version. The first version, when no migration was used, is always 1. Increase this version number whenever you perform a change that is not backwards compatible without migration code.\nImplement the transformation of the old JSON structure to the new JSON structure in the transform(fromVersion, jsonNode)transform(fromVersion, jsonNode) method. The JsonNode is mutable so you can add and remove fields, or change values. Note that you have to cast to specific sub-classes such as ObjectNode and ArrayNode to get access to mutators.\nThe migration class must be defined in configuration file:\ncopysourcepekko.serialization.jackson.migrations {\n  \"com.myservice.event.ItemAdded\" = \"com.myservice.event.ItemAddedMigration\"\n}\nThe same thing could have been done for the note field, adding a default value of \"\" in the ItemAddedMigration.","title":"Add Mandatory Field"},{"location":"/serialization-jackson.html#rename-field","text":"Let’s say that we want to rename the productId field to itemId in the previous example.\nScala copysourcecase class ItemAdded(shoppingCartId: String, itemId: String, quantity: Int) extends MySerializable Java copysourcepublic class ItemAdded implements MySerializable {\n  public final String shoppingCartId;\n\n  public final String itemId;\n\n  public final int quantity;\n\n  public ItemAdded(String shoppingCartId, String itemId, int quantity) {\n    this.shoppingCartId = shoppingCartId;\n    this.itemId = itemId;\n    this.quantity = quantity;\n  }\n}\nThe migration code would look like:\nScala copysourceimport org.apache.pekko.serialization.jackson.JacksonMigration\nimport com.fasterxml.jackson.databind.JsonNode\nimport com.fasterxml.jackson.databind.node.ObjectNode\n\nclass ItemAddedMigration extends JacksonMigration {\n\n  override def currentVersion: Int = 2\n\n  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {\n    val root = json.asInstanceOf[ObjectNode]\n    if (fromVersion <= 1) {\n      root.set[JsonNode](\"itemId\", root.get(\"productId\"))\n      root.remove(\"productId\")\n    }\n    root\n  }\n} Java copysource import org.apache.pekko.serialization.jackson.JacksonMigration;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\npublic class ItemAddedMigration extends JacksonMigration {\n\n  @Override\n  public int currentVersion() {\n    return 2;\n  }\n\n  @Override\n  public JsonNode transform(int fromVersion, JsonNode json) {\n    ObjectNode root = (ObjectNode) json;\n    if (fromVersion <= 1) {\n      root.set(\"itemId\", root.get(\"productId\"));\n      root.remove(\"productId\");\n    }\n    return root;\n  }\n}","title":"Rename Field"},{"location":"/serialization-jackson.html#structural-changes","text":"In a similar way we can do arbitrary structural changes.\nOld class:\nScala copysourcecase class Customer(name: String, street: String, city: String, zipCode: String, country: String) extends MySerializable Java copysourcepublic class Customer implements MySerializable {\n  public final String name;\n  public final String street;\n  public final String city;\n  public final String zipCode;\n  public final String country;\n\n  public Customer(String name, String street, String city, String zipCode, String country) {\n    this.name = name;\n    this.street = street;\n    this.city = city;\n    this.zipCode = zipCode;\n    this.country = country;\n  }\n}\nNew class:\nScala copysourcecase class Customer(name: String, shippingAddress: Address, billingAddress: Option[Address]) extends MySerializable Java copysourcepublic class Customer implements MySerializable {\n  public final String name;\n  public final Address shippingAddress;\n  public final Optional<Address> billingAddress;\n\n  public Customer(String name, Address shippingAddress, Optional<Address> billingAddress) {\n    this.name = name;\n    this.shippingAddress = shippingAddress;\n    this.billingAddress = billingAddress;\n  }\n}\nwith the Address class:\nScala copysourcecase class Address(street: String, city: String, zipCode: String, country: String) extends MySerializable Java copysourcepublic class Address {\n  public final String street;\n  public final String city;\n  public final String zipCode;\n  public final String country;\n\n  public Address(String street, String city, String zipCode, String country) {\n    this.street = street;\n    this.city = city;\n    this.zipCode = zipCode;\n    this.country = country;\n  }\n}\nThe migration code would look like:\nScala copysourceimport org.apache.pekko.serialization.jackson.JacksonMigration\nimport com.fasterxml.jackson.databind.JsonNode\nimport com.fasterxml.jackson.databind.node.ObjectNode\n\nclass CustomerMigration extends JacksonMigration {\n\n  override def currentVersion: Int = 2\n\n  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {\n    val root = json.asInstanceOf[ObjectNode]\n    if (fromVersion <= 1) {\n      val shippingAddress = root.`with`(\"shippingAddress\")\n      shippingAddress.set[JsonNode](\"street\", root.get(\"street\"))\n      shippingAddress.set[JsonNode](\"city\", root.get(\"city\"))\n      shippingAddress.set[JsonNode](\"zipCode\", root.get(\"zipCode\"))\n      shippingAddress.set[JsonNode](\"country\", root.get(\"country\"))\n      root.remove(\"street\")\n      root.remove(\"city\")\n      root.remove(\"zipCode\")\n      root.remove(\"country\")\n    }\n    root\n  }\n} Java copysourceimport org.apache.pekko.serialization.jackson.JacksonMigration;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\npublic class CustomerMigration extends JacksonMigration {\n\n  @Override\n  public int currentVersion() {\n    return 2;\n  }\n\n  @Override\n  public JsonNode transform(int fromVersion, JsonNode json) {\n    ObjectNode root = (ObjectNode) json;\n    if (fromVersion <= 1) {\n      ObjectNode shippingAddress = root.with(\"shippingAddress\");\n      shippingAddress.set(\"street\", root.get(\"street\"));\n      shippingAddress.set(\"city\", root.get(\"city\"));\n      shippingAddress.set(\"zipCode\", root.get(\"zipCode\"));\n      shippingAddress.set(\"country\", root.get(\"country\"));\n      root.remove(\"street\");\n      root.remove(\"city\");\n      root.remove(\"zipCode\");\n      root.remove(\"country\");\n    }\n    return root;\n  }\n}","title":"Structural Changes"},{"location":"/serialization-jackson.html#rename-class","text":"It is also possible to rename the class. For example, let’s rename OrderAdded to OrderPlaced.\nOld class:\nScala copysourcecase class OrderAdded(shoppingCartId: String) extends MySerializable Java copysourcepublic class OrderAdded implements MySerializable {\n  public final String shoppingCartId;\n\n  @JsonCreator\n  public OrderAdded(String shoppingCartId) {\n    this.shoppingCartId = shoppingCartId;\n  }\n}\nNew class:\nScala copysourcecase class OrderPlaced(shoppingCartId: String) extends MySerializable Java copysourcepublic class OrderPlaced implements MySerializable {\n  public final String shoppingCartId;\n\n  @JsonCreator\n  public OrderPlaced(String shoppingCartId) {\n    this.shoppingCartId = shoppingCartId;\n  }\n}\nThe migration code would look like:\nScala copysourceclass OrderPlacedMigration extends JacksonMigration {\n\n  override def currentVersion: Int = 2\n\n  override def transformClassName(fromVersion: Int, className: String): String =\n    classOf[OrderPlaced].getName\n\n  override def transform(fromVersion: Int, json: JsonNode): JsonNode = json\n} Java copysourcepublic class OrderPlacedMigration extends JacksonMigration {\n\n  @Override\n  public int currentVersion() {\n    return 2;\n  }\n\n  @Override\n  public String transformClassName(int fromVersion, String className) {\n    return OrderPlaced.class.getName();\n  }\n\n  @Override\n  public JsonNode transform(int fromVersion, JsonNode json) {\n    return json;\n  }\n}\nNote the override of the transformClassName(fromVersion, className)transformClassName(fromVersion, className) method to define the new class name.\nThat type of migration must be configured with the old class name as key. The actual class can be removed.\ncopysourcepekko.serialization.jackson.migrations {\n  \"com.myservice.event.OrderAdded\" = \"com.myservice.event.OrderPlacedMigration\"\n}","title":"Rename Class"},{"location":"/serialization-jackson.html#remove-from-serialization-bindings","text":"When a class is not used for serialization any more it can be removed from serialization-bindings but to still allow deserialization it must then be listed in the allowed-class-prefix configuration. This is useful for example during rolling update with serialization changes, or when reading old stored data. It can also be used when changing from Jackson serializer to another serializer (e.g. Protobuf) and thereby changing the serialization binding, but it should still be possible to deserialize old data with Jackson.\ncopysourcepekko.serialization.jackson.allowed-class-prefix =\n  [\"com.myservice.event.OrderAdded\", \"com.myservice.command\"]\nIt’s a list of class names or prefixes of class names.","title":"Remove from serialization-bindings"},{"location":"/serialization-jackson.html#rolling-updates","text":"When doing a rolling update, for a period of time there are two different binaries running in production. If the schema has evolved requiring a new schema version, the data serialized by the new binary will be unreadable from the old binary. This situation causes transient errors on the processes running the old binary. This service degradation is usually fine since the rolling update will eventually complete and all old processes will be replaced with the new binary. To avoid this service degradation you can also use forward-one support in your schema evolutions.\nTo complete a no-degradation rolling update, you need to make two deployments. First, deploy a new binary which can read the new schema but still uses the old schema. Then, deploy a second binary which serializes data using the new schema and drops the downcasting code from the migration.\nLet’s take, for example, the case above where we renamed a field.\nThe starting schema is:\nScala copysourcecase class ItemAdded(shoppingCartId: String, productId: String, quantity: Int) extends MySerializable Java copysourcepublic class ItemAdded implements MySerializable {\n  public final String shoppingCartId;\n  public final String productId;\n  public final int quantity;\n\n  public ItemAdded(String shoppingCartId, String productId, int quantity) {\n    this.shoppingCartId = shoppingCartId;\n    this.productId = productId;\n    this.quantity = quantity;\n  }\n}\nIn a first deployment, we still don’t make any change to the event class:\nScala copysourcecase class ItemAdded(shoppingCartId: String, productId: String, quantity: Int) extends MySerializable Java copysourcepublic class ItemAdded implements MySerializable {\n  public final String shoppingCartId;\n  public final String productId;\n  public final int quantity;\n\n  public ItemAdded(String shoppingCartId, String productId, int quantity) {\n    this.shoppingCartId = shoppingCartId;\n    this.productId = productId;\n    this.quantity = quantity;\n  }\n}\nbut we introduce a migration that can read the newer schema which is versioned 2:\nScala copysourceimport org.apache.pekko.serialization.jackson.JacksonMigration\nimport com.fasterxml.jackson.databind.JsonNode\nimport com.fasterxml.jackson.databind.node.ObjectNode\n\nclass ItemAddedMigration extends JacksonMigration {\n\n  // Data produced in this node is still produced using the version 1 of the schema\n  override def currentVersion: Int = 1\n\n  override def supportedForwardVersion: Int = 2\n\n  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {\n    val root = json.asInstanceOf[ObjectNode]\n    if (fromVersion == 2) {\n      // When receiving an event of version 2 we down-cast it to the version 1 of the schema\n      root.set[JsonNode](\"productId\", root.get(\"itemId\"))\n      root.remove(\"itemId\")\n    }\n    root\n  }\n} Java copysource import org.apache.pekko.serialization.jackson.JacksonMigration;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\npublic class ItemAddedMigration extends JacksonMigration {\n\n  // Data produced in this node is still produced using the version 1 of the schema\n  @Override\n  public int currentVersion() {\n    return 1;\n  }\n\n  @Override\n  public int supportedForwardVersion() {\n    return 2;\n  }\n\n  @Override\n  public JsonNode transform(int fromVersion, JsonNode json) {\n    ObjectNode root = (ObjectNode) json;\n    if (fromVersion == 2) {\n      // When receiving an event of version 2 we down-cast it to the version 1 of the schema\n      root.set(\"productId\", root.get(\"itemId\"));\n      root.remove(\"itemId\");\n    }\n    return root;\n  }\n}\nOnce all running nodes have the new migration code which can read version 2 of ItemAdded we can proceed with the second step. So, we deploy the updated event:\nScala copysourcecase class ItemAdded(shoppingCartId: String, itemId: String, quantity: Int) extends MySerializable Java copysourcepublic class ItemAdded implements MySerializable {\n  public final String shoppingCartId;\n\n  public final String itemId;\n\n  public final int quantity;\n\n  public ItemAdded(String shoppingCartId, String itemId, int quantity) {\n    this.shoppingCartId = shoppingCartId;\n    this.itemId = itemId;\n    this.quantity = quantity;\n  }\n}\nand the final migration code which no longer needs forward-compatibility code:\nScala copysourceimport org.apache.pekko.serialization.jackson.JacksonMigration\nimport com.fasterxml.jackson.databind.JsonNode\nimport com.fasterxml.jackson.databind.node.ObjectNode\n\nclass ItemAddedMigration extends JacksonMigration {\n\n  override def currentVersion: Int = 2\n\n  override def transform(fromVersion: Int, json: JsonNode): JsonNode = {\n    val root = json.asInstanceOf[ObjectNode]\n    if (fromVersion <= 1) {\n      root.set[JsonNode](\"itemId\", root.get(\"productId\"))\n      root.remove(\"productId\")\n    }\n    root\n  }\n} Java copysource import org.apache.pekko.serialization.jackson.JacksonMigration;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\npublic class ItemAddedMigration extends JacksonMigration {\n\n  @Override\n  public int currentVersion() {\n    return 2;\n  }\n\n  @Override\n  public JsonNode transform(int fromVersion, JsonNode json) {\n    ObjectNode root = (ObjectNode) json;\n    if (fromVersion <= 1) {\n      root.set(\"itemId\", root.get(\"productId\"));\n      root.remove(\"productId\");\n    }\n    return root;\n  }\n}","title":"Rolling updates"},{"location":"/serialization-jackson.html#jackson-modules","text":"The following Jackson modules are enabled by default:\ncopysourcepekko.serialization.jackson {\n\n  # The Jackson JSON serializer will register these modules.\n  jackson-modules += \"org.apache.pekko.serialization.jackson.PekkoJacksonModule\"\n  # PekkoTypedJacksonModule optionally included if pekko-actor-typed is in classpath\n  jackson-modules += \"org.apache.pekko.serialization.jackson.PekkoTypedJacksonModule\"\n  # PekkoStreamsModule optionally included if pekko-streams is in classpath\n  jackson-modules += \"org.apache.pekko.serialization.jackson.PekkoStreamJacksonModule\"\n  jackson-modules += \"com.fasterxml.jackson.module.paramnames.ParameterNamesModule\"\n  jackson-modules += \"com.fasterxml.jackson.datatype.jdk8.Jdk8Module\"\n  jackson-modules += \"com.fasterxml.jackson.datatype.jsr310.JavaTimeModule\"\n  jackson-modules += \"com.fasterxml.jackson.module.scala.DefaultScalaModule\"\n}\nYou can amend the configuration pekko.serialization.jackson.jackson-modules to enable other modules.\nThe ParameterNamesModule requires that the -parameters Java compiler option is enabled.","title":"Jackson Modules"},{"location":"/serialization-jackson.html#compression","text":"JSON can be rather verbose and for large messages it can be beneficial to compress large payloads. For the jackson-json binding the default configuration is:\ncopysource# Compression settings for the jackson-json binding\npekko.serialization.jackson.jackson-json.compression {\n  # Compression algorithm.\n  # - off  : no compression\n  # - gzip : using common java gzip\n  # - lz4 : using lz4-java\n  algorithm = gzip\n\n  # If compression is enabled with the `algorithm` setting the payload is compressed\n  # when it's larger than this value.\n  compress-larger-than = 32 KiB\n}\nSupported compression algorithms are: gzip, lz4. Use ‘off’ to disable compression. Gzip is generally slower than lz4. Messages larger than the compress-larger-than property are compressed.\nCompression can be disabled by setting the algorithm property to off. It will still be able to decompress payloads that were compressed when serialized, e.g. if this configuration is changed.\nFor the jackson-cbor and custom bindings other than jackson-json compression is by default disabled, but can be enabled in the same way as the configuration shown above but replacing jackson-json with the binding name (for example jackson-cbor).","title":"Compression"},{"location":"/serialization-jackson.html#using-pekko-serialization-for-embedded-types","text":"For types that already have a Pekko Serializer defined that are embedded in types serialized with Jackson the PekkoSerializationSerializerPekkoSerializationSerializer and PekkoSerializationDeserializerPekkoSerializationDeserializer can be used to Pekko Serialization for individual fields.\nThe serializer/deserializer are not enabled automatically. The @JsonSerialize and @JsonDeserialize annotation needs to be added to the fields containing the types to be serialized with Pekko Serialization.\nThe type will be embedded as an object with the fields:\nserId - the serializer id serManifest - the manifest for the type payload - base64 encoded bytes","title":"Using Pekko Serialization for embedded types"},{"location":"/serialization-jackson.html#additional-configuration","text":"","title":"Additional configuration"},{"location":"/serialization-jackson.html#configuration-per-binding","text":"By default the configuration for the Jackson serializers and their ObjectMappers is defined in the pekko.serialization.jackson section. It is possible to override that configuration in a more specific pekko.serialization.jackson.<binding name> section.\ncopysourcepekko.serialization.jackson.jackson-json {\n  serialization-features {\n    WRITE_DATES_AS_TIMESTAMPS = off\n  }\n}\npekko.serialization.jackson.jackson-cbor {\n  serialization-features {\n    WRITE_DATES_AS_TIMESTAMPS = on\n  }\n}\nIt’s also possible to define several bindings and use different configuration for them. For example, different settings for remote messages and persisted events.\ncopysourcepekko.actor {\n  serializers {\n    jackson-json-message = \"org.apache.pekko.serialization.jackson.JacksonJsonSerializer\"\n    jackson-json-event   = \"org.apache.pekko.serialization.jackson.JacksonJsonSerializer\"\n  }\n  serialization-identifiers {\n    jackson-json-message = 9001\n    jackson-json-event = 9002\n  }\n  serialization-bindings {\n    \"com.myservice.MyMessage\" = jackson-json-message\n    \"com.myservice.MyEvent\" = jackson-json-event\n  }\n}\npekko.serialization.jackson {\n  jackson-json-message {\n    serialization-features {\n      WRITE_DATES_AS_TIMESTAMPS = on\n    }\n  }\n  jackson-json-event {\n    serialization-features {\n      WRITE_DATES_AS_TIMESTAMPS = off\n    }\n  }\n}","title":"Configuration per binding"},{"location":"/serialization-jackson.html#manifest-less-serialization","text":"When using the Jackson serializer for persistence, given that the fully qualified class name is stored in the manifest, this can result in a lot of wasted disk and IO used, especially when the events are small. To address this, a type-in-manifest flag can be turned off, which will result in the class name not appearing in the manifest.\nWhen deserializing, the Jackson serializer will use the type defined in deserialization-type, if present, otherwise it will look for exactly one serialization binding class, and use that. For this to be useful, generally that single type must be a Polymorphic type, with all type information necessary to deserialize to the various sub types contained in the JSON message.\nWhen switching serializers, there will be periods of time when you may have no serialization bindings declared for the type. In such circumstances, you must use the deserialization-type configuration attribute to specify which type should be used to deserialize messages.\nSince this configuration can only be applied to a single root type, you will usually only want to apply it to a per binding configuration, not to the regular jackson-json or jackson-cbor configurations.\ncopysourcepekko.actor {\n  serializers {\n    jackson-json-event = \"org.apache.pekko.serialization.jackson.JacksonJsonSerializer\"\n  }\n  serialization-identifiers {\n    jackson-json-event = 9001\n  }\n  serialization-bindings {\n    \"com.myservice.MyEvent\" = jackson-json-event\n  }\n}\npekko.serialization.jackson {\n  jackson-json-event {\n    type-in-manifest = off\n    # Since there is exactly one serialization binding declared for this\n    # serializer above, this is optional, but if there were none or many,\n    # this would be mandatory.\n    deserialization-type = \"com.myservice.MyEvent\"\n  }\n}\nNote that Pekko remoting already implements manifest compression, and so this optimization will have no significant impact for messages sent over remoting. It’s only useful for messages serialized for other purposes, such as persistence or distributed data.","title":"Manifest-less serialization"},{"location":"/serialization-jackson.html#additional-features","text":"Additional Jackson serialization features can be enabled/disabled in configuration. The default values from Jackson are used aside from the the following that are changed in Pekko’s default configuration.\ncopysourcepekko.serialization.jackson {\n  # Configuration of the ObjectMapper serialization features.\n  # See com.fasterxml.jackson.databind.SerializationFeature\n  # Enum values corresponding to the SerializationFeature and their boolean value.\n  serialization-features {\n    # Date/time in ISO-8601 (rfc3339) yyyy-MM-dd'T'HH:mm:ss.SSSZ format\n    # as defined by com.fasterxml.jackson.databind.util.StdDateFormat\n    # For interoperability it's better to use the ISO format, i.e. WRITE_DATES_AS_TIMESTAMPS=off,\n    # but WRITE_DATES_AS_TIMESTAMPS=on has better performance.\n    WRITE_DATES_AS_TIMESTAMPS = off\n    WRITE_DURATIONS_AS_TIMESTAMPS = off\n    FAIL_ON_EMPTY_BEANS = off\n  }\n\n  # Configuration of the ObjectMapper deserialization features.\n  # See com.fasterxml.jackson.databind.DeserializationFeature\n  # Enum values corresponding to the DeserializationFeature and their boolean value.\n  deserialization-features {\n    FAIL_ON_UNKNOWN_PROPERTIES = off\n  }\n\n  # Configuration of the ObjectMapper mapper features.\n  # See com.fasterxml.jackson.databind.MapperFeature\n  # Enum values corresponding to the MapperFeature and their\n  # boolean values, for example:\n  #\n  # mapper-features {\n  #   SORT_PROPERTIES_ALPHABETICALLY = on\n  # }\n  mapper-features {}\n\n  # Configuration of the ObjectMapper JsonParser features.\n  # See com.fasterxml.jackson.core.JsonParser.Feature\n  # Enum values corresponding to the JsonParser.Feature and their\n  # boolean value, for example:\n  #\n  # json-parser-features {\n  #   ALLOW_SINGLE_QUOTES = on\n  # }\n  json-parser-features {}\n\n  # Configuration of the ObjectMapper JsonParser features.\n  # See com.fasterxml.jackson.core.JsonGenerator.Feature\n  # Enum values corresponding to the JsonGenerator.Feature and\n  # their boolean value, for example:\n  #\n  # json-generator-features {\n  #   WRITE_NUMBERS_AS_STRINGS = on\n  # }\n  json-generator-features {}\n\n  # Configuration of the JsonFactory StreamReadFeature.\n  # See com.fasterxml.jackson.core.StreamReadFeature\n  # Enum values corresponding to the StreamReadFeatures and\n  # their boolean value, for example:\n  #\n  # stream-read-features {\n  #   STRICT_DUPLICATE_DETECTION = on\n  # }\n  stream-read-features {}\n\n  # Configuration of the JsonFactory StreamWriteFeature.\n  # See com.fasterxml.jackson.core.StreamWriteFeature\n  # Enum values corresponding to the StreamWriteFeatures and\n  # their boolean value, for example:\n  #\n  # stream-write-features {\n  #   WRITE_BIGDECIMAL_AS_PLAIN = on\n  # }\n  stream-write-features {}\n\n  # Configuration of the JsonFactory JsonReadFeature.\n  # See com.fasterxml.jackson.core.json.JsonReadFeature\n  # Enum values corresponding to the JsonReadFeatures and\n  # their boolean value, for example:\n  #\n  # json-read-features {\n  #   ALLOW_SINGLE_QUOTES = on\n  # }\n  json-read-features {}\n\n  # Configuration of the JsonFactory JsonWriteFeature.\n  # See com.fasterxml.jackson.core.json.JsonWriteFeature\n  # Enum values corresponding to the JsonWriteFeatures and\n  # their boolean value, for example:\n  #\n  # json-write-features {\n  #   WRITE_NUMBERS_AS_STRINGS = on\n  # }\n  json-write-features {}\n\n  # Configuration of the JsonFactory Visibility.\n  # See com.fasterxml.jackson.annotation.PropertyAccessor\n  # and com.fasterxml.jackson.annotation.JsonAutoDetect.Visibility\n  # Enum values. For example, to serialize only public fields\n  # overwrite the default values with:\n  #\n  # visibility {\n  #   FIELD = PUBLIC_ONLY\n  # }\n  # Default: all fields (including private and protected) are serialized.\n  visibility {\n    FIELD = ANY\n  }\n\n  # Deprecated, use `allowed-class-prefix` instead\n  whitelist-class-prefix = []\n\n  # Additional classes that are allowed even if they are not defined in `serialization-bindings`.\n  # This is useful when a class is not used for serialization any more and therefore removed\n  # from `serialization-bindings`, but should still be possible to deserialize.\n  allowed-class-prefix = ${pekko.serialization.jackson.whitelist-class-prefix}\n\n\n  # settings for compression of the payload\n  compression {\n    # Compression algorithm.\n    # - off  : no compression\n    # - gzip : using common java gzip\n    algorithm = off\n\n    # If compression is enabled with the `algorithm` setting the payload is compressed\n    # when it's larger than this value.\n    compress-larger-than = 0 KiB\n  }\n\n  # Whether the type should be written to the manifest.\n  # If this is off, then either deserialization-type must be defined, or there must be exactly\n  # one serialization binding declared for this serializer, and the type in that binding will be\n  # used as the deserialization type. This feature will only work if that type either is a\n  # concrete class, or if it is a supertype that uses Jackson polymorphism (ie, the\n  # @JsonTypeInfo annotation) to store type information in the JSON itself. The intention behind\n  # disabling this is to remove extraneous type information (ie, fully qualified class names) when\n  # serialized objects are persisted in Pekko persistence or replicated using Pekko distributed\n  # data. Note that Pekko remoting already has manifest compression optimizations that address this,\n  # so for types that just get sent over remoting, this offers no optimization.\n  type-in-manifest = on\n\n  # The type to use for deserialization.\n  # This is only used if type-in-manifest is disabled. If set, this type will be used to\n  # deserialize all messages. This is useful if the binding configuration you want to use when\n  # disabling type in manifest cannot be expressed as a single type. Examples of when you might\n  # use this include when changing serializers, so you don't want this serializer used for\n  # serialization and you haven't declared any bindings for it, but you still want to be able to\n  # deserialize messages that were serialized with this serializer, as well as situations where\n  # you only want some sub types of a given Jackson polymorphic type to be serialized using this\n  # serializer.\n  deserialization-type = \"\"\n\n  # Specific settings for jackson-json binding can be defined in this section to\n  # override the settings in 'pekko.serialization.jackson'\n  jackson-json {}\n\n  # Specific settings for jackson-cbor binding can be defined in this section to\n  # override the settings in 'pekko.serialization.jackson'\n  jackson-cbor {}\n\n  # Issue #28918 for compatibility with data serialized with JacksonCborSerializer in\n  # Pekko 2.6.4 or earlier, which was plain JSON format.\n  jackson-cbor-264 = ${pekko.serialization.jackson.jackson-cbor}\n\n}","title":"Additional features"},{"location":"/serialization-jackson.html#date-time-format","text":"WRITE_DATES_AS_TIMESTAMPS and WRITE_DURATIONS_AS_TIMESTAMPS are by default disabled, which means that date/time fields are serialized in ISO-8601 (rfc3339) yyyy-MM-dd'T'HH:mm:ss.SSSZ format instead of numeric arrays. This is better for interoperability but it is slower. If you don’t need the ISO format for interoperability with external systems you can change the following configuration for better performance of date/time fields.\ncopysourcepekko.serialization.jackson.serialization-features {\n  WRITE_DATES_AS_TIMESTAMPS = on\n  WRITE_DURATIONS_AS_TIMESTAMPS = on\n}\nJackson is still be able to deserialize the other format independent of this setting.","title":"Date/time format"},{"location":"/multi-jvm-testing.html","text":"","title":"Multi JVM Testing"},{"location":"/multi-jvm-testing.html#multi-jvm-testing","text":"Supports running applications (objects with main methods) and ScalaTest tests in multiple JVMs at the same time. Useful for integration testing where multiple systems communicate with each other.","title":"Multi JVM Testing"},{"location":"/multi-jvm-testing.html#setup","text":"The multi-JVM testing is an sbt plugin that you can find at https://github.com/sbt/sbt-multi-jvm. To configure it in your project you should do the following steps:\nAdd it as a plugin by adding the following to your project/plugins.sbt: addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-multi-jvm\" % \"0.4.0\")\n Add multi-JVM testing to build.sbt or project/Build.scala by enabling MultiJvmPlugin and setting the MultiJvm config. ```none\nlazy val root = (project in file(\".\"))\n  .enablePlugins(MultiJvmPlugin)\n  .configs(MultiJvm)\n```\nPlease note that by default MultiJvm test sources are located in src/multi-jvm/..., and not in src/test/....","title":"Setup"},{"location":"/multi-jvm-testing.html#running-tests","text":"The multi-JVM tasks are similar to the normal tasks: test, testOnly, and run, but are under the multi-jvm configuration.\nSo in Pekko, to run all the multi-JVM tests in the pekko-remote project use (at the sbt prompt):\nremote-tests/multi-jvm:test\nOr one can change to the pekko-remote-tests project first, and then run the tests:\nproject remote-tests\nmulti-jvm:test\nTo run individual tests use testOnly:\nmulti-jvm:testOnly org.apache.pekko.remote.RandomRoutedRemoteActor\nMore than one test name can be listed to run multiple specific tests. Tab-completion in sbt makes it easy to complete the test names.\nIt’s also possible to specify JVM options with testOnly by including those options after the test names and --. For example:\nmulti-jvm:testOnly org.apache.pekko.remote.RandomRoutedRemoteActor -- -Dsome.option=something","title":"Running tests"},{"location":"/multi-jvm-testing.html#creating-application-tests","text":"The tests are discovered, and combined, through a naming convention. MultiJvm test sources are located in src/multi-jvm/.... A test is named with the following pattern:\n{TestName}MultiJvm{NodeName}\nThat is, each test has MultiJvm in the middle of its name. The part before it groups together tests/applications under a single TestName that will run together. The part after, the NodeName, is a distinguishing name for each forked JVM.\nSo to create a 3-node test called Sample, you can create three applications like the following:\npackage sample\n\nobject SampleMultiJvmNode1 {\n  def main(args: Array[String]) {\n    println(\"Hello from node 1\")\n  }\n}\n\nobject SampleMultiJvmNode2 {\n  def main(args: Array[String]) {\n    println(\"Hello from node 2\")\n  }\n}\n\nobject SampleMultiJvmNode3 {\n  def main(args: Array[String]) {\n    println(\"Hello from node 3\")\n  }\n}\nWhen you call multi-jvm:run sample.Sample at the sbt prompt, three JVMs will be spawned, one for each node. It will look like this:\n> multi-jvm:run sample.Sample\n...\n[info] * sample.Sample\n[JVM-1] Hello from node 1\n[JVM-2] Hello from node 2\n[JVM-3] Hello from node 3\n[success] Total time: ...","title":"Creating application tests"},{"location":"/multi-jvm-testing.html#changing-defaults","text":"You can specify JVM options for the forked JVMs:\njvmOptions in MultiJvm := Seq(\"-Xmx256M\")\nYou can change the name of the multi-JVM test source directory by adding the following configuration to your project:\nunmanagedSourceDirectories in MultiJvm :=\n   Seq(baseDirectory(_ / \"src/some_directory_here\")).join.value\nYou can change what the MultiJvm identifier is. For example, to change it to ClusterTest use the multiJvmMarker setting:\nmultiJvmMarker in MultiJvm := \"ClusterTest\"\nYour tests should now be named {TestName}ClusterTest{NodeName}.","title":"Changing Defaults"},{"location":"/multi-jvm-testing.html#configuration-of-the-jvm-instances","text":"You can define specific JVM options for each of the spawned JVMs. You do that by creating a file named after the node in the test with suffix .opts and put them in the same directory as the test.\nFor example, to feed the JVM options -Dpekko.remote.port=9991 and -Xmx256m to the SampleMultiJvmNode1 let’s create three *.opts files and add the options to them. Separate multiple options with space.\nSampleMultiJvmNode1.opts:\n-Dpekko.remote.port=9991 -Xmx256m\nSampleMultiJvmNode2.opts:\n-Dpekko.remote.port=9992 -Xmx256m\nSampleMultiJvmNode3.opts:\n-Dpekko.remote.port=9993 -Xmx256m","title":"Configuration of the JVM instances"},{"location":"/multi-jvm-testing.html#scalatest","text":"There is also support for creating ScalaTest tests rather than applications. To do this use the same naming convention as above, but create ScalaTest suites rather than objects with main methods. You need to have ScalaTest on the classpath. Here is a similar example to the one above but using ScalaTest:\npackage sample\n\nimport org.scalatest.wordspec.AnyWordSpec\nimport org.scalatest.matchers.must.Matchers\n\nclass SpecMultiJvmNode1 extends AnyWordSpec with Matchers {\n  \"A node\" should {\n    \"be able to say hello\" in {\n      val message = \"Hello from node 1\"\n      message must be(\"Hello from node 1\")\n    }\n  }\n}\n\nclass SpecMultiJvmNode2 extends AnyWordSpec with Matchers {\n  \"A node\" should {\n    \"be able to say hello\" in {\n      val message = \"Hello from node 2\"\n      message must be(\"Hello from node 2\")\n    }\n  }\n}\nTo run just these tests you would call multi-jvm:testOnly sample.Spec at the sbt prompt.","title":"ScalaTest"},{"location":"/multi-jvm-testing.html#multi-node-additions","text":"There has also been some additions made to the SbtMultiJvm plugin to accommodate the may change module multi node testing, described in that section.","title":"Multi Node Additions"},{"location":"/multi-jvm-testing.html#example-project","text":"Cluster example project is an example project that can be downloaded, and with instructions of how to run.\nThis project illustrates Cluster features and also includes Multi JVM Testing with the sbt-multi-jvm plugin.","title":"Example project"},{"location":"/multi-node-testing.html","text":"","title":"Multi Node Testing"},{"location":"/multi-node-testing.html#multi-node-testing","text":"","title":"Multi Node Testing"},{"location":"/multi-node-testing.html#module-info","text":"To use Multi Node Testing, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-multi-node-testkit\" % PekkoVersion % Test Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-multi-node-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  testImplementation \"org.apache.pekko:pekko-multi-node-testkit_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Multi-node Testkit Artifact org.apache.pekko pekko-multi-node-testkit 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.remote.testkit License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/multi-node-testing.html#multi-node-testing-concepts","text":"When we talk about multi node testing in Pekko we mean the process of running coordinated tests on multiple actor systems in different JVMs. The multi node testing kit consist of three main parts.\nThe Test Conductor. that coordinates and controls the nodes under test. The Multi Node Spec. that is a convenience wrapper for starting the TestConductorTestConductor and letting all nodes connect to it. The SbtMultiJvm Plugin. that starts tests in multiple JVMs possibly on multiple machines.","title":"Multi Node Testing Concepts"},{"location":"/multi-node-testing.html#the-test-conductor","text":"The basis for the multi node testing is the TestConductorTestConductor. It is a Pekko Extension that plugs in to the network stack and it is used to coordinate the nodes participating in the test and provides several features including:\nNode Address Lookup: Finding out the full path to another test node (No need to share configuration between test nodes) Node Barrier Coordination: Waiting for other nodes at named barriers. Network Failure Injection: Throttling traffic, dropping packets, unplugging and plugging nodes back in.\nThis is a schematic overview of the test conductor.\nThe test conductor server is responsible for coordinating barriers and sending commands to the test conductor clients that act upon them, e.g. throttling network traffic to/from another client. More information on the possible operations is available in the remote.testconductor.Conductorremote.testconductor.Conductor API documentation.","title":"The Test Conductor"},{"location":"/multi-node-testing.html#the-multi-node-spec","text":"The Multi Node Spec consists of two parts. The MultiNodeConfigMultiNodeConfig that is responsible for common configuration and enumerating and naming the nodes under test. The MultiNodeSpec that contains a number of convenience functions for making the test nodes interact with each other. More information on the possible operations is available in the remote.testkit.MultiNodeSpecremote.testkit.MultiNodeSpec API documentation.\nThe setup of the MultiNodeSpec is configured through java system properties that you set on all JVMs that’s going to run a node under test. These can be set on the JVM command line with -Dproperty=value.\nThese are the available properties\nmultinode.max-nodes The maximum number of nodes that a test can have. multinode.host The host name or IP for this node. Must be resolvable using InetAddress.getByName. multinode.port The port number for this node. Defaults to 0 which will use a random port. multinode.server-host The host name or IP for the server node. Must be resolvable using InetAddress.getByName. multinode.server-port The port number for the server node. Defaults to 4711. multinode.index The index of this node in the sequence of roles defined for the test. The index 0 is special and that machine will be the server. All failure injection and throttling must be done from this node.","title":"The Multi Node Spec"},{"location":"/multi-node-testing.html#the-sbtmultijvm-plugin","text":"The SbtMultiJvm Plugin has been updated to be able to run multi node tests, by automatically generating the relevant multinode.* properties. This means that you can run multi node tests on a single machine without any special configuration by running them as normal multi-jvm tests. These tests can then be run distributed over multiple machines without any changes by using the multi-node additions to the plugin.","title":"The SbtMultiJvm Plugin"},{"location":"/multi-node-testing.html#multi-node-specific-additions","text":"The plugin also has a number of new multi-node-* sbt tasks and settings to support running tests on multiple machines. The necessary test classes and dependencies are packaged for distribution to other machines with SbtAssembly into a jar file with a name on the format <projectName>_<scalaVersion>-<projectVersion>-multi-jvm-assembly.jar\nNote To be able to distribute and kick off the tests on multiple machines, it is assumed that both host and target systems are POSIX like systems with ssh and rsync available.\nThese are the available sbt multi-node settings\nmultiNodeHosts A sequence of hosts to use for running the test, on the form user@host:java where host is the only required part. Will override settings from file. multiNodeHostsFileName A file to use for reading in the hosts to use for running the test. One per line on the same format as above. Defaults to multi-node-test.hosts in the base project directory. multiNodeTargetDirName A name for the directory on the target machine, where to copy the jar file. Defaults to multi-node-test in the base directory of the ssh user used to rsync the jar file. multiNodeJavaName The name of the default Java executable on the target machines. Defaults to java.\nHere are some examples of how you define hosts\nlocalhost The current user on localhost using the default java. user1@host1 User user1 on host host1 with the default java. user2@host2:/usr/lib/jvm/java-7-openjdk-amd64/bin/java User user2 on host host2 using java 7. host3:/usr/lib/jvm/java-6-openjdk-amd64/bin/java The current user on host host3 using java 6.","title":"Multi Node Specific Additions"},{"location":"/multi-node-testing.html#running-the-multi-node-tests","text":"To run all the multi node test in multi-node mode (i.e. distributing the jar files and kicking off the tests remotely) from inside sbt, use the multiNodeTest task:\nmultiNodeTest\nTo run all of them in multi-jvm mode (i.e. all JVMs on the local machine) do:\nmulti-jvm:test\nTo run individual tests use the multiNodeTestOnly task:\nmultiNodeTestOnly your.MultiNodeTest\nTo run individual tests in the multi-jvm mode do:\nmulti-jvm:testOnly your.MultiNodeTest\nMore than one test name can be listed to run multiple specific tests. Tab completion in sbt makes it easy to complete the test names.","title":"Running the Multi Node Tests"},{"location":"/multi-node-testing.html#a-multi-node-testing-example","text":"First we need some scaffolding to hook up the MultiNodeSpecMultiNodeSpec with your favorite test framework. Lets define a trait STMultiNodeSpec that uses ScalaTest to start and stop MultiNodeSpec.\ncopysourcepackage org.apache.pekko.remote.testkit\n\nimport scala.language.implicitConversions\n\nimport org.scalatest.BeforeAndAfterAll\nimport org.scalatest.matchers.should.Matchers\nimport org.scalatest.wordspec.AnyWordSpecLike\n\n/**\n * Hooks up MultiNodeSpec with ScalaTest\n */\ntrait STMultiNodeSpec extends MultiNodeSpecCallbacks with AnyWordSpecLike with Matchers with BeforeAndAfterAll {\n  self: MultiNodeSpec =>\n\n  override def beforeAll() = multiNodeSpecBeforeAll()\n\n  override def afterAll() = multiNodeSpecAfterAll()\n\n  // Might not be needed anymore if we find a nice way to tag all logging from a node\n  override implicit def convertToWordSpecStringWrapper(s: String): WordSpecStringWrapper =\n    new WordSpecStringWrapper(s\"$s (on node '${self.myself.name}', $getClass)\")\n}\nThen we need to define a configuration. Lets use two nodes \"node1 and \"node2\" and call it MultiNodeSampleConfig.\ncopysourcepackage org.apache.pekko.remote.sample\n\nimport org.apache.pekko.remote.testkit.{ MultiNodeConfig, STMultiNodeSpec }\n\nobject MultiNodeSampleConfig extends MultiNodeConfig {\n  val node1 = role(\"node1\")\n  val node2 = role(\"node2\")\n}\nAnd then finally to the node test code. That starts the two nodes, and demonstrates a barrier, and a remote actor message send/receive.\ncopysourcepackage org.apache.pekko.remote.sample\n\nimport org.apache.pekko\nimport pekko.actor.{ Actor, Props }\nimport pekko.remote.testkit.MultiNodeSpec\nimport pekko.testkit.ImplicitSender\n\nclass MultiNodeSampleSpecMultiJvmNode1 extends MultiNodeSample\nclass MultiNodeSampleSpecMultiJvmNode2 extends MultiNodeSample\n\nobject MultiNodeSample {\n  class Ponger extends Actor {\n    def receive = {\n      case \"ping\" => sender() ! \"pong\"\n    }\n  }\n}\n\nclass MultiNodeSample extends MultiNodeSpec(MultiNodeSampleConfig) with STMultiNodeSpec with ImplicitSender {\n\n  import MultiNodeSample._\n  import MultiNodeSampleConfig._\n\n  def initialParticipants = roles.size\n\n  \"A MultiNodeSample\" must {\n\n    \"wait for all nodes to enter a barrier\" in {\n      enterBarrier(\"startup\")\n    }\n\n    \"send to and receive from a remote node\" in {\n      runOn(node1) {\n        enterBarrier(\"deployed\")\n        val ponger = system.actorSelection(node(node2) / \"user\" / \"ponger\")\n        ponger ! \"ping\"\n        import scala.concurrent.duration._\n        expectMsg(10.seconds, \"pong\")\n      }\n\n      runOn(node2) {\n        system.actorOf(Props[Ponger](), \"ponger\")\n        enterBarrier(\"deployed\")\n      }\n\n      enterBarrier(\"finished\")\n    }\n  }\n}","title":"A Multi Node Testing Example"},{"location":"/multi-node-testing.html#things-to-keep-in-mind","text":"There are a couple of things to keep in mind when writing multi node tests or else your tests might behave in surprising ways.\nDon’t issue a shutdown of the first node. The first node is the controller and if it shuts down your test will break. To be able to use blackhole, passThrough, and throttle you must activate the failure injector and throttler transport adapters by specifying testTransport(on = true)testTransport(true) in your MultiNodeConfig. Throttling, shutdown and other failure injections can only be done from the first node, which again is the controller. Don’t ask for the address of a node using node(address) after the node has been shut down. Grab the address before shutting down the node. Don’t use MultiNodeSpec methods like address lookup, barrier entry et.c. from other threads than the main test thread. This also means that you shouldn’t use them from inside an actor, a future, or a scheduled task.","title":"Things to Keep in Mind"},{"location":"/multi-node-testing.html#configuration","text":"There are several configuration properties for the Multi-Node Testing module, please refer to the reference configuration.","title":"Configuration"},{"location":"/remoting-artery.html","text":"","title":"Artery Remoting"},{"location":"/remoting-artery.html#artery-remoting","text":"Note Remoting is the mechanism by which Actors on different nodes talk to each other internally. When building a Pekko application, you would usually not use the Remoting concepts directly, but instead use the more high-level Pekko Cluster utilities or technology-agnostic protocols such as HTTP, gRPC etc.\nIf migrating from classic remoting see what’s new in Artery","title":"Artery Remoting"},{"location":"/remoting-artery.html#dependency","text":"To use Artery Remoting, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-remote\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-remote_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-remote_${versions.ScalaBinary}\"\n}\nOne option is to use Artery with Aeron, see Selecting a transport. The Aeron dependency needs to be explicitly added if using the aeron-udp transport:\nsbt libraryDependencies ++= Seq(\n  \"io.aeron\" % \"aeron-driver\" % \"1.38.1\",\n  \"io.aeron\" % \"aeron-client\" % \"1.38.1\"\n) Maven <dependencies>\n  <dependency>\n    <groupId>io.aeron</groupId>\n    <artifactId>aeron-driver</artifactId>\n    <version>1.38.1</version>\n  </dependency>\n  <dependency>\n    <groupId>io.aeron</groupId>\n    <artifactId>aeron-client</artifactId>\n    <version>1.38.1</version>\n  </dependency>\n</dependencies> Gradle dependencies {\n  implementation \"io.aeron:aeron-driver:1.38.1\"\n  implementation \"io.aeron:aeron-client:1.38.1\"\n}","title":"Dependency"},{"location":"/remoting-artery.html#configuration","text":"To enable remote capabilities in your Pekko project you should, at a minimum, add the following changes to your application.conf file:\npekko {\n  actor {\n    # provider=remote is possible, but prefer cluster\n    provider = cluster \n  }\n  remote {\n    artery {\n      transport = tcp # See Selecting a transport below\n      canonical.hostname = \"127.0.0.1\"\n      canonical.port = 25520\n    }\n  }\n}\nAs you can see in the example above there are four things you need to add to get started:\nChange provider from local. We recommend using Pekko Cluster over using remoting directly. Enable Artery to use it as the remoting implementation Add host name - the machine you want to run the actor system on; this host name is exactly what is passed to remote systems in order to identify this system and consequently used for connecting back to this system if need be, hence set it to a reachable IP address or resolvable name in case you want to communicate across the network. Add port number - the port the actor system should listen on, set to 0 to have it chosen automatically\nNote The port number needs to be unique for each actor system on the same machine even if the actor systems have different names. This is because each actor system has its own networking subsystem listening for connections and handling messages as not to interfere with other actor systems.\nThe example above only illustrates the bare minimum of properties you have to add to enable remoting. All settings are described in Remote Configuration.","title":"Configuration"},{"location":"/remoting-artery.html#introduction","text":"We recommend Pekko Cluster over using remoting directly. As remoting is the underlying module that allows for Cluster, it is still useful to understand details about it though.\nNote This page describes the remoting subsystem, codenamed Artery that has replaced the classic remoting implementation.\nRemoting enables Actor systems on different hosts or JVMs to communicate with each other. By enabling remoting the system will start listening on a provided network address and also gains the ability to connect to other systems through the network. From the application’s perspective there is no API difference between local or remote systems, ActorRefActorRef instances that point to remote systems look exactly the same as local ones: they can be sent messages to, watched, etc. Every ActorRef contains hostname and port information and can be passed around even on the network. This means that on a network every ActorRef is a unique identifier of an actor on that network.\nYou need to enable serialization for your actor messages. Serialization with Jackson is a good choice in many cases and our recommendation if you don’t have other preference.\nRemoting is not a server-client technology. All systems using remoting can contact any other system on the network if they possess an ActorRef pointing to those system. This means that every system that is remoting enabled acts as a “server” to which arbitrary systems on the same network can connect to.","title":"Introduction"},{"location":"/remoting-artery.html#selecting-a-transport","text":"There are three alternatives of which underlying transport to use. It is configured by property pekko.remote.artery.transport with the possible values:\ntcp - Based on Pekko Streams TCP (default if other not configured) tls-tcp - Same as tcp with encryption using Pekko Streams TLS aeron-udp - Based on Aeron (UDP)\nIf you are uncertain of what to select a good choice is to use the default, which is tcp.\nThe Aeron (UDP) transport is a high performance transport and should be used for systems that require high throughput and low latency. It uses more CPU than TCP when the system is idle or at low message rates. There is no encryption for Aeron.\nThe TCP and TLS transport is implemented using Pekko Streams TCP/TLS. This is the choice when encryption is needed, but it can also be used with plain TCP without TLS. It’s also the obvious choice when UDP can’t be used. It has very good performance (high throughput and low latency) but latency at high throughput might not be as good as the Aeron transport. It has less operational complexity than the Aeron transport and less risk of trouble in container environments.\nAeron requires 64bit JVM to work reliably and is only officially supported on Linux, Mac and Windows. It may work on other Unixes e.g. Solaris but insufficient testing has taken place for it to be officially supported. If you’re on a Big Endian processor, such as Sparc, it is recommended to use TCP.\nNote Rolling update is not supported when changing from one transport to another.","title":"Selecting a transport"},{"location":"/remoting-artery.html#canonical-address","text":"In order for remoting to work properly, where each system can send messages to any other system on the same network (for example a system forwards a message to a third system, and the third replies directly to the sender system) it is essential for every system to have a unique, globally reachable address and port. This address is part of the unique name of the system and will be used by other systems to open a connection to it and send messages. This means that if a host has multiple names (different DNS records pointing to the same IP address) then only one of these can be canonical. If a message arrives to a system but it contains a different hostname than the expected canonical name then the message will be dropped. If multiple names for a system would be allowed, then equality checks among ActorRefActorRef instances would no longer to be trusted and this would violate the fundamental assumption that an actor has a globally unique reference on a given network. As a consequence, this also means that localhost addresses (e.g. 127.0.0.1) cannot be used in general (apart from local development) since they are not unique addresses in a real network.\nIn cases, where Network Address Translation (NAT) is used or other network bridging is involved, it is important to configure the system so that it understands that there is a difference between his externally visible, canonical address and between the host-port pair that is used to listen for connections. See Pekko behind NAT or in a Docker container for details.","title":"Canonical address"},{"location":"/remoting-artery.html#acquiring-references-to-remote-actors","text":"In order to communicate with an actor, it is necessary to have its ActorRefActorRef. In the local case it is usually the creator of the actor (the caller of actorOf()) is who gets the ActorRef for an actor that it can then send to other actors. In other words:\nAn Actor can get a remote Actor’s reference by receiving a message from it (as it’s available as sender()getSender() then), or inside of a remote message (e.g. PleaseReply(message: String, remoteActorRef: ActorRef))\nAlternatively, an actor can look up another located at a known path using ActorSelectionActorSelection. These methods are available even in remoting enabled systems:\nRemote Lookup : used to look up an actor on a remote node with actorSelection(path)actorSelection(path) Remote Creation : used to create an actor on a remote node with actorOf(Props(...), actorName)actorOf(Props(...), actorName)\nIn the next sections the two alternatives are described in detail.","title":"Acquiring references to remote actors"},{"location":"/remoting-artery.html#looking-up-remote-actors","text":"actorSelection(path)actorSelection(path) will obtain an ActorSelectionActorSelection to an Actor on a remote node, e.g.:\nScala val selection =\n  context.actorSelection(\"pekko://actorSystemName@10.0.0.1:25520/user/actorName\")\n Java ActorSelection selection =\n  context.actorSelection(\"pekko://actorSystemName@10.0.0.1:25520/user/actorName\");\nAs you can see from the example above the following pattern is used to find an actor on a remote node:\npekko://<actor system>@<hostname>:<port>/<actor path>\nNote Unlike with earlier remoting, the protocol field is always pekko as pluggable transports are no longer supported.\nOnce you obtained a selection to the actor you can interact with it in the same way you would with a local actor, e.g.:\nScala selection ! \"Pretty awesome feature\"\n Java selection.tell(\"Pretty awesome feature\", getSelf());\nTo acquire an ActorRefActorRef for an ActorSelectionActorSelection you need to send a message to the selection and use the sender()getSender() reference of the reply from the actor. There is a built-in IdentifyIdentify message that all Actors will understand and automatically reply to with a ActorIdentityActorIdentity message containing the ActorRef. This can also be done with the resolveOneresolveOne method of the ActorSelection, which returns a FutureCompletionStage of the matching ActorRef.\nFor more details on how actor addresses and paths are formed and used, please refer to Actor References, Paths and Addresses.\nNote Message sends to actors that are actually in the sending actor system do not get delivered via the remote actor ref provider. They’re delivered directly, by the local actor ref provider. Aside from providing better performance, this also means that if the hostname you configure remoting to listen as cannot actually be resolved from within the very same actor system, such messages will (perhaps counterintuitively) be delivered just fine.","title":"Looking up Remote Actors"},{"location":"/remoting-artery.html#remote-security","text":"An ActorSystemActorSystem should not be exposed via Pekko Remote (Artery) over plain Aeron/UDP or TCP to an untrusted network (e.g. Internet). It should be protected by network security, such as a firewall. If that is not considered as enough protection TLS with mutual authentication should be enabled.\nBest practice is that Pekko remoting nodes should only be accessible from the adjacent network. Note that if TLS is enabled with mutual authentication there is still a risk that an attacker can gain access to a valid certificate by compromising any node with certificates issued by the same internal PKI tree.\nBy default, Java serialization is disabled in Pekko. That is also security best-practice because of its multiple known attack surfaces.","title":"Remote Security"},{"location":"/remoting-artery.html#configuring-ssl-tls-for-pekko-remoting","text":"In addition to what is described here, you can read the blog post addressing this aspect for Pekko Securing Pekko cluster communication in Kubernetes.\nSSL can be used as the remote transport by using the tls-tcp transport:\npekko.remote.artery {\n  transport = tls-tcp\n}\nNext the actual SSL/TLS parameters have to be configured:\npekko.remote.artery {\n  transport = tls-tcp\n\n  ssl.config-ssl-engine {\n    key-store = \"/example/path/to/mykeystore.jks\"\n    trust-store = \"/example/path/to/mytruststore.jks\"\n\n    key-store-password = ${SSL_KEY_STORE_PASSWORD}\n    key-password = ${SSL_KEY_PASSWORD}\n    trust-store-password = ${SSL_TRUST_STORE_PASSWORD}\n\n    protocol = \"TLSv1.2\"\n\n    enabled-algorithms = [TLS_DHE_RSA_WITH_AES_128_GCM_SHA256]\n  }\n}\nAlways use substitution from environment variables for passwords. Don’t define real passwords in config files.\nAccording to RFC 7525 the recommended algorithms to use with TLS 1.2 (as of writing this document) are:\nTLS_DHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_DHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\nYou should always check the latest information about security and algorithm recommendations though before you configure your system.\nSince a Pekko remoting is inherently peer-to-peer both the key-store as well as trust-store need to be configured on each remoting node participating in the cluster.\nThe official Java Secure Socket Extension documentation as well as the Oracle documentation on creating KeyStore and TrustStores are both great resources to research when setting up security on the JVM. Please consult those resources when troubleshooting and configuring SSL.\nMutual authentication between TLS peers is enabled by default. Mutual authentication means that the the passive side (the TLS server side) of a connection will also request and verify a certificate from the connecting peer. Without this mode only the client side is requesting and verifying certificates. While Pekko is a peer-to-peer technology, each connection between nodes starts out from one side (the “client”) towards the other (the “server”).\nNote that if TLS is enabled with mutual authentication there is still a risk that an attacker can gain access to a valid certificate by compromising any node with certificates issued by the same internal PKI tree.\nIt’s recommended that you enable hostname verification with pekko.remote.artery.ssl.config-ssl-engine.hostname-verification=on. When enabled it will verify that the destination hostname matches the hostname in the peer’s certificate.\nIn deployments where hostnames are dynamic and not known up front it can make sense to leave the hostname verification off.\nYou have a few choices how to set up certificates and hostname verification:\nHave a single set of keys and a single certificate for all nodes and disable hostname checking The single set of keys and the single certificate is distributed to all nodes. The certificate can be self-signed as it is distributed both as a certificate for authentication but also as the trusted certificate. If the keys/certificate are lost, someone else can connect to your cluster. Adding nodes to the cluster is simple as the key material can be deployed / distributed to the new node. Have a single set of keys and a single certificate for all nodes that contains all of the host names and enable hostname checking. This means that only the hosts mentioned in the certificate can connect to the cluster. It cannot be checked, though, if the node you talk to is actually the node it is supposed to be (or if it is one of the other nodes). This seems like a minor restriction as you’ll have to trust all cluster nodes the same in an Pekko cluster anyway. The certificate can be self-signed in which case the same single certificate is distributed and trusted on all nodes (but see the next bullet) Adding a new node means that its host name needs to conform to the trusted host names in the certificate. That either means to foresee new hosts, use a wildcard certificate, or use a full CA in the first place, so you can later issue more certificates if more nodes are to be added (but then you already get into the territory of the next solution). If a certificate is stolen, it can only be used to connect to the cluster from a node reachable via a hostname that is trusted in the certificate. It would require tampering with DNS to allow other nodes to get access to the cluster (however, tampering DNS might be easier in an internal setting than on internet scale). Have a CA and then keys/certificates, one for each node, and enable host name checking. Basically like internet HTTPS but that you only trust the internal CA and then issue certificates for each new node. Needs a PKI, the CA certificate is trusted on all nodes, the individual certificates are used for authentication. Only the CA certificate and the key/certificate for a node is distributed. If keys/certificates are stolen, only the same node can access the cluster (unless DNS is tampered with as well). You can revoke single certificates.\nSee also a description of the settings in the Remote Configuration section.\nNote When using SHA1PRNG on Linux it’s recommended specify -Djava.security.egd=file:/dev/urandom as argument to the JVM to prevent blocking. It is NOT as secure because it reuses the seed.","title":"Configuring SSL/TLS for Pekko Remoting"},{"location":"/remoting-artery.html#untrusted-mode","text":"As soon as an actor system can connect to another remotely, it may in principle send any possible message to any actor contained within that remote system. One example may be sending a PoisonPillPoisonPill to the system guardian, shutting that system down. This is not always desired, and it can be disabled with the following setting:\npekko.remote.artery.untrusted-mode = on\nThis disallows sending of system messages (actor life-cycle commands, DeathWatch, etc.) and any message extending PossiblyHarmfulPossiblyHarmful to the system on which this flag is set. Should a client send them nonetheless they are dropped and logged (at DEBUG level in order to reduce the possibilities for a denial of service attack). PossiblyHarmful covers the predefined messages like PoisonPillPoisonPill and KillKill, but it can also be added as a marker trait to user-defined messages.\nWarning Untrusted mode does not give full protection against attacks by itself. It makes it slightly harder to perform malicious or unintended actions but it should be noted that Java serialization should still not be enabled. Additional protection can be achieved when running in an untrusted network by network security (e.g. firewalls) and/or enabling TLS with mutual authentication.\nMessages sent with actor selection are by default discarded in untrusted mode, but permission to receive actor selection messages can be granted to specific actors defined in configuration:\npekko.remote.artery.trusted-selection-paths = [\"/user/receptionist\", \"/user/namingService\"]\nThe actual message must still not be of type PossiblyHarmful.\nIn summary, the following operations are ignored by a system configured in untrusted mode when incoming via the remoting layer:\nremote deployment (which also means no remote supervision) remote DeathWatch system.stop()system.stop(), PoisonPillPoisonPill, KillKill sending any message which extends from the PossiblyHarmfulPossiblyHarmful marker interface, which includes TerminatedTerminated messages sent with actor selection, unless destination defined in trusted-selection-paths.\nNote Enabling the untrusted mode does not remove the capability of the client to freely choose the target of its message sends, which means that messages not prohibited by the above rules can be sent to any actor in the remote system. It is good practice for a client-facing system to only contain a well-defined set of entry point actors, which then forward requests (possibly after performing validation) to another actor system containing the actual worker actors. If messaging between these two server-side systems is done using local ActorRefActorRef (they can be exchanged safely between actor systems within the same JVM), you can restrict the messages on this interface by marking them PossiblyHarmfulPossiblyHarmful so that a client cannot forge them.","title":"Untrusted Mode"},{"location":"/remoting-artery.html#quarantine","text":"Pekko remoting is using TCP or Aeron as underlying message transport. Aeron is using UDP and adds among other things reliable delivery and session semantics, very similar to TCP. This means that the order of the messages are preserved, which is needed for the Actor message ordering guarantees. Under normal circumstances all messages will be delivered but there are cases when messages may not be delivered to the destination:\nduring a network partition when the TCP connection or the Aeron session is broken, this automatically recovered once the partition is over when sending too many messages without flow control and thereby filling up the outbound send queue (outbound-message-queue-size config) if serialization or deserialization of a message fails (only that message will be dropped) if an unexpected exception occurs in the remoting infrastructure\nIn short, Actor message delivery is “at-most-once” as described in Message Delivery Reliability\nSome messages in Pekko are called system messages and those cannot be dropped because that would result in an inconsistent state between the systems. Such messages are used for essentially two features; remote death watch and remote deployment. These messages are delivered by Pekko remoting with “exactly-once” guarantee by confirming each message and resending unconfirmed messages. If a system message anyway cannot be delivered the association with the destination system is irrecoverable failed, and Terminated is signaled for all watched actors on the remote system. It is placed in a so called quarantined state. Quarantine usually does not happen if remote watch or remote deployment is not used.\nEach ActorSystemActorSystem instance has an unique identifier (UID), which is important for differentiating between incarnations of a system when it is restarted with the same hostname and port. It is the specific incarnation (UID) that is quarantined. The only way to recover from this state is to restart one of the actor systems.\nMessages that are sent to and received from a quarantined system will be dropped. However, it is possible to send messages with actorSelection to the address of a quarantined system, which is useful to probe if the system has been restarted.\nAn association will be quarantined when:\nCluster node is removed from the cluster membership. Remote failure detector triggers, i.e. remote watch is used. This is different when Pekko Cluster is used. The unreachable observation by the cluster failure detector can go back to reachable if the network partition heals. A cluster member is not quarantined when the failure detector triggers. Overflow of the system message delivery buffer, e.g. because of too many watch requests at the same time (system-message-buffer-size config). Unexpected exception occurs in the control subchannel of the remoting infrastructure.\nThe UID of the ActorSystemActorSystem is exchanged in a two-way handshake when the first message is sent to a destination. The handshake will be retried until the other system replies and no other messages will pass through until the handshake is completed. If the handshake cannot be established within a timeout (handshake-timeout config) the association is stopped (freeing up resources). Queued messages will be dropped if the handshake cannot be established. It will not be quarantined, because the UID is unknown. New handshake attempt will start when next message is sent to the destination.\nHandshake requests are actually also sent periodically to be able to establish a working connection when the destination system has been restarted.","title":"Quarantine"},{"location":"/remoting-artery.html#watching-remote-actors","text":"Watching a remote actor is API wise not different than watching a local actor, as described in Lifecycle Monitoring aka DeathWatch. However, it is important to note, that unlike in the local case, remoting has to handle when a remote actor does not terminate in a graceful way sending a system message to notify the watcher actor about the event, but instead being hosted on a system which stopped abruptly (crashed). These situations are handled by the built-in failure detector.","title":"Watching Remote Actors"},{"location":"/remoting-artery.html#failure-detector","text":"Under the hood remote death watch uses heartbeat messages and a failure detector to generate TerminatedTerminated message from network failures and JVM crashes, in addition to graceful termination of watched actor.\nThe heartbeat arrival times is interpreted by an implementation of The Phi Accrual Failure Detector.\nThe suspicion level of failure is given by a value called phi. The basic idea of the phi failure detector is to express the value of phi on a scale that is dynamically adjusted to reflect current network conditions.\nThe value of phi is calculated as:\nphi = -log10(1 - F(timeSinceLastHeartbeat))\nwhere F is the cumulative distribution function of a normal distribution with mean and standard deviation estimated from historical heartbeat inter-arrival times.\nIn the Remote Configuration you can adjust the pekko.remote.watch-failure-detector.threshold to define when a phi value is considered to be a failure.\nA low threshold is prone to generate many false positives but ensures a quick detection in the event of a real crash. Conversely, a high threshold generates fewer mistakes but needs more time to detect actual crashes. The default threshold is 10 and is appropriate for most situations. However in cloud environments, such as Amazon EC2, the value could be increased to 12 in order to account for network issues that sometimes occur on such platforms.\nThe following chart illustrates how phi increase with increasing time since the previous heartbeat.\nPhi is calculated from the mean and standard deviation of historical inter arrival times. The previous chart is an example for standard deviation of 200 ms. If the heartbeats arrive with less deviation the curve becomes steeper, i.e. it is possible to determine failure more quickly. The curve looks like this for a standard deviation of 100 ms.\nTo be able to survive sudden abnormalities, such as garbage collection pauses and transient network failures the failure detector is configured with a margin, pekko.remote.watch-failure-detector.acceptable-heartbeat-pause. You may want to adjust the Remote Configuration of this depending on you environment. This is how the curve looks like for acceptable-heartbeat-pause configured to 3 seconds.","title":"Failure Detector"},{"location":"/remoting-artery.html#serialization","text":"You need to enable serialization for your actor messages. Serialization with Jackson is a good choice in many cases and our recommendation if you don’t have other preference.","title":"Serialization"},{"location":"/remoting-artery.html#bytebuffer-based-serialization","text":"Artery introduces a new serialization mechanism which allows the ByteBufferSerializerByteBufferSerializer to directly write into a shared java.nio.ByteBuffer instead of being forced to allocate and return an Array[Byte] for each serialized message. For high-throughput messaging this API change can yield significant performance benefits, so we recommend changing your serializers to use this new mechanism.\nThis new API also plays well with new versions of Google Protocol Buffers and other serialization libraries, which gained the ability to serialize directly into and from ByteBuffers.\nAs the new feature only changes how bytes are read and written, and the rest of the serialization infrastructure remained the same, we recommend reading the Serialization documentation first.\nImplementing an org.apache.pekko.serialization.ByteBufferSerializer works the same way as any other serializer,\nScala copysourcetrait ByteBufferSerializer {\n\n  /**\n   * Serializes the given object into the `ByteBuffer`.\n   */\n  def toBinary(o: AnyRef, buf: ByteBuffer): Unit\n\n  /**\n   * Produces an object from a `ByteBuffer`, with an optional type-hint;\n   * the class should be loaded using ActorSystem.dynamicAccess.\n   */\n  @throws(classOf[NotSerializableException])\n  def fromBinary(buf: ByteBuffer, manifest: String): AnyRef\n\n} Java copysourceinterface ByteBufferSerializer {\n  /** Serializes the given object into the `ByteBuffer`. */\n  void toBinary(Object o, ByteBuffer buf);\n\n  /**\n   * Produces an object from a `ByteBuffer`, with an optional type-hint; the class should be\n   * loaded using ActorSystem.dynamicAccess.\n   */\n  Object fromBinary(ByteBuffer buf, String manifest);\n}\nImplementing a serializer for Artery is therefore as simple as implementing this interface, and binding the serializer as usual (which is explained in Serialization).\nImplementations should typically extend SerializerWithStringManifestSerializerWithStringManifest and in addition to the ByteBuffer based toBinarytoBinary and fromBinaryfromBinary methods also implement the array based toBinarytoBinary and fromBinaryfromBinary methods. The array based methods will be used when ByteBuffer is not used, e.g. in Pekko Persistence.\nNote that the array based methods can be implemented by delegation like this:\nScala copysourceimport java.nio.ByteBuffer\nimport org.apache.pekko\nimport pekko.serialization.ByteBufferSerializer\nimport pekko.serialization.SerializerWithStringManifest\n\nclass ExampleByteBufSerializer extends SerializerWithStringManifest with ByteBufferSerializer {\n  override def identifier: Int = 1337\n  override def manifest(o: AnyRef): String = \"naive-toStringImpl\"\n\n  // Implement this method for compatibility with `SerializerWithStringManifest`.\n  override def toBinary(o: AnyRef): Array[Byte] = {\n    // in production code, acquire this from a BufferPool\n    val buf = ByteBuffer.allocate(256)\n\n    toBinary(o, buf)\n    buf.flip()\n    val bytes = new Array[Byte](buf.remaining)\n    buf.get(bytes)\n    bytes\n  }\n\n  // Implement this method for compatibility with `SerializerWithStringManifest`.\n  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =\n    fromBinary(ByteBuffer.wrap(bytes), manifest)\n\n  // Actual implementation in the ByteBuffer versions of to/fromBinary:\n  override def toBinary(o: AnyRef, buf: ByteBuffer): Unit = ??? // implement actual logic here\n  override def fromBinary(buf: ByteBuffer, manifest: String): AnyRef = ??? // implement actual logic here\n} Java copysourceimport org.apache.pekko.serialization.ByteBufferSerializer;\nimport org.apache.pekko.serialization.SerializerWithStringManifest;\n\nclass ExampleByteBufSerializer extends SerializerWithStringManifest\n    implements ByteBufferSerializer {\n\n  @Override\n  public int identifier() {\n    return 1337;\n  }\n\n  @Override\n  public String manifest(Object o) {\n    return \"serialized-\" + o.getClass().getSimpleName();\n  }\n\n  @Override\n  public byte[] toBinary(Object o) {\n    // in production code, acquire this from a BufferPool\n    final ByteBuffer buf = ByteBuffer.allocate(256);\n\n    toBinary(o, buf);\n    buf.flip();\n    final byte[] bytes = new byte[buf.remaining()];\n    buf.get(bytes);\n    return bytes;\n  }\n\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) {\n    return fromBinary(ByteBuffer.wrap(bytes), manifest);\n  }\n\n  @Override\n  public void toBinary(Object o, ByteBuffer buf) {\n    // Implement actual serialization here\n  }\n\n  @Override\n  public Object fromBinary(ByteBuffer buf, String manifest) {\n    // Implement actual deserialization here\n    return null;\n  }\n}","title":"ByteBuffer based serialization"},{"location":"/remoting-artery.html#routers-with-remote-destinations","text":"It is absolutely feasible to combine remoting with Routing.\nA pool of remote deployed routees can be configured as:\ncopysourcepekko.actor.deployment {\n  /parent/remotePool {\n    router = round-robin-pool\n    nr-of-instances = 10\n    target.nodes = [\"tcp://app@10.0.0.2:2552\", \"pekko://app@10.0.0.3:2552\"]\n  }\n}\nThis configuration setting will clone the actor defined in the Props of the remotePool 10 times and deploy it evenly distributed across the two given target nodes.\nWhen using a pool of remote deployed routees you must ensure that all parameters of the Props can be serialized.\nA group of remote actors can be configured as:\ncopysourcepekko.actor.deployment {\n  /parent/remoteGroup2 {\n    router = round-robin-group\n    routees.paths = [\n      \"pekko://app@10.0.0.1:2552/user/workers/w1\",\n      \"pekko://app@10.0.0.2:2552/user/workers/w1\",\n      \"pekko://app@10.0.0.3:2552/user/workers/w1\"]\n  }\n}\nThis configuration setting will send messages to the defined remote actor paths. It requires that you create the destination actors on the remote nodes with matching paths. That is not done by the router.","title":"Routers with Remote Destinations"},{"location":"/remoting-artery.html#what-is-new-in-artery","text":"Artery is a reimplementation of the old remoting module aimed at improving performance and stability. It is mostly source compatible with the old implementation and it is a drop-in replacement in many cases. Main features of Artery compared to the previous implementation:\nBased on Pekko Streams TCP/TLS or Aeron (UDP) instead of Netty TCP Focused on high-throughput, low-latency communication Isolation of internal control messages from user messages improving stability and reducing false failure detection in case of heavy traffic by using a dedicated subchannel. Mostly allocation-free operation Support for a separate subchannel for large messages to avoid interference with smaller messages Compression of actor paths on the wire to reduce overhead for smaller messages Support for faster serialization/deserialization using ByteBuffers directly Built-in Java Flight Recorder (JFR) to help debugging implementation issues without polluting users logs with implementation specific events Providing protocol stability across major Pekko versions to support rolling updates of large-scale systems\nThe main incompatible change from the previous implementation that the protocol field of the string representation of an ActorRefActorRef is always pekko instead of the previously used pekko.tcp or pekko.ssl.tcp. Configuration properties are also different.","title":"What is new in Artery"},{"location":"/remoting-artery.html#performance-tuning","text":"","title":"Performance tuning"},{"location":"/remoting-artery.html#lanes","text":"Message serialization and deserialization can be a bottleneck for remote communication. Therefore there is support for parallel inbound and outbound lanes to perform serialization and other tasks for different destination actors in parallel. Using multiple lanes is of most value for the inbound messages, since all inbound messages from all remote systems share the same inbound stream. For outbound messages there is already one stream per remote destination system, so multiple outbound lanes only add value when sending to different actors in same destination system.\nThe selection of lane is based on consistent hashing of the recipient ActorRef to preserve message ordering per receiver.\nNote that lowest latency can be achieved with inbound-lanes=1 and outbound-lanes=1 because multiple lanes introduce an asynchronous boundary.\nAlso note that the total amount of parallel tasks are bound by the remote-dispatcher and the thread pool size should not exceed the number of CPU cores minus headroom for actually processing the messages in the application, i.e. in practice the the pool size should be less than half of the number of cores.\nSee inbound-lanes and outbound-lanes in the reference configuration for default values.","title":"Lanes"},{"location":"/remoting-artery.html#dedicated-subchannel-for-large-messages","text":"All the communication between user defined remote actors are isolated from the channel of Pekko internal messages so a large user message cannot block an urgent system message. While this provides good isolation for Pekko services, all user communications by default happen through a shared network connection. When some actors send large messages this can cause other messages to suffer higher latency as they need to wait until the full message has been transported on the shared channel (and hence, shared bottleneck). In these cases it is usually helpful to separate actors that have different QoS requirements: large messages vs. low latency.\nPekko remoting provides a dedicated channel for large messages if configured. Since actor message ordering must not be violated the channel is actually dedicated for actors instead of messages, to ensure all of the messages arrive in send order. It is possible to assign actors on given paths to use this dedicated channel by using path patterns that have to be specified in the actor system’s configuration on both the sending and the receiving side:\npekko.remote.artery.large-message-destinations = [\n   \"/user/largeMessageActor\",\n   \"/user/largeMessagesGroup/*\",\n   \"/user/anotherGroup/*/largeMesssages\",\n   \"/user/thirdGroup/**\",\n   \"/temp/session-ask-actor*\"\n]\n*NOTE: Support for * inside of an actor path (ie. /temp/session-ask-actor*) is only available in 2.6.18+\nThis means that all messages sent to the following actors will pass through the dedicated, large messages channel:\n/user/largeMessageActor /user/largeMessageActorGroup/actor1 /user/largeMessageActorGroup/actor2 /user/anotherGroup/actor1/largeMessages /user/anotherGroup/actor2/largeMessages /user/thirdGroup/actor3/ /user/thirdGroup/actor4/actor5 /temp/session-ask-actor$abc\nMessages destined for actors not matching any of these patterns are sent using the default channel as before.\nTo notice large messages you can enable logging of message types with payload size in bytes larger than the configured log-frame-size-exceeding.\npekko.remote.artery {\n  log-frame-size-exceeding = 10000b\n}\nExample log messages:\n[INFO] Payload size for [java.lang.String] is [39068] bytes. Sent to Actor[pekko://Sys@localhost:53039/user/destination#-1908386800]\n[INFO] New maximum payload size for [java.lang.String] is [44068] bytes. Sent to Actor[pekko://Sys@localhost:53039/user/destination#-1908386800].\nThe large messages channel can still not be used for extremely large messages, a few MB per message at most. An alternative is to use the Reliable delivery that has support for automatically splitting up large messages and assemble them again on the receiving side.","title":"Dedicated subchannel for large messages"},{"location":"/remoting-artery.html#external-shared-aeron-media-driver","text":"The Aeron transport is running in a so called media driver. By default, Pekko starts the media driver embedded in the same JVM process as application. This is convenient and simplifies operational concerns by only having one process to start and monitor.\nThe media driver may use rather much CPU resources. If you run more than one Pekko application JVM on the same machine it can therefore be wise to share the media driver by running it as a separate process.\nThe media driver has also different resource usage characteristics than a normal application and it can therefore be more efficient and stable to run the media driver as a separate process.\nGiven that Aeron jar files are in the classpath the standalone media driver can be started with:\njava io.aeron.driver.MediaDriver\nThe needed classpath:\nAgrona-0.5.4.jar:aeron-driver-1.0.1.jar:aeron-client-1.0.1.jar\nYou find those jar files on Maven Central, or you can create a package with your preferred build tool.\nYou can pass Aeron properties as command line -D system properties:\n-Daeron.dir=/dev/shm/aeron\nYou can also define Aeron properties in a file:\njava io.aeron.driver.MediaDriver config/aeron.properties\nAn example of such a properties file:\naeron.mtu.length=16384\naeron.socket.so_sndbuf=2097152\naeron.socket.so_rcvbuf=2097152\naeron.rcv.buffer.length=16384\naeron.rcv.initial.window.length=2097152\nagrona.disable.bounds.checks=true\n\naeron.threading.mode=SHARED_NETWORK\n\n# low latency settings\n#aeron.threading.mode=DEDICATED\n#aeron.sender.idle.strategy=org.agrona.concurrent.BusySpinIdleStrategy\n#aeron.receiver.idle.strategy=org.agrona.concurrent.BusySpinIdleStrategy\n\n# use same director in pekko.remote.artery.advanced.aeron-dir config\n# of the Pekko application\naeron.dir=/dev/shm/aeron\nRead more about the media driver in the Aeron documentation.\nTo use the external media driver from the Pekko application you need to define the following two configuration properties:\npekko.remote.artery.advanced.aeron {\n  embedded-media-driver = off\n  aeron-dir = /dev/shm/aeron\n}\nThe aeron-dir must match the directory you started the media driver with, i.e. the aeron.dir property.\nSeveral Pekko applications can then be configured to use the same media driver by pointing to the same directory.\nNote that if the media driver process is stopped the Pekko applications that are using it will also be stopped.","title":"External, shared Aeron media driver"},{"location":"/remoting-artery.html#aeron-tuning","text":"See Aeron documentation about Performance Testing.","title":"Aeron Tuning"},{"location":"/remoting-artery.html#fine-tuning-cpu-usage-latency-tradeoff","text":"Artery has been designed for low latency and as a result it can be CPU hungry when the system is mostly idle. This is not always desirable. When using the Aeron transport it is possible to tune the tradeoff between CPU usage and latency with the following configuration:\n# Values can be from 1 to 10, where 10 strongly prefers low latency\n# and 1 strongly prefers less CPU usage\npekko.remote.artery.advanced.aeron.idle-cpu-level = 1\nBy setting this value to a lower number, it tells Pekko to do longer “sleeping” periods on its thread dedicated for spin-waiting and hence reducing CPU load when there is no immediate task to execute at the cost of a longer reaction time to an event when it actually happens. It is worth to be noted though that during a continuously high-throughput period this setting makes not much difference as the thread mostly has tasks to execute. This also means that under high throughput (but below maximum capacity) the system might have less latency than at low message rates.","title":"Fine-tuning CPU usage latency tradeoff"},{"location":"/remoting-artery.html#remote-configuration","text":"There are lots of configuration properties that are related to remoting in Pekko. We refer to the reference configuration for more information.\nNote Setting properties like the listening IP and port number programmatically is best done by using something like the following: copysourceConfigFactory.parseString(\"pekko.remote.artery.canonical.hostname=\\\"1.2.3.4\\\"\")\n    .withFallback(ConfigFactory.load());","title":"Remote Configuration"},{"location":"/remoting-artery.html#pekko-behind-nat-or-in-a-docker-container","text":"In setups involving Network Address Translation (NAT), Load Balancers or Docker containers the hostname and port pair that Pekko binds to will be different than the “logical” host name and port pair that is used to connect to the system from the outside. This requires special configuration that sets both the logical and the bind pairs for remoting.\npekko {\n  remote {\n    artery {\n      canonical.hostname = my.domain.com      # external (logical) hostname\n      canonical.port = 8000                   # external (logical) port\n\n      bind.hostname = local.address # internal (bind) hostname\n      bind.port = 25520              # internal (bind) port\n    }\n }\n}\nYou can look at the Cluster with docker-compose example project Cluster with docker-compose example project to see what this looks like in practice.","title":"Pekko behind NAT or in a Docker container"},{"location":"/remoting-artery.html#running-in-docker-kubernetes","text":"When using aeron-udp in a containerized environment special care must be taken that the media driver runs on a ram disk. This by default is located in /dev/shm which on most physical Linux machines will be mounted as half the size of the system memory.\nDocker and Kubernetes mount a 64Mb ram disk. This is unlikely to be large enough. For docker this can be overridden with --shm-size=\"512mb\".\nIn Kubernetes there is no direct support (yet) for setting shm size. Instead mount an EmptyDir with type Memory to /dev/shm for example in a deployment.yml:\nspec:\n  containers:\n  - name: artery-udp-cluster\n    // rest of container spec...\n    volumeMounts:\n    - mountPath: /dev/shm\n      name: media-driver\n  volumes:\n  - name: media-driver\n    emptyDir:\n      medium: Memory\n      name: media-driver\nThere is currently no way to limit the size of a memory empty dir but there is a pull request for adding it.\nAny space used in the mount will count towards your container’s memory usage.","title":"Running in Docker/Kubernetes"},{"location":"/remoting-artery.html#flight-recorder","text":"When running on JDK 11 Artery specific flight recording is available through the Java Flight Recorder (JFR). The flight recorder is automatically enabled by detecting JDK 11 but can be disabled if needed by setting pekko.java-flight-recorder.enabled = false.\nLow overhead Artery specific events are emitted by default when JFR is enabled, higher overhead events needs a custom settings template and are not enabled automatically with the profiling JFR template. To enable those create a copy of the profiling template and enable all Pekko sub category events, for example through the JMC GUI.","title":"Flight Recorder"},{"location":"/remoting.html","text":"","title":"Classic Remoting (Deprecated)"},{"location":"/remoting.html#classic-remoting-deprecated-","text":"Warning Classic remoting has been deprecated. Please use Artery instead.\nNote Remoting is the mechanism by which Actors on different nodes talk to each other internally. When building a Pekko application, you would usually not use the Remoting concepts directly, but instead use the more high-level Pekko Cluster utilities or technology-agnostic protocols such as HTTP, gRPC etc.","title":"Classic Remoting (Deprecated)"},{"location":"/remoting.html#module-info","text":"To use Pekko Remoting, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-remote\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-remote_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-remote_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Remoting Artifact org.apache.pekko pekko-remote 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.remote License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko\nClassic remoting depends on Netty. This needs to be explicitly added as a dependency so that users not using classic remoting do not have to have Netty on the classpath:\nsbt libraryDependencies += \"io.netty\" % \"netty\" % \"3.10.6.Final\" Maven <dependencies>\n  <dependency>\n    <groupId>io.netty</groupId>\n    <artifactId>netty</artifactId>\n    <version>3.10.6.Final</version>\n  </dependency>\n</dependencies> Gradle dependencies {\n  implementation \"io.netty:netty:3.10.6.Final\"\n}","title":"Module info"},{"location":"/remoting.html#configuration","text":"To enable classic remoting in your Pekko project you should, at a minimum, add the following changes to your application.conf file:\npekko {\n  actor {\n    # provider=remote is possible, but prefer cluster\n    provider = cluster\n  }\n  remote.artery.enabled = false\n  remote.classic {\n    enabled-transports = [\"pekko.remote.classic.netty.tcp\"]\n    netty.tcp {\n      hostname = \"127.0.0.1\"\n      port = 2552\n    }\n }\n}\nAs you can see in the example above there are four things you need to add to get started:\nChange provider from local. We recommend using Pekko Cluster over using remoting directly. Disable artery remoting. Artery is the default remoting implementation since 2.6.0 Add host name - the machine you want to run the actor system on; this host name is exactly what is passed to remote systems in order to identify this system and consequently used for connecting back to this system if need be, hence set it to a reachable IP address or resolvable name in case you want to communicate across the network. Add port number - the port the actor system should listen on, set to 0 to have it chosen automatically\nNote The port number needs to be unique for each actor system on the same machine even if the actor systems have different names. This is because each actor system has its own networking subsystem listening for connections and handling messages as not to interfere with other actor systems.\nThe example above only illustrates the bare minimum of properties you have to add to enable remoting. All settings are described in Remote Configuration.","title":"Configuration"},{"location":"/remoting.html#introduction","text":"We recommend Pekko Cluster over using remoting directly. As remoting is the underlying module that allows for Cluster, it is still useful to understand details about it though.\nFor an introduction of remoting capabilities of Pekko please see Location Transparency.\nNote As explained in that chapter Pekko remoting is designed for communication in a peer-to-peer fashion and it is not a good fit for client-server setups. In particular Pekko Remoting does not work transparently with Network Address Translation, Load Balancers, or in Docker containers. For symmetric communication in these situations network and/or Pekko configuration will have to be changed as described in Pekko behind NAT or in a Docker container.\nYou need to enable serialization for your actor messages. Serialization with Jackson is a good choice in many cases and our recommendation if you don’t have other preference.","title":"Introduction"},{"location":"/remoting.html#types-of-remote-interaction","text":"Pekko has two ways of using remoting:\nLookup : used to look up an actor on a remote node with actorSelection(path) Creation : used to create an actor on a remote node with actorOf(Props(...), actorName)\nIn the next sections the two alternatives are described in detail.","title":"Types of Remote Interaction"},{"location":"/remoting.html#looking-up-remote-actors","text":"actorSelection(path) will obtain an ActorSelection to an Actor on a remote node, e.g.:\nScala val selection =\n  context.actorSelection(\"pekko.tcp://actorSystemName@10.0.0.1:2552/user/actorName\")\n Java ActorSelection selection =\n  context.actorSelection(\"pekko.tcp://app@10.0.0.1:2552/user/serviceA/worker\");\nAs you can see from the example above the following pattern is used to find an actor on a remote node:\npekko.<protocol>://<actor system name>@<hostname>:<port>/<actor path>\nOnce you obtained a selection to the actor you can interact with it in the same way you would with a local actor, e.g.:\nScala selection ! \"Pretty awesome feature\"\n Java selection.tell(\"Pretty awesome feature\", getSelf());\nTo acquire an ActorRef for an ActorSelection you need to send a message to the selection and use the sender reference of the reply from the actor. There is a built-in Identify message that all Actors will understand and automatically reply to with a ActorIdentity message containing the ActorRef. This can also be done with the resolveOne method of the ActorSelection, which returns a FutureCompletionStage of the matching ActorRef.\nNote For more details on how actor addresses and paths are formed and used, please refer to Actor References, Paths and Addresses.\nNote Message sends to actors that are actually in the sending actor system do not get delivered via the remote actor ref provider. They’re delivered directly, by the local actor ref provider. Aside from providing better performance, this also means that if the hostname you configure remoting to listen as cannot actually be resolved from within the very same actor system, such messages will (perhaps counterintuitively) be delivered just fine.","title":"Looking up Remote Actors"},{"location":"/remoting.html#creating-actors-remotely","text":"If you want to use the creation functionality in Pekko remoting you have to further amend the application.conf file in the following way (only showing deployment section):\npekko {\n  actor {\n    deployment {\n      /sampleActor {\n        remote = \"pekko.tcp://sampleActorSystem@127.0.0.1:2553\"\n      }\n    }\n  }\n}\nThe configuration above instructs Pekko to react when an actor with path /sampleActor is created, i.e. using system.actorOf(Props(...), \"sampleActor\")system.actorOf(new Props(...), \"sampleActor\"). This specific actor will not be directly instantiated, but instead the remote daemon of the remote system will be asked to create the actor, which in this sample corresponds to sampleActorSystem@127.0.0.1:2553.\nOnce you have configured the properties above you would do the following in code:\nScala copysource val actor = system.actorOf(Props[SampleActor](), \"sampleActor\")\nactor ! \"Pretty slick\" Java copysource ActorRef actor = system.actorOf(Props.create(SampleActor.class), \"sampleActor\");\nactor.tell(\"Pretty slick\", ActorRef.noSender());\nThe actor class SampleActor has to be available to the runtimes using it, i.e. the classloader of the actor systems has to have a JAR containing the class.\nWhen using remote deployment of actors you must ensure that all parameters of the Props can be serialized.\nNote In order to ensure serializability of Props when passing constructor arguments to the actor being created, do not make the factory ana non-static inner class: this will inherently capture a reference to its enclosing object, which in most cases is not serializable. It is best to create a factory method in the companion object of the actor’s classmake a static inner class which implements Creator<T extends Actor>. Serializability of all Props can be tested by setting the configuration item pekko.actor.serialize-creators=on. Only Props whose deploy has LocalScope are exempt from this check.\nNote You can use asterisks as wildcard matches for the actor path sections, so you could specify: /*/sampleActor and that would match all sampleActor on that level in the hierarchy. You can also use wildcard in the last position to match all actors at a certain level: /someParent/*. Non-wildcard matches always have higher priority to match than wildcards, so: /foo/bar is considered more specific than /foo/* and only the highest priority match is used. Please note that it cannot be used to partially match section, like this: /foo*/bar, /f*o/bar etc.","title":"Creating Actors Remotely"},{"location":"/remoting.html#programmatic-remote-deployment","text":"To allow dynamically deployed systems, it is also possible to include deployment configuration in the Props which are used to create an actor: this information is the equivalent of a deployment section from the configuration file, and if both are given, the external configuration takes precedence.\nWith these imports:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.{ Address, AddressFromURIString, Deploy, Props }\nimport pekko.remote.RemoteScope Java copysourceimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.Address;\nimport org.apache.pekko.actor.AddressFromURIString;\nimport org.apache.pekko.actor.Deploy;\nimport org.apache.pekko.actor.Props;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.remote.RemoteScope;\nand a remote address like this:\nScala copysourceval one = AddressFromURIString(\"pekko://sys@host:1234\")\nval two = Address(\"pekko\", \"sys\", \"host\", 1234) // this gives the same Java copysourceAddress addr = new Address(\"akka\", \"sys\", \"host\", 1234);\naddr = AddressFromURIString.parse(\"akka://sys@host:1234\"); // the same\nyou can advise the system to create a child on that remote node like so:\nScala copysourceval ref = system.actorOf(Props[SampleActor]().withDeploy(Deploy(scope = RemoteScope(address)))) Java copysourceProps props = Props.create(SampleActor.class).withDeploy(new Deploy(new RemoteScope(addr)));\nActorRef ref = system.actorOf(props);","title":"Programmatic Remote Deployment"},{"location":"/remoting.html#remote-deployment-allow-list","text":"As remote deployment can potentially be abused by both users and even attackers an allow list feature is available to guard the ActorSystem from deploying unexpected actors. Please note that remote deployment is not remote code loading, the Actors class to be deployed onto a remote system needs to be present on that remote system. This still however may pose a security risk, and one may want to restrict remote deployment to only a specific set of known actors by enabling the allow list feature.\nTo enable remote deployment allow list set the pekko.remote.deployment.enable-allow-list value to on. The list of allowed classes has to be configured on the “remote” system, in other words on the system onto which others will be attempting to remote deploy Actors. That system, locally, knows best which Actors it should or should not allow others to remote deploy onto it. The full settings section may for example look like this:\ncopysourcepekko.remote.deployment {\n  enable-allow-list = on\n  \n  allowed-actor-classes = [\n    \"NOT_ON_CLASSPATH\", # verify we don't throw if a class not on classpath is listed here\n    \"org.apache.pekko.remote.classic.RemoteDeploymentAllowListSpec.EchoAllowed\"\n  ]\n}\nActor classes not included in the allow list will not be allowed to be remote deployed onto this system.","title":"Remote deployment allow list"},{"location":"/remoting.html#lifecycle-and-failure-recovery-model","text":"Each link with a remote system can be in one of the four states as illustrated above. Before any communication happens with a remote system at a given Address the state of the association is Idle. The first time a message is attempted to be sent to the remote system or an inbound connection is accepted the state of the link transitions to Active denoting that the two systems has messages to send or receive and no failures were encountered so far. When a communication failure happens and the connection is lost between the two systems the link becomes Gated.\nIn this state the system will not attempt to connect to the remote host and all outbound messages will be dropped. The time while the link is in the Gated state is controlled by the setting pekko.remote.retry-gate-closed-for: after this time elapses the link state transitions to Idle again. Gate is one-sided in the sense that whenever a successful inbound connection is accepted from a remote system during Gate it automatically transitions to Active and communication resumes immediately.\nIn the face of communication failures that are unrecoverable because the state of the participating systems are inconsistent, the remote system becomes Quarantined. Unlike Gate, quarantining is permanent and lasts until one of the systems is restarted. After a restart communication can be resumed again and the link can become Active again.","title":"Lifecycle and Failure Recovery Model"},{"location":"/remoting.html#watching-remote-actors","text":"Watching a remote actor is not different than watching a local actor, as described in Lifecycle Monitoring aka DeathWatch.","title":"Watching Remote Actors"},{"location":"/remoting.html#failure-detector","text":"Please see:\nPhi Accrual Failure Detector implementation for details Using the Failure Detector below for usage","title":"Failure Detector"},{"location":"/remoting.html#using-the-failure-detector","text":"Remoting uses the org.apache.pekko.remote.PhiAccrualFailureDetector failure detector by default, or you can provide your by implementing the org.apache.pekko.remote.FailureDetector and configuring it:\npekko.remote.watch-failure-detector.implementation-class = \"com.example.CustomFailureDetector\"\nIn the Remote Configuration you may want to adjust these depending on you environment:\nWhen a phi value is considered to be a failure pekko.remote.watch-failure-detector.threshold Margin of error for sudden abnormalities pekko.remote.watch-failure-detector.acceptable-heartbeat-pause","title":"Using the Failure Detector"},{"location":"/remoting.html#serialization","text":"You need to enable serialization for your actor messages. Serialization with Jackson is a good choice in many cases and our recommendation if you don’t have other preference.","title":"Serialization"},{"location":"/remoting.html#routers-with-remote-destinations","text":"It is absolutely feasible to combine remoting with Routing.\nA pool of remote deployed routees can be configured as:\ncopysourcepekko.actor.deployment {\n  /parent/remotePool {\n    router = round-robin-pool\n    nr-of-instances = 10\n    target.nodes = [\"pekko://app@10.0.0.2:2552\", \"pekko://app@10.0.0.3:2552\"]\n  }\n}\nThis configuration setting will clone the actor defined in the Props of the remotePool 10 times and deploy it evenly distributed across the two given target nodes.\nWhen using a pool of remote deployed routees you must ensure that all parameters of the Props can be serialized.\nA group of remote actors can be configured as:\ncopysourcepekko.actor.deployment {\n  /parent/remoteGroup {\n    router = round-robin-group\n    routees.paths = [\n      \"pekko://app@10.0.0.1:2552/user/workers/w1\",\n      \"pekko://app@10.0.0.2:2552/user/workers/w1\",\n      \"pekko://app@10.0.0.3:2552/user/workers/w1\"]\n  }\n}\nThis configuration setting will send messages to the defined remote actor paths. It requires that you create the destination actors on the remote nodes with matching paths. That is not done by the router.","title":"Routers with Remote Destinations"},{"location":"/remoting.html#remote-events","text":"It is possible to listen to events that occur in Pekko Remote, and to subscribe/unsubscribe to these events you register as listener to the below described types in on the ActorSystem.eventStream.\nNote To subscribe to any remote event, subscribe to RemotingLifecycleEvent. To subscribe to events related only to the lifecycle of associations, subscribe to org.apache.pekko.remote.AssociationEvent.\nNote The use of term “Association” instead of “Connection” reflects that the remoting subsystem may use connectionless transports, but an association similar to transport layer connections is maintained between endpoints by the Pekko protocol.\nBy default an event listener is registered which logs all of the events described below. This default was chosen to help setting up a system, but it is quite common to switch this logging off once that phase of the project is finished.\nNote In order to disable the logging, set pekko.remote.classic.log-remote-lifecycle-events = off in your application.conf.\nTo be notified when an association is over (“disconnected”) listen to DisassociatedEvent which holds the direction of the association (inbound or outbound) and the addresses of the involved parties.\nTo be notified when an association is successfully established (“connected”) listen to AssociatedEvent which holds the direction of the association (inbound or outbound) and the addresses of the involved parties.\nTo intercept errors directly related to associations, listen to AssociationErrorEvent which holds the direction of the association (inbound or outbound), the addresses of the involved parties and the Throwable cause.\nTo be notified when the remoting subsystem is ready to accept associations, listen to RemotingListenEvent which contains the addresses the remoting listens on.\nTo be notified when the current system is quarantined by the remote system, listen to ThisActorSystemQuarantinedEvent, which includes the addresses of local and remote ActorSystems.\nTo be notified when the remoting subsystem has been shut down, listen to RemotingShutdownEvent.\nTo intercept generic remoting related errors, listen to RemotingErrorEvent which holds the Throwable cause.","title":"Remote Events"},{"location":"/remoting.html#remote-security","text":"An ActorSystem should not be exposed via Pekko Remote over plain TCP to an untrusted network (e.g. Internet). It should be protected by network security, such as a firewall. If that is not considered as enough protection TLS with mutual authentication should be enabled.\nBest practice is that Pekko remoting nodes should only be accessible from the adjacent network. Note that if TLS is enabled with mutual authentication there is still a risk that an attacker can gain access to a valid certificate by compromising any node with certificates issued by the same internal PKI tree.\nBy default, Java serialization is disabled in Pekko. That is also security best-practice because of its multiple known attack surfaces.","title":"Remote Security"},{"location":"/remoting.html#configuring-ssl-tls-for-pekko-remoting","text":"SSL can be used as the remote transport by adding pekko.remote.classic.netty.ssl to the enabled-transport configuration section. An example of setting up the default Netty based SSL driver as default:\npekko {\n  remote.classic {\n    enabled-transports = [pekko.remote.classic.netty.ssl]\n  }\n}\nNext the actual SSL/TLS parameters have to be configured:\npekko {\n  remote.classic {\n    netty.ssl {\n      hostname = \"127.0.0.1\"\n      port = \"3553\"\n\n      security {\n        key-store = \"/example/path/to/mykeystore.jks\"\n        trust-store = \"/example/path/to/mytruststore.jks\"\n\n        key-store-password = ${SSL_KEY_STORE_PASSWORD}\n        key-password = ${SSL_KEY_PASSWORD}\n        trust-store-password = ${SSL_TRUST_STORE_PASSWORD}\n\n        protocol = \"TLSv1.2\"\n\n        enabled-algorithms = [TLS_DHE_RSA_WITH_AES_128_GCM_SHA256]\n      }\n    }\n  }\n}\nAlways use substitution from environment variables for passwords. Don’t define real passwords in config files.\nAccording to RFC 7525 the recommended algorithms to use with TLS 1.2 (as of writing this document) are:\nTLS_DHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_DHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\nYou should always check the latest information about security and algorithm recommendations though before you configure your system.\nSince a Pekko remoting is inherently peer-to-peer both the key-store as well as trust-store need to be configured on each remoting node participating in the cluster.\nThe official Java Secure Socket Extension documentation as well as the Oracle documentation on creating KeyStore and TrustStores are both great resources to research when setting up security on the JVM. Please consult those resources when troubleshooting and configuring SSL.\nSince Pekko 2.5.0 mutual authentication between TLS peers is enabled by default.\nMutual authentication means that the the passive side (the TLS server side) of a connection will also request and verify a certificate from the connecting peer. Without this mode only the client side is requesting and verifying certificates. While Pekko is a peer-to-peer technology, each connection between nodes starts out from one side (the “client”) towards the other (the “server”).\nNote that if TLS is enabled with mutual authentication there is still a risk that an attacker can gain access to a valid certificate by compromising any node with certificates issued by the same internal PKI tree.\nSee also a description of the settings in the Remote Configuration section.\nNote When using SHA1PRNG on Linux it’s recommended specify -Djava.security.egd=file:/dev/urandom as argument to the JVM to prevent blocking. It is NOT as secure because it reuses the seed.","title":"Configuring SSL/TLS for Pekko Remoting"},{"location":"/remoting.html#untrusted-mode","text":"As soon as an actor system can connect to another remotely, it may in principle send any possible message to any actor contained within that remote system. One example may be sending a PoisonPill to the system guardian, shutting that system down. This is not always desired, and it can be disabled with the following setting:\npekko.remote.classic.untrusted-mode = on\nThis disallows sending of system messages (actor life-cycle commands, DeathWatch, etc.) and any message extending PossiblyHarmful to the system on which this flag is set. Should a client send them nonetheless they are dropped and logged (at DEBUG level in order to reduce the possibilities for a denial of service attack). PossiblyHarmful covers the predefined messages like PoisonPill and Kill, but it can also be added as a marker trait to user-defined messages.\nWarning Untrusted mode does not give full protection against attacks by itself. It makes it slightly harder to perform malicious or unintended actions but it should be noted that Java serialization should still not be enabled. Additional protection can be achieved when running in an untrusted network by network security (e.g. firewalls) and/or enabling TLS with mutual authentication.\nMessages sent with actor selection are by default discarded in untrusted mode, but permission to receive actor selection messages can be granted to specific actors defined in configuration:\npekko.remote.classic.trusted-selection-paths = [\"/user/receptionist\", \"/user/namingService\"]\nThe actual message must still not be of type PossiblyHarmful.\nIn summary, the following operations are ignored by a system configured in untrusted mode when incoming via the remoting layer:\nremote deployment (which also means no remote supervision) remote DeathWatch system.stop(), PoisonPill, Kill sending any message which extends from the PossiblyHarmful marker interface, which includes Terminated messages sent with actor selection, unless destination defined in trusted-selection-paths.\nNote Enabling the untrusted mode does not remove the capability of the client to freely choose the target of its message sends, which means that messages not prohibited by the above rules can be sent to any actor in the remote system. It is good practice for a client-facing system to only contain a well-defined set of entry point actors, which then forward requests (possibly after performing validation) to another actor system containing the actual worker actors. If messaging between these two server-side systems is done using local ActorRef (they can be exchanged safely between actor systems within the same JVM), you can restrict the messages on this interface by marking them PossiblyHarmful so that a client cannot forge them.","title":"Untrusted Mode"},{"location":"/remoting.html#remote-configuration","text":"There are lots of configuration properties that are related to remoting in Pekko. We refer to the reference configuration for more information.\nNote Setting properties like the listening IP and port number programmatically is best done by using something like the following: copysourceConfigFactory.parseString(\"pekko.remote.classic.netty.tcp.hostname=\\\"1.2.3.4\\\"\")\n    .withFallback(ConfigFactory.load());","title":"Remote Configuration"},{"location":"/remoting.html#pekko-behind-nat-or-in-a-docker-container","text":"In setups involving Network Address Translation (NAT), Load Balancers or Docker containers the hostname and port pair that Pekko binds to will be different than the “logical” host name and port pair that is used to connect to the system from the outside. This requires special configuration that sets both the logical and the bind pairs for remoting.\npekko.remote.classic.netty.tcp {\n      hostname = my.domain.com      # external (logical) hostname\n      port = 8000                   # external (logical) port\n\n      bind-hostname = local.address # internal (bind) hostname\n      bind-port = 2552              # internal (bind) port\n}\nKeep in mind that local.address will most likely be in one of private network ranges:\n10.0.0.0 - 10.255.255.255 (network class A) 172.16.0.0 - 172.31.255.255 (network class B) 192.168.0.0 - 192.168.255.255 (network class C)\nFor further details see RFC 1597 and RFC 1918.","title":"Pekko behind NAT or in a Docker container"},{"location":"/split-brain-resolver.html","text":"","title":"Split Brain Resolver"},{"location":"/split-brain-resolver.html#split-brain-resolver","text":"When operating a Pekko cluster you must consider how to handle network partitions (a.k.a. split brain scenarios) and machine crashes (including JVM and hardware failures). This is crucial for correct behavior if you use Cluster Singleton or Cluster Sharding, especially together with Pekko Persistence.\nThe Split Brain Resolver video is a good starting point for learning why it is important to use a correct downing provider and how the Split Brain Resolver works.","title":"Split Brain Resolver"},{"location":"/split-brain-resolver.html#module-info","text":"To use Pekko Split Brain Resolver is part of pekko-cluster and you probably already have that dependency included. Otherwise, add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster (classic) Artifact org.apache.pekko pekko-cluster 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/split-brain-resolver.html#enable-the-split-brain-resolver","text":"You need to enable the Split Brain Resolver by configuring it as downing provider in the configuration of the ActorSystem (application.conf):\npekko.cluster.downing-provider-class = \"org.apache.pekko.cluster.sbr.SplitBrainResolverProvider\"\nYou should also consider the different available downing strategies.","title":"Enable the Split Brain Resolver"},{"location":"/split-brain-resolver.html#the-problem","text":"A fundamental problem in distributed systems is that network partitions (split brain scenarios) and machine crashes are indistinguishable for the observer, i.e. a node can observe that there is a problem with another node, but it cannot tell if it has crashed and will never be available again or if there is a network issue that might or might not heal again after a while. Temporary and permanent failures are indistinguishable because decisions must be made in finite time, and there always exists a temporary failure that lasts longer than the time limit for the decision.\nA third type of problem is if a process is unresponsive, e.g. because of overload, CPU starvation or long garbage collection pauses. This is also indistinguishable from network partitions and crashes. The only signal we have for decision is “no reply in given time for heartbeats” and this means that phenomena causing delays or lost heartbeats are indistinguishable from each other and must be handled in the same way.\nWhen there is a crash, we would like to remove the affected node immediately from the cluster membership. When there is a network partition or unresponsive process we would like to wait for a while in the hope that it is a transient problem that will heal again, but at some point, we must give up and continue with the nodes on one side of the partition and shut down nodes on the other side. Also, certain features are not fully available during partitions so it might not matter that the partition is transient or not if it just takes too long. Those two goals are in conflict with each other and there is a trade-off between how quickly we can remove a crashed node and premature action on transient network partitions.\nThis is a difficult problem to solve given that the nodes on the different sides of the network partition cannot communicate with each other. We must ensure that both sides can make this decision by themselves and that they take the same decision about which part will keep running and which part will shut itself down.\nAnother type of problem that makes it difficult to see the “right” picture is when some nodes are not fully connected and cannot communicate directly to each other but information can be disseminated between them via other nodes.\nThe Pekko cluster has a failure detector that will notice network partitions and machine crashes (but it cannot distinguish the two). It uses periodic heartbeat messages to check if other nodes are available and healthy. These observations by the failure detector are referred to as a node being unreachable and it may become reachable again if the failure detector observes that it can communicate with it again.\nThe failure detector in itself is not enough for making the right decision in all situations. The naive approach is to remove an unreachable node from the cluster membership after a timeout. This works great for crashes and short transient network partitions, but not for long network partitions. Both sides of the network partition will see the other side as unreachable and after a while remove it from its cluster membership. Since this happens on both sides the result is that two separate disconnected clusters have been created. This approach is provided by the opt-in (off by default) auto-down feature in the OSS version of Pekko Cluster.\nIf you use the timeout based auto-down feature in combination with Cluster Singleton or Cluster Sharding that would mean that two singleton instances or two sharded entities with the same identifier would be running. One would be running: one in each cluster. For example when used together with Pekko Persistence that could result in that two instances of a persistent actor with the same persistenceId are running and writing concurrently to the same stream of persistent events, which will have fatal consequences when replaying these events.\nThe default setting in Pekko Cluster is to not remove unreachable nodes automatically and the recommendation is that the decision of what to do should be taken by a human operator or an external monitoring system. This is a valid solution, but not very convenient if you do not have this staff or external system for other reasons.\nIf the unreachable nodes are not downed at all they will still be part of the cluster membership. Meaning that Cluster Singleton and Cluster Sharding will not failover to another node. While there are unreachable nodes new nodes that are joining the cluster will not be promoted to full worthy members (with status Up). Similarly, leaving members will not be removed until all unreachable nodes have been resolved. In other words, keeping unreachable members for an unbounded time is undesirable.\nWith that introduction of the problem domain, it is time to look at the provided strategies for handling network partition, unresponsive nodes and crashed nodes.","title":"The Problem"},{"location":"/split-brain-resolver.html#strategies","text":"By default the Keep Majority strategy will be used because it works well for most systems. However, it’s worth considering the other available strategies and pick a strategy that fits the characteristics of your system. For example, in a Kubernetes environment the Lease strategy can be a good choice.\nEvery strategy has a failure scenario where it makes a “wrong” decision. This section describes the different strategies and guidelines of when to use what.\nWhen there is uncertainty it selects to down more nodes than necessary, or even downing of all nodes. Therefore Split Brain Resolver should always be combined with a mechanism to automatically start up nodes that have been shutdown, and join them to the existing cluster or form a new cluster again.\nYou enable a strategy with the configuration property pekko.cluster.split-brain-resolver.active-strategy.","title":"Strategies"},{"location":"/split-brain-resolver.html#stable-after","text":"All strategies are inactive until the cluster membership and the information about unreachable nodes have been stable for a certain time period. Continuously adding more nodes while there is a network partition does not influence this timeout, since the status of those nodes will not be changed to Up while there are unreachable nodes. Joining nodes are not counted in the logic of the strategies.\ncopysource # To enable the split brain resolver you first need to enable the provider in your application.conf:\n# pekko.cluster.downing-provider-class = \"org.apache.pekko.cluster.sbr.SplitBrainResolverProvider\"\n\npekko.cluster.split-brain-resolver {\n  # Select one of the available strategies (see descriptions below):\n  # static-quorum, keep-majority, keep-oldest, down-all, lease-majority\n  active-strategy = keep-majority\n\n  # Time margin after which shards or singletons that belonged to a downed/removed\n  # partition are created in surviving partition. The purpose of this margin is that\n  # in case of a network partition the persistent actors in the non-surviving partitions\n  # must be stopped before corresponding persistent actors are started somewhere else.\n  # This is useful if you implement downing strategies that handle network partitions,\n  # e.g. by keeping the larger side of the partition and shutting down the smaller side.\n  # Decision is taken by the strategy when there has been no membership or\n  # reachability changes for this duration, i.e. the cluster state is stable.\n  stable-after = 20s\n\n  # When reachability observations by the failure detector are changed the SBR decisions\n  # are deferred until there are no changes within the 'stable-after' duration.\n  # If this continues for too long it might be an indication of an unstable system/network\n  # and it could result in delayed or conflicting decisions on separate sides of a network\n  # partition.\n  # As a precaution for that scenario all nodes are downed if no decision is made within\n  # `stable-after + down-all-when-unstable` from the first unreachability event.\n  # The measurement is reset if all unreachable have been healed, downed or removed, or\n  # if there are no changes within `stable-after * 2`.\n  # The value can be on, off, or a duration.\n  # By default it is 'on' and then it is derived to be 3/4 of stable-after, but not less than\n  # 4 seconds.\n  down-all-when-unstable = on\n\n}\nSet pekko.cluster.split-brain-resolver.stable-after to a shorter duration to have quicker removal of crashed nodes, at the price of risking too early action on transient network partitions that otherwise would have healed. Do not set this to a shorter duration than the membership dissemination time in the cluster, which depends on the cluster size. Recommended minimum duration for different cluster sizes:\ncluster size stable-after 5 7 s 10 10 s 20 13 s 50 17 s 100 20 s 1000 30 s\nThe different strategies may have additional settings that are described below.\nNote It is important that you use the same configuration on all nodes.\nThe side of the split that decides to shut itself down will use the cluster down command to initiate the removal of a cluster member. When that has been spread among the reachable nodes it will be removed from the cluster membership.\nIt’s good to terminate the ActorSystem and exit the JVM when the node is removed from the cluster.\nThat is handled by Coordinated Shutdown but to exit the JVM it’s recommended that you enable:\npekko.coordinated-shutdown.exit-jvm = on\nNote Some legacy containers may block calls to System.exit(..) and you may have to find an alternate way to shut the app down. For example, when running Pekko on top of a Spring / Tomcat setup, you could replace the call to System.exit(..) with a call to Spring’s ApplicationContext .close() method (or with a HTTP call to Tomcat Manager’s API to un-deploy the app).","title":"Stable after"},{"location":"/split-brain-resolver.html#keep-majority","text":"The strategy named keep-majority will down the unreachable nodes if the current node is in the majority part based on the last known membership information. Otherwise down the reachable nodes, i.e. the own part. If the parts are of equal size the part containing the node with the lowest address is kept.\nThis strategy is a good choice when the number of nodes in the cluster change dynamically and you can therefore not use static-quorum.\nThis strategy also handles the edge case that may occur when there are membership changes at the same time as the network partition occurs. For example, the status of two members are changed to Up on one side but that information is not disseminated to the other side before the connection is broken. Then one side sees two more nodes and both sides might consider themselves having a majority. It will detect this situation and make the safe decision to down all nodes on the side that could be in minority if the joining nodes were changed to Up on the other side. Note that this has the drawback that if the joining nodes were not changed to Up and becoming a majority on the other side then each part will shut down itself, terminating the whole cluster.\nNote that if there are more than two partitions and none is in majority each part will shut down itself, terminating the whole cluster.\nIf more than half of the nodes crash at the same time the other running nodes will down themselves because they think that they are not in majority, and thereby the whole cluster is terminated.\nThe decision can be based on nodes with a configured role instead of all nodes in the cluster. This can be useful when some types of nodes are more valuable than others. You might for example have some nodes responsible for persistent data and some nodes with stateless worker services. Then it probably more important to keep as many persistent data nodes as possible even though it means shutting down more worker nodes.\nConfiguration:\npekko.cluster.split-brain-resolver.active-strategy=keep-majority\ncopysourcepekko.cluster.split-brain-resolver.keep-majority {\n  # if the 'role' is defined the decision is based only on members with that 'role'\n  role = \"\"\n}","title":"Keep Majority"},{"location":"/split-brain-resolver.html#static-quorum","text":"The strategy named static-quorum will down the unreachable nodes if the number of remaining nodes are greater than or equal to a configured quorum-size. Otherwise, it will down the reachable nodes, i.e. it will shut down that side of the partition. In other words, the quorum-size defines the minimum number of nodes that the cluster must have to be operational.\nThis strategy is a good choice when you have a fixed number of nodes in the cluster, or when you can define a fixed number of nodes with a certain role.\nFor example, in a 9 node cluster you will configure the quorum-size to 5. If there is a network split of 4 and 5 nodes the side with 5 nodes will survive and the other 4 nodes will be downed. After that, in the 5 node cluster, no more failures can be handled, because the remaining cluster size would be less than 5. In the case of another failure in that 5 node cluster all nodes will be downed.\nTherefore it is important that you join new nodes when old nodes have been removed.\nAnother consequence of this is that if there are unreachable nodes when starting up the cluster, before reaching this limit, the cluster may shut itself down immediately. This is not an issue if you start all nodes at approximately the same time or use the pekko.cluster.min-nr-of-members to define required number of members before the leader changes member status of ‘Joining’ members to ‘Up’ You can tune the timeout after which downing decisions are made using the stable-after setting.\nYou should not add more members to the cluster than quorum-size * 2 - 1. A warning is logged if this recommendation is violated. If the exceeded cluster size remains when a SBR decision is needed it will down all nodes because otherwise there is a risk that both sides may down each other and thereby form two separate clusters.\nFor rolling updates it’s best to leave the cluster gracefully via Coordinated Shutdown (SIGTERM). For successful leaving SBR will not be used (no downing) but if there is an unreachability problem at the same time as the rolling update is in progress there could be an SBR decision. To avoid that the total number of members limit is not exceeded during the rolling update it’s recommended to leave and fully remove one node before adding a new one, when using static-quorum.\nIf the cluster is split into 3 (or more) parts each part that is smaller than then configured quorum-size will down itself and possibly shutdown the whole cluster.\nIf more nodes than the configured quorum-size crash at the same time the other running nodes will down themselves because they think that they are not in the majority, and thereby the whole cluster is terminated.\nThe decision can be based on nodes with a configured role instead of all nodes in the cluster. This can be useful when some types of nodes are more valuable than others. You might, for example, have some nodes responsible for persistent data and some nodes with stateless worker services. Then it probably more important to keep as many persistent data nodes as possible even though it means shutting down more worker nodes.\nThere is another use of the role as well. By defining a role for a few (e.g. 7) stable nodes in the cluster and using that in the configuration of static-quorum you will be able to dynamically add and remove other nodes without this role and still have good decisions of what nodes to keep running and what nodes to shut down in the case of network partitions. The advantage of this approach compared to keep-majority (described below) is that you do not risk splitting the cluster into two separate clusters, i.e. a split brain*. You must still obey the rule of not starting too many nodes with this role as described above. It also suffers the risk of shutting down all nodes if there is a failure when there are not enough nodes with this role remaining in the cluster, as described above.\nConfiguration:\npekko.cluster.split-brain-resolver.active-strategy=static-quorum\ncopysourcepekko.cluster.split-brain-resolver.static-quorum {\n  # minimum number of nodes that the cluster must have\n  quorum-size = undefined\n\n  # if the 'role' is defined the decision is based only on members with that 'role'\n  role = \"\"\n}","title":"Static Quorum"},{"location":"/split-brain-resolver.html#keep-oldest","text":"The strategy named keep-oldest will down the part that does not contain the oldest member. The oldest member is interesting because the active Cluster Singleton instance is running on the oldest member.\nThere is one exception to this rule if down-if-alone is configured to on. Then, if the oldest node has partitioned from all other nodes the oldest will down itself and keep all other nodes running. The strategy will not down the single oldest node when it is the only remaining node in the cluster.\nNote that if the oldest node crashes the others will remove it from the cluster when down-if-alone is on, otherwise they will down themselves if the oldest node crashes, i.e. shut down the whole cluster together with the oldest node.\nThis strategy is good to use if you use Cluster Singleton and do not want to shut down the node where the singleton instance runs. If the oldest node crashes a new singleton instance will be started on the next oldest node. The drawback is that the strategy may keep only a few nodes in a large cluster. For example, if one part with the oldest consists of 2 nodes and the other part consists of 98 nodes then it will keep 2 nodes and shut down 98 nodes.\nThis strategy also handles the edge case that may occur when there are membership changes at the same time as the network partition occurs. For example, the status of the oldest member is changed to Exiting on one side but that information is not disseminated to the other side before the connection is broken. It will detect this situation and make the safe decision to down all nodes on the side that sees the oldest as Leaving. Note that this has the drawback that if the oldest was Leaving and not changed to Exiting then each part will shut down itself, terminating the whole cluster.\nThe decision can be based on nodes with a configured role instead of all nodes in the cluster, i.e. using the oldest member (singleton) within the nodes with that role.\nConfiguration:\npekko.cluster.split-brain-resolver.active-strategy=keep-oldest\ncopysourcepekko.cluster.split-brain-resolver.keep-oldest {\n  # Enable downing of the oldest node when it is partitioned from all other nodes\n  down-if-alone = on\n\n  # if the 'role' is defined the decision is based only on members with that 'role',\n  # i.e. using the oldest member (singleton) within the nodes with that role\n  role = \"\"\n}","title":"Keep Oldest"},{"location":"/split-brain-resolver.html#down-all","text":"The strategy named down-all will down all nodes.\nThis strategy can be a safe alternative if the network environment is highly unstable with unreachability observations that can’t be fully trusted, and including frequent occurrences of indirectly connected nodes. Due to the instability there is an increased risk of different information on different sides of partitions and therefore the other strategies may result in conflicting decisions. In such environments it can be better to shutdown all nodes and start up a new fresh cluster.\nShutting down all nodes means that the system will be completely unavailable until nodes have been restarted and formed a new cluster. This strategy is not recommended for large clusters (> 10 nodes) because any minor problem will shutdown all nodes, and that is more likely to happen in larger clusters since there are more nodes that may fail.\nSee also Down all when unstable and indirectly connected nodes.","title":"Down All"},{"location":"/split-brain-resolver.html#lease","text":"The strategy named lease-majority is using a distributed lease (lock) to decide what nodes that are allowed to survive. Only one SBR instance can acquire the lease make the decision to remain up. The other side will not be able to aquire the lease and will therefore down itself.\nBest effort is to keep the side that has most nodes, i.e. the majority side. This is achieved by adding a delay before trying to acquire the lease on the minority side.\nThere is currently one supported implementation of the lease which is backed by a Custom Resource Definition (CRD) in Kubernetes. It is described in the Kubernetes Lease documentation.\nThis strategy is very safe since coordination is added by an external arbiter. The trade-off compared to other strategies is that it requires additional infrastructure for implementing the lease and it reduces the availability of a decision to that of the system backing the lease store.\nSimilar to other strategies it is important that decisions are not deferred for too long because the nodes that couldn’t acquire the lease must decide to down themselves, see Down all when unstable.\nIn some cases the lease will be unavailable when needed for a decision from all SBR instances, e.g. because it is on another side of a network partition, and then all nodes will be downed.\nConfiguration:\npekko {\n  cluster {\n    downing-provider-class = \"org.apache.pekko.cluster.sbr.SplitBrainResolverProvider\"\n    split-brain-resolver {\n      active-strategy = \"lease-majority\"\n      lease-majority {\n        lease-implementation = \"pekko.coordination.lease.kubernetes\"\n      }\n    }\n  }\n}\ncopysourcepekko.cluster.split-brain-resolver.lease-majority {\n  lease-implementation = \"\"\n\n  # The recommended format for the lease name is \"<service-name>-pekko-sbr\".\n  # When lease-name is not defined, the name will be set to \"<actor-system-name>-pekko-sbr\"\n  lease-name = \"\"\n\n  # This delay is used on the minority side before trying to acquire the lease,\n  # as an best effort to try to keep the majority side.\n  acquire-lease-delay-for-minority = 2s\n\n  # Release the lease after this duration.\n  release-after = 40s\n\n  # If the 'role' is defined the majority/minority is based only on members with that 'role'.\n  role = \"\"\n}\nSee also configuration and additional dependency in Kubernetes Lease","title":"Lease"},{"location":"/split-brain-resolver.html#indirectly-connected-nodes","text":"In a malfunctional network there can be situations where nodes are observed as unreachable via some network links but they are still indirectly connected via other nodes, i.e. it’s not a clean network partition (or node crash).\nWhen this situation is detected the Split Brain Resolvers will keep fully connected nodes and down all the indirectly connected nodes.\nIf there is a combination of indirectly connected nodes and a clean network partition it will combine the above decision with the ordinary decision, e.g. keep majority, after excluding suspicious failure detection observations.","title":"Indirectly connected nodes"},{"location":"/split-brain-resolver.html#down-all-when-unstable","text":"When reachability observations by the failure detector are changed the SBR decisions are deferred until there are no changes within the stable-after duration. If this continues for too long it might be an indication of an unstable system/network and it could result in delayed or conflicting decisions on separate sides of a network partition.\nAs a precaution for that scenario all nodes are downed if no decision is made within stable-after + down-all-when-unstable from the first unreachability event. The measurement is reset if all unreachable have been healed, downed or removed, or if there are no changes within stable-after * 2.\nThis is enabled by default for all strategies and by default the duration is derived to be 3/4 of stable-after.\nThe below property can be defined as a duration of for how long the changes are acceptable to continue after the stable-after or it can be set to off to disable this feature.\npekko.cluster.split-brain-resolver {\n  down-all-when-unstable = 15s\n  stable-after = 20s\n}\nWarning It is recommended to keep down-all-when-unstable enabled and not set it to a longer duration than stable-after (down-removal-margin) because that can result in delayed decisions on the side that should have been downed, e.g. in the case of a clean network partition followed by continued instability on the side that should be downed. That could result in that members are removed from one side but are still running on the other side.","title":"Down all when unstable"},{"location":"/split-brain-resolver.html#multiple-data-centers","text":"Pekko Cluster has support for multiple data centers, where the cluster membership is managed by each data center separately and independently of network partitions across different data centers. The Split Brain Resolver is embracing that strategy and will not count nodes or down nodes in another data center.\nWhen there is a network partition across data centers the typical solution is to wait the partition out until it heals, i.e. do nothing. Other decisions should be performed by an external monitoring tool or human operator.","title":"Multiple data centers"},{"location":"/split-brain-resolver.html#cluster-singleton-and-cluster-sharding","text":"The purpose of Cluster Singleton and Cluster Sharding is to run at most one instance of a given actor at any point in time. When such an instance is shut down a new instance is supposed to be started elsewhere in the cluster. It is important that the new instance is not started before the old instance has been stopped. This is especially important when the singleton or the sharded instance is persistent, since there must only be one active writer of the journaled events of a persistent actor instance.\nSince the strategies on different sides of a network partition cannot communicate with each other and they may take the decision at slightly different points in time there must be a time based margin that makes sure that the new instance is not started before the old has been stopped.\nYou would like to configure this to a short duration to have quick failover, but that will increase the risk of having multiple singleton/sharded instances running at the same time and it may take a different amount of time to act on the decision (dissemination of the down/removal). The duration is by default the same as the stable-after property (see Stable after above). It is recommended to leave this value as is, but it can also be separately overriden with the pekko.cluster.down-removal-margin property.\nAnother concern for setting this stable-after/pekko.cluster.down-removal-margin is dealing with JVM pauses e.g. garbage collection. When a node is unresponsive it is not known if it is due to a pause, overload, a crash or a network partition. If it is pause that lasts longer than stable-after * 2 it gives time for SBR to down the node and for singletons and shards to be started on other nodes. When the node un-pauses there will be a short time before it sees its self as down where singletons and sharded actors are still running. It is therefore important to understand the max pause time your application is likely to incur and make sure it is smaller than stable-margin.\nIf you choose to set a separate value for down-removal-margin, the recommended minimum duration for different cluster sizes are:\ncluster size down-removal-margin 5 7 s 10 10 s 20 13 s 50 17 s 100 20 s 1000 30 s","title":"Cluster Singleton and Cluster Sharding"},{"location":"/split-brain-resolver.html#expected-failover-time","text":"As you have seen, there are several configured timeouts that add to the total failover latency. With default configuration those are:\nfailure detection 5 seconds stable-after 20 seconds down-removal-margin (by default the same as stable-after) 20 seconds\nIn total, you can expect the failover time of a singleton or sharded instance to be around 45 seconds with default configuration. The default configuration is sized for a cluster of 100 nodes. If you have around 10 nodes you can reduce the stable-after to around 10 seconds, resulting in an expected failover time of around 25 seconds.","title":"Expected Failover Time"},{"location":"/coordination.html","text":"","title":"Coordination"},{"location":"/coordination.html#coordination","text":"Pekko Coordination is a set of tools for distributed coordination.","title":"Coordination"},{"location":"/coordination.html#module-info","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-coordination\" % PekkoVersion Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-coordination_${versions.ScalaBinary}\"\n} Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-coordination_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies>\nProject Info: Pekko Coordination Artifact org.apache.pekko pekko-coordination 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.coordination License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/coordination.html#lease","text":"The lease is a pluggable API for a distributed lock.","title":"Lease"},{"location":"/coordination.html#using-a-lease","text":"Leases are loaded with:\nLease name Config location to indicate which implementation should be loaded Owner name\nAny lease implementation should provide the following guarantees:\nA lease with the same name loaded multiple times, even on different nodes, is the same lease Only one owner can acquire the lease at a time\nTo acquire a lease:\nScala copysourceval lease = LeaseProvider(system).getLease(\"<name of the lease>\", \"docs-lease\", \"owner\")\nval acquired: Future[Boolean] = lease.acquire()\nval stillAcquired: Boolean = lease.checkLease()\nval released: Future[Boolean] = lease.release() Java copysourceLease lease =\n    LeaseProvider.get(system).getLease(\"<name of the lease>\", \"jdocs-lease\", \"<owner name>\");\nCompletionStage<Boolean> acquired = lease.acquire();\nboolean stillAcquired = lease.checkLease();\nCompletionStage<Boolean> released = lease.release();\nAcquiring a lease returns a FutureCompletionStage as lease implementations typically are implemented via a third party system such as the Kubernetes API server or Zookeeper.\nOnce a lease is acquired, checkLease can be called to ensure that the lease is still acquired. As lease implementations are based on other distributed systems, a lease can be lost due to a timeout with the third party system. This operation is not asynchronous, so it can be called before performing any action for which having the lease is important.\nA lease has an owner. If the same owner tries to acquire the lease multiple times, it will succeed i.e. leases are reentrant.\nIt is important to pick a lease name that will be unique for your use case. If a lease needs to be unique for each node in a Cluster the cluster host port can be used:\nScala copysourceval owner = Cluster(system).selfAddress.hostPort Java copysource// String owner = Cluster.get(system).selfAddress().hostPort();\nFor use cases where multiple different leases on the same node then something unique must be added to the name. For example a lease can be used with Cluster Sharding and in this case the shard Id is included in the lease name for each shard.","title":"Using a lease"},{"location":"/coordination.html#setting-a-lease-heartbeat","text":"If a node with a lease crashes or is unresponsive the heartbeat-timeout is how long before other nodes can acquire the lease. Without this timeout operator intervention would be needed to release a lease in the case of a node crash. This is the safest option but not practical in all cases.\nThe value should be greater than the max expected JVM pause e.g. garbage collection, otherwise a lease can be acquired by another node and then when the original node becomes responsive again there will be a short time before the original lease owner can take action e.g. shutdown shards or singletons.","title":"Setting a lease heartbeat"},{"location":"/coordination.html#usages-in-other-pekko-modules","text":"Leases can be used for Split Brain Resolver, Cluster Singleton, and Cluster Sharding.","title":"Usages in other Pekko modules"},{"location":"/coordination.html#lease-implementations","text":"Kubernetes API","title":"Lease implementations"},{"location":"/coordination.html#implementing-a-lease","text":"Implementations should extend the org.apache.pekko.coordination.lease.scaladsl.Leaseorg.apache.pekko.coordination.lease.javadsl.Lease\nScala copysourceclass SampleLease(settings: LeaseSettings) extends Lease(settings) {\n\n  override def acquire(): Future[Boolean] = {\n    Future.successful(true)\n  }\n\n  override def acquire(leaseLostCallback: Option[Throwable] => Unit): Future[Boolean] = {\n    Future.successful(true)\n  }\n\n  override def release(): Future[Boolean] = {\n    Future.successful(true)\n  }\n\n  override def checkLease(): Boolean = {\n    true\n  }\n} Java copysourcestatic class SampleLease extends Lease {\n\n  private LeaseSettings settings;\n\n  public SampleLease(LeaseSettings settings) {\n    this.settings = settings;\n  }\n\n  @Override\n  public LeaseSettings getSettings() {\n    return settings;\n  }\n\n  @Override\n  public CompletionStage<Boolean> acquire() {\n    return CompletableFuture.completedFuture(true);\n  }\n\n  @Override\n  public CompletionStage<Boolean> acquire(Consumer<Optional<Throwable>> leaseLostCallback) {\n    return CompletableFuture.completedFuture(true);\n  }\n\n  @Override\n  public CompletionStage<Boolean> release() {\n    return CompletableFuture.completedFuture(true);\n  }\n\n  @Override\n  public boolean checkLease() {\n    return true;\n  }\n}\nThe methods should provide the following guarantees:\nacquire should complete with: true if the lease has been acquired, false if the lease is taken by another owner, or fail if it can’t communicate with the third party system implementing the lease. release should complete with: true if the lease has definitely been released, false if the lease has definitely not been released, or fail if it is unknown if the lease has been released. checkLease should return true if the lease has been acquired, should return false until an acquire FutureCompletionStage has completed, and should return false if the lease is lost due to an error communicating with the third party. Check lease should not block. The acquire lease lost callback should only be called after an acquire FutureCompletionStage has completed and should be called if the lease is lost e.g. due to losing communication with the third party system.\nIn addition, it is expected that a lease implementation will include a time to live mechanism meaning that a lease won’t be held for ever in case the node crashes. If a user prefers to have outside intervention in this case for maximum safety then the time to live can be set to infinite.\nThe configuration must define the lease-class property for the FQCN of the lease implementation.\nThe lease implementation should have support for the following properties where the defaults come from pekko.coordination.lease:\ncopysource# if the node that acquired the leases crashes, how long should the lease be held before another owner can get it\nheartbeat-timeout = 120s\n\n# interval for communicating with the third party to confirm the lease is still held\nheartbeat-interval = 12s\n\n# lease implementations are expected to time out acquire and release calls or document\n# that they do not implement an operation timeout\nlease-operation-timeout = 5s\nThis configuration location is passed into getLease.\nScala copysourcepekko.actor.provider = cluster\ndocs-lease {\n  lease-class = \"docs.coordination.SampleLease\"\n  heartbeat-timeout = 100s\n  heartbeat-interval = 1s\n  lease-operation-timeout = 1s\n  # Any lease specific configuration\n} Java copysourcepekko.actor.provider = cluster\ndocs-lease {\n  lease-class = \"docs.coordination.SampleLease\"\n  heartbeat-timeout = 100s\n  heartbeat-interval = 1s\n  lease-operation-timeout = 1s\n  # Any lease specific configuration\n}","title":"Implementing a lease"},{"location":"/typed/choosing-cluster.html","text":"","title":"Choosing Pekko Cluster"},{"location":"/typed/choosing-cluster.html#choosing-pekko-cluster","text":"An architectural choice you have to make is if you are going to use a microservices architecture or a traditional distributed application. This choice will influence how you should use Pekko Cluster.","title":"Choosing Pekko Cluster"},{"location":"/typed/choosing-cluster.html#microservices","text":"Microservices architecture has many attractive properties, such as the independent nature of microservices allows for multiple smaller and more focused teams that can deliver new functionality more frequently and can respond quicker to business opportunities. Reactive Microservices should be isolated, autonomous, and have a single responsibility as identified by Jonas Bonér in the book Reactive Microsystems: The Evolution of Microservices at Scale.\nIn a microservices architecture, you should consider communication within a service and between services.\nIn general we recommend against using Pekko Cluster and actor messaging between different services because that would result in a too tight code coupling between the services and difficulties deploying these independent of each other, which is one of the main reasons for using a microservices architecture. See the discussion on Internal and External Communication for some background on this.\nNodes of a single service (collectively called a cluster) require less decoupling. They share the same code and are deployed together, as a set, by a single team or individual. There might be two versions running concurrently during a rolling deployment, but deployment of the entire set has a single point of control. For this reason, intra-service communication can take advantage of Pekko Cluster, failure management and actor messaging, which is convenient to use and has great performance.\nBetween different services Pekko HTTP or Pekko gRPC can be used for synchronous (yet non-blocking) communication and Pekko Streams Kafka or other Pekko Connectors for integration asynchronous communication. All those communication mechanisms work well with streaming of messages with end-to-end back-pressure, and the synchronous communication tools can also be used for single request response interactions. It is also important to note that when using these tools both sides of the communication do not have to be implemented with Pekko, nor does the programming language matter.","title":"Microservices"},{"location":"/typed/choosing-cluster.html#traditional-distributed-application","text":"We acknowledge that microservices also introduce many new challenges and it’s not the only way to build applications. A traditional distributed application may have less complexity and work well in many cases. For example for a small startup, with a single team, building an application where time to market is everything. Pekko Cluster can efficiently be used for building such distributed application.\nIn this case, you have a single deployment unit, built from a single code base (or using traditional binary dependency management to modularize) but deployed across many nodes using a single cluster. Tighter coupling is OK, because there is a central point of deployment and control. In some cases, nodes may have specialized runtime roles which means that the cluster is not totally homogenous (e.g., “front-end” and “back-end” nodes, or dedicated master/worker nodes) but if these are run from the same built artifacts this is just a runtime behavior and doesn’t cause the same kind of problems you might get from tight coupling of totally separate artifacts.\nA tightly coupled distributed application has served the industry and many Pekko users well for years and is still a valid choice.","title":"Traditional distributed application"},{"location":"/typed/choosing-cluster.html#distributed-monolith","text":"There is also an anti-pattern that is sometimes called “distributed monolith”. You have multiple services that are built and deployed independently from each other, but they have a tight coupling that makes this very risky, such as a shared cluster, shared code and dependencies for service API calls, or a shared database schema. There is a false sense of autonomy because of the physical separation of the code and deployment units, but you are likely to encounter problems because of changes in the implementation of one service leaking into the behavior of others. See Ben Christensen’s Don’t Build a Distributed Monolith.\nOrganizations that find themselves in this situation often react by trying to centrally coordinate deployment of multiple services, at which point you have lost the principal benefit of microservices while taking on the costs. You are in a halfway state with things that aren’t really separable being built and deployed in a separate way. Some people do this, and some manage to make it work, but it’s not something we would recommend and it needs to be carefully managed.","title":"Distributed monolith"},{"location":"/typed/index-persistence.html","text":"","title":"Persistence (Event Sourcing)"},{"location":"/typed/index-persistence.html#persistence-event-sourcing-","text":"Event Sourcing Module info Introduction Example and core API Effects and Side Effects Cluster Sharding and EventSourcedBehavior Accessing the ActorContext Changing Behavior Replies Serialization Recovery Tagging Event adapters Wrapping EventSourcedBehavior Journal failures Stash Scaling out Configuration Example project Replicated Event Sourcing Relaxing the single-writer principle for availability API Resolving conflicting updates Side effects How it works Sharded Replicated Event Sourced entities Tagging events and running projections Direct Replication of Events Hot Standby Examples Journal Support CQRS Style Guide Event handlers in the state Command handlers in the state Optional initial state Mutable state Snapshotting Snapshots Snapshot failures Snapshot deletion Event deletion Testing Module info Unit testing Persistence TestKit Integration testing EventSourced behaviors as finite state machines Schema Evolution for Event Sourced Actors Dependency Introduction Schema evolution in event-sourced systems Picking the right serialization format Schema evolution in action Apache Persistence Query Dependency Introduction Design overview Read Journals Performance and denormalization Query plugins Scaling out Example project Persistence Query for LevelDB Dependency Introduction How to get the ReadJournal Supported Queries Configuration Persistence Plugins Eager initialization of persistence plugin Pre-packaged plugins Persistence - Building a storage backend Journal plugin API Snapshot store plugin API Plugin TCK Corrupt event logs Replicated Event Sourcing Examples Auction example Shopping cart example","title":"Persistence (Event Sourcing)"},{"location":"/typed/persistence.html","text":"","title":"Event Sourcing"},{"location":"/typed/persistence.html#event-sourcing","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Pekko Persistence.","title":"Event Sourcing"},{"location":"/typed/persistence.html#module-info","text":"To use Pekko Persistence, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-persistence-typed\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-persistence-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-typed_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-typed_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-persistence-testkit_${versions.ScalaBinary}\"\n}\nYou also have to select journal plugin and optionally snapshot store plugin, see Persistence Plugins.\nProject Info: Pekko Event Sourcing (typed) Artifact org.apache.pekko pekko-persistence-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.persistence.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/persistence.html#introduction","text":"Pekko Persistence enables stateful actors to persist their state so that it can be recovered when an actor is either restarted, such as after a JVM crash, by a supervisor or a manual stop-start, or migrated within a cluster. The key concept behind Pekko Persistence is that only the events that are persisted by the actor are stored, not the actual state of the actor (although actor state snapshot support is available). The events are persisted by appending to storage (nothing is ever mutated) which allows for very high transaction rates and efficient replication. A stateful actor is recovered by replaying the stored events to the actor, allowing it to rebuild its state. This can be either the full history of changes or starting from a checkpoint in a snapshot, which can dramatically reduce recovery times.\nPekko Persistence also supports Durable State Behaviors, which is based on persistence of the latest state of the actor. In this implementation, the latest state is persisted, instead of events. Hence this is more similar to CRUD based applications.\nThe Microservices with Pekko tutorial illustrates how to implement an Event Sourced CQRS application with Pekko Persistence and Pekko Projections.\nNote The General Data Protection Regulation (GDPR) requires that personal information must be deleted at the request of users. Deleting or modifying events that carry personal information would be difficult. Data shredding can be used to forget information instead of deleting or modifying it. This is achieved by encrypting the data with a key for a given data subject id (person) and deleting the key when that data subject is to be forgotten.","title":"Introduction"},{"location":"/typed/persistence.html#event-sourcing-concepts","text":"See an introduction to Event Sourcing at MSDN.\nAnother excellent article about “thinking in Events” is Events As First-Class Citizens by Randy Shoup. It is a short and recommended read if you’re starting developing Events based applications.\nWhat follows is Pekko’s implementation via event sourced actors.\nAn event sourced actor (also known as a persistent actor) receives a (non-persistent) command which is first validated if it can be applied to the current state. Here validation can mean anything, from simple inspection of a command message’s fields up to a conversation with several external services, for example. If validation succeeds, events are generated from the command, representing the effect of the command. These events are then persisted and, after successful persistence, used to change the actor’s state. When the event sourced actor needs to be recovered, only the persisted events are replayed of which we know that they can be successfully applied. In other words, events cannot fail when being replayed to a persistent actor, in contrast to commands. Event sourced actors may also process commands that do not change application state such as query commands for example.","title":"Event Sourcing concepts"},{"location":"/typed/persistence.html#example-and-core-api","text":"Let’s start with a simple example. The minimum required for a EventSourcedBehaviorEventSourcedBehavior is:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.typed.scaladsl.EventSourcedBehavior\nimport pekko.persistence.typed.PersistenceId\n\nobject MyPersistentBehavior {\n  sealed trait Command\n  sealed trait Event\n  final case class State()\n\n  def apply(): Behavior[Command] =\n    EventSourcedBehavior[Command, Event, State](\n      persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n      emptyState = State(),\n      commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n      eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n} Java copysourcepublic class MyPersistentBehavior\n    extends EventSourcedBehavior<\n        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {\n\n  interface Command {}\n\n  interface Event {}\n\n  public static class State {}\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return new MyPersistentBehavior(persistenceId);\n  }\n\n  private MyPersistentBehavior(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  @Override\n  public State emptyState() {\n    return new State();\n  }\n\n  @Override\n  public CommandHandler<Command, Event, State> commandHandler() {\n    return (state, command) -> {\n      throw new RuntimeException(\"TODO: process the command & return an Effect\");\n    };\n  }\n\n  @Override\n  public EventHandler<State, Event> eventHandler() {\n    return (state, event) -> {\n      throw new RuntimeException(\"TODO: process the event return the next state\");\n    };\n  }\n}\nThe first important thing to notice is the BehaviorBehavior of a persistent actor is typed to the type of the Command because this is the type of message a persistent actor should receive. In Pekko, this is now enforced by the type system.\nThe components that make up an EventSourcedBehaviorEventSourcedBehavior are:\npersistenceId is the stable unique identifier for the persistent actor. emptyState defines the State when the entity is first created e.g. a Counter would start with 0 as state. commandHandler defines how to handle command by producing Effects e.g. persisting events, stopping the persistent actor. eventHandler returns the new state given the current state when an event has been persisted.\nNote that the concrete class does not contain any fields with state like a regular POJO. All state of the EventSourcedBehavior must be represented in the State or else they will not be persisted and therefore be lost when the actor is stopped or restarted. Updates to the State are always performed in the eventHandler based on the events.\nNext we’ll discuss each of these in detail.","title":"Example and core API"},{"location":"/typed/persistence.html#persistenceid","text":"The PersistenceIdPersistenceId is the stable unique identifier for the persistent actor in the backend event journal and snapshot store.\nCluster Sharding is typically used together with EventSourcedBehavior to ensure that there is only one active entity for each PersistenceId (entityId). There are techniques to ensure this uniqueness, an example of which can be found in the Persistence example in the Cluster Sharding documentation. This illustrates how to construct the PersistenceId from the entityTypeKey and entityId provided by the EntityContextEntityContext.\nThe entityId in Cluster Sharding is the business domain identifier of the entity. The entityId might not be unique enough to be used as the PersistenceId by itself. For example two different types of entities may have the same entityId. To create a unique PersistenceId the entityId should be prefixed with a stable name of the entity type, which typically is the same as the EntityTypeKey.name that is used in Cluster Sharding. There are PersistenceId.applyPersistenceId.of factory methods to help with constructing such PersistenceId from an entityTypeHint and entityId.\nThe default separator when concatenating the entityTypeHint and entityId is |, but a custom separator is supported.\nNote The | separator is also used in Lagom’s scaladsl.PersistentEntity but no separator is used in Lagom’s javadsl.PersistentEntity. For compatibility with Lagom’s javadsl.PersistentEntity you should use \"\" as the separator.\nA custom identifier can be created with PersistenceId.ofUniqueIdPersistenceId.ofUniqueId.","title":"PersistenceId"},{"location":"/typed/persistence.html#command-handler","text":"The command handler is a function with 2 parameters, the current State and the incoming Command.\nA command handler returns an EffectEffect directive that defines what event or events, if any, to persist. Effects are created using a factory that is returned via the Effect() method the Effect factory.\nThe two most commonly used effects are:\npersist will persist one single event or several events atomically, i.e. all events are stored or none of them are stored if there is an error none no events are to be persisted, for example a read-only command\nMore effects are explained in Effects and Side Effects.\nIn addition to returning the primary Effect for the command EventSourcedBehaviors can also chain side effects that are to be performed after successful persist which is achieved with the thenRun function e.g. Effect.persist(..).thenRunEffect().persist(..).thenRun.","title":"Command handler"},{"location":"/typed/persistence.html#event-handler","text":"When an event has been persisted successfully the new state is created by applying the event to the current state with the eventHandler. In the case of multiple persisted events, the eventHandler is called with each event in the same order as they were passed to Effect.persist(..)Effect().persist(..).\nThe state is typically defined as an immutable class and then the event handler returns a new instance of the state. You may choose to use a mutable class for the state, and then the event handler may update the state instance and return the same instance. Both immutable and mutable state is supported.\nThe same event handler is also used when the entity is started up to recover its state from the stored events.\nThe event handler must only update the state and never perform side effects, as those would also be executed during recovery of the persistent actor. Side effects should be performed in thenRun from the command handler after persisting the event or from the RecoveryCompletedRecoveryCompleted after Recovery.","title":"Event handler"},{"location":"/typed/persistence.html#completing-the-example","text":"Let’s fill in the details of the example.\nCommand and event:\nScala copysourcesealed trait Command\nfinal case class Add(data: String) extends Command\ncase object Clear extends Command\n\nsealed trait Event\nfinal case class Added(data: String) extends Event\ncase object Cleared extends Event Java copysourceinterface Command {}\n\npublic static class Add implements Command {\n  public final String data;\n\n  public Add(String data) {\n    this.data = data;\n  }\n}\n\npublic enum Clear implements Command {\n  INSTANCE\n}\n\ninterface Event {}\n\npublic static class Added implements Event {\n  public final String data;\n\n  public Added(String data) {\n    this.data = data;\n  }\n}\n\npublic enum Cleared implements Event {\n  INSTANCE\n}\nState is a List containing the 5 latest items:\nScala copysourcefinal case class State(history: List[String] = Nil) Java copysourcepublic static class State {\n  private final List<String> items;\n\n  private State(List<String> items) {\n    this.items = items;\n  }\n\n  public State() {\n    this.items = new ArrayList<>();\n  }\n\n  public State addItem(String data) {\n    List<String> newItems = new ArrayList<>(items);\n    newItems.add(0, data);\n    // keep 5 items\n    List<String> latest = newItems.subList(0, Math.min(5, newItems.size()));\n    return new State(latest);\n  }\n}\nThe command handler persists the Add payload in an Added event:\nScala copysourceimport org.apache.pekko.persistence.typed.scaladsl.Effect\n\nval commandHandler: (State, Command) => Effect[Event, State] = { (state, command) =>\n  command match {\n    case Add(data) => Effect.persist(Added(data))\n    case Clear     => Effect.persist(Cleared)\n  }\n} Java copysource@Override\npublic CommandHandler<Command, Event, State> commandHandler() {\n  return newCommandHandlerBuilder()\n      .forAnyState()\n      .onCommand(Add.class, command -> Effect().persist(new Added(command.data)))\n      .onCommand(Clear.class, command -> Effect().persist(Cleared.INSTANCE))\n      .build();\n}\nThe event handler appends the item to the state and keeps 5 items. This is called after successfully persisting the event in the database:\nScala copysourceval eventHandler: (State, Event) => State = { (state, event) =>\n  event match {\n    case Added(data) => state.copy((data :: state.history).take(5))\n    case Cleared     => State(Nil)\n  }\n} Java copysource@Override\npublic EventHandler<State, Event> eventHandler() {\n  return newEventHandlerBuilder()\n      .forAnyState()\n      .onEvent(Added.class, (state, event) -> state.addItem(event.data))\n      .onEvent(Cleared.class, () -> new State())\n      .build();\n}\nThese are used to create an EventSourcedBehavior: These are defined in an EventSourcedBehavior:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.typed.scaladsl.EventSourcedBehavior\nimport pekko.persistence.typed.PersistenceId\n\ndef apply(id: String): Behavior[Command] =\n  EventSourcedBehavior[Command, Event, State](\n    persistenceId = PersistenceId.ofUniqueId(id),\n    emptyState = State(Nil),\n    commandHandler = commandHandler,\n    eventHandler = eventHandler) Java copysourceimport org.apache.pekko.persistence.typed.javadsl.EventSourcedBehavior;\nimport org.apache.pekko.persistence.typed.PersistenceId;\n\npublic class MyPersistentBehavior\n    extends EventSourcedBehavior<\n        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {\n\n  // commands, events and state defined here\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return new MyPersistentBehavior(persistenceId);\n  }\n\n  private MyPersistentBehavior(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  @Override\n  public State emptyState() {\n    return new State();\n  }\n\n  @Override\n  public CommandHandler<Command, Event, State> commandHandler() {\n    return newCommandHandlerBuilder()\n        .forAnyState()\n        .onCommand(Add.class, command -> Effect().persist(new Added(command.data)))\n        .onCommand(Clear.class, command -> Effect().persist(Cleared.INSTANCE))\n        .build();\n  }\n\n  @Override\n  public EventHandler<State, Event> eventHandler() {\n    return newEventHandlerBuilder()\n        .forAnyState()\n        .onEvent(Added.class, (state, event) -> state.addItem(event.data))\n        .onEvent(Cleared.class, () -> new State())\n        .build();\n  }\n}","title":"Completing the example"},{"location":"/typed/persistence.html#effects-and-side-effects","text":"A command handler returns an EffectEffect directive that defines what event or events, if any, to persist. Effects are created using a factory that is returned via the Effect() method the Effect factory and can be one of:\npersistpersist will persist one single event or several events atomically, i.e. all events are stored or none of them are stored if there is an error nonenone no events are to be persisted, for example a read-only command unhandledunhandled the command is unhandled (not supported) in current state stopstop stop this actor stashstash the current command is stashed unstashAllunstashAll process the commands that were stashed with Effect.stashEffect().stash replyreply send a reply message to the given ActorRefActorRef\nNote that only one of those can be chosen per incoming command. It is not possible to both persist and say none/unhandled.\nIn addition to returning the primary Effect for the command EventSourcedBehaviorEventSourcedBehaviors can also chain side effects that are to be performed after successful persist which is achieved with the thenRunthenRun function e.g. Effect.persist(..).thenRunEffect().persist(..).thenRun.\nIn the example below the state is sent to the subscriber ActorRef. Note that the new state after applying the event is passed as parameter of the thenRun function. In the case where multiple events have been persisted, the state passed to thenRun is the updated state after all events have been handled.\nAll thenRun registered callbacks are executed sequentially after successful execution of the persist statement (or immediately, in case of none and unhandled).\nIn addition to thenRun the following actions can also be performed after successful persist:\nthenStopthenStop the actor will be stopped thenUnstashAllthenUnstashAll process the commands that were stashed with Effect.stashEffect().stash thenReplythenReply send a reply message to the given ActorRefActorRef\nExample of effects:\nScala copysourcedef onCommand(subscriber: ActorRef[State], state: State, command: Command): Effect[Event, State] = {\n  command match {\n    case Add(data) =>\n      Effect.persist(Added(data)).thenRun(newState => subscriber ! newState)\n    case Clear =>\n      Effect.persist(Cleared).thenRun((newState: State) => subscriber ! newState).thenStop()\n  }\n} Java copysourceprivate final ActorRef<State> subscriber;\n\n@Override\npublic CommandHandler<Command, Event, State> commandHandler() {\n  return newCommandHandlerBuilder()\n      .forAnyState()\n      .onCommand(Add.class, this::onAdd)\n      .onCommand(Clear.class, this::onClear)\n      .build();\n}\n\nprivate Effect<Event, State> onAdd(Add command) {\n  return Effect()\n      .persist(new Added(command.data))\n      .thenRun(newState -> subscriber.tell(newState));\n}\n\nprivate Effect<Event, State> onClear(Clear command) {\n  return Effect()\n      .persist(Cleared.INSTANCE)\n      .thenRun(newState -> subscriber.tell(newState))\n      .thenStop();\n}\nMost of the time this will be done with the thenRunthenRun method on the Effect above. You can factor out common side effects into functions and reuse for several commands. For example:\nScala copysource// Example factoring out a chained effect to use in several places with `thenRun`\nval commonChainedEffects: Mood => Unit = _ => println(\"Command processed\")\n// Then in a command handler:\nEffect\n  .persist(Remembered(\"Yep\")) // persist event\n  .thenRun(commonChainedEffects) // add on common chained effect Java copysource// Example factoring out a chained effect to use in several places with `thenRun`\nstatic final Procedure<ExampleState> commonChainedEffect =\n    state -> System.out.println(\"Command handled!\");\n\n      @Override\n      public CommandHandler<MyCommand, MyEvent, ExampleState> commandHandler() {\n        return newCommandHandlerBuilder()\n            .forStateType(ExampleState.class)\n            .onCommand(\n                Cmd.class,\n                (state, cmd) ->\n                    Effect()\n                        .persist(new Evt(cmd.data))\n                        .thenRun(() -> cmd.replyTo.tell(new Ack()))\n                        .thenRun(commonChainedEffect))\n            .build();\n      }","title":"Effects and Side Effects"},{"location":"/typed/persistence.html#side-effects-ordering-and-guarantees","text":"Any side effects are executed on an at-most-once basis and will not be executed if the persist fails.\nSide effects are not run when the actor is restarted or started again after being stopped. You may inspect the state when receiving the RecoveryCompletedRecoveryCompleted signal and execute side effects that have not been acknowledged at that point. That may possibly result in executing side effects more than once.\nThe side effects are executed sequentially, it is not possible to execute side effects in parallel, unless they call out to something that is running concurrently (for example sending a message to another actor).\nIt’s possible to execute a side effects before persisting the event, but that can result in that the side effect is performed but the event is not stored if the persist fails.","title":"Side effects ordering and guarantees"},{"location":"/typed/persistence.html#atomic-writes","text":"It is possible to store several events atomically by using the persistpersist effect with a list of events. That means that all events passed to that method are stored or none of them are stored if there is an error.\nThe recovery of a persistent actor will therefore never be done partially with only a subset of events persisted by a single persistpersist effect.\nSome journals may not support atomic writes of several events and they will then reject the persist with multiple events. This is signalled to an EventSourcedBehaviorEventSourcedBehavior via an EventRejectedExceptionEventRejectedException (typically with a UnsupportedOperationException) and can be handled with a supervisor.","title":"Atomic writes"},{"location":"/typed/persistence.html#cluster-sharding-and-eventsourcedbehavior","text":"Cluster Sharding is an excellent fit to spread persistent actors over a cluster, addressing them by id. It makes it possible to have more persistent actors exist in the cluster than what would fit in the memory of one node. Cluster sharding improves the resilience of the cluster. If a node crashes, the persistent actors are quickly started on a new node and can resume operations.\nThe EventSourcedBehaviorEventSourcedBehavior can then be run as with any plain actor as described in actors documentation, but since Pekko Persistence is based on the single-writer principle the persistent actors are typically used together with Cluster Sharding. For a particular persistenceId only one persistent actor instance should be active at one time. If multiple instances were to persist events at the same time, the events would be interleaved and might not be interpreted correctly on replay. Cluster Sharding ensures that there is only one active entity for each id. The Cluster Sharding example illustrates this common combination.","title":"Cluster Sharding and EventSourcedBehavior"},{"location":"/typed/persistence.html#accessing-the-actorcontext","text":"If the EventSourcedBehaviorEventSourcedBehavior needs to use the ActorContextActorContext, for example to spawn child actors, it can be obtained by wrapping construction with Behaviors.setupBehaviors.setup:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.typed.scaladsl.Effect\nimport pekko.persistence.typed.scaladsl.EventSourcedBehavior.CommandHandler\n\ndef apply(): Behavior[String] =\n  Behaviors.setup { context =>\n    EventSourcedBehavior[String, String, State](\n      persistenceId = PersistenceId.ofUniqueId(\"myPersistenceId\"),\n      emptyState = State(),\n      commandHandler = CommandHandler.command { cmd =>\n        context.log.info(\"Got command {}\", cmd)\n        Effect.none\n      },\n      eventHandler = {\n        case (state, _) => state\n      })\n  } Java copysourcepublic class MyPersistentBehavior\n    extends EventSourcedBehavior<\n        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return Behaviors.setup(ctx -> new MyPersistentBehavior(persistenceId, ctx));\n  }\n\n  // this makes the context available to the command handler etc.\n  private final ActorContext<Command> context;\n\n  // optionally if you only need `ActorContext.getSelf()`\n  private final ActorRef<Command> self;\n\n  public MyPersistentBehavior(PersistenceId persistenceId, ActorContext<Command> ctx) {\n    super(persistenceId);\n    this.context = ctx;\n    this.self = ctx.getSelf();\n  }\n\n}","title":"Accessing the ActorContext"},{"location":"/typed/persistence.html#changing-behavior","text":"After processing a message, actors are able to return the BehaviorBehavior that is used for the next message.\nAs you can see in the above examples this is not supported by persistent actors. Instead, the state is returned by eventHandler. The reason a new behavior can’t be returned is that behavior is part of the actor’s state and must also carefully be reconstructed during recovery. If it would have been supported it would mean that the behavior must be restored when replaying events and also encoded in the state anyway when snapshots are used. That would be very prone to mistakes and thus not allowed in Pekko Persistence.\nFor basic actors you can use the same set of command handlers independent of what state the entity is in, as shown in above example. For more complex actors it’s useful to be able to change the behavior in the sense that different functions for processing commands may be defined depending on what state the actor is in. This is useful when implementing finite state machine (FSM) like entities.\nThe next example demonstrates how to define different behavior based on the current State. It shows an actor that represents the state of a blog post. Before a post is started the only command it can process is to AddPost. Once it is started then one can look it up with GetPost, modify it with ChangeBody or publish it with Publish.\nThe state is captured by:\nScala copysourcesealed trait State\n\ncase object BlankState extends State\n\nfinal case class DraftState(content: PostContent) extends State {\n  def withBody(newBody: String): DraftState =\n    copy(content = content.copy(body = newBody))\n\n  def postId: String = content.postId\n}\n\nfinal case class PublishedState(content: PostContent) extends State {\n  def postId: String = content.postId\n} Java copysourceinterface State {}\n\nenum BlankState implements State {\n  INSTANCE\n}\n\nstatic class DraftState implements State {\n  final PostContent content;\n\n  DraftState(PostContent content) {\n    this.content = content;\n  }\n\n  DraftState withContent(PostContent newContent) {\n    return new DraftState(newContent);\n  }\n\n  DraftState withBody(String newBody) {\n    return withContent(new PostContent(postId(), content.title, newBody));\n  }\n\n  String postId() {\n    return content.postId;\n  }\n}\n\nstatic class PublishedState implements State {\n  final PostContent content;\n\n  PublishedState(PostContent content) {\n    this.content = content;\n  }\n\n  PublishedState withContent(PostContent newContent) {\n    return new PublishedState(newContent);\n  }\n\n  PublishedState withBody(String newBody) {\n    return withContent(new PostContent(postId(), content.title, newBody));\n  }\n\n  String postId() {\n    return content.postId;\n  }\n}\nThe commands, of which only a subset are valid depending on the state:\nScala copysourcesealed trait Command\nfinal case class AddPost(content: PostContent, replyTo: ActorRef[StatusReply[AddPostDone]]) extends Command\nfinal case class AddPostDone(postId: String)\nfinal case class GetPost(replyTo: ActorRef[PostContent]) extends Command\nfinal case class ChangeBody(newBody: String, replyTo: ActorRef[Done]) extends Command\nfinal case class Publish(replyTo: ActorRef[Done]) extends Command\nfinal case class PostContent(postId: String, title: String, body: String) Java copysourcepublic interface Command {}\npublic static class AddPost implements Command {\n  final PostContent content;\n  final ActorRef<AddPostDone> replyTo;\n\n  public AddPost(PostContent content, ActorRef<AddPostDone> replyTo) {\n    this.content = content;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class AddPostDone implements Command {\n  final String postId;\n\n  public AddPostDone(String postId) {\n    this.postId = postId;\n  }\n}\npublic static class GetPost implements Command {\n  final ActorRef<PostContent> replyTo;\n\n  public GetPost(ActorRef<PostContent> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class ChangeBody implements Command {\n  final String newBody;\n  final ActorRef<Done> replyTo;\n\n  public ChangeBody(String newBody, ActorRef<Done> replyTo) {\n    this.newBody = newBody;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class Publish implements Command {\n  final ActorRef<Done> replyTo;\n\n  public Publish(ActorRef<Done> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class PostContent implements Command {\n  final String postId;\n  final String title;\n  final String body;\n\n  public PostContent(String postId, String title, String body) {\n    this.postId = postId;\n    this.title = title;\n    this.body = body;\n  }\n}\nThe command handler to process each command is decided by the state class (or state predicate) that is given to the forStateType of the CommandHandlerBuilder and the match cases in the builders. The command handler to process each command is decided by first looking at the state and then the command. It typically becomes two levels of pattern matching, first on the state and then on the command. Delegating to methods is a good practice because the one-line cases give a nice overview of the message dispatch.\nScala copysourceprivate val commandHandler: (State, Command) => Effect[Event, State] = { (state, command) =>\n  state match {\n\n    case BlankState =>\n      command match {\n        case cmd: AddPost => addPost(cmd)\n        case _            => Effect.unhandled\n      }\n\n    case draftState: DraftState =>\n      command match {\n        case cmd: ChangeBody  => changeBody(draftState, cmd)\n        case Publish(replyTo) => publish(draftState, replyTo)\n        case GetPost(replyTo) => getPost(draftState, replyTo)\n        case AddPost(_, replyTo) =>\n          Effect.unhandled.thenRun(_ => replyTo ! StatusReply.Error(\"Cannot add post while in draft state\"))\n      }\n\n    case publishedState: PublishedState =>\n      command match {\n        case GetPost(replyTo) => getPost(publishedState, replyTo)\n        case AddPost(_, replyTo) =>\n          Effect.unhandled.thenRun(_ => replyTo ! StatusReply.Error(\"Cannot add post, already published\"))\n        case _ => Effect.unhandled\n      }\n  }\n}\n\nprivate def addPost(cmd: AddPost): Effect[Event, State] = {\n  val evt = PostAdded(cmd.content.postId, cmd.content)\n  Effect.persist(evt).thenRun { _ =>\n    // After persist is done additional side effects can be performed\n    cmd.replyTo ! StatusReply.Success(AddPostDone(cmd.content.postId))\n  }\n}\n\nprivate def changeBody(state: DraftState, cmd: ChangeBody): Effect[Event, State] = {\n  val evt = BodyChanged(state.postId, cmd.newBody)\n  Effect.persist(evt).thenRun { _ =>\n    cmd.replyTo ! Done\n  }\n}\n\nprivate def publish(state: DraftState, replyTo: ActorRef[Done]): Effect[Event, State] = {\n  Effect.persist(Published(state.postId)).thenRun { _ =>\n    println(s\"Blog post ${state.postId} was published\")\n    replyTo ! Done\n  }\n}\n\nprivate def getPost(state: DraftState, replyTo: ActorRef[PostContent]): Effect[Event, State] = {\n  replyTo ! state.content\n  Effect.none\n}\n\nprivate def getPost(state: PublishedState, replyTo: ActorRef[PostContent]): Effect[Event, State] = {\n  replyTo ! state.content\n  Effect.none\n} Java copysource@Override\npublic CommandHandler<Command, Event, State> commandHandler() {\n  CommandHandlerBuilder<Command, Event, State> builder = newCommandHandlerBuilder();\n\n  builder.forStateType(BlankState.class).onCommand(AddPost.class, this::onAddPost);\n\n  builder\n      .forStateType(DraftState.class)\n      .onCommand(ChangeBody.class, this::onChangeBody)\n      .onCommand(Publish.class, this::onPublish)\n      .onCommand(GetPost.class, this::onGetPost);\n\n  builder\n      .forStateType(PublishedState.class)\n      .onCommand(ChangeBody.class, this::onChangeBody)\n      .onCommand(GetPost.class, this::onGetPost);\n\n  builder.forAnyState().onCommand(AddPost.class, (state, cmd) -> Effect().unhandled());\n\n  return builder.build();\n}\n\nprivate Effect<Event, State> onAddPost(AddPost cmd) {\n  PostAdded event = new PostAdded(cmd.content.postId, cmd.content);\n  return Effect()\n      .persist(event)\n      .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));\n}\n\nprivate Effect<Event, State> onChangeBody(DraftState state, ChangeBody cmd) {\n  BodyChanged event = new BodyChanged(state.postId(), cmd.newBody);\n  return Effect().persist(event).thenRun(() -> cmd.replyTo.tell(Done.getInstance()));\n}\n\nprivate Effect<Event, State> onChangeBody(PublishedState state, ChangeBody cmd) {\n  BodyChanged event = new BodyChanged(state.postId(), cmd.newBody);\n  return Effect().persist(event).thenRun(() -> cmd.replyTo.tell(Done.getInstance()));\n}\n\nprivate Effect<Event, State> onPublish(DraftState state, Publish cmd) {\n  return Effect()\n      .persist(new Published(state.postId()))\n      .thenRun(\n          () -> {\n            System.out.println(\"Blog post published: \" + state.postId());\n            cmd.replyTo.tell(Done.getInstance());\n          });\n}\n\nprivate Effect<Event, State> onGetPost(DraftState state, GetPost cmd) {\n  cmd.replyTo.tell(state.content);\n  return Effect().none();\n}\n\nprivate Effect<Event, State> onGetPost(PublishedState state, GetPost cmd) {\n  cmd.replyTo.tell(state.content);\n  return Effect().none();\n}\nThe event handler:\nScala copysourceprivate val eventHandler: (State, Event) => State = { (state, event) =>\n  state match {\n\n    case BlankState =>\n      event match {\n        case PostAdded(_, content) =>\n          DraftState(content)\n        case _ => throw new IllegalStateException(s\"unexpected event [$event] in state [$state]\")\n      }\n\n    case draftState: DraftState =>\n      event match {\n\n        case BodyChanged(_, newBody) =>\n          draftState.withBody(newBody)\n\n        case Published(_) =>\n          PublishedState(draftState.content)\n\n        case _ => throw new IllegalStateException(s\"unexpected event [$event] in state [$state]\")\n      }\n\n    case _: PublishedState =>\n      // no more changes after published\n      throw new IllegalStateException(s\"unexpected event [$event] in state [$state]\")\n  }\n} Java copysource@Override\npublic EventHandler<State, Event> eventHandler() {\n\n  EventHandlerBuilder<State, Event> builder = newEventHandlerBuilder();\n\n  builder\n      .forStateType(BlankState.class)\n      .onEvent(PostAdded.class, event -> new DraftState(event.content));\n\n  builder\n      .forStateType(DraftState.class)\n      .onEvent(BodyChanged.class, (state, chg) -> state.withBody(chg.newBody))\n      .onEvent(Published.class, (state, event) -> new PublishedState(state.content));\n\n  builder\n      .forStateType(PublishedState.class)\n      .onEvent(BodyChanged.class, (state, chg) -> state.withBody(chg.newBody));\n\n  return builder.build();\n}\nAnd finally the behavior is created from the EventSourcedBehavior.apply:\nScala copysourceobject BlogPostEntity {\n  // commands, events, state defined here\n\n  def apply(entityId: String, persistenceId: PersistenceId): Behavior[Command] = {\n    Behaviors.setup { context =>\n      context.log.info(\"Starting BlogPostEntity {}\", entityId)\n      EventSourcedBehavior[Command, Event, State](persistenceId, emptyState = BlankState, commandHandler, eventHandler)\n    }\n  }\n\n  // commandHandler and eventHandler defined here\n} Java copysourcepublic class BlogPostEntity\n    extends EventSourcedBehavior<\n        BlogPostEntity.Command, BlogPostEntity.Event, BlogPostEntity.State> {\n  // commands, events and state as in above snippets\n\n  public static Behavior<Command> create(String entityId, PersistenceId persistenceId) {\n    return Behaviors.setup(\n        context -> {\n          context.getLog().info(\"Starting BlogPostEntity {}\", entityId);\n          return new BlogPostEntity(persistenceId);\n        });\n  }\n\n  private BlogPostEntity(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  @Override\n  public State emptyState() {\n    return BlankState.INSTANCE;\n  }\n\n  // commandHandler, eventHandler as in above snippets\n}\nThis can be taken one or two steps further by defining the event and command handlers in the state class as illustrated in event handlers in the state and command handlers in the state.\nThere is also an example illustrating an optional initial state.","title":"Changing Behavior"},{"location":"/typed/persistence.html#replies","text":"The Request-Response interaction pattern is very common for persistent actors, because you typically want to know if the command was rejected due to validation errors and when accepted you want a confirmation when the events have been successfully stored.\nTherefore you typically include a ActorRefActorRef[ReplyMessageType]<ReplyMessageType>. If the command can either have a successful response or a validation error returned, the generic response type StatusReplyStatusReply[ReplyType] <ReplyType> can be used. If the successful reply does not contain a value but is more of an acknowledgement a pre defined StatusReply.AckStatusReply.ack() of type StatusReply[Done]StatusReply<Done> can be used.\nAfter validation errors or after persisting events, using a thenRunthenRun side effect, the reply message can be sent to the ActorRefActorRef.\nScala copysourcefinal case class AddPost(content: PostContent, replyTo: ActorRef[StatusReply[AddPostDone]]) extends Command\nfinal case class AddPostDone(postId: String) Java copysourcepublic static class AddPost implements Command {\n  final PostContent content;\n  final ActorRef<AddPostDone> replyTo;\n\n  public AddPost(PostContent content, ActorRef<AddPostDone> replyTo) {\n    this.content = content;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class AddPostDone implements Command {\n  final String postId;\n\n  public AddPostDone(String postId) {\n    this.postId = postId;\n  }\n}\nScala copysourceval evt = PostAdded(cmd.content.postId, cmd.content)\nEffect.persist(evt).thenRun { _ =>\n  // After persist is done additional side effects can be performed\n  cmd.replyTo ! StatusReply.Success(AddPostDone(cmd.content.postId))\n} Java copysourcePostAdded event = new PostAdded(cmd.content.postId, cmd.content);\nreturn Effect()\n    .persist(event)\n    .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));\nSince this is such a common pattern there is a reply effect for this purpose. It has the nice property that it can be used to enforce that replies are not forgotten when implementing the EventSourcedBehaviorEventSourcedBehavior. If it’s defined with EventSourcedBehavior.withEnforcedRepliesEventSourcedBehaviorWithEnforcedReplies there will be compilation errors if the returned effect isn’t a ReplyEffectReplyEffect, which can be created with Effect.replyEffect().reply, Effect.noReplyEffect().noReply, Effect.thenReplyEffect().thenReply, or Effect.thenNoReplyEffect().thenNoReply.\nScala copysourcedef apply(accountNumber: String, persistenceId: PersistenceId): Behavior[Command] = {\n  EventSourcedBehavior.withEnforcedReplies(persistenceId, EmptyAccount, commandHandler(accountNumber), eventHandler)\n} Java copysourcepublic class AccountEntity\n    extends EventSourcedBehaviorWithEnforcedReplies<\n        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account> {\nThe commands must have a field of ActorRefActorRef[ReplyMessageType]<ReplyMessageType> that can then be used to send a reply.\nScala copysourcesealed trait Command extends CborSerializable\nfinal case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command Java copysourceinterface Command extends CborSerializable {}\nThe ReplyEffectReplyEffect is created with Effect.replyEffect().reply, Effect.noReplyEffect().noReply, Effect.thenReplyEffect().thenReply, or Effect.thenNoReplyEffect().thenNoReply.\nNote that command handlers are defined with newCommandHandlerWithReplyBuilder when using EventSourcedBehaviorWithEnforcedReplies, as opposed to newCommandHandlerBuilder when using EventSourcedBehavior.\nScala copysourceprivate def withdraw(acc: OpenedAccount, cmd: Withdraw): ReplyEffect[Event, Account] = {\n  if (acc.canWithdraw(cmd.amount))\n    Effect.persist(Withdrawn(cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n  else\n    Effect.reply(cmd.replyTo)(\n      StatusReply.Error(s\"Insufficient balance ${acc.balance} to be able to withdraw ${cmd.amount}\"))\n} Java copysourceprivate ReplyEffect<Event, Account> withdraw(OpenedAccount account, Withdraw command) {\n  if (!account.canWithdraw(command.amount)) {\n    return Effect()\n        .reply(\n            command.replyTo,\n            StatusReply.error(\"not enough funds to withdraw \" + command.amount));\n  } else {\n    return Effect()\n        .persist(new Withdrawn(command.amount))\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n}\nThese effects will send the reply message even when EventSourcedBehavior.withEnforcedRepliesEventSourcedBehaviorWithEnforcedReplies is not used, but then there will be no compilation errors if the reply decision is left out.\nNote that the noReply is a way of making conscious decision that a reply shouldn’t be sent for a specific command or the reply will be sent later, perhaps after some asynchronous interaction with other actors or services.","title":"Replies"},{"location":"/typed/persistence.html#serialization","text":"The same serialization mechanism as for actor messages is also used for persistent actors. When picking a serialization solution for the events you should also consider that it must be possible to read old events when the application has evolved. Strategies for that can be found in the schema evolution.\nYou need to enable serialization for your commands (messages), events, and state (snapshot). Serialization with Jackson is a good choice in many cases and our recommendation if you don’t have other preference.","title":"Serialization"},{"location":"/typed/persistence.html#recovery","text":"An event sourced actor is automatically recovered on start and on restart by replaying journaled events. New messages sent to the actor during recovery do not interfere with replayed events. They are stashed and received by the EventSourcedBehaviorEventSourcedBehavior after the recovery phase completes.\nThe number of concurrent recoveries that can be in progress at the same time is limited to not overload the system and the backend data store. When exceeding the limit the actors will wait until other recoveries have been completed. This is configured by:\npekko.persistence.max-concurrent-recoveries = 50\nThe event handler is used for updating the state when replaying the journaled events.\nIt is strongly discouraged to perform side effects in the event handler, so side effects should be performed once recovery has completed as a reaction to the RecoveryCompletedRecoveryCompleted signal in the receiveSignal handler by overriding receiveSignal\nScala copysourceEventSourcedBehavior[Command, Event, State](\n  persistenceId = persistenceId,\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .receiveSignal {\n    case (state, RecoveryCompleted) =>\n      throw new NotImplementedError(\"TODO: add some end-of-recovery side-effect here\")\n  } Java copysource @Override\npublic SignalHandler<State> signalHandler() {\n  return newSignalHandlerBuilder()\n      .onSignal(\n          RecoveryCompleted.instance(),\n          state -> {\n            throw new RuntimeException(\"TODO: add some end-of-recovery side-effect here\");\n          })\n      .build();\n}\nThe RecoveryCompleted contains the current State.\nThe actor will always receive a RecoveryCompleted signal, even if there are no events in the journal and the snapshot store is empty, or if it’s a new persistent actor with a previously unused PersistenceId.\nSnapshots can be used for optimizing recovery times.","title":"Recovery"},{"location":"/typed/persistence.html#replay-filter","text":"There could be cases where event streams are corrupted and multiple writers (i.e. multiple persistent actor instances) journaled different messages with the same sequence number. In such a case, you can configure how you filter replayed messages from multiple writers, upon recovery.\nIn your configuration, under the pekko.persistence.journal.xxx.replay-filter section (where xxx is your journal plugin id), you can select the replay filter mode from one of the following values:\nrepair-by-discard-old fail warn off\nFor example, if you configure the replay filter for leveldb plugin, it looks like this:\n# The replay filter can detect a corrupt event stream by inspecting\n# sequence numbers and writerUuid when replaying events.\npekko.persistence.journal.leveldb.replay-filter {\n  # What the filter should do when detecting invalid events.\n  # Supported values:\n  # `repair-by-discard-old` : discard events from old writers,\n  #                           warning is logged\n  # `fail` : fail the replay, error is logged\n  # `warn` : log warning but emit events untouched\n  # `off` : disable this feature completely\n  mode = repair-by-discard-old\n}","title":"Replay filter"},{"location":"/typed/persistence.html#disable-recovery","text":"You can also completely disable the recovery of events and snapshots:\nScala copysourceEventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .withRecovery(Recovery.disabled) Java copysource@Override\npublic Recovery recovery() {\n  return Recovery.disabled();\n}\nPlease refer to snapshots if you need to disable only the snapshot recovery, or you need to select specific snapshots.\nIn any case, the highest sequence number will always be recovered so you can keep persisting new events without corrupting your event log.","title":"Disable recovery"},{"location":"/typed/persistence.html#tagging","text":"Persistence allows you to use event tags without using an EventAdapter:\nScala copysourceEventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .withTagger(_ => Set(\"tag1\", \"tag2\")) Java copysource@Override\npublic Set<String> tagsFor(Event event) {\n  Set<String> tags = new HashSet<>();\n  tags.add(\"tag1\");\n  tags.add(\"tag2\");\n  return tags;\n}","title":"Tagging"},{"location":"/typed/persistence.html#event-adapters","text":"Event adapters can be programmatically added to your EventSourcedBehaviorEventSourcedBehaviors that can convert from your Event type to another type that is then passed to the journal.\nDefining an event adapter is done by extending an EventAdapter:\nScala copysourcecase class Wrapper[T](event: T)\nclass WrapperEventAdapter[T] extends EventAdapter[T, Wrapper[T]] {\n  override def toJournal(e: T): Wrapper[T] = Wrapper(e)\n  override def fromJournal(p: Wrapper[T], manifest: String): EventSeq[T] = EventSeq.single(p.event)\n  override def manifest(event: T): String = \"\"\n} Java copysourcepublic static class Wrapper<T> {\n  private final T event;\n\n  public Wrapper(T event) {\n    this.event = event;\n  }\n\n  public T getEvent() {\n    return event;\n  }\n}\n\npublic static class EventAdapterExample\n    extends EventAdapter<SimpleEvent, Wrapper<SimpleEvent>> {\n  @Override\n  public Wrapper<SimpleEvent> toJournal(SimpleEvent simpleEvent) {\n    return new Wrapper<>(simpleEvent);\n  }\n\n  @Override\n  public String manifest(SimpleEvent event) {\n    return \"\";\n  }\n\n  @Override\n  public EventSeq<SimpleEvent> fromJournal(\n      Wrapper<SimpleEvent> simpleEventWrapper, String manifest) {\n    return EventSeq.single(simpleEventWrapper.getEvent());\n  }\n}\nThen install it on an EventSourcedBehavior:\nScala copysourceEventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .eventAdapter(new WrapperEventAdapter[Event]) Java copysource@Override\npublic EventAdapter<SimpleEvent, Wrapper<SimpleEvent>> eventAdapter() {\n  return new EventAdapterExample();\n}","title":"Event adapters"},{"location":"/typed/persistence.html#wrapping-eventsourcedbehavior","text":"When creating an EventSourcedBehaviorEventSourcedBehavior, it is possible to wrap EventSourcedBehavior in other behaviors such as Behaviors.setupBehaviors.setup in order to access the ActorContextActorContext object. For instance to access the actor logging upon taking snapshots for debug purpose.\nScala copysourceBehaviors.setup[Command] { context =>\n  EventSourcedBehavior[Command, Event, State](\n    persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n    emptyState = State(),\n    commandHandler =\n      (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n    eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n    .snapshotWhen((state, _, _) => {\n      context.log.info2(\"Snapshot actor {} => state: {}\", context.self.path.name, state)\n      true\n    })\n} Java copysourcepublic class MyPersistentBehavior\n    extends EventSourcedBehavior<\n        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {\n\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return Behaviors.setup(context -> new MyPersistentBehavior(persistenceId, context));\n  }\n\n  private final ActorContext<Command> context;\n\n  private MyPersistentBehavior(PersistenceId persistenceId, ActorContext<Command> context) {\n    super(\n        persistenceId,\n        SupervisorStrategy.restartWithBackoff(\n            Duration.ofSeconds(10), Duration.ofSeconds(30), 0.2));\n    this.context = context;\n  }\n\n  @Override\n  public boolean shouldSnapshot(State state, Event event, long sequenceNr) {\n    context\n        .getLog()\n        .info(\"Snapshot actor {} => state: {}\", context.getSelf().path().name(), state);\n    return true;\n  }\n}","title":"Wrapping EventSourcedBehavior"},{"location":"/typed/persistence.html#journal-failures","text":"By default an EventSourcedBehaviorEventSourcedBehavior will stop if an exception is thrown from the journal. It is possible to override this with any BackoffSupervisorStrategyBackoffSupervisorStrategy. It is not possible to use the normal supervision wrapping for this as it isn’t valid to resume a behavior on a journal failure as it is not known if the event was persisted.\nScala copysourceEventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .onPersistFailure(\n    SupervisorStrategy.restartWithBackoff(minBackoff = 10.seconds, maxBackoff = 60.seconds, randomFactor = 0.1)) Java copysourcepublic class MyPersistentBehavior\n    extends EventSourcedBehavior<\n        MyPersistentBehavior.Command, MyPersistentBehavior.Event, MyPersistentBehavior.State> {\n\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return new MyPersistentBehavior(persistenceId);\n  }\n\n  private MyPersistentBehavior(PersistenceId persistenceId) {\n    super(\n        persistenceId,\n        SupervisorStrategy.restartWithBackoff(\n            Duration.ofSeconds(10), Duration.ofSeconds(30), 0.2));\n  }\n\n}\nIf there is a problem with recovering the state of the actor from the journal, a RecoveryFailedRecoveryFailed signal is emitted to the receiveSignal handler receiveSignal method and the actor will be stopped (or restarted with backoff).","title":"Journal failures"},{"location":"/typed/persistence.html#journal-rejections","text":"Journals can reject events. The difference from a failure is that the journal must decide to reject an event before trying to persist it e.g. because of a serialization exception. If an event is rejected it definitely won’t be in the journal. This is signalled to an EventSourcedBehaviorEventSourcedBehavior via an EventRejectedExceptionEventRejectedException and can be handled with a supervisor. Not all journal implementations use rejections and treat these kind of problems also as journal failures.","title":"Journal rejections"},{"location":"/typed/persistence.html#stash","text":"When persisting events with persistpersist it is guaranteed that the EventSourcedBehaviorEventSourcedBehavior will not receive further commands until after the events have been confirmed to be persisted and additional side effects have been run. Incoming messages are stashed automatically until the persist is completed.\nCommands are also stashed during recovery and will not interfere with replayed events. Commands will be received when recovery has been completed.\nThe stashing described above is handled automatically, but there is also a possibility to stash commands when they are received to defer processing of them until later. One example could be waiting for some external condition or interaction to complete before processing additional commands. That is accomplished by returning a stashstash effect and later use thenUnstashAllthenUnstashAll.\nLet’s use an example of a task manager to illustrate how the stashing effects can be used. It handles three commands; StartTask, NextStep and EndTask. Those commands are associated with a given taskId and the manager processes one taskId at a time. A task is started when receiving StartTask, and continues when receiving NextStep commands until the final EndTask is received. Commands with another taskId than the one in progress are deferred by stashing them. When EndTask is processed a new task can start and the stashed commands are processed.\nScala copysourceobject TaskManager {\n\n  sealed trait Command\n  final case class StartTask(taskId: String) extends Command\n  final case class NextStep(taskId: String, instruction: String) extends Command\n  final case class EndTask(taskId: String) extends Command\n\n  sealed trait Event\n  final case class TaskStarted(taskId: String) extends Event\n  final case class TaskStep(taskId: String, instruction: String) extends Event\n  final case class TaskCompleted(taskId: String) extends Event\n\n  final case class State(taskIdInProgress: Option[String])\n\n  def apply(persistenceId: PersistenceId): Behavior[Command] =\n    EventSourcedBehavior[Command, Event, State](\n      persistenceId = persistenceId,\n      emptyState = State(None),\n      commandHandler = (state, command) => onCommand(state, command),\n      eventHandler = (state, event) => applyEvent(state, event))\n      .onPersistFailure(SupervisorStrategy.restartWithBackoff(1.second, 30.seconds, 0.2))\n\n  private def onCommand(state: State, command: Command): Effect[Event, State] = {\n    state.taskIdInProgress match {\n      case None =>\n        command match {\n          case StartTask(taskId) =>\n            Effect.persist(TaskStarted(taskId))\n          case _ =>\n            Effect.unhandled\n        }\n\n      case Some(inProgress) =>\n        command match {\n          case StartTask(taskId) =>\n            if (inProgress == taskId)\n              Effect.none // duplicate, already in progress\n            else\n              // other task in progress, wait with new task until later\n              Effect.stash()\n\n          case NextStep(taskId, instruction) =>\n            if (inProgress == taskId)\n              Effect.persist(TaskStep(taskId, instruction))\n            else\n              // other task in progress, wait with new task until later\n              Effect.stash()\n\n          case EndTask(taskId) =>\n            if (inProgress == taskId)\n              Effect.persist(TaskCompleted(taskId)).thenUnstashAll() // continue with next task\n            else\n              // other task in progress, wait with new task until later\n              Effect.stash()\n        }\n    }\n  }\n\n  private def applyEvent(state: State, event: Event): State = {\n    event match {\n      case TaskStarted(taskId) => State(Option(taskId))\n      case TaskStep(_, _)      => state\n      case TaskCompleted(_)    => State(None)\n    }\n  }\n} Java copysourcepublic class TaskManager\n    extends EventSourcedBehavior<TaskManager.Command, TaskManager.Event, TaskManager.State> {\n\n  public interface Command {}\n\n  public static final class StartTask implements Command {\n    public final String taskId;\n\n    public StartTask(String taskId) {\n      this.taskId = taskId;\n    }\n  }\n\n  public static final class NextStep implements Command {\n    public final String taskId;\n    public final String instruction;\n\n    public NextStep(String taskId, String instruction) {\n      this.taskId = taskId;\n      this.instruction = instruction;\n    }\n  }\n\n  public static final class EndTask implements Command {\n    public final String taskId;\n\n    public EndTask(String taskId) {\n      this.taskId = taskId;\n    }\n  }\n\n  public interface Event {}\n\n  public static final class TaskStarted implements Event {\n    public final String taskId;\n\n    public TaskStarted(String taskId) {\n      this.taskId = taskId;\n    }\n  }\n\n  public static final class TaskStep implements Event {\n    public final String taskId;\n    public final String instruction;\n\n    public TaskStep(String taskId, String instruction) {\n      this.taskId = taskId;\n      this.instruction = instruction;\n    }\n  }\n\n  public static final class TaskCompleted implements Event {\n    public final String taskId;\n\n    public TaskCompleted(String taskId) {\n      this.taskId = taskId;\n    }\n  }\n\n  public static class State {\n    public final Optional<String> taskIdInProgress;\n\n    public State(Optional<String> taskIdInProgress) {\n      this.taskIdInProgress = taskIdInProgress;\n    }\n  }\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return new TaskManager(persistenceId);\n  }\n\n  public TaskManager(PersistenceId persistenceId) {\n    super(\n        persistenceId,\n        SupervisorStrategy.restartWithBackoff(\n            Duration.ofSeconds(1), Duration.ofSeconds(30), 0.2));\n  }\n\n  @Override\n  public State emptyState() {\n    return new State(Optional.empty());\n  }\n\n  @Override\n  public CommandHandler<Command, Event, State> commandHandler() {\n    return newCommandHandlerBuilder()\n        .forAnyState()\n        .onCommand(StartTask.class, this::onStartTask)\n        .onCommand(NextStep.class, this::onNextStep)\n        .onCommand(EndTask.class, this::onEndTask)\n        .build();\n  }\n\n  private Effect<Event, State> onStartTask(State state, StartTask command) {\n    if (state.taskIdInProgress.isPresent()) {\n      if (state.taskIdInProgress.get().equals(command.taskId))\n        return Effect().none(); // duplicate, already in progress\n      else return Effect().stash(); // other task in progress, wait with new task until later\n    } else {\n      return Effect().persist(new TaskStarted(command.taskId));\n    }\n  }\n\n  private Effect<Event, State> onNextStep(State state, NextStep command) {\n    if (state.taskIdInProgress.isPresent()) {\n      if (state.taskIdInProgress.get().equals(command.taskId))\n        return Effect().persist(new TaskStep(command.taskId, command.instruction));\n      else return Effect().stash(); // other task in progress, wait with new task until later\n    } else {\n      return Effect().unhandled();\n    }\n  }\n\n  private Effect<Event, State> onEndTask(State state, EndTask command) {\n    if (state.taskIdInProgress.isPresent()) {\n      if (state.taskIdInProgress.get().equals(command.taskId))\n        return Effect()\n            .persist(new TaskCompleted(command.taskId))\n            .thenUnstashAll(); // continue with next task\n      else return Effect().stash(); // other task in progress, wait with new task until later\n    } else {\n      return Effect().unhandled();\n    }\n  }\n\n  @Override\n  public EventHandler<State, Event> eventHandler() {\n    return newEventHandlerBuilder()\n        .forAnyState()\n        .onEvent(TaskStarted.class, (state, event) -> new State(Optional.of(event.taskId)))\n        .onEvent(TaskStep.class, (state, event) -> state)\n        .onEvent(TaskCompleted.class, (state, event) -> new State(Optional.empty()))\n        .build();\n  }\n}\nYou should be careful to not send more messages to a persistent actor than it can keep up with, otherwise the stash buffer will fill up and when reaching its maximum capacity the commands will be dropped. The capacity can be configured with:\npekko.persistence.typed.stash-capacity = 10000\nNote that the stashed commands are kept in an in-memory buffer, so in case of a crash they will not be processed.\nStashed commands are discarded in case the actor (entity) is passivated or rebalanced by Cluster Sharding. Stashed commands are discarded in case the actor is restarted (or stopped) due to a thrown exception while processing a command or side effect after persisting. Stashed commands are preserved and processed later in case of a failure while storing events but only if an onPersistFailure backoff supervisor strategy is defined.\nIt’s allowed to stash messages while unstashing. Those newly added commands will not be processed by the unstashAllunstashAll effect that was in progress and have to be unstashed by another unstashAll.","title":"Stash"},{"location":"/typed/persistence.html#scaling-out","text":"In a use case where the number of persistent actors needed is higher than what would fit in the memory of one node or where resilience is important so that if a node crashes the persistent actors are quickly started on a new node and can resume operations Cluster Sharding is an excellent fit to spread persistent actors over a cluster and address them by id.\nPekko Persistence is based on the single-writer principle. For a particular PersistenceIdPersistenceId only one EventSourcedBehaviorEventSourcedBehavior instance should be active at one time. If multiple instances were to persist events at the same time, the events would be interleaved and might not be interpreted correctly on replay. Cluster Sharding ensures that there is only one active entity (EventSourcedBehavior) for each id within a data center. Replicated Event Sourcing supports active-active persistent entities across data centers.","title":"Scaling out"},{"location":"/typed/persistence.html#configuration","text":"There are several configuration properties for the persistence module, please refer to the reference configuration.\nThe journal and snapshot store plugins have specific configuration, see reference documentation of the chosen plugin.","title":"Configuration"},{"location":"/typed/persistence.html#example-project","text":"Persistence example project Persistence example project is an example project that can be downloaded, and with instructions of how to run. This project contains a Shopping Cart sample illustrating how to use Pekko Persistence.\nThe Shopping Cart sample is expanded further in the Microservices with Pekko tutorial. In that sample the events are tagged to be consumed by even processors to build other representations from the events, or publish the events to other services.\nMulti-DC Persistence example project Multi-DC Persistence example project illustrates how to use Replicated Event Sourcing that supports active-active persistent entities across data centers.","title":"Example project"},{"location":"/typed/replicated-eventsourcing.html","text":"","title":"Replicated Event Sourcing"},{"location":"/typed/replicated-eventsourcing.html#replicated-event-sourcing","text":"Event Sourcing with EventSourcedBehaviors is based on the single writer principle, which means that there can only be one active instance of a EventSourcedBehavior with a given persistenceId. Otherwise, multiple instances would store interleaving events based on different states, and when these events would later be replayed it would not be possible to reconstruct the correct state.\nThis restriction means that in the event of network partitions, and for a short time during rolling re-deploys, some EventSourcedBehavior actors are unavailable.\nReplicated Event Sourcing enables running multiple replicas of each entity. There is automatic replication of every event persisted to all replicas.\nFor instance, a replica can be run per:\nData Center Availability zone or rack\nThe motivations are:\nRedundancy to tolerate failures in one location and still be operational Serve requests from a location near the user to provide better responsiveness Balance the load over many servers\nHowever, the event handler must be able to handle concurrent events as when replication is enabled the single-writer guarantee is not maintained like it is with a normal EventSourcedBehavior.\nThe state of a replicated EventSourcedBehavior is eventually consistent. Event replication may be delayed due to network partitions and outages, which means that the event handler and those reading the state must be designed to handle this.\nTo be able to use Replicated Event Sourcing the journal and snapshot store used is required to have specific support for the metadata that the replication needs (see Journal Support).","title":"Replicated Event Sourcing"},{"location":"/typed/replicated-eventsourcing.html#relaxing-the-single-writer-principle-for-availability","text":"Taking the example of using Replicated Event Sourcing to run a replica per data center.\nWhen there is no network partitions and no concurrent writes the events stored by an EventSourcedBehavior at one replica can be replicated and consumed by another (corresponding) replica in another data center without any concerns. Such replicated events can simply be applied to the local state.\nThe interesting part begins when there are concurrent writes by EventSourcedBehavior replicas. That is more likely to happen when there is a network partition, but it can also happen when there are no network issues. They simply write at the “same time” before the events from the other side have been replicated and consumed.\nThe event handler logic for applying events to the state of the entity must be aware of that such concurrent updates can occur, and it must be modeled to handle such conflicts. This means that it should typically have the same characteristics as a Conflict Free Replicated Data Type (CRDT). With a CRDT there are by definition no conflicts, the events can always be applied. The library provides some general purpose CRDTs, but the logic of how to apply events can also be defined by an application specific function.\nFor example, sometimes it’s enough to use application specific timestamps to decide which update should win.\nTo assist in implementing the event handler the Replicated Event Sourcing detects these conflicts.","title":"Relaxing the single-writer principle for availability"},{"location":"/typed/replicated-eventsourcing.html#api","text":"The same API as regular EventSourcedBehaviorsA very similar API to the regular EventSourcedBehavior is used to define the logic.\nTo enable an entity for Replicated Event Sourcing let it extend ReplicatedEventSourcedBehavior instead of EventSourcedBehavior and use the factory methods on org.apache.pekko.persistence.typed.scaladsl.ReplicatedEventSourcingorg.apache.pekko.persistence.typed.javadsl.ReplicatedEventSourcing.\nAll replicas need to be known up front:\nScala copysourceval DCA = ReplicaId(\"DC-A\")\nval DCB = ReplicaId(\"DC-B\")\nval AllReplicas = Set(DCA, DCB) Java copysourcepublic static final ReplicaId DCA = new ReplicaId(\"DCA\");\npublic static final ReplicaId DCB = new ReplicaId(\"DCB\");\n\npublic static final Set<ReplicaId> ALL_REPLICAS =\n    Collections.unmodifiableSet(new HashSet<>(Arrays.asList(DCA, DCB)));\nThen to enable replication create the event sourced behavior with the factory method:\nScala copysourcedef apply(\n    system: ActorSystem[_],\n    entityId: String,\n    replicaId: ReplicaId): EventSourcedBehavior[Command, State, Event] = {\n  ReplicatedEventSourcing.perReplicaJournalConfig(\n    ReplicationId(\"MyReplicatedEntity\", entityId, replicaId),\n    Map(DCA -> \"journalForDCA\", DCB -> \"journalForDCB\")) { replicationContext =>\n    EventSourcedBehavior[Command, State, Event](???, ???, ???, ???)\n  }\n}\n Java copysourcepublic class MyReplicatedBehavior\n    extends ReplicatedEventSourcedBehavior<\n        MyReplicatedBehavior.Command, MyReplicatedBehavior.Event, MyReplicatedBehavior.State> {\n  public static Behavior<Command> create(String entityId, ReplicaId replicaId) {\n    Map<ReplicaId, String> allReplicasAndQueryPlugins = new HashMap<>();\n    allReplicasAndQueryPlugins.put(DCA, \"journalForDCA\");\n    allReplicasAndQueryPlugins.put(DCB, \"journalForDCB\");\n\n    return ReplicatedEventSourcing.perReplicaJournalConfig(\n        new ReplicationId(\"MyReplicatedEntity\", entityId, replicaId),\n        allReplicasAndQueryPlugins,\n        MyReplicatedBehavior::new);\n  }\n\n  private MyReplicatedBehavior(ReplicationContext replicationContext) {\n    super(replicationContext);\n  }\nThe factory takes in:\nentityId: this will be used as part of the underlying persistenceId replicaId: Which replica this instance is allReplicasAndQueryPlugins: All Replicas and the query plugin used to read their events A factory function to create an instance of the EventSourcedBehaviorReplicatedEventSourcedBehavior\nIn this scenario each replica reads from each other’s database effectively providing cross region replication for any database that has a Pekko Persistence plugin. Alternatively if all the replicas use the same journal, e.g. for testing or if it is a distributed database such as Cassandra, the withSharedJournal factory can be used.\nScala copysourcedef apply(\n    system: ActorSystem[_],\n    entityId: String,\n    replicaId: ReplicaId): EventSourcedBehavior[Command, State, Event] = {\n  ReplicatedEventSourcing.commonJournalConfig(\n    ReplicationId(\"MyReplicatedEntity\", entityId, replicaId),\n    AllReplicas,\n    queryPluginId) { replicationContext =>\n    EventSourcedBehavior[Command, State, Event](???, ???, ???, ???)\n  }\n} Java copysourcepublic static Behavior<Command> create(\n    String entityId, ReplicaId replicaId, String queryPluginId) {\n  return ReplicatedEventSourcing.commonJournalConfig(\n      new ReplicationId(\"MyReplicatedEntity\", entityId, replicaId),\n      ALL_REPLICAS,\n      queryPluginId,\n      MyReplicatedBehavior::new);\n}\nThe function passed to both factory methods return an EventSourcedBehavior and provide access to the ReplicationContextReplicationContext that has the following methods: entityId replicaId allReplicas persistenceId - to provide to the EventSourcedBehavior factory. This must be used. As well as methods that can only be used in the event handler. The values these methods return relate to the event that is being processed.\nThe function passed to both factory methods is invoked with a special ReplicationContextReplicationContext that needs to be passed to the concrete ReplicatedEventSourcedBehavior and on to the super constructor. The context gives access to: entityId replicaId allReplicas persistenceId As well as methods that can only be used in the event handler, accessed through getReplicationContext. The values these methods return relate to the event that is being processed.\norigin: The ReplicaId that originally created the event concurrent: Whether the event was concurrent with another event as in the second diagram above recoveryRunning: Whether a recovery is running. Can be used to send commands back to self for side effects that should only happen once. currentTimeMillis: similar to System.currentTimeMillis but guaranteed never to go backwards\nThe factory returns a Behavior that can be spawned like any other behavior.","title":"API"},{"location":"/typed/replicated-eventsourcing.html#resolving-conflicting-updates","text":"","title":"Resolving conflicting updates"},{"location":"/typed/replicated-eventsourcing.html#conflict-free-replicated-data-types","text":"Writing code to resolve conflicts can be complicated to get right. One well-understood technique to create eventually-consistent systems is to model your state as a Conflict Free Replicated Data Type, a CRDT. There are two types of CRDTs; operation-based and state-based. For Replicated Event Sourcing the operation-based is a good fit, since the events represent the operations. Note that this is distinct from the CRDT’s implemented in Pekko Distributed Data, which are state-based rather than operation-based.\nThe rule for operation-based CRDT’s is that the operations must be commutative — in other words, applying the same events (which represent the operations) in any order should always produce the same final state. You may assume each event is applied only once, with causal delivery order.\nThe following CRDTs are included that can you can use as the state or part of the state in the entity:\nLwwTimeLwwTime CounterCounter ORSetORSet\nPekko serializers are included for all these types and can be used to serialize when embedded in Jackson.\nAn example would be a movies watch list that is represented by the general purpose ORSetORSet CRDT. ORSet is short for Observed Remove Set. Elements can be added and removed any number of times. Concurrent add wins over remove. It is an operation based CRDT where the delta of an operation (add/remove) can be represented as an event.\nSuch movies watch list example:\nScala copysourceobject MovieWatchList {\n  sealed trait Command\n  final case class AddMovie(movieId: String) extends Command\n  final case class RemoveMovie(movieId: String) extends Command\n  final case class GetMovieList(replyTo: ActorRef[MovieList]) extends Command\n  final case class MovieList(movieIds: Set[String])\n\n  def apply(entityId: String, replicaId: ReplicaId, allReplicaIds: Set[ReplicaId]): Behavior[Command] = {\n    ReplicatedEventSourcing.commonJournalConfig(\n      ReplicationId(\"movies\", entityId, replicaId),\n      allReplicaIds,\n      PersistenceTestKitReadJournal.Identifier) { replicationContext =>\n      EventSourcedBehavior[Command, ORSet.DeltaOp, ORSet[String]](\n        replicationContext.persistenceId,\n        ORSet.empty(replicationContext.replicaId),\n        (state, cmd) => commandHandler(state, cmd),\n        (state, event) => eventHandler(state, event))\n    }\n  }\n\n  private def commandHandler(state: ORSet[String], cmd: Command): Effect[ORSet.DeltaOp, ORSet[String]] = {\n    cmd match {\n      case AddMovie(movieId) =>\n        Effect.persist(state + movieId)\n      case RemoveMovie(movieId) =>\n        Effect.persist(state - movieId)\n      case GetMovieList(replyTo) =>\n        replyTo ! MovieList(state.elements)\n        Effect.none\n    }\n  }\n\n  private def eventHandler(state: ORSet[String], event: ORSet.DeltaOp): ORSet[String] = {\n    state.applyOperation(event)\n  }\n\n} Java copysourcepublic final class MovieWatchList\n    extends ReplicatedEventSourcedBehavior<MovieWatchList.Command, ORSet.DeltaOp, ORSet<String>> {\n\n  interface Command {}\n\n  public static class AddMovie implements Command {\n    public final String movieId;\n\n    public AddMovie(String movieId) {\n      this.movieId = movieId;\n    }\n  }\n\n  public static class RemoveMovie implements Command {\n    public final String movieId;\n\n    public RemoveMovie(String movieId) {\n      this.movieId = movieId;\n    }\n  }\n\n  public static class GetMovieList implements Command {\n    public final ActorRef<MovieList> replyTo;\n\n    public GetMovieList(ActorRef<MovieList> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class MovieList {\n    public final Set<String> movieIds;\n\n    public MovieList(Set<String> movieIds) {\n      this.movieIds = Collections.unmodifiableSet(movieIds);\n    }\n  }\n\n  public static Behavior<Command> create(\n      String entityId, ReplicaId replicaId, Set<ReplicaId> allReplicas) {\n    return ReplicatedEventSourcing.commonJournalConfig(\n        new ReplicationId(\"movies\", entityId, replicaId),\n        allReplicas,\n        PersistenceTestKitReadJournal.Identifier(),\n        MovieWatchList::new);\n  }\n\n  private MovieWatchList(ReplicationContext replicationContext) {\n    super(replicationContext);\n  }\n\n  @Override\n  public ORSet<String> emptyState() {\n    return ORSet.empty(getReplicationContext().replicaId());\n  }\n\n  @Override\n  public CommandHandler<Command, ORSet.DeltaOp, ORSet<String>> commandHandler() {\n    return newCommandHandlerBuilder()\n        .forAnyState()\n        .onCommand(\n            AddMovie.class, (state, command) -> Effect().persist(state.add(command.movieId)))\n        .onCommand(\n            RemoveMovie.class,\n            (state, command) -> Effect().persist(state.remove(command.movieId)))\n        .onCommand(\n            GetMovieList.class,\n            (state, command) -> {\n              command.replyTo.tell(new MovieList(state.getElements()));\n              return Effect().none();\n            })\n        .build();\n  }\n\n  @Override\n  public EventHandler<ORSet<String>, ORSet.DeltaOp> eventHandler() {\n    return newEventHandlerBuilder().forAnyState().onAnyEvent(ORSet::applyOperation);\n  }\n}\nThe Auction example is a more comprehensive example that illustrates how application-specific rules can be used to implement an entity with CRDT semantics.","title":"Conflict free replicated data types"},{"location":"/typed/replicated-eventsourcing.html#last-writer-wins","text":"Sometimes, it is enough to use timestamps to decide which update should win. Such approach relies on synchronized clocks, and clocks of different machines will always be slightly out of sync. Timestamps should therefore only be used when the choice of value is not important for concurrent updates occurring within the clock skew.\nIn general, last writer wins means that the event is used if the timestamp of the event is later (higher) than the timestamp of previous local update, otherwise the event is discarded. There is no built-in support for last writer wins, because it must often be combined with more application specific aspects.\nThere is a small utility class LwwTimeLwwTime that can be useful for implementing last writer wins semantics. It contains a timestamp representing current time when the event was persisted and an identifier of the replica that persisted it. When comparing two LwwTimeLwwTime the greatest timestamp wins. The replica identifier is used if the two timestamps are equal, and then the one from the replicaId sorted first in alphanumeric order wins.\nScala copysourceprivate def eventHandler(\n    ctx: ActorContext[Command],\n    replicationContext: ReplicationContext,\n    state: BlogState,\n    event: Event): BlogState = {\n  ctx.log.info(s\"${replicationContext.entityId}:${replicationContext.replicaId} Received event $event\")\n  event match {\n    case PostAdded(_, content, timestamp) =>\n      if (timestamp.isAfter(state.contentTimestamp)) {\n        val s = state.withContent(content, timestamp)\n        ctx.log.info(\"Updating content. New content is {}\", s)\n        s\n      } else {\n        ctx.log.info(\"Ignoring event as timestamp is older\")\n        state\n      }\n    case BodyChanged(_, newContent, timestamp) =>\n      if (timestamp.isAfter(state.contentTimestamp))\n        state.withContent(newContent, timestamp)\n      else state\n    case Published(_) =>\n      state.copy(published = true)\n  }\n} Java copysource@Override\npublic EventHandler<BlogState, Event> eventHandler() {\n  return newEventHandlerBuilder()\n      .forAnyState()\n      .onEvent(PostAdded.class, this::onPostAdded)\n      .onEvent(BodyChanged.class, this::onBodyChanged)\n      .onEvent(Published.class, this::onPublished)\n      .build();\n}\n\nprivate BlogState onPostAdded(BlogState state, PostAdded event) {\n  if (event.timestamp.isAfter(state.contentTimestamp)) {\n    BlogState s = state.withContent(event.content, event.timestamp);\n    context.getLog().info(\"Updating content. New content is {}\", s);\n    return s;\n  } else {\n    context.getLog().info(\"Ignoring event as timestamp is older\");\n    return state;\n  }\n}\n\nprivate BlogState onBodyChanged(BlogState state, BodyChanged event) {\n  if (event.timestamp.isAfter(state.contentTimestamp)) {\n    return state.withContent(event.content, event.timestamp);\n  } else {\n    return state;\n  }\n}\n\nprivate BlogState onPublished(BlogState state, Published event) {\n  return state.publish();\n}\nWhen creating the LwwTime it is good to have a monotonically increasing timestamp, and for that the increase method in LwwTime can be used:\nScala copysourceprivate def commandHandler(\n    ctx: ActorContext[Command],\n    replicationContext: ReplicationContext,\n    state: BlogState,\n    cmd: Command): Effect[Event, BlogState] = {\n  cmd match {\n    case AddPost(_, content, replyTo) =>\n      val evt =\n        PostAdded(\n          replicationContext.entityId,\n          content,\n          state.contentTimestamp.increase(replicationContext.currentTimeMillis(), replicationContext.replicaId))\n      Effect.persist(evt).thenRun { _ =>\n        replyTo ! AddPostDone(replicationContext.entityId)\n      }\n    case ChangeBody(_, newContent, replyTo) =>\n      val evt =\n        BodyChanged(\n          replicationContext.entityId,\n          newContent,\n          state.contentTimestamp.increase(replicationContext.currentTimeMillis(), replicationContext.replicaId))\n      Effect.persist(evt).thenRun { _ =>\n        replyTo ! Done\n      }\n    case p: Publish =>\n      Effect.persist(Published(\"id\")).thenRun { _ =>\n        p.replyTo ! Done\n      }\n    case gp: GetPost =>\n      ctx.log.info(\"GetPost {}\", state.content)\n      state.content.foreach(content => gp.replyTo ! content)\n      Effect.none\n  }\n} Java copysource@Override\npublic CommandHandler<Command, Event, BlogState> commandHandler() {\n  return newCommandHandlerBuilder()\n      .forAnyState()\n      .onCommand(AddPost.class, this::onAddPost)\n      .onCommand(ChangeBody.class, this::onChangeBody)\n      .onCommand(Publish.class, this::onPublish)\n      .onCommand(GetPost.class, this::onGetPost)\n      .build();\n}\n\nprivate Effect<Event, BlogState> onAddPost(BlogState state, AddPost command) {\n  PostAdded evt =\n      new PostAdded(\n          getReplicationContext().entityId(),\n          command.content,\n          state.contentTimestamp.increase(\n              getReplicationContext().currentTimeMillis(),\n              getReplicationContext().replicaId()));\n  return Effect()\n      .persist(evt)\n      .thenRun(() -> command.replyTo.tell(new AddPostDone(getReplicationContext().entityId())));\n}\n\nprivate Effect<Event, BlogState> onChangeBody(BlogState state, ChangeBody command) {\n  BodyChanged evt =\n      new BodyChanged(\n          getReplicationContext().entityId(),\n          command.newContent,\n          state.contentTimestamp.increase(\n              getReplicationContext().currentTimeMillis(),\n              getReplicationContext().replicaId()));\n  return Effect().persist(evt).thenRun(() -> command.replyTo.tell(Done.getInstance()));\n}\n\nprivate Effect<Event, BlogState> onPublish(BlogState state, Publish command) {\n  Published evt = new Published(getReplicationContext().entityId());\n  return Effect().persist(evt).thenRun(() -> command.replyTo.tell(Done.getInstance()));\n}\n\nprivate Effect<Event, BlogState> onGetPost(BlogState state, GetPost command) {\n  context.getLog().info(\"GetPost {}\", state.content);\n  if (state.content.isPresent()) command.replyTo.tell(state.content.get());\n  return Effect().none();\n}\nThe nature of last writer wins means that if you only have one timestamp for the state the events must represent an update of the full state. Otherwise, there is a risk that the state in different replicas will be different and not eventually converge.\nAn example of that would be an entity representing a blog post and the fields author and title could be updated separately with events AuthorChanged(newAuthor: String)new AuthorChanged(newAuthor) and TitleChanged(newTitle: String)new TitleChanged(newTitle).\nLet’s say the blog post is created and the initial state of title=Pekko, author=unknown is in sync in both replicas DC-A and `DC-B.\nIn DC-A author is changed to “Bob” at time 100. Before that event has been replicated over to DC-B the title is updated to “Pekko News” at time 101 in DC-B. When the events have been replicated the result will be:\nDC-A: The title update is later so the event is used and new state is title=Pekko News, author=Bob\nDC-B: The author update is earlier so the event is discarded and state is title=Pekko News, author=unknown\nThe problem here is that the partial update of the state is not applied on both sides, so the states have diverged and will not become the same.\nTo solve this with last writer wins the events must carry the full state, such as AuthorChanged(newContent: PostContent)new AuthorChanged(newContent) and TitleChanged(newContent: PostContent)new TitleChanged(newContent). Then the result would eventually be title=Pekko News, author=unknown on both sides. The author update is lost but that is because the changes were performed concurrently. More important is that the state is eventually consistent.\nIncluding the full state in each event is often not desired. An event typically represent a change, a delta. Then one can use several timestamps, one for each set of fields that can be updated together. In the above example one could use one timestamp for the title and another for the author. Then the events could represent changes to parts of the full state, such as AuthorChanged(newAuthor: String)new AuthorChanged(newAuthor) and TitleChanged(newTitle: String)new TitleChanged(newTitle).","title":"Last writer wins"},{"location":"/typed/replicated-eventsourcing.html#side-effects","text":"In most cases it is recommended to do side effects as described for EventSourcedBehaviors.\nSide effects from the event handler are generally discouraged because the event handlers are also used during replay and when consuming replicated events and that would result in undesired re-execution of the side effects.\nUses cases for doing side effects in the event handler:\nDoing a side effect only in a single replica Doing a side effect once all replicas have seen an event A side effect for a replicated event A side effect when a conflict has occurred\nThere is no built in support for knowing an event has been replicated to all replicas but it can be modelled in your state. For some use cases you may need to trigger side effects after consuming replicated events. For example when an auction has been closed in all data centers and all bids have been replicated.\nThe contains the current replica, the origin replica for the event processes, and if a recovery is running. These can be used to implement side effects that take place once events are fully replicated. If the side effect should happen only once then a particular replica can be designated to do it. The Auction example uses these techniques.","title":"Side effects"},{"location":"/typed/replicated-eventsourcing.html#how-it-works","text":"You don’t have to read this section to be able to use the feature, but to use the abstraction efficiently and for the right type of use cases it can be good to understand how it’s implemented. For example, it should give you the right expectations of the overhead that the solution introduces compared to using just EventSourcedBehaviors.","title":"How it works"},{"location":"/typed/replicated-eventsourcing.html#causal-delivery-order","text":"Causal delivery order means that events persisted in one replica are read in the same order in other replicas. The order of concurrent events is undefined, which should be no problem when using CRDT’s and otherwise will be detected via the ReplicationContext concurrent method.\nFor example:\nDC-1: write e1\nDC-2: read e1, write e2\nDC-1: read e2, write e3\nIn the above example the causality is e1 -> e2 -> e3. Also in a third replica DC-3 these events will be read in the same order e1, e2, e3.\nAnother example with concurrent events:\nDC1: write e1\nDC2: read e1, write e2\nDC1: write e3 (e2 and e3 are concurrent)\nDC1: read e2\nDC2: read e3\ne2 and e3 are concurrent, i.e. they don’t have a causal relation: DC1 sees them in the order “e1, e3, e2”, while DC2 sees them as “e1, e2, e3”.\nA third replica may also see the events as either “e1, e3, e2” or as “e1, e2, e3”.","title":"Causal delivery order"},{"location":"/typed/replicated-eventsourcing.html#concurrent-updates","text":"Replicated Event Sourcing automatically tracks causality between events from different replicas using version vectors.\nEach replica “owns” a slot in the version vector and increases its counter when an event is persisted. The version vector is stored with the event, and when a replicated event is consumed the version vector of the event is merged with the local version vector.\nWhen comparing two version vectors v1 and v2:\nv1 is SAME as v2 iff for all i v1(i) == v2(i) v1is BEFORE v2 iff for all i v1(i) <= v2(i) and there exist a j such that v1(j) < v2(j) v1is AFTER v2 iff for all i v1(i) >= v2(i) and there exist a j such that v1(j) > v2(j) v1is CONCURRENT with v2 otherwise","title":"Concurrent updates"},{"location":"/typed/replicated-eventsourcing.html#sharded-replicated-event-sourced-entities","text":"There are three ways to integrate replicated event sourced entities with sharding:\nEnsure that each replica has a unique entity id by using the replica id as part of the entity id Use multi datacenter to run a full copy of sharding per replica Use roles to run a full copy of sharding per replica\nTo simplify all three cases the ReplicatedShardingExtensionReplicatedShardingExtension is available from the pekko-cluster-sharding-typed module.\nScala copysourceReplicatedEntityProvider[Command](\"MyEntityType\", Set(ReplicaId(\"DC-A\"), ReplicaId(\"DC-B\"))) {\n  (entityTypeKey, replicaId) =>\n    ReplicatedEntity(replicaId,\n      Entity(entityTypeKey) { entityContext =>\n        // the sharding entity id contains the business entityId, entityType, and replica id\n        // which you'll need to create a ReplicatedEventSourcedBehavior\n        val replicationId = ReplicationId.fromString(entityContext.entityId)\n        MyEventSourcedBehavior(replicationId)\n      })\n} Java copysourcereturn ReplicatedEntityProvider.create(\n    Command.class,\n    \"MyReplicatedType\",\n    ALL_REPLICAS,\n    (entityTypeKey, replicaId) ->\n        ReplicatedEntity.create(\n            replicaId,\n            Entity.of(\n                entityTypeKey,\n                entityContext ->\n                    myEventSourcedBehavior(\n                        ReplicationId.fromString(entityContext.getEntityId())))));\nThis will run an instance of sharding and per replica and each entity id contains the replica id and the type name. Replicas could be on the same node if they end up in the same shard or if the shards get allocated to the same node.\nTo prevent this roles can be used. You could for instance add a cluster role per availability zone / rack and have a replica per rack.\nScala copysourceval provider = ReplicatedEntityProvider.perRole(\"MyEntityType\", Set(ReplicaId(\"DC-A\"), ReplicaId(\"DC-B\"))) {\n  replicationId =>\n    MyEventSourcedBehavior(replicationId)\n} Java copysourcereturn ReplicatedEntityProvider.create(\n    Command.class,\n    \"MyReplicatedType\",\n    ALL_REPLICAS,\n    (entityTypeKey, replicaId) ->\n        ReplicatedEntity.create(\n            replicaId,\n            Entity.of(\n                    entityTypeKey,\n                    entityContext ->\n                        myEventSourcedBehavior(\n                            ReplicationId.fromString(entityContext.getEntityId())))\n                .withRole(replicaId.id())));\nLastly if your Pekko Cluster is setup across DCs you can run a replica per DC.\nScala copysourceReplicatedEntityProvider.perDataCenter(\"MyEntityType\", Set(ReplicaId(\"DC-A\"), ReplicaId(\"DC-B\"))) { replicationId =>\n  MyEventSourcedBehavior(replicationId)\n} Java copysourceReplicatedEntityProvider.create(\n    Command.class,\n    \"MyReplicatedType\",\n    ALL_REPLICAS,\n    (entityTypeKey, replicaId) ->\n        ReplicatedEntity.create(\n            replicaId,\n            Entity.of(\n                    entityTypeKey,\n                    entityContext ->\n                        myEventSourcedBehavior(\n                            ReplicationId.fromString(entityContext.getEntityId())))\n                .withDataCenter(replicaId.id())));\nRegardless of which replication strategy you use sending messages to the replicated entities is the same.\ninit returns an ReplicatedShardingReplicatedSharding instance which gives access to EntityRefEntityRefs for each of the replicas for arbitrary routing logic:\nScala copysourceval myReplicatedSharding: ReplicatedSharding[Command] =\n  ReplicatedShardingExtension(system).init(provider)\n\nval entityRefs: Map[ReplicaId, EntityRef[Command]] = myReplicatedSharding.entityRefsFor(\"myEntityId\") Java copysourceReplicatedShardingExtension extension = ReplicatedShardingExtension.get(system);\n\nReplicatedSharding<Command> replicatedSharding = extension.init(provider());\n\nMap<ReplicaId, EntityRef<Command>> myEntityId =\n    replicatedSharding.getEntityRefsFor(\"myEntityId\");\nMore advanced routing among the replicas is currently left as an exercise for the reader.","title":"Sharded Replicated Event Sourced entities"},{"location":"/typed/replicated-eventsourcing.html#tagging-events-and-running-projections","text":"Just like for regular EventSourcedBehaviors it is possible to tag events along with persisting them. This is useful for later retrival of events for a given tag. The same API for tagging provided for EventSourcedBehavior can be used for replicated event sourced behaviors as well. Tagging is useful in practice to build queries that lead to other data representations or aggregations of these event streams that can more directly serve user queries – known as building the “read side” in CQRS based applications.\nCreating read side projections is possible through Pekko Projection or through direct usage of the events by tag queries.\nThe tagging is invoked in each replicas, which requires some special care in using tags, or else the same event will be tagged one time for each replica and show up in the event by tag stream one time for each replica. In addition to this the tags will be written in the respective journal of the replicas, which means that unless they all share a single journal the tag streams will be local to the replica even if the same tag is used on multiple replicas.\nOne strategy for dealing with this is to include the replica id in the tag name, this means there will be a tagged stream of events per replica that contains all replicated events, but since the events can arrive in different order, they can also come in different order per replica tag.\nAnother strategy would be to tag only the events that are local to the replica and not events that are replicated. Either using a tag that will be the same for all replicas, leading to a single stream of tagged events where the events from each replica is present only once, or with a tag including the replica id meaning that there will be a stream of tagged events with the events accepted locally for each replica.\nDetermining the replica id of the replicated actor itself and the origin replica id of an event is possible through the ReplicationContextReplicationContext when the tagger callback is invoked like this:\nScala copysourceReplicatedEventSourcing.commonJournalConfig(\n  ReplicationId(\"TaggingSpec\", entityId, replica),\n  allReplicas,\n  queryPluginId)(replicationContext =>\n  EventSourcedBehavior[Command, String, State](\n    replicationContext.persistenceId,\n    State(Set.empty),\n    (state, command) =>\n      command match {\n        case Add(string, ack) =>\n          if (state.strings.contains(string)) Effect.none.thenRun(_ => ack ! Done)\n          else Effect.persist(string).thenRun(_ => ack ! Done)\n        case GetStrings(replyTo) =>\n          replyTo ! state.strings\n          Effect.none\n      },\n    (state, event) => state.copy(strings = state.strings + event))\n    // use withTagger to define tagging logic\n    .withTagger(event =>\n      // don't apply tags if event was replicated here, it already will appear in queries by tag\n      // as the origin replica would have tagged it already\n      if (replicationContext.origin != replicationContext.replicaId) Set.empty\n      else if (event.length > 10) Set(\"long-strings\", \"strings\")\n      else Set(\"strings\"))) Java copysource@Override\npublic Set<String> tagsFor(String event) {\n  // don't apply tags if event was replicated here, it already will appear in queries by tag\n  // as the origin replica would have tagged it already\n  if (getReplicationContext().replicaId() != getReplicationContext().origin()) {\n    return new HashSet<>();\n  } else {\n    Set<String> tags = new HashSet<>();\n    tags.add(\"strings\");\n    if (event.length() > 10) tags.add(\"long-strings\");\n    return tags;\n  }\n}\nIn this sample we are using a shared journal, and single tag but only tagging local events to it and therefore ending up with a single stream of tagged events from all replicas without duplicates.","title":"Tagging events and running projections"},{"location":"/typed/replicated-eventsourcing.html#direct-replication-of-events","text":"Each replica will read the events from all the other copies from the database. When used with Cluster Sharding, and to make the sharing of events with other replicas more efficient, each replica publishes the events across the Pekko cluster directly to other replicas. The delivery of events across the cluster is not guaranteed so the query to the journal is still needed but can be configured to poll the database less often since most events will arrive at the replicas through the cluster.\nThe direct replication of events feature is enabled by default when using Cluster Sharding. To disable this feature you first need to:\ndisable event publishing on the EventSourcedBehavior with withEventPublishing(false)overriding withEventPublishing from ReplicatedEventSourcedBehavior to return false , and then disable direct replication through withDirectReplication(false) on ReplicatedEntityProviderReplicatedEntityProvider\nThe “event publishing” feature publishes each event to the local system event bus as a side effect after it has been written.","title":"Direct Replication of Events"},{"location":"/typed/replicated-eventsourcing.html#hot-standby","text":"If all writes occur to one replica and the other replicas are not started there might be many replicated events to catch up with when they are later started. Therefore it can be good to activate all replicas when there is some activity.\nThis can be achieved automatically when ReplicatedSharding is used and direct replication of events is enabled as described in Direct Replication of Events. When each written event is forwarded to the other replicas it will trigger them to start if they are not already started.","title":"Hot Standby"},{"location":"/typed/replicated-eventsourcing.html#examples","text":"More examples can be found in Replicated Event Sourcing Examples","title":"Examples"},{"location":"/typed/replicated-eventsourcing.html#journal-support","text":"For a journal plugin to support replication it needs to store and read metadata for each event if it is defined in the metadata field. To attach the metadata after writing it, PersistentRepr.withMetadata is used. The JournalSpecJournalSpec in the Persistence TCK provides a capability flag supportsMetadata to toggle verification that metadata is handled correctly.\nFor a snapshot plugin to support replication it needs to store and read metadata for the snapshot if it is defined in the metadata field. To attach the metadata when reading the snapshot the org.apache.pekko.persistence.SnapshotMetadata.apply factory overload taking a metadata parameter is used. The SnapshotStoreSpecSnapshotStoreSpec in the Persistence TCK provides a capability flag supportsMetadata to toggle verification that metadata is handled correctly.\nThe following plugins support Replicated Event Sourcing:\nPekko Persistence Cassandra versions 1.0.3+ Pekko Persistence Spanner versions 1.0.0-RC4+ Pekko Persistence JDBC versions 5.0.0+","title":"Journal Support"},{"location":"/typed/cqrs.html","text":"","title":"CQRS"},{"location":"/typed/cqrs.html#cqrs","text":"EventSourcedBehaviors along with Pekko Projections can be used to implement Command Query Responsibility Segregation (CQRS). The Microservices with Pekko tutorial explains how to use Event Sourcing and Projections together. For implementing CQRS using DurableStateBehavior, please take a look at the corresponding CQRS documentation.","title":"CQRS"},{"location":"/typed/persistence-style.html","text":"","title":"Style Guide"},{"location":"/typed/persistence-style.html#style-guide","text":"","title":"Style Guide"},{"location":"/typed/persistence-style.html#event-handlers-in-the-state","text":"The section about Changing Behavior described how commands and events can be handled differently depending on the state. One can take that one step further and define the event handler inside the state classes. In the next section the command handlers are also defined in the state.\nThe state can be seen as your domain object and it should contain the core business logic. Then it’s a matter of taste if event handlers and command handlers should be defined in the state or be kept outside of it.\nHere we are using a bank account as the example domain. It has 3 state classes that are representing the lifecycle of the account; EmptyAccount, OpenedAccount, and ClosedAccount.\nScala copysourceobject AccountEntity {\n  // Command\n  sealed trait Command extends CborSerializable\n  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command\n  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n\n  // Reply\n  final case class CurrentBalance(balance: BigDecimal) extends CborSerializable\n\n  // Event\n  sealed trait Event extends CborSerializable\n  case object AccountCreated extends Event\n  case class Deposited(amount: BigDecimal) extends Event\n  case class Withdrawn(amount: BigDecimal) extends Event\n  case object AccountClosed extends Event\n\n  val Zero = BigDecimal(0)\n\n  // State\n  sealed trait Account extends CborSerializable {\n    def applyEvent(event: Event): Account\n  }\n  case object EmptyAccount extends Account {\n    override def applyEvent(event: Event): Account = event match {\n      case AccountCreated => OpenedAccount(Zero)\n      case _              => throw new IllegalStateException(s\"unexpected event [$event] in state [EmptyAccount]\")\n    }\n  }\n  case class OpenedAccount(balance: BigDecimal) extends Account {\n    require(balance >= Zero, \"Account balance can't be negative\")\n\n    override def applyEvent(event: Event): Account =\n      event match {\n        case Deposited(amount) => copy(balance = balance + amount)\n        case Withdrawn(amount) => copy(balance = balance - amount)\n        case AccountClosed     => ClosedAccount\n        case AccountCreated    => throw new IllegalStateException(s\"unexpected event [$event] in state [OpenedAccount]\")\n      }\n\n    def canWithdraw(amount: BigDecimal): Boolean = {\n      balance - amount >= Zero\n    }\n\n  }\n  case object ClosedAccount extends Account {\n    override def applyEvent(event: Event): Account =\n      throw new IllegalStateException(s\"unexpected event [$event] in state [ClosedAccount]\")\n  }\n\n  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:\n  val TypeKey: EntityTypeKey[Command] =\n    EntityTypeKey[Command](\"Account\")\n\n  // Note that after defining command, event and state classes you would probably start here when writing this.\n  // When filling in the parameters of EventSourcedBehavior.apply you can use IntelliJ alt+Enter > createValue\n  // to generate the stub with types for the command and event handlers.\n\n  def apply(accountNumber: String, persistenceId: PersistenceId): Behavior[Command] = {\n    EventSourcedBehavior.withEnforcedReplies(persistenceId, EmptyAccount, commandHandler(accountNumber), eventHandler)\n  }\n\n  private def commandHandler(accountNumber: String): (Account, Command) => ReplyEffect[Event, Account] = {\n    (state, cmd) =>\n      state match {\n        case EmptyAccount =>\n          cmd match {\n            case c: CreateAccount => createAccount(c)\n            case _                => Effect.unhandled.thenNoReply() // CreateAccount before handling any other commands\n          }\n\n        case acc @ OpenedAccount(_) =>\n          cmd match {\n            case c: Deposit      => deposit(c)\n            case c: Withdraw     => withdraw(acc, c)\n            case c: GetBalance   => getBalance(acc, c)\n            case c: CloseAccount => closeAccount(acc, c)\n            case c: CreateAccount =>\n              Effect.reply(c.replyTo)(StatusReply.Error(s\"Account $accountNumber is already created\"))\n          }\n\n        case ClosedAccount =>\n          cmd match {\n            case c: Deposit =>\n              replyClosed(accountNumber, c.replyTo)\n            case c: Withdraw =>\n              replyClosed(accountNumber, c.replyTo)\n            case GetBalance(replyTo) =>\n              Effect.reply(replyTo)(CurrentBalance(Zero))\n            case CloseAccount(replyTo) =>\n              replyClosed(accountNumber, replyTo)\n            case CreateAccount(replyTo) =>\n              replyClosed(accountNumber, replyTo)\n          }\n      }\n  }\n\n  private def replyClosed(\n      accountNumber: String,\n      replyTo: ActorRef[StatusReply[Done]]): ReplyEffect[Event, Account] = {\n    Effect.reply(replyTo)(StatusReply.Error(s\"Account $accountNumber is closed\"))\n  }\n\n  private val eventHandler: (Account, Event) => Account = { (state, event) =>\n    state.applyEvent(event)\n  }\n\n  private def createAccount(cmd: CreateAccount): ReplyEffect[Event, Account] = {\n    Effect.persist(AccountCreated).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n  }\n\n  private def deposit(cmd: Deposit): ReplyEffect[Event, Account] = {\n    Effect.persist(Deposited(cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n  }\n\n  private def withdraw(acc: OpenedAccount, cmd: Withdraw): ReplyEffect[Event, Account] = {\n    if (acc.canWithdraw(cmd.amount))\n      Effect.persist(Withdrawn(cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n    else\n      Effect.reply(cmd.replyTo)(\n        StatusReply.Error(s\"Insufficient balance ${acc.balance} to be able to withdraw ${cmd.amount}\"))\n  }\n\n  private def getBalance(acc: OpenedAccount, cmd: GetBalance): ReplyEffect[Event, Account] = {\n    Effect.reply(cmd.replyTo)(CurrentBalance(acc.balance))\n  }\n\n  private def closeAccount(acc: OpenedAccount, cmd: CloseAccount): ReplyEffect[Event, Account] = {\n    if (acc.balance == Zero)\n      Effect.persist(AccountClosed).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n    else\n      Effect.reply(cmd.replyTo)(StatusReply.Error(\"Can't close account with non-zero balance\"))\n  }\n\n} Java copysourcepublic class AccountEntity\n    extends EventSourcedBehaviorWithEnforcedReplies<\n        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account> {\n\n  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =\n      EntityTypeKey.create(Command.class, \"Account\");\n\n  // Command\n  interface Command extends CborSerializable {}\n\n  public static class CreateAccount implements Command {\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    @JsonCreator\n    public CreateAccount(ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Deposit implements Command {\n    public final BigDecimal amount;\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    public Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n      this.amount = amount;\n    }\n  }\n\n  public static class Withdraw implements Command {\n    public final BigDecimal amount;\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    public Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {\n      this.amount = amount;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class GetBalance implements Command {\n    public final ActorRef<CurrentBalance> replyTo;\n\n    @JsonCreator\n    public GetBalance(ActorRef<CurrentBalance> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class CloseAccount implements Command {\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    @JsonCreator\n    public CloseAccount(ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  // Reply\n  public static class CurrentBalance implements CborSerializable {\n    public final BigDecimal balance;\n\n    @JsonCreator\n    public CurrentBalance(BigDecimal balance) {\n      this.balance = balance;\n    }\n  }\n\n  // Event\n  interface Event extends CborSerializable {}\n\n  public enum AccountCreated implements Event {\n    INSTANCE\n  }\n\n  public static class Deposited implements Event {\n    public final BigDecimal amount;\n\n    @JsonCreator\n    Deposited(BigDecimal amount) {\n      this.amount = amount;\n    }\n  }\n\n  public static class Withdrawn implements Event {\n    public final BigDecimal amount;\n\n    @JsonCreator\n    Withdrawn(BigDecimal amount) {\n      this.amount = amount;\n    }\n  }\n\n  public static class AccountClosed implements Event {}\n\n  // State\n  interface Account extends CborSerializable {}\n\n  public static class EmptyAccount implements Account {\n    OpenedAccount openedAccount() {\n      return new OpenedAccount(BigDecimal.ZERO);\n    }\n  }\n\n  public static class OpenedAccount implements Account {\n    final BigDecimal balance;\n\n    @JsonCreator\n    public OpenedAccount(BigDecimal balance) {\n      this.balance = balance;\n    }\n\n    OpenedAccount makeDeposit(BigDecimal amount) {\n      return new OpenedAccount(balance.add(amount));\n    }\n\n    boolean canWithdraw(BigDecimal amount) {\n      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);\n    }\n\n    OpenedAccount makeWithdraw(BigDecimal amount) {\n      if (!canWithdraw(amount))\n        throw new IllegalStateException(\"Account balance can't be negative\");\n      return new OpenedAccount(balance.subtract(amount));\n    }\n\n    ClosedAccount closedAccount() {\n      return new ClosedAccount();\n    }\n  }\n\n  public static class ClosedAccount implements Account {}\n\n  public static AccountEntity create(String accountNumber, PersistenceId persistenceId) {\n    return new AccountEntity(accountNumber, persistenceId);\n  }\n\n  private final String accountNumber;\n\n  private AccountEntity(String accountNumber, PersistenceId persistenceId) {\n    super(persistenceId);\n    this.accountNumber = accountNumber;\n  }\n\n  @Override\n  public Account emptyState() {\n    return new EmptyAccount();\n  }\n\n  @Override\n  public CommandHandlerWithReply<Command, Event, Account> commandHandler() {\n    CommandHandlerWithReplyBuilder<Command, Event, Account> builder =\n        newCommandHandlerWithReplyBuilder();\n\n    builder.forStateType(EmptyAccount.class).onCommand(CreateAccount.class, this::createAccount);\n\n    builder\n        .forStateType(OpenedAccount.class)\n        .onCommand(Deposit.class, this::deposit)\n        .onCommand(Withdraw.class, this::withdraw)\n        .onCommand(GetBalance.class, this::getBalance)\n        .onCommand(CloseAccount.class, this::closeAccount);\n\n    builder\n        .forStateType(ClosedAccount.class)\n        .onAnyCommand(() -> Effect().unhandled().thenNoReply());\n\n    return builder.build();\n  }\n\n  private ReplyEffect<Event, Account> createAccount(EmptyAccount account, CreateAccount command) {\n    return Effect()\n        .persist(AccountCreated.INSTANCE)\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n\n  private ReplyEffect<Event, Account> deposit(OpenedAccount account, Deposit command) {\n    return Effect()\n        .persist(new Deposited(command.amount))\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n\n  private ReplyEffect<Event, Account> withdraw(OpenedAccount account, Withdraw command) {\n    if (!account.canWithdraw(command.amount)) {\n      return Effect()\n          .reply(\n              command.replyTo,\n              StatusReply.error(\"not enough funds to withdraw \" + command.amount));\n    } else {\n      return Effect()\n          .persist(new Withdrawn(command.amount))\n          .thenReply(command.replyTo, account2 -> StatusReply.ack());\n    }\n  }\n\n  private ReplyEffect<Event, Account> getBalance(OpenedAccount account, GetBalance command) {\n    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));\n  }\n\n  private ReplyEffect<Event, Account> closeAccount(OpenedAccount account, CloseAccount command) {\n    if (account.balance.equals(BigDecimal.ZERO)) {\n      return Effect()\n          .persist(new AccountClosed())\n          .thenReply(command.replyTo, account2 -> StatusReply.ack());\n    } else {\n      return Effect()\n          .reply(command.replyTo, StatusReply.error(\"balance must be zero for closing account\"));\n    }\n  }\n\n  @Override\n  public EventHandler<Account, Event> eventHandler() {\n    EventHandlerBuilder<Account, Event> builder = newEventHandlerBuilder();\n\n    builder\n        .forStateType(EmptyAccount.class)\n        .onEvent(AccountCreated.class, (account, created) -> account.openedAccount());\n\n    builder\n        .forStateType(OpenedAccount.class)\n        .onEvent(Deposited.class, (account, deposited) -> account.makeDeposit(deposited.amount))\n        .onEvent(Withdrawn.class, (account, withdrawn) -> account.makeWithdraw(withdrawn.amount))\n        .onEvent(AccountClosed.class, (account, closed) -> account.closedAccount());\n\n    return builder.build();\n  }\n}\nNotice how the eventHandler delegates to the applyEvent in the Account (state), which is implemented in the concrete EmptyAccount, OpenedAccount, and ClosedAccount. Notice how the eventHandler delegates to methods in the concrete Account (state) classes; EmptyAccount, OpenedAccount, and ClosedAccount.\nCommand handlers in the state We can take the previous bank account example one step further by handling the commands in the state too. Scala copysourceobject AccountEntity {\n  // Command\n  sealed trait Command extends CborSerializable\n  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command\n  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n\n  // Reply\n  final case class CurrentBalance(balance: BigDecimal)\n\n  // Event\n  sealed trait Event extends CborSerializable\n  case object AccountCreated extends Event\n  case class Deposited(amount: BigDecimal) extends Event\n  case class Withdrawn(amount: BigDecimal) extends Event\n  case object AccountClosed extends Event\n\n  val Zero = BigDecimal(0)\n\n  // type alias to reduce boilerplate\n  type ReplyEffect = pekko.persistence.typed.scaladsl.ReplyEffect[Event, Account]\n\n  // State\n  sealed trait Account extends CborSerializable {\n    def applyCommand(cmd: Command): ReplyEffect\n    def applyEvent(event: Event): Account\n  }\n  case object EmptyAccount extends Account {\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case CreateAccount(replyTo) =>\n          Effect.persist(AccountCreated).thenReply(replyTo)(_ => StatusReply.Ack)\n        case _ =>\n          // CreateAccount before handling any other commands\n          Effect.unhandled.thenNoReply()\n      }\n\n    override def applyEvent(event: Event): Account =\n      event match {\n        case AccountCreated => OpenedAccount(Zero)\n        case _              => throw new IllegalStateException(s\"unexpected event [$event] in state [EmptyAccount]\")\n      }\n  }\n  case class OpenedAccount(balance: BigDecimal) extends Account {\n    require(balance >= Zero, \"Account balance can't be negative\")\n\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case Deposit(amount, replyTo) =>\n          Effect.persist(Deposited(amount)).thenReply(replyTo)(_ => StatusReply.Ack)\n\n        case Withdraw(amount, replyTo) =>\n          if (canWithdraw(amount))\n            Effect.persist(Withdrawn(amount)).thenReply(replyTo)(_ => StatusReply.Ack)\n          else\n            Effect.reply(replyTo)(StatusReply.Error(s\"Insufficient balance $balance to be able to withdraw $amount\"))\n\n        case GetBalance(replyTo) =>\n          Effect.reply(replyTo)(CurrentBalance(balance))\n\n        case CloseAccount(replyTo) =>\n          if (balance == Zero)\n            Effect.persist(AccountClosed).thenReply(replyTo)(_ => StatusReply.Ack)\n          else\n            Effect.reply(replyTo)(StatusReply.Error(\"Can't close account with non-zero balance\"))\n\n        case CreateAccount(replyTo) =>\n          Effect.reply(replyTo)(StatusReply.Error(\"Account is already created\"))\n\n      }\n\n    override def applyEvent(event: Event): Account =\n      event match {\n        case Deposited(amount) => copy(balance = balance + amount)\n        case Withdrawn(amount) => copy(balance = balance - amount)\n        case AccountClosed     => ClosedAccount\n        case AccountCreated    => throw new IllegalStateException(s\"unexpected event [$event] in state [OpenedAccount]\")\n      }\n\n    def canWithdraw(amount: BigDecimal): Boolean = {\n      balance - amount >= Zero\n    }\n\n  }\n  case object ClosedAccount extends Account {\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case c: Deposit =>\n          replyClosed(c.replyTo)\n        case c: Withdraw =>\n          replyClosed(c.replyTo)\n        case GetBalance(replyTo) =>\n          Effect.reply(replyTo)(CurrentBalance(Zero))\n        case CloseAccount(replyTo) =>\n          replyClosed(replyTo)\n        case CreateAccount(replyTo) =>\n          replyClosed(replyTo)\n      }\n\n    private def replyClosed(replyTo: ActorRef[StatusReply[Done]]): ReplyEffect =\n      Effect.reply(replyTo)(StatusReply.Error(s\"Account is closed\"))\n\n    override def applyEvent(event: Event): Account =\n      throw new IllegalStateException(s\"unexpected event [$event] in state [ClosedAccount]\")\n  }\n\n  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:\n  val TypeKey: EntityTypeKey[Command] =\n    EntityTypeKey[Command](\"Account\")\n\n  def apply(persistenceId: PersistenceId): Behavior[Command] = {\n    EventSourcedBehavior.withEnforcedReplies[Command, Event, Account](\n      persistenceId,\n      EmptyAccount,\n      (state, cmd) => state.applyCommand(cmd),\n      (state, event) => state.applyEvent(event))\n  }\n\n} Notice how the command handler is delegating to applyCommand in the Account (state), which is implemented in the concrete EmptyAccount, OpenedAccount, and ClosedAccount.","title":"Event handlers in the state"},{"location":"/typed/persistence-style.html#optional-initial-state","text":"Sometimes, it’s not desirable to use a separate state class for the empty initial state, but rather treat that as there is no state yet. null can then be used as the emptyState, but be aware of that the state parameter will then be null for the first commands and events until the first event has be persisted to create the non-null state. It’s possible to use Optional instead of null but that results in rather much boilerplate to unwrap the Optional state parameter and therefore null is probably preferred. The following example illustrates using null as the emptyState. Option[State] can be used as the state type and None as the emptyState. Pattern matching is then used in command and event handlers at the outer layer before delegating to the state or other methods.\nScala copysourceobject AccountEntity {\n  // Command\n  sealed trait Command extends CborSerializable\n  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command\n  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n\n  // Reply\n  final case class CurrentBalance(balance: BigDecimal) extends CborSerializable\n\n  // Event\n  sealed trait Event extends CborSerializable\n  case object AccountCreated extends Event\n  case class Deposited(amount: BigDecimal) extends Event\n  case class Withdrawn(amount: BigDecimal) extends Event\n  case object AccountClosed extends Event\n\n  val Zero = BigDecimal(0)\n\n  // type alias to reduce boilerplate\n  type ReplyEffect = pekko.persistence.typed.scaladsl.ReplyEffect[Event, Option[Account]]\n\n  // State\n  sealed trait Account extends CborSerializable {\n    def applyCommand(cmd: Command): ReplyEffect\n    def applyEvent(event: Event): Account\n  }\n  case class OpenedAccount(balance: BigDecimal) extends Account {\n    require(balance >= Zero, \"Account balance can't be negative\")\n\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case Deposit(amount, replyTo) =>\n          Effect.persist(Deposited(amount)).thenReply(replyTo)(_ => StatusReply.Ack)\n\n        case Withdraw(amount, replyTo) =>\n          if (canWithdraw(amount))\n            Effect.persist(Withdrawn(amount)).thenReply(replyTo)(_ => StatusReply.Ack)\n          else\n            Effect.reply(replyTo)(StatusReply.Error(s\"Insufficient balance $balance to be able to withdraw $amount\"))\n\n        case GetBalance(replyTo) =>\n          Effect.reply(replyTo)(CurrentBalance(balance))\n\n        case CloseAccount(replyTo) =>\n          if (balance == Zero)\n            Effect.persist(AccountClosed).thenReply(replyTo)(_ => StatusReply.Ack)\n          else\n            Effect.reply(replyTo)(StatusReply.Error(\"Can't close account with non-zero balance\"))\n\n        case CreateAccount(replyTo) =>\n          Effect.reply(replyTo)(StatusReply.Error(\"Account is already created\"))\n\n      }\n\n    override def applyEvent(event: Event): Account =\n      event match {\n        case Deposited(amount) => copy(balance = balance + amount)\n        case Withdrawn(amount) => copy(balance = balance - amount)\n        case AccountClosed     => ClosedAccount\n        case AccountCreated    => throw new IllegalStateException(s\"unexpected event [$event] in state [OpenedAccount]\")\n      }\n\n    def canWithdraw(amount: BigDecimal): Boolean = {\n      balance - amount >= Zero\n    }\n\n  }\n  case object ClosedAccount extends Account {\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case c: Deposit =>\n          replyClosed(c.replyTo)\n        case c: Withdraw =>\n          replyClosed(c.replyTo)\n        case GetBalance(replyTo) =>\n          Effect.reply(replyTo)(CurrentBalance(Zero))\n        case CloseAccount(replyTo) =>\n          replyClosed(replyTo)\n        case CreateAccount(replyTo) =>\n          replyClosed(replyTo)\n      }\n\n    private def replyClosed(replyTo: ActorRef[StatusReply[Done]]): ReplyEffect =\n      Effect.reply(replyTo)(StatusReply.Error(s\"Account is closed\"))\n\n    override def applyEvent(event: Event): Account =\n      throw new IllegalStateException(s\"unexpected event [$event] in state [ClosedAccount]\")\n  }\n\n  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:\n  val TypeKey: EntityTypeKey[Command] =\n    EntityTypeKey[Command](\"Account\")\n\n  def apply(persistenceId: PersistenceId): Behavior[Command] = {\n    EventSourcedBehavior.withEnforcedReplies[Command, Event, Option[Account]](\n      persistenceId,\n      None,\n      (state, cmd) =>\n        state match {\n          case None          => onFirstCommand(cmd)\n          case Some(account) => account.applyCommand(cmd)\n        },\n      (state, event) =>\n        state match {\n          case None          => Some(onFirstEvent(event))\n          case Some(account) => Some(account.applyEvent(event))\n        })\n  }\n\n  def onFirstCommand(cmd: Command): ReplyEffect = {\n    cmd match {\n      case CreateAccount(replyTo) =>\n        Effect.persist(AccountCreated).thenReply(replyTo)(_ => StatusReply.Ack)\n      case _ =>\n        // CreateAccount before handling any other commands\n        Effect.unhandled.thenNoReply()\n    }\n  }\n\n  def onFirstEvent(event: Event): Account = {\n    event match {\n      case AccountCreated => OpenedAccount(Zero)\n      case _              => throw new IllegalStateException(s\"unexpected event [$event] in state [EmptyAccount]\")\n    }\n  }\n\n} Java copysourcepublic class AccountEntity\n    extends EventSourcedBehaviorWithEnforcedReplies<\n        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account> {\n\n  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =\n      EntityTypeKey.create(Command.class, \"Account\");\n\n  // Command\n  interface Command extends CborSerializable {}\n\n  public static class CreateAccount implements Command {\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    @JsonCreator\n    public CreateAccount(ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Deposit implements Command {\n    public final BigDecimal amount;\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    public Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n      this.amount = amount;\n    }\n  }\n\n  public static class Withdraw implements Command {\n    public final BigDecimal amount;\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    public Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {\n      this.amount = amount;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class GetBalance implements Command {\n    public final ActorRef<CurrentBalance> replyTo;\n\n    @JsonCreator\n    public GetBalance(ActorRef<CurrentBalance> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class CloseAccount implements Command {\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    @JsonCreator\n    public CloseAccount(ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  // Reply\n  public static class CurrentBalance implements CborSerializable {\n    public final BigDecimal balance;\n\n    @JsonCreator\n    public CurrentBalance(BigDecimal balance) {\n      this.balance = balance;\n    }\n  }\n\n  // Event\n  interface Event extends CborSerializable {}\n\n  public enum AccountCreated implements Event {\n    INSTANCE\n  }\n\n  public static class Deposited implements Event {\n    public final BigDecimal amount;\n\n    @JsonCreator\n    Deposited(BigDecimal amount) {\n      this.amount = amount;\n    }\n  }\n\n  public static class Withdrawn implements Event {\n    public final BigDecimal amount;\n\n    @JsonCreator\n    Withdrawn(BigDecimal amount) {\n      this.amount = amount;\n    }\n  }\n\n  public static class AccountClosed implements Event {}\n\n  // State\n  interface Account extends CborSerializable {}\n\n  public static class OpenedAccount implements Account {\n    public final BigDecimal balance;\n\n    public OpenedAccount() {\n      this.balance = BigDecimal.ZERO;\n    }\n\n    @JsonCreator\n    public OpenedAccount(BigDecimal balance) {\n      this.balance = balance;\n    }\n\n    OpenedAccount makeDeposit(BigDecimal amount) {\n      return new OpenedAccount(balance.add(amount));\n    }\n\n    boolean canWithdraw(BigDecimal amount) {\n      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);\n    }\n\n    OpenedAccount makeWithdraw(BigDecimal amount) {\n      if (!canWithdraw(amount))\n        throw new IllegalStateException(\"Account balance can't be negative\");\n      return new OpenedAccount(balance.subtract(amount));\n    }\n\n    ClosedAccount closedAccount() {\n      return new ClosedAccount();\n    }\n  }\n\n  public static class ClosedAccount implements Account {}\n\n  public static AccountEntity create(String accountNumber, PersistenceId persistenceId) {\n    return new AccountEntity(accountNumber, persistenceId);\n  }\n\n  private final String accountNumber;\n\n  private AccountEntity(String accountNumber, PersistenceId persistenceId) {\n    super(persistenceId);\n    this.accountNumber = accountNumber;\n  }\n\n  @Override\n  public Account emptyState() {\n    return null;\n  }\n\n  @Override\n  public CommandHandlerWithReply<Command, Event, Account> commandHandler() {\n    CommandHandlerWithReplyBuilder<Command, Event, Account> builder =\n        newCommandHandlerWithReplyBuilder();\n\n    builder.forNullState().onCommand(CreateAccount.class, this::createAccount);\n\n    builder\n        .forStateType(OpenedAccount.class)\n        .onCommand(Deposit.class, this::deposit)\n        .onCommand(Withdraw.class, this::withdraw)\n        .onCommand(GetBalance.class, this::getBalance)\n        .onCommand(CloseAccount.class, this::closeAccount);\n\n    builder\n        .forStateType(ClosedAccount.class)\n        .onAnyCommand(() -> Effect().unhandled().thenNoReply());\n\n    return builder.build();\n  }\n\n  private ReplyEffect<Event, Account> createAccount(CreateAccount command) {\n    return Effect()\n        .persist(AccountCreated.INSTANCE)\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n\n  private ReplyEffect<Event, Account> deposit(OpenedAccount account, Deposit command) {\n    return Effect()\n        .persist(new Deposited(command.amount))\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n\n  private ReplyEffect<Event, Account> withdraw(OpenedAccount account, Withdraw command) {\n    if (!account.canWithdraw(command.amount)) {\n      return Effect()\n          .reply(\n              command.replyTo,\n              StatusReply.error(\"not enough funds to withdraw \" + command.amount));\n    } else {\n      return Effect()\n          .persist(new Withdrawn(command.amount))\n          .thenReply(command.replyTo, account2 -> StatusReply.ack());\n    }\n  }\n\n  private ReplyEffect<Event, Account> getBalance(OpenedAccount account, GetBalance command) {\n    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));\n  }\n\n  private ReplyEffect<Event, Account> closeAccount(OpenedAccount account, CloseAccount command) {\n    if (account.balance.equals(BigDecimal.ZERO)) {\n      return Effect()\n          .persist(new AccountClosed())\n          .thenReply(command.replyTo, account2 -> StatusReply.ack());\n    } else {\n      return Effect()\n          .reply(command.replyTo, StatusReply.error(\"balance must be zero for closing account\"));\n    }\n  }\n\n  @Override\n  public EventHandler<Account, Event> eventHandler() {\n    EventHandlerBuilder<Account, Event> builder = newEventHandlerBuilder();\n\n    builder.forNullState().onEvent(AccountCreated.class, () -> new OpenedAccount());\n\n    builder\n        .forStateType(OpenedAccount.class)\n        .onEvent(Deposited.class, (account, deposited) -> account.makeDeposit(deposited.amount))\n        .onEvent(Withdrawn.class, (account, withdrawn) -> account.makeWithdraw(withdrawn.amount))\n        .onEvent(AccountClosed.class, (account, closed) -> account.closedAccount());\n\n    return builder.build();\n  }\n}\nMutable state The state can be mutable or immutable. When it is immutable the event handler returns a new instance of the state for each change. When using mutable state it’s important to not send the full state instance as a message to another actor, e.g. as a reply to a command. Messages must be immutable to avoid concurrency problems. The above examples are using immutable state classes and below is corresponding example with mutable state. Java copysourcepublic class AccountEntity\n    extends EventSourcedBehaviorWithEnforcedReplies<\n        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account> {\n\n  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =\n      EntityTypeKey.create(Command.class, \"Account\");\n\n  // Command\n  interface Command extends CborSerializable {}\n\n  public static class CreateAccount implements Command {\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    @JsonCreator\n    public CreateAccount(ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Deposit implements Command {\n    public final BigDecimal amount;\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    public Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n      this.amount = amount;\n    }\n  }\n\n  public static class Withdraw implements Command {\n    public final BigDecimal amount;\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    public Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {\n      this.amount = amount;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class GetBalance implements Command {\n    public final ActorRef<CurrentBalance> replyTo;\n\n    @JsonCreator\n    public GetBalance(ActorRef<CurrentBalance> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class CloseAccount implements Command {\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    @JsonCreator\n    public CloseAccount(ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  // Reply\n  public static class CurrentBalance implements CborSerializable {\n    public final BigDecimal balance;\n\n    @JsonCreator\n    public CurrentBalance(BigDecimal balance) {\n      this.balance = balance;\n    }\n  }\n\n  // Event\n  interface Event extends CborSerializable {}\n\n  public enum AccountCreated implements Event {\n    INSTANCE\n  }\n\n  public static class Deposited implements Event {\n    public final BigDecimal amount;\n\n    @JsonCreator\n    Deposited(BigDecimal amount) {\n      this.amount = amount;\n    }\n  }\n\n  public static class Withdrawn implements Event {\n    public final BigDecimal amount;\n\n    @JsonCreator\n    Withdrawn(BigDecimal amount) {\n      this.amount = amount;\n    }\n  }\n\n  public static class AccountClosed implements Event {}\n\n  // State\n  interface Account extends CborSerializable {}\n\n  public static class EmptyAccount implements Account {\n    OpenedAccount openedAccount() {\n      return new OpenedAccount();\n    }\n  }\n\n  public static class OpenedAccount implements Account {\n    private BigDecimal balance = BigDecimal.ZERO;\n\n    public BigDecimal getBalance() {\n      return balance;\n    }\n\n    void makeDeposit(BigDecimal amount) {\n      balance = balance.add(amount);\n    }\n\n    boolean canWithdraw(BigDecimal amount) {\n      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);\n    }\n\n    void makeWithdraw(BigDecimal amount) {\n      if (!canWithdraw(amount))\n        throw new IllegalStateException(\"Account balance can't be negative\");\n      balance = balance.subtract(amount);\n    }\n\n    ClosedAccount closedAccount() {\n      return new ClosedAccount();\n    }\n  }\n\n  public static class ClosedAccount implements Account {}\n\n  public static AccountEntity create(String accountNumber, PersistenceId persistenceId) {\n    return new AccountEntity(accountNumber, persistenceId);\n  }\n\n  private final String accountNumber;\n\n  private AccountEntity(String accountNumber, PersistenceId persistenceId) {\n    super(persistenceId);\n    this.accountNumber = accountNumber;\n  }\n\n  @Override\n  public Account emptyState() {\n    return new EmptyAccount();\n  }\n\n  @Override\n  public CommandHandlerWithReply<Command, Event, Account> commandHandler() {\n    CommandHandlerWithReplyBuilder<Command, Event, Account> builder =\n        newCommandHandlerWithReplyBuilder();\n\n    builder.forStateType(EmptyAccount.class).onCommand(CreateAccount.class, this::createAccount);\n\n    builder\n        .forStateType(OpenedAccount.class)\n        .onCommand(Deposit.class, this::deposit)\n        .onCommand(Withdraw.class, this::withdraw)\n        .onCommand(GetBalance.class, this::getBalance)\n        .onCommand(CloseAccount.class, this::closeAccount);\n\n    builder\n        .forStateType(ClosedAccount.class)\n        .onAnyCommand(() -> Effect().unhandled().thenNoReply());\n\n    return builder.build();\n  }\n\n  private ReplyEffect<Event, Account> createAccount(EmptyAccount account, CreateAccount command) {\n    return Effect()\n        .persist(AccountCreated.INSTANCE)\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n\n  private ReplyEffect<Event, Account> deposit(OpenedAccount account, Deposit command) {\n    return Effect()\n        .persist(new Deposited(command.amount))\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n\n  private ReplyEffect<Event, Account> withdraw(OpenedAccount account, Withdraw command) {\n    if (!account.canWithdraw(command.amount)) {\n      return Effect()\n          .reply(\n              command.replyTo,\n              StatusReply.error(\"not enough funds to withdraw \" + command.amount));\n    } else {\n      return Effect()\n          .persist(new Withdrawn(command.amount))\n          .thenReply(command.replyTo, account2 -> StatusReply.ack());\n    }\n  }\n\n  private ReplyEffect<Event, Account> getBalance(OpenedAccount account, GetBalance command) {\n    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));\n  }\n\n  private ReplyEffect<Event, Account> closeAccount(OpenedAccount account, CloseAccount command) {\n    if (account.getBalance().equals(BigDecimal.ZERO)) {\n      return Effect()\n          .persist(new AccountClosed())\n          .thenReply(command.replyTo, account2 -> StatusReply.ack());\n    } else {\n      return Effect()\n          .reply(command.replyTo, StatusReply.error(\"balance must be zero for closing account\"));\n    }\n  }\n\n  @Override\n  public EventHandler<Account, Event> eventHandler() {\n    EventHandlerBuilder<Account, Event> builder = newEventHandlerBuilder();\n\n    builder\n        .forStateType(EmptyAccount.class)\n        .onEvent(AccountCreated.class, (account, event) -> account.openedAccount());\n\n    builder\n        .forStateType(OpenedAccount.class)\n        .onEvent(\n            Deposited.class,\n            (account, deposited) -> {\n              account.makeDeposit(deposited.amount);\n              return account;\n            })\n        .onEvent(\n            Withdrawn.class,\n            (account, withdrawn) -> {\n              account.makeWithdraw(withdrawn.amount);\n              return account;\n            })\n        .onEvent(AccountClosed.class, (account, closed) -> account.closedAccount());\n\n    return builder.build();\n  }\n}","title":"Optional initial state"},{"location":"/typed/persistence-snapshot.html","text":"","title":"Snapshotting"},{"location":"/typed/persistence-snapshot.html#snapshotting","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Pekko Persistence.","title":"Snapshotting"},{"location":"/typed/persistence-snapshot.html#snapshots","text":"As you model your domain using event sourced actors, you may notice that some actors may be prone to accumulating extremely long event logs and experiencing long recovery times. Sometimes, the right approach may be to split out into a set of shorter lived actors. However, when this is not an option, you can use snapshots to reduce recovery times drastically.\nPersistent actors can save snapshots of internal state every N events or when a given predicate of the state is fulfilled.\nScala copysource import org.apache.pekko.persistence.typed.scaladsl.Effect\n\nEventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => state) // do something based on a particular state\n  .snapshotWhen {\n    case (state, BookingCompleted(_), sequenceNumber) => true\n    case (state, event, sequenceNumber)               => false\n  }\n  .withRetention(RetentionCriteria.snapshotEvery(numberOfEvents = 100, keepNSnapshots = 2)) Java copysource@Override // override retentionCriteria in EventSourcedBehavior\npublic RetentionCriteria retentionCriteria() {\n  return RetentionCriteria.snapshotEvery(100, 2);\n}\nScala copysourceEventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .snapshotWhen {\n    case (state, BookingCompleted(_), sequenceNumber) => true\n    case (state, event, sequenceNumber)               => false\n  } Java copysource@Override // override shouldSnapshot in EventSourcedBehavior\npublic boolean shouldSnapshot(State state, Event event, long sequenceNr) {\n  return event instanceof BookingCompleted;\n}\nWhen a snapshot is triggered, incoming commands are stashed until the snapshot has been saved. This means that the state can safely be mutable although the serialization and storage of the state is performed asynchronously. The state instance will not be updated by new events until after the snapshot has been saved.\nDuring recovery, the persistent actor is using the latest saved snapshot to initialize the state. Thereafter the events after the snapshot are replayed using the event handler to recover the persistent actor to its current (i.e. latest) state.\nIf not specified, they default to SnapshotSelectionCriteria.LatestSnapshotSelectionCriteria.latest() which selects the latest (youngest) snapshot. It’s possible to override the selection of which snapshot to use for recovery like this:\nScala copysourceimport org.apache.pekko.persistence.typed.SnapshotSelectionCriteria\n\nEventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .withRecovery(Recovery.withSnapshotSelectionCriteria(SnapshotSelectionCriteria.none)) Java copysource@Override\npublic Recovery recovery() {\n  return Recovery.withSnapshotSelectionCriteria(SnapshotSelectionCriteria.none());\n}\nTo disable snapshot-based recovery, applications can use SnapshotSelectionCriteria.NoneSnapshotSelectionCriteria.none(). A recovery where no saved snapshot matches the specified SnapshotSelectionCriteria will replay all journaled events. This can be useful if snapshot serialization format has changed in an incompatible way. It should typically not be used when events have been deleted.\nIn order to use snapshots, a default snapshot-store (pekko.persistence.snapshot-store.plugin) must be configured, or you can pick a snapshot store for for a specific EventSourcedBehavior by defining it with withSnapshotPluginId of the EventSourcedBehavioroverriding snapshotPluginId in the EventSourcedBehavior.\nBecause some use cases may not benefit from or need snapshots, it is perfectly valid not to not configure a snapshot store. However, Pekko will log a warning message when this situation is detected and then continue to operate until an actor tries to store a snapshot, at which point the operation will fail.","title":"Snapshots"},{"location":"/typed/persistence-snapshot.html#snapshot-failures","text":"Saving snapshots can either succeed or fail – this information is reported back to the persistent actor via the SnapshotCompleted or SnapshotFailed signal. Snapshot failures are logged by default but do not cause the actor to stop or restart.\nIf there is a problem with recovering the state of the actor from the journal when the actor is started, RecoveryFailed signal is emitted (logging the error by default), and the actor will be stopped. Note that failure to load snapshot is also treated like this, but you can disable loading of snapshots if you for example know that serialization format has changed in an incompatible way.","title":"Snapshot failures"},{"location":"/typed/persistence-snapshot.html#optional-snapshots","text":"By default, the persistent actor will unconditionally be stopped if the snapshot can’t be loaded in the recovery. It is possible to make snapshot loading optional. This can be useful when it is alright to ignore snapshot in case of for example deserialization errors. When snapshot loading fails it will instead recover by replaying all events.\nEnable this feature by setting snapshot-is-optional = true in the snapshot store configuration.\nWarning Don’t set snapshot-is-optional = true if events have been deleted because that would result in wrong recovered state if snapshot load fails.","title":"Optional snapshots"},{"location":"/typed/persistence-snapshot.html#snapshot-deletion","text":"To free up space, an event sourced actor can automatically delete older snapshots based on the given RetentionCriteria.\nScala copysource import org.apache.pekko.persistence.typed.scaladsl.Effect\n\nEventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => state) // do something based on a particular state\n  .snapshotWhen {\n    case (state, BookingCompleted(_), sequenceNumber) => true\n    case (state, event, sequenceNumber)               => false\n  }\n  .withRetention(RetentionCriteria.snapshotEvery(numberOfEvents = 100, keepNSnapshots = 2)) Java copysource@Override // override retentionCriteria in EventSourcedBehavior\npublic RetentionCriteria retentionCriteria() {\n  return RetentionCriteria.snapshotEvery(100, 2);\n}\n@Override // override shouldSnapshot in EventSourcedBehavior\npublic boolean shouldSnapshot(State state, Event event, long sequenceNr) {\n  return event instanceof BookingCompleted;\n}\nSnapshot deletion is triggered after saving a new snapshot.\nThe above example will save snapshots automatically every numberOfEvents = 100. Snapshots that have sequence number less than the sequence number of the saved snapshot minus keepNSnapshots * numberOfEvents (100 * 2) are automatically deleted.\nIn addition, it will also save a snapshot when the persisted event is BookingCompleted. Automatic snapshotting based on numberOfEvents can be used without specifying snapshotWhenshouldSnapshot. Snapshots triggered by the snapshotWhenshouldSnapshot predicate will not trigger deletion of old snapshots.\nOn async deletion, either a DeleteSnapshotsCompleted or DeleteSnapshotsFailed signal is emitted. You can react to signal outcomes by using with receiveSignal handler by overriding receiveSignal. By default, successful completion is logged by the system at log level debug, failures at log level warning.\nScala copysource EventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .withRetention(RetentionCriteria.snapshotEvery(numberOfEvents = 100, keepNSnapshots = 2))\n  .receiveSignal { // optionally respond to signals\n    case (state, _: SnapshotFailed)        => // react to failure\n    case (state, _: DeleteSnapshotsFailed) => // react to failure\n  } Java copysource@Override\npublic SignalHandler<State> signalHandler() {\n  return newSignalHandlerBuilder()\n      .onSignal(\n          SnapshotFailed.class,\n          (state, completed) -> {\n            throw new RuntimeException(\"TODO: add some on-snapshot-failed side-effect here\");\n          })\n      .onSignal(\n          DeleteSnapshotsFailed.class,\n          (state, completed) -> {\n            throw new RuntimeException(\n                \"TODO: add some on-delete-snapshot-failed side-effect here\");\n          })\n      .onSignal(\n          DeleteEventsFailed.class,\n          (state, completed) -> {\n            throw new RuntimeException(\n                \"TODO: add some on-delete-snapshot-failed side-effect here\");\n          })\n      .build();\n}","title":"Snapshot deletion"},{"location":"/typed/persistence-snapshot.html#event-deletion","text":"Deleting events in Event Sourcing based applications is typically either not used at all, or used in conjunction with snapshotting. By deleting events you will lose the history of how the system changed before it reached current state, which is one of the main reasons for using Event Sourcing in the first place.\nIf snapshot-based retention is enabled, after a snapshot has been successfully stored, a delete of the events (journaled by a single event sourced actor) up until the sequence number of the data held by that snapshot can be issued.\nTo elect to use this, enable withDeleteEventsOnSnapshot of the RetentionCriteria which is disabled by default.\nScala copysource EventSourcedBehavior[Command, Event, State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n  eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n  .withRetention(RetentionCriteria.snapshotEvery(numberOfEvents = 100, keepNSnapshots = 2).withDeleteEventsOnSnapshot)\n  .receiveSignal { // optionally respond to signals\n    case (state, _: SnapshotFailed)        => // react to failure\n    case (state, _: DeleteSnapshotsFailed) => // react to failure\n    case (state, _: DeleteEventsFailed)    => // react to failure\n  } Java copysource@Override // override retentionCriteria in EventSourcedBehavior\npublic RetentionCriteria retentionCriteria() {\n  return RetentionCriteria.snapshotEvery(100, 2).withDeleteEventsOnSnapshot();\n}\nEvent deletion is triggered after saving a new snapshot. Old events would be deleted prior to old snapshots being deleted.\nOn async deletion, either a DeleteEventsCompleted or DeleteEventsFailed signal is emitted. You can react to signal outcomes by using with receiveSignal handler by overriding receiveSignal. By default, successful completion is logged by the system at log level debug, failures at log level warning.\nMessage deletion does not affect the highest sequence number of the journal, even if all messages were deleted from it after a delete occurs.\nNote It is up to the journal implementation whether events are actually removed from storage.","title":"Event deletion"},{"location":"/typed/persistence-testing.html","text":"","title":"Testing"},{"location":"/typed/persistence-testing.html#testing","text":"","title":"Testing"},{"location":"/typed/persistence-testing.html#module-info","text":"To use Pekko Persistence TestKit, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-persistence-typed\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-persistence-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-typed_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-typed_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-persistence-testkit_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Persistence Testkit Artifact org.apache.pekko pekko-persistence-testkit 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.persistence.testkit License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/persistence-testing.html#unit-testing","text":"Note! The EventSourcedBehaviorTestKit is a new feature, api may have changes breaking source compatibility in future versions.\nUnit testing of EventSourcedBehavior can be done with the EventSourcedBehaviorTestKitEventSourcedBehaviorTestKit. It supports running one command at a time and you can assert that the synchronously returned result is as expected. The result contains the events emitted by the command and the new state after applying the events. It also has support for verifying the reply to a command.\nYou need to configure the ActorSystem with the EventSourcedBehaviorTestKit.config. The configuration enables the in-memory journal and snapshot storage.\nScala copysourceclass AccountExampleDocSpec\n    extends ScalaTestWithActorTestKit(EventSourcedBehaviorTestKit.config) Java copysource@ClassRule\npublic static final TestKitJunitResource testKit =\n    new TestKitJunitResource(EventSourcedBehaviorTestKit.config());\n\nprivate EventSourcedBehaviorTestKit<\n        AccountEntity.Command, AccountEntity.Event, AccountEntity.Account>\n    eventSourcedTestKit =\n        EventSourcedBehaviorTestKit.create(\n            testKit.system(), AccountEntity.create(\"1\", PersistenceId.of(\"Account\", \"1\")));\nA full test for the AccountEntity, which is shown in the Persistence Style Guide, may look like this:\nScala copysourceimport org.apache.pekko\nimport pekko.Done\nimport pekko.persistence.testkit.scaladsl.EventSourcedBehaviorTestKit\nimport pekko.persistence.typed.PersistenceId\nimport pekko.actor.testkit.typed.scaladsl.LogCapturing\nimport pekko.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit\nimport pekko.pattern.StatusReply\nimport org.scalatest.BeforeAndAfterEach\nimport org.scalatest.wordspec.AnyWordSpecLike\n\nclass AccountExampleDocSpec\n    extends ScalaTestWithActorTestKit(EventSourcedBehaviorTestKit.config)\n    with AnyWordSpecLike\n    with BeforeAndAfterEach\n    with LogCapturing {\n\n  private val eventSourcedTestKit =\n    EventSourcedBehaviorTestKit[AccountEntity.Command, AccountEntity.Event, AccountEntity.Account](\n      system,\n      AccountEntity(\"1\", PersistenceId(\"Account\", \"1\")))\n\n  override protected def beforeEach(): Unit = {\n    super.beforeEach()\n    eventSourcedTestKit.clear()\n  }\n\n  \"Account\" must {\n\n    \"be created with zero balance\" in {\n      val result = eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.CreateAccount(_))\n      result.reply shouldBe StatusReply.Ack\n      result.event shouldBe AccountEntity.AccountCreated\n      result.stateOfType[AccountEntity.OpenedAccount].balance shouldBe 0\n    }\n\n    \"handle Withdraw\" in {\n      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.CreateAccount(_))\n\n      val result1 = eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Deposit(100, _))\n      result1.reply shouldBe StatusReply.Ack\n      result1.event shouldBe AccountEntity.Deposited(100)\n      result1.stateOfType[AccountEntity.OpenedAccount].balance shouldBe 100\n\n      val result2 = eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Withdraw(10, _))\n      result2.reply shouldBe StatusReply.Ack\n      result2.event shouldBe AccountEntity.Withdrawn(10)\n      result2.stateOfType[AccountEntity.OpenedAccount].balance shouldBe 90\n    }\n\n    \"reject Withdraw overdraft\" in {\n      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.CreateAccount(_))\n      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Deposit(100, _))\n\n      val result = eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Withdraw(110, _))\n      result.reply.isError shouldBe true\n      result.hasNoEvents shouldBe true\n    }\n\n    \"handle GetBalance\" in {\n      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.CreateAccount(_))\n      eventSourcedTestKit.runCommand[StatusReply[Done]](AccountEntity.Deposit(100, _))\n\n      val result = eventSourcedTestKit.runCommand[AccountEntity.CurrentBalance](AccountEntity.GetBalance(_))\n      result.reply.balance shouldBe 100\n      result.hasNoEvents shouldBe true\n    }\n  }\n} Java copysourceimport java.math.BigDecimal;\nimport org.apache.pekko.actor.testkit.typed.javadsl.LogCapturing;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestKitJunitResource;\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.persistence.testkit.javadsl.EventSourcedBehaviorTestKit;\nimport org.apache.pekko.persistence.testkit.javadsl.EventSourcedBehaviorTestKit.CommandResultWithReply;\nimport org.apache.pekko.persistence.typed.PersistenceId;\n\nimport org.junit.Before;\nimport org.junit.ClassRule;\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class AccountExampleDocTest\n{\n\n  @ClassRule\n  public static final TestKitJunitResource testKit =\n      new TestKitJunitResource(EventSourcedBehaviorTestKit.config());\n\n  private EventSourcedBehaviorTestKit<\n          AccountEntity.Command, AccountEntity.Event, AccountEntity.Account>\n      eventSourcedTestKit =\n          EventSourcedBehaviorTestKit.create(\n              testKit.system(), AccountEntity.create(\"1\", PersistenceId.of(\"Account\", \"1\")));\n\n  @Rule public final LogCapturing logCapturing = new LogCapturing();\n\n  @Before\n  public void beforeEach() {\n    eventSourcedTestKit.clear();\n  }\n\n  @Test\n  public void createWithEmptyBalance() {\n    CommandResultWithReply<\n            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>\n        result = eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);\n    assertEquals(StatusReply.ack(), result.reply());\n    assertEquals(AccountEntity.AccountCreated.INSTANCE, result.event());\n    assertEquals(BigDecimal.ZERO, result.stateOfType(AccountEntity.OpenedAccount.class).balance);\n  }\n\n  @Test\n  public void createWithUnHandle() {\n    CommandResultWithReply<\n            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>\n        result = eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);\n    assertFalse(result.hasNoReply());\n  }\n\n  @Test\n  public void handleWithdraw() {\n    eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);\n\n    CommandResultWithReply<\n            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>\n        result1 =\n            eventSourcedTestKit.runCommand(\n                replyTo -> new AccountEntity.Deposit(BigDecimal.valueOf(100), replyTo));\n    assertEquals(StatusReply.ack(), result1.reply());\n    assertEquals(\n        BigDecimal.valueOf(100), result1.eventOfType(AccountEntity.Deposited.class).amount);\n    assertEquals(\n        BigDecimal.valueOf(100), result1.stateOfType(AccountEntity.OpenedAccount.class).balance);\n\n    CommandResultWithReply<\n            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>\n        result2 =\n            eventSourcedTestKit.runCommand(\n                replyTo -> new AccountEntity.Withdraw(BigDecimal.valueOf(10), replyTo));\n    assertEquals(StatusReply.ack(), result2.reply());\n    assertEquals(BigDecimal.valueOf(10), result2.eventOfType(AccountEntity.Withdrawn.class).amount);\n    assertEquals(\n        BigDecimal.valueOf(90), result2.stateOfType(AccountEntity.OpenedAccount.class).balance);\n  }\n\n  @Test\n  public void rejectWithdrawOverdraft() {\n    eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);\n    eventSourcedTestKit.runCommand(\n        (ActorRef<StatusReply<Done>> replyTo) ->\n            new AccountEntity.Deposit(BigDecimal.valueOf(100), replyTo));\n\n    CommandResultWithReply<\n            AccountEntity.Command, AccountEntity.Event, AccountEntity.Account, StatusReply<Done>>\n        result =\n            eventSourcedTestKit.runCommand(\n                replyTo -> new AccountEntity.Withdraw(BigDecimal.valueOf(110), replyTo));\n    assertTrue(result.reply().isError());\n    assertTrue(result.hasNoEvents());\n  }\n\n  @Test\n  public void handleGetBalance() {\n    eventSourcedTestKit.runCommand(AccountEntity.CreateAccount::new);\n    eventSourcedTestKit.runCommand(\n        (ActorRef<StatusReply<Done>> replyTo) ->\n            new AccountEntity.Deposit(BigDecimal.valueOf(100), replyTo));\n\n    CommandResultWithReply<\n            AccountEntity.Command,\n            AccountEntity.Event,\n            AccountEntity.Account,\n            AccountEntity.CurrentBalance>\n        result = eventSourcedTestKit.runCommand(AccountEntity.GetBalance::new);\n    assertEquals(BigDecimal.valueOf(100), result.reply().balance);\n  }\n}\nSerialization of commands, events and state are verified automatically. The serialization checks can be customized with the SerializationSettings when creating the EventSourcedBehaviorTestKit. By default, the serialization roundtrip is checked but the equality of the result of the serialization is not checked. equals must be implemented (or using case class) in the commands, events and state if verifyEquality is enabled.\nTo test recovery the restart method of the EventSourcedBehaviorTestKit can be used. It will restart the behavior, which will then recover from stored snapshot and events from previous commands. It’s also possible to populate the storage with events or simulate failures by using the underlying PersistenceTestKitPersistenceTestKit.","title":"Unit testing"},{"location":"/typed/persistence-testing.html#persistence-testkit","text":"Note! The PersistenceTestKit is a new feature, api may have changes breaking source compatibility in future versions.\nPersistence testkit allows to check events saved in a storage, emulate storage operations and exceptions. To use the testkit you need to add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence-testkit\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-testkit_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-testkit_${versions.ScalaBinary}\"\n}\nThere are two testkit classes which have similar api:\nPersistenceTestKitPersistenceTestKit class is for events SnapshotTestKitSnapshotTestKit class is for snapshots\nThe testkit classes have two corresponding plugins which emulate the behavior of the storages:\nPersistenceTestKitPluginPersistenceTestKitPlugin class emulates a events storage PersistenceTestKitSnapshotPluginPersistenceTestKitSnapshotPlugin class emulates a snapshots storage\nNote! The corresponding plugins must be configured in the actor system which is used to initialize the particular testkit class:\nScala copysource val yourConfiguration = ConfigFactory.defaultApplication()\n\nval system =\n  ActorSystem(??? /*some behavior*/, \"test-system\", PersistenceTestKitPlugin.config.withFallback(yourConfiguration))\n\nval testKit = PersistenceTestKit(system)\n Java copysourcepublic class PersistenceTestKitConfig {\n\n  Config conf =\n      PersistenceTestKitPlugin.getInstance()\n          .config()\n          .withFallback(ConfigFactory.defaultApplication());\n\n  ActorSystem<Command> system = ActorSystem.create(new SomeBehavior(), \"example\", conf);\n\n  PersistenceTestKit testKit = PersistenceTestKit.create(system);\n}\nand\nScala copysource val yourConfiguration = ConfigFactory.defaultApplication()\n\nval system = ActorSystem(\n  ??? /*some behavior*/,\n  \"test-system\",\n  PersistenceTestKitSnapshotPlugin.config.withFallback(yourConfiguration))\n\nval testKit = SnapshotTestKit(system)\n Java copysourcepublic class SnapshotTestKitConfig {\n\n  Config conf =\n      PersistenceTestKitSnapshotPlugin.getInstance()\n          .config()\n          .withFallback(ConfigFactory.defaultApplication());\n\n  ActorSystem<Command> system = ActorSystem.create(new SomeBehavior(), \"example\", conf);\n\n  SnapshotTestKit testKit = SnapshotTestKit.create(system);\n}\nA typical scenario is to create a persistent actor, send commands to it and check that it persists events as it is expected:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.testkit.typed.scaladsl.ScalaTestWithActorTestKit\nimport pekko.persistence.testkit.PersistenceTestKitPlugin\nimport pekko.persistence.testkit.scaladsl.PersistenceTestKit\n\nclass PersistenceTestKitSampleSpec\n    extends ScalaTestWithActorTestKit(PersistenceTestKitPlugin.config.withFallback(ConfigFactory.defaultApplication()))\n    with AnyWordSpecLike\n    with BeforeAndAfterEach {\n\n  val persistenceTestKit = PersistenceTestKit(system)\n\n  override def beforeEach(): Unit = {\n    persistenceTestKit.clearAll()\n  }\n\n  \"Persistent actor\" should {\n\n    \"persist all events\" in {\n\n      val persistenceId = PersistenceId.ofUniqueId(\"your-persistence-id\")\n      val persistentActor = spawn(\n        EventSourcedBehavior[Cmd, Evt, State](\n          persistenceId,\n          emptyState = State.empty,\n          commandHandler = (_, cmd) => Effect.persist(Evt(cmd.data)),\n          eventHandler = (state, evt) => state.updated(evt)))\n      val cmd = Cmd(\"data\")\n\n      persistentActor ! cmd\n\n      val expectedPersistedEvent = Evt(cmd.data)\n      persistenceTestKit.expectNextPersisted(persistenceId.id, expectedPersistedEvent)\n    }\n\n  }\n} Java copysourcepublic class PersistenceTestKitSampleTest extends AbstractJavaTest {\n\n  @ClassRule\n  public static final TestKitJunitResource testKit =\n      new TestKitJunitResource(\n          PersistenceTestKitPlugin.getInstance()\n              .config()\n              .withFallback(ConfigFactory.defaultApplication()));\n\n  PersistenceTestKit persistenceTestKit = PersistenceTestKit.create(testKit.system());\n\n  @Before\n  public void beforeEach() {\n    persistenceTestKit.clearAll();\n  }\n\n  @Test\n  public void test() {\n    PersistenceId persistenceId = PersistenceId.ofUniqueId(\"some-id\");\n    ActorRef<YourPersistentBehavior.Cmd> ref =\n        testKit.spawn(YourPersistentBehavior.create(persistenceId));\n\n    YourPersistentBehavior.Cmd cmd = new YourPersistentBehavior.Cmd(\"data\");\n    ref.tell(cmd);\n    YourPersistentBehavior.Evt expectedEventPersisted = new YourPersistentBehavior.Evt(cmd.data);\n\n    persistenceTestKit.expectNextPersisted(persistenceId.id(), expectedEventPersisted);\n  }\n}\n\nclass YourPersistentBehavior\n    extends EventSourcedBehavior<\n        YourPersistentBehavior.Cmd, YourPersistentBehavior.Evt, YourPersistentBehavior.State> {\n\n  static final class Cmd implements CborSerializable {\n\n    public final String data;\n\n    @JsonCreator\n    public Cmd(String data) {\n      this.data = data;\n    }\n  }\n\n  static final class Evt implements CborSerializable {\n\n    public final String data;\n\n    @JsonCreator\n    public Evt(String data) {\n      this.data = data;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n      if (this == o) return true;\n      if (o == null || getClass() != o.getClass()) return false;\n\n      Evt evt = (Evt) o;\n\n      return data.equals(evt.data);\n    }\n\n    @Override\n    public int hashCode() {\n      return data.hashCode();\n    }\n  }\n\n  static final class State implements CborSerializable {}\n\n  static Behavior<Cmd> create(PersistenceId persistenceId) {\n    return Behaviors.setup(context -> new YourPersistentBehavior(persistenceId));\n  }\n\n  private YourPersistentBehavior(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  @Override\n  public State emptyState() {\n    // some state\n    return new State();\n  }\n\n  @Override\n  public CommandHandler<Cmd, Evt, State> commandHandler() {\n    return newCommandHandlerBuilder()\n        .forAnyState()\n        .onCommand(Cmd.class, command -> Effect().persist(new Evt(command.data)))\n        .build();\n  }\n\n  @Override\n  public EventHandler<State, Evt> eventHandler() {\n    // TODO handle events\n    return newEventHandlerBuilder().forAnyState().onEvent(Evt.class, (state, evt) -> state).build();\n  }\n}\nYou can safely use persistence testkit in combination with main pekko testkit.\nThe main methods of the api allow to (see PersistenceTestKitPersistenceTestKit and SnapshotTestKitSnapshotTestKit for more details):\ncheck if the given event/snapshot object is the next persisted in the storage. read a sequence of persisted events/snapshots. check that no events/snapshots have been persisted in the storage. throw the default exception from the storage on attempt to persist, read or delete the following event/snapshot. clear the events/snapshots persisted in the storage. reject the events, but not snapshots (rejections are not supported for snapshots in the original api). set your own policy which emulates the work of the storage. Policy determines what to do when persistence needs to execute some operation on the storage (i.e. read, delete, etc.). get all the events/snapshots persisted in the storage put the events/snapshots in the storage to test recovery","title":"Persistence TestKit"},{"location":"/typed/persistence-testing.html#setting-your-own-policy-for-the-storage","text":"You can implement and set your own policy for the storage to control its actions on particular operations, for example you can fail or reject events on your own conditions. Implement the ProcessingPolicy[EventStorage.JournalOperation]ProcessingPolicy<EventStorage.JournalOperation> traitinterface for event storage or ProcessingPolicy[SnapshotStorage.SnapshotOperation]ProcessingPolicy<SnapshotStorage.SnapshotOperation> traitinterface for snapshot storage, and set it with withPolicy() method.\nScala copysourceclass PersistenceTestKitSampleSpecWithPolicy\n    extends ScalaTestWithActorTestKit(PersistenceTestKitPlugin.config.withFallback(ConfigFactory.defaultApplication()))\n    with AnyWordSpecLike\n    with BeforeAndAfterEach {\n\n  val persistenceTestKit = PersistenceTestKit(system)\n\n  override def beforeEach(): Unit = {\n    persistenceTestKit.clearAll()\n    persistenceTestKit.resetPolicy()\n  }\n\n  \"Testkit policy\" should {\n\n    \"fail all operations with custom exception\" in {\n      val policy = new EventStorage.JournalPolicies.PolicyType {\n\n        class CustomFailure extends RuntimeException\n\n        override def tryProcess(persistenceId: String, processingUnit: JournalOperation): ProcessingResult =\n          processingUnit match {\n            case WriteEvents(_) => StorageFailure(new CustomFailure)\n            case _              => ProcessingSuccess\n          }\n      }\n      persistenceTestKit.withPolicy(policy)\n\n      val persistenceId = PersistenceId.ofUniqueId(\"your-persistence-id\")\n      val persistentActor = spawn(\n        EventSourcedBehavior[Cmd, Evt, State](\n          persistenceId,\n          emptyState = State.empty,\n          commandHandler = (_, cmd) => Effect.persist(Evt(cmd.data)),\n          eventHandler = (state, evt) => state.updated(evt)))\n\n      persistentActor ! Cmd(\"data\")\n      persistenceTestKit.expectNothingPersisted(persistenceId.id)\n\n    }\n  }\n} Java copysourcepublic class PersistenceTestKitPolicySampleTest extends AbstractJavaTest {\n\n  @ClassRule\n  public static final TestKitJunitResource testKit =\n      new TestKitJunitResource(\n          PersistenceTestKitPlugin.getInstance()\n              .config()\n              .withFallback(ConfigFactory.defaultApplication()));\n\n  PersistenceTestKit persistenceTestKit = PersistenceTestKit.create(testKit.system());\n\n  @Before\n  public void beforeEach() {\n    persistenceTestKit.clearAll();\n    persistenceTestKit.resetPolicy();\n  }\n\n  @Test\n  public void test() {\n    SampleEventStoragePolicy policy = new SampleEventStoragePolicy();\n    persistenceTestKit.withPolicy(policy);\n\n    PersistenceId persistenceId = PersistenceId.ofUniqueId(\"some-id\");\n    ActorRef<YourPersistentBehavior.Cmd> ref =\n        testKit.spawn(YourPersistentBehavior.create(persistenceId));\n\n    YourPersistentBehavior.Cmd cmd = new YourPersistentBehavior.Cmd(\"data\");\n    ref.tell(cmd);\n\n    persistenceTestKit.expectNothingPersisted(persistenceId.id());\n  }\n\n  static class SampleEventStoragePolicy implements ProcessingPolicy<JournalOperation> {\n    @Override\n    public ProcessingResult tryProcess(String processId, JournalOperation processingUnit) {\n      if (processingUnit instanceof WriteEvents) {\n        return StorageFailure.create();\n      } else {\n        return ProcessingSuccess.getInstance();\n      }\n    }\n  }\n}\ntryProcess() method of the ProcessingPolicyProcessingPolicy has two arguments: persistence id and the storage operation.\nEvent storage has the following operations:\nReadEventsReadEvents Read the events from the storage. WriteEventsWriteEvents Write the events to the storage. DeleteEventsDeleteEvents Delete the events from the storage. ReadSeqNumReadSeqNum Read the highest sequence number for particular persistence id.\nSnapshot storage has the following operations:\nReadSnapshotReadSnapshot Read the snapshot from the storage. WriteSnapshotWriteSnapshot Writhe the snapshot to the storage. DeleteSnapshotsByCriteriaDeleteSnapshotsByCriteria Delete snapshots in the storage by criteria. DeleteSnapshotByMetaDeleteSnapshotByMeta Delete particular snapshot from the storage by its metadata.\nThe tryProcess() method must return one of the processing results:\nProcessingSuccessProcessingSuccess Successful completion of the operation. All the events will be saved/read/deleted. StorageFailureStorageFailure Emulates exception from the storage. RejectReject Emulates rejection from the storage.\nNote that snapshot storage does not have rejections. If you return Reject in the tryProcess() of the snapshot storage policy, it will have the same effect as the StorageFailure.\nHere is an example of the policy for an event storage:\nScala copysourceimport org.apache.pekko.persistence.testkit._\n\nclass SampleEventStoragePolicy extends EventStorage.JournalPolicies.PolicyType {\n\n  // you can use internal state, it does not need to be thread safe\n  var count = 1\n\n  override def tryProcess(persistenceId: String, processingUnit: JournalOperation): ProcessingResult =\n    if (count < 10) {\n      count += 1\n      // check the type of operation and react with success or with reject or with failure.\n      // if you return ProcessingSuccess the operation will be performed, otherwise not.\n      processingUnit match {\n        case ReadEvents(batch) if batch.nonEmpty => ProcessingSuccess\n        case WriteEvents(batch) if batch.size > 1 =>\n          ProcessingSuccess\n        case ReadSeqNum      => StorageFailure()\n        case DeleteEvents(_) => Reject()\n        case _               => StorageFailure()\n      }\n    } else {\n      ProcessingSuccess\n    }\n\n} Java copysourceclass SampleEventStoragePolicy implements ProcessingPolicy<JournalOperation> {\n\n  // you can use internal state, it does not need to be thread safe\n  int count = 1;\n\n  @Override\n  public ProcessingResult tryProcess(String processId, JournalOperation processingUnit) {\n    // check the type of operation and react with success or with reject or with failure.\n    // if you return ProcessingSuccess the operation will be performed, otherwise not.\n    if (count < 10) {\n      count += 1;\n      if (processingUnit instanceof ReadEvents) {\n        ReadEvents read = (ReadEvents) processingUnit;\n        if (read.batch().nonEmpty()) {\n          ProcessingSuccess.getInstance();\n        } else {\n          return StorageFailure.create();\n        }\n      } else if (processingUnit instanceof WriteEvents) {\n        return ProcessingSuccess.getInstance();\n      } else if (processingUnit instanceof DeleteEvents) {\n        return ProcessingSuccess.getInstance();\n      } else if (processingUnit.equals(ReadSeqNum.getInstance())) {\n        return Reject.create();\n      }\n      // you can set your own exception\n      return StorageFailure.create(new RuntimeException(\"your exception\"));\n    } else {\n      return ProcessingSuccess.getInstance();\n    }\n  }\n}\nHere is an example of the policy for a snapshot storage:\nScala copysourceclass SampleSnapshotStoragePolicy extends SnapshotStorage.SnapshotPolicies.PolicyType {\n\n  // you can use internal state, it does not need to be thread safe\n  var count = 1\n\n  override def tryProcess(persistenceId: String, processingUnit: SnapshotOperation): ProcessingResult =\n    if (count < 10) {\n      count += 1\n      // check the type of operation and react with success or with reject or with failure.\n      // if you return ProcessingSuccess the operation will be performed, otherwise not.\n      processingUnit match {\n        case ReadSnapshot(_, payload) if payload.nonEmpty =>\n          ProcessingSuccess\n        case WriteSnapshot(meta, payload) if meta.sequenceNr > 10 =>\n          ProcessingSuccess\n        case DeleteSnapshotsByCriteria(_) => StorageFailure()\n        case DeleteSnapshotByMeta(meta) if meta.sequenceNr < 10 =>\n          ProcessingSuccess\n        case _ => StorageFailure()\n      }\n    } else {\n      ProcessingSuccess\n    }\n} Java copysourceclass SnapshotStoragePolicy implements ProcessingPolicy<SnapshotOperation> {\n\n  // you can use internal state, it doesn't need to be thread safe\n  int count = 1;\n\n  @Override\n  public ProcessingResult tryProcess(String processId, SnapshotOperation processingUnit) {\n    // check the type of operation and react with success or with failure.\n    // if you return ProcessingSuccess the operation will be performed, otherwise not.\n    if (count < 10) {\n      count += 1;\n      if (processingUnit instanceof ReadSnapshot) {\n        ReadSnapshot read = (ReadSnapshot) processingUnit;\n        if (read.getSnapshot().isPresent()) {\n          ProcessingSuccess.getInstance();\n        } else {\n          return StorageFailure.create();\n        }\n      } else if (processingUnit instanceof WriteSnapshot) {\n        return ProcessingSuccess.getInstance();\n      } else if (processingUnit instanceof DeleteSnapshotsByCriteria) {\n        return ProcessingSuccess.getInstance();\n      } else if (processingUnit instanceof DeleteSnapshotByMeta) {\n        return ProcessingSuccess.getInstance();\n      }\n      // you can set your own exception\n      return StorageFailure.create(new RuntimeException(\"your exception\"));\n    } else {\n      return ProcessingSuccess.getInstance();\n    }\n  }\n}","title":"Setting your own policy for the storage"},{"location":"/typed/persistence-testing.html#configuration-of-persistence-testkit","text":"There are several configuration properties for persistence testkit, please refer to the reference configuration","title":"Configuration of Persistence TestKit"},{"location":"/typed/persistence-testing.html#integration-testing","text":"EventSourcedBehavior actors can be tested with the ActorTestKit together with other actors. The in-memory journal and snapshot storage from the Persistence TestKit can be used also for integration style testing of a single ActorSystem, for example when using Cluster Sharding with a single Cluster node.\nFor tests that involve more than one Cluster node you have to use another journal and snapshot store. While it’s possible to use the Persistence Plugin Proxy it’s often better and more realistic to use a real database.\nThe CQRS example includes tests that are using Pekko Persistence Cassandra.","title":"Integration testing"},{"location":"/typed/persistence-testing.html#plugin-initialization","text":"Some Persistence plugins create tables automatically, but has the limitation that it can’t be done concurrently from several ActorSystems. That can be a problem if the test creates a Cluster and all nodes tries to initialize the plugins at the same time. To coordinate initialization you can use the PersistenceInit utility.\nPersistenceInit is part of pekko-persistence-testkit and you need to add the dependency to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence-testkit\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-testkit_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-testkit_${versions.ScalaBinary}\"\n}\nScala copysourceimport org.apache.pekko.persistence.testkit.scaladsl.PersistenceInit\n\nimport scala.concurrent.Await\nimport scala.concurrent.Future\nimport scala.concurrent.duration._\n\nval timeout = 5.seconds\nval done: Future[Done] = PersistenceInit.initializeDefaultPlugins(system, timeout)\nAwait.result(done, timeout) Java copysourceimport org.apache.pekko.persistence.testkit.javadsl.PersistenceInit;\nimport org.apache.pekko.Done;\n\nimport java.time.Duration;\nimport java.util.concurrent.CompletionStage;\nimport java.util.concurrent.TimeUnit;\n\nDuration timeout = Duration.ofSeconds(5);\nCompletionStage<Done> done =\n    PersistenceInit.initializeDefaultPlugins(testKit.system(), timeout);\ndone.toCompletableFuture().get(timeout.getSeconds(), TimeUnit.SECONDS);","title":"Plugin initialization"},{"location":"/typed/persistence-fsm.html","text":"","title":"EventSourced behaviors as finite state machines"},{"location":"/typed/persistence-fsm.html#eventsourced-behaviors-as-finite-state-machines","text":"An EventSourcedBehaviorEventSourcedBehavior can be used to represent a persistent FSM. If you’re migrating an existing classic persistent FSM to EventSourcedBehavior see the migration guide.\nTo demonstrate this consider an example of a shopping application. A customer can be in the following states:\nLooking around Shopping (has something in their basket) Inactive Paid\nScala copysourcesealed trait State\ncase class LookingAround(cart: ShoppingCart) extends State\ncase class Shopping(cart: ShoppingCart) extends State\ncase class Inactive(cart: ShoppingCart) extends State\ncase class Paid(cart: ShoppingCart) extends State Java copysourceabstract static class State {\n  public final ShoppingCart cart;\n\n  protected State(ShoppingCart cart) {\n    this.cart = cart;\n  }\n}\n\npublic static class LookingAround extends State {\n  public LookingAround(ShoppingCart cart) {\n    super(cart);\n  }\n}\n\npublic static class Shopping extends State {\n  public Shopping(ShoppingCart cart) {\n    super(cart);\n  }\n}\n\npublic static class Inactive extends State {\n  public Inactive(ShoppingCart cart) {\n    super(cart);\n  }\n}\n\npublic static class Paid extends State {\n  public Paid(ShoppingCart cart) {\n    super(cart);\n  }\n}\nAnd the commands that can result in state changes:\nAdd item Buy Leave Timeout (internal command to discard abandoned purchases)\nAnd the following read only commands:\nGet current cart\nScala copysourcesealed trait Command\ncase class AddItem(item: Item) extends Command\ncase object Buy extends Command\ncase object Leave extends Command\ncase class GetCurrentCart(replyTo: ActorRef[ShoppingCart]) extends Command\nprivate case object Timeout extends Command Java copysourceinterface Command {}\n\npublic static class AddItem implements Command {\n  public final Item item;\n\n  public AddItem(Item item) {\n    this.item = item;\n  }\n}\n\npublic static class GetCurrentCart implements Command {\n  public final ActorRef<ShoppingCart> replyTo;\n\n  public GetCurrentCart(ActorRef<ShoppingCart> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic enum Buy implements Command {\n  INSTANCE\n}\n\npublic enum Leave implements Command {\n  INSTANCE\n}\n\nprivate enum Timeout implements Command {\n  INSTANCE\n}\nThe command handler of the EventSourcedBehavior is used to convert the commands that change the state of the FSM to events, and reply to commands.\nThe command handler:The forStateType command handler can be used:\nScala copysourcedef commandHandler(timers: TimerScheduler[Command])(state: State, command: Command): Effect[DomainEvent, State] =\n  state match {\n    case LookingAround(cart) =>\n      command match {\n        case AddItem(item) =>\n          Effect.persist(ItemAdded(item)).thenRun(_ => timers.startSingleTimer(StateTimeout, Timeout, 1.second))\n        case GetCurrentCart(replyTo) =>\n          replyTo ! cart\n          Effect.none\n        case _ =>\n          Effect.none\n      }\n    case Shopping(cart) =>\n      command match {\n        case AddItem(item) =>\n          Effect.persist(ItemAdded(item)).thenRun(_ => timers.startSingleTimer(StateTimeout, Timeout, 1.second))\n        case Buy =>\n          Effect.persist(OrderExecuted).thenRun(_ => timers.cancel(StateTimeout))\n        case Leave =>\n          Effect.persist(OrderDiscarded).thenStop()\n        case GetCurrentCart(replyTo) =>\n          replyTo ! cart\n          Effect.none\n        case Timeout =>\n          Effect.persist(CustomerInactive)\n      }\n    case Inactive(_) =>\n      command match {\n        case AddItem(item) =>\n          Effect.persist(ItemAdded(item)).thenRun(_ => timers.startSingleTimer(StateTimeout, Timeout, 1.second))\n        case Timeout =>\n          Effect.persist(OrderDiscarded)\n        case _ =>\n          Effect.none\n      }\n    case Paid(cart) =>\n      command match {\n        case Leave =>\n          Effect.stop()\n        case GetCurrentCart(replyTo) =>\n          replyTo ! cart\n          Effect.none\n        case _ =>\n          Effect.none\n      }\n  } Java copysource  CommandHandlerBuilder<Command, DomainEvent, State> builder = newCommandHandlerBuilder();\n\n  builder.forStateType(LookingAround.class).onCommand(AddItem.class, this::addItem);\n\n  builder\n      .forStateType(Shopping.class)\n      .onCommand(AddItem.class, this::addItem)\n      .onCommand(Buy.class, this::buy)\n      .onCommand(Leave.class, this::discardShoppingCart)\n      .onCommand(Timeout.class, this::timeoutShopping);\n\n  builder\n      .forStateType(Inactive.class)\n      .onCommand(AddItem.class, this::addItem)\n      .onCommand(Timeout.class, () -> Effect().persist(OrderDiscarded.INSTANCE).thenStop());\n\n  builder.forStateType(Paid.class).onCommand(Leave.class, () -> Effect().stop());\n\n  builder.forAnyState().onCommand(GetCurrentCart.class, this::getCurrentCart);\n  return builder.build();\n}\nThe event handler is used to change state once the events have been persisted. When the EventSourcedBehavior is restarted the events are replayed to get back into the correct state.\nScala copysourcedef eventHandler(state: State, event: DomainEvent): State = {\n  state match {\n    case la @ LookingAround(cart) =>\n      event match {\n        case ItemAdded(item) => Shopping(cart.addItem(item))\n        case _               => la\n      }\n    case Shopping(cart) =>\n      event match {\n        case ItemAdded(item)  => Shopping(cart.addItem(item))\n        case OrderExecuted    => Paid(cart)\n        case OrderDiscarded   => state // will be stopped\n        case CustomerInactive => Inactive(cart)\n      }\n    case i @ Inactive(cart) =>\n      event match {\n        case ItemAdded(item) => Shopping(cart.addItem(item))\n        case OrderDiscarded  => i // will be stopped\n        case _               => i\n      }\n    case Paid(_) => state // no events after paid\n  }\n} Java copysource@Override\npublic EventHandler<State, DomainEvent> eventHandler() {\n  EventHandlerBuilder<State, DomainEvent> eventHandlerBuilder = newEventHandlerBuilder();\n\n  eventHandlerBuilder\n      .forStateType(LookingAround.class)\n      .onEvent(ItemAdded.class, item -> new Shopping(new ShoppingCart(item.getItem())));\n\n  eventHandlerBuilder\n      .forStateType(Shopping.class)\n      .onEvent(\n          ItemAdded.class, (state, item) -> new Shopping(state.cart.addItem(item.getItem())))\n      .onEvent(OrderExecuted.class, (state, item) -> new Paid(state.cart))\n      .onEvent(OrderDiscarded.class, (state, item) -> state) // will be stopped\n      .onEvent(CustomerInactive.class, (state, event) -> new Inactive(state.cart));\n\n  eventHandlerBuilder\n      .forStateType(Inactive.class)\n      .onEvent(\n          ItemAdded.class, (state, item) -> new Shopping(state.cart.addItem(item.getItem())))\n      .onEvent(OrderDiscarded.class, (state, item) -> state); // will be stopped\n\n  return eventHandlerBuilder.build();\n}","title":"EventSourced behaviors as finite state machines"},{"location":"/persistence-schema-evolution.html","text":"","title":"Schema Evolution for Event Sourced Actors"},{"location":"/persistence-schema-evolution.html#schema-evolution-for-event-sourced-actors","text":"","title":"Schema Evolution for Event Sourced Actors"},{"location":"/persistence-schema-evolution.html#dependency","text":"This documentation page touches upon Pekko Persistence, so to follow those examples you will want to depend on:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-persistence\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-persistence-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-persistence-testkit_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/persistence-schema-evolution.html#introduction","text":"When working on long running projects using Persistence, or any kind of Event Sourcing architectures, schema evolution becomes one of the more important technical aspects of developing your application. The requirements as well as our own understanding of the business domain may (and will) change in time.\nIn fact, if a project matures to the point where you need to evolve its schema to adapt to changing business requirements you can view this as first signs of its success – if you wouldn’t need to adapt anything over an apps lifecycle that could mean that no-one is really using it actively.\nIn this chapter we will investigate various schema evolution strategies and techniques from which you can pick and choose the ones that match your domain and challenge at hand.\nNote This page proposes a number of possible solutions to the schema evolution problem and explains how some of the utilities Pekko provides can be used to achieve this, it is by no means a complete (closed) set of solutions. Sometimes, based on the capabilities of your serialization formats, you may be able to evolve your schema in different ways than outlined in the sections below. If you discover useful patterns or techniques for schema evolution feel free to submit Pull Requests to this page to extend it.","title":"Introduction"},{"location":"/persistence-schema-evolution.html#schema-evolution-in-event-sourced-systems","text":"In recent years we have observed a tremendous move towards immutable append-only datastores, with event-sourcing being the prime technique successfully being used in these settings. For an excellent overview why and how immutable data makes scalability and systems design much simpler you may want to read Pat Helland’s excellent Immutability Changes Everything whitepaper.\nSince with Event Sourcing the events are immutable and usually never deleted – the way schema evolution is handled differs from how one would go about it in a mutable database setting (e.g. in typical CRUD database applications).\nThe system needs to be able to continue to work in the presence of “old” events which were stored under the “old” schema. We also want to limit complexity in the business logic layer, exposing a consistent view over all of the events of a given type to PersistentActorAbstractPersistentActor s and persistence queries. This allows the business logic layer to focus on solving business problems instead of having to explicitly deal with different schemas.\nIn summary, schema evolution in event sourced systems exposes the following characteristics:\nAllow the system to continue operating without large scale migrations to be applied, Allow the system to read “old” events from the underlying storage, however present them in a “new” view to the application logic, Transparently promote events to the latest versions during recovery (or queries) such that the business logic need not consider multiple versions of events","title":"Schema evolution in event-sourced systems"},{"location":"/persistence-schema-evolution.html#types-of-schema-evolution","text":"Before we explain the various techniques that can be used to safely evolve the schema of your persistent events over time, we first need to define what the actual problem is, and what the typical styles of changes are.\nSince events are never deleted, we need to have a way to be able to replay (read) old events, in such way that does not force the PersistentActorAbstractPersistentActor to be aware of all possible versions of an event that it may have persisted in the past. Instead, we want the Actors to work on some form of “latest” version of the event and provide some means of either converting old “versions” of stored events into this “latest” event type, or constantly evolve the event definition - in a backwards compatible way - such that the new deserialization code can still read old events.\nThe most common schema changes you will likely are:\nadding a field to an event type, remove or rename field in event type, remove event type, split event into multiple smaller events.\nThe following sections will explain some patterns which can be used to safely evolve your schema when facing those changes.","title":"Types of schema evolution"},{"location":"/persistence-schema-evolution.html#picking-the-right-serialization-format","text":"Picking the serialization format is a very important decision you will have to make while building your application. It affects which kind of evolutions are simple (or hard) to do, how much work is required to add a new datatype, and, last but not least, serialization performance.\nIf you find yourself realising you have picked “the wrong” serialization format, it is always possible to change the format used for storing new events, however you would have to keep the old deserialization code in order to be able to replay events that were persisted using the old serialization scheme. It is possible to “rebuild” an event-log from one serialization format to another one, however it may be a more involved process if you need to perform this on a live system.\nSerialization with Jackson is a good choice in many cases and our recommendation if you don’t have other preference. It also has support for Schema Evolution.\nGoogle Protocol Buffers is good if you want more control over the schema evolution of your messages, but it requires more work to develop and maintain the mapping between serialized representation and domain representation.\nBinary serialization formats that we have seen work well for long-lived applications include the very flexible IDL based: Google Protocol Buffers, Apache Thrift or Apache Avro. Avro schema evolution is more “entire schema” based, instead of single fields focused like in protobuf or thrift, and usually requires using some kind of schema registry.\nThere are plenty excellent blog posts explaining the various trade-offs between popular serialization formats, one post we would like to highlight is the very well illustrated Schema evolution in Avro, Protocol Buffers and Thrift by Martin Kleppmann.","title":"Picking the right serialization format"},{"location":"/persistence-schema-evolution.html#provided-default-serializers","text":"Pekko Persistence provides Google Protocol Buffers based serializers (using Pekko Serialization) for its own message types such as PersistentReprPersistentRepr, AtomicWriteAtomicWrite and snapshots. Journal plugin implementations may choose to use those provided serializers, or pick a serializer which suits the underlying database better.\nNote Serialization is NOT handled automatically by Pekko Persistence itself. Instead, it only provides the above described serializers, and in case a AsyncWriteJournalAsyncWriteJournal plugin implementation chooses to use them directly, the above serialization scheme will be used. Please refer to your write journal’s documentation to learn more about how it handles serialization! For example, some journals may choose to not use Pekko Serialization at all and instead store the data in a format that is more “native” for the underlying datastore, e.g. using JSON or some other kind of format that the target datastore understands directly.\nThe below figure explains how the default serialization scheme works, and how it fits together with serializing the user provided message itself, which we will from here on refer to as the payload (highlighted in yellow):\nPekko Persistence provided serializers wrap the user payload in an envelope containing all persistence-relevant information. If the Journal uses provided Protobuf serializers for the wrapper types (e.g. PersistentRepr), then the payload will be serialized using the user configured serializer, and if none is provided explicitly, Java serialization will be used for it.\nThe blue colored regions of the PersistentMessage indicate what is serialized using the generated protocol buffers serializers, and the yellow payload indicates the user provided event (by calling persist(payload)(...)persist(payload,...). As you can see, the PersistentMessage acts as an envelope around the payload, adding various fields related to the origin of the event (persistenceId, sequenceNr and more).\nMore advanced techniques (e.g. Remove event class and ignore events) will dive into using the manifests for increasing the flexibility of the persisted vs. exposed types even more. However for now we will focus on the simpler evolution techniques, concerning only configuring the payload serializers.\nBy default the payload will be serialized using Java Serialization. This is fine for testing and initial phases of your development (while you’re still figuring out things, and the data will not need to stay persisted forever). However, once you move to production you should really pick a different serializer for your payloads.\nWarning Do not rely on Java serialization for serious application development! It does not lean itself well to evolving schemas over long periods of time, and its performance is also not very high (it never was designed for high-throughput scenarios).","title":"Provided default serializers"},{"location":"/persistence-schema-evolution.html#configuring-payload-serializers","text":"This section aims to highlight the complete basics on how to define custom serializers using Pekko Serialization. Many journal plugin implementations use Pekko Serialization, thus it is tremendously important to understand how to configure it to work with your event classes.\nNote Read the Pekko Serialization docs to learn more about defining custom serializers.\nThe below snippet explains in the minimal amount of lines how a custom serializer can be registered. For more in-depth explanations on how serialization picks the serializer to use etc, please refer to its documentation.\nFirst we start by defining our domain model class, here representing a person:\nScala copysourcefinal case class Person(name: String, surname: String) Java copysourcestatic class Person {\n  public final String name;\n  public final String surname;\n\n  public Person(String name, String surname) {\n    this.name = name;\n    this.surname = surname;\n  }\n}\nNext we implement a serializer (or extend an existing one to be able to handle the new Person class):\nScala copysource/**\n * Simplest possible serializer, uses a string representation of the Person class.\n *\n * Usually a serializer like this would use a library like:\n * protobuf, kryo, avro, cap'n proto, flatbuffers, SBE or some other dedicated serializer backend\n * to perform the actual to/from bytes marshalling.\n */\nclass SimplestPossiblePersonSerializer extends SerializerWithStringManifest {\n  val Utf8 = Charset.forName(\"UTF-8\")\n\n  val PersonManifest = classOf[Person].getName\n\n  // unique identifier of the serializer\n  def identifier = 1234567\n\n  // extract manifest to be stored together with serialized object\n  override def manifest(o: AnyRef): String = o.getClass.getName\n\n  // serialize the object\n  override def toBinary(obj: AnyRef): Array[Byte] = obj match {\n    case p: Person => s\"\"\"${p.name}|${p.surname}\"\"\".getBytes(Utf8)\n    case _         => throw new IllegalArgumentException(s\"Unable to serialize to bytes, clazz was: ${obj.getClass}!\")\n  }\n\n  // deserialize the object, using the manifest to indicate which logic to apply\n  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =\n    manifest match {\n      case PersonManifest =>\n        val nameAndSurname = new String(bytes, Utf8)\n        val Array(name, surname) = nameAndSurname.split(\"[|]\")\n        Person(name, surname)\n      case _ =>\n        throw new NotSerializableException(\n          s\"Unable to deserialize from bytes, manifest was: $manifest! Bytes length: \" +\n          bytes.length)\n    }\n\n}\n Java copysource/**\n * Simplest possible serializer, uses a string representation of the Person class.\n *\n * <p>Usually a serializer like this would use a library like: protobuf, kryo, avro, cap'n\n * proto, flatbuffers, SBE or some other dedicated serializer backend to perform the actual\n * to/from bytes marshalling.\n */\nstatic class SimplestPossiblePersonSerializer extends SerializerWithStringManifest {\n  private final Charset utf8 = StandardCharsets.UTF_8;\n\n  private final String personManifest = Person.class.getName();\n\n  // unique identifier of the serializer\n  @Override\n  public int identifier() {\n    return 1234567;\n  }\n\n  // extract manifest to be stored together with serialized object\n  @Override\n  public String manifest(Object o) {\n    return o.getClass().getName();\n  }\n\n  // serialize the object\n  @Override\n  public byte[] toBinary(Object obj) {\n    if (obj instanceof Person) {\n      Person p = (Person) obj;\n      return (p.name + \"|\" + p.surname).getBytes(utf8);\n    } else {\n      throw new IllegalArgumentException(\n          \"Unable to serialize to bytes, clazz was: \" + obj.getClass().getName());\n    }\n  }\n\n  // deserialize the object, using the manifest to indicate which logic to apply\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) throws NotSerializableException {\n    if (personManifest.equals(manifest)) {\n      String nameAndSurname = new String(bytes, utf8);\n      String[] parts = nameAndSurname.split(\"[|]\");\n      return new Person(parts[0], parts[1]);\n    } else {\n      throw new NotSerializableException(\n          \"Unable to deserialize from bytes, manifest was: \"\n              + manifest\n              + \"! Bytes length: \"\n              + bytes.length);\n    }\n  }\n}\nAnd finally we register the serializer and bind it to handle the docs.persistence.Person class:\ncopysource# application.conf\npekko {\n  actor {\n    serializers {\n      person = \"docs.persistence.SimplestPossiblePersonSerializer\"\n    }\n\n    serialization-bindings {\n      \"docs.persistence.Person\" = person\n    }\n  }\n}\nDeserialization will be performed by the same serializer which serialized the message initially because of the identifier being stored together with the message.\nPlease refer to the Pekko Serialization documentation for more advanced use of serializers, especially the Serializer with String Manifest section since it is very useful for Persistence based applications dealing with schema evolutions, as we will see in some of the examples below.","title":"Configuring payload serializers"},{"location":"/persistence-schema-evolution.html#schema-evolution-in-action","text":"In this section we will discuss various schema evolution techniques using concrete examples and explaining some of the various options one might go about handling the described situation. The list below is by no means a complete guide, so feel free to adapt these techniques depending on your serializer’s capabilities and/or other domain specific limitations.\nNote Serialization with Jackson has good support for Schema Evolution and many of the scenarios described here can be solved with that Jackson transformation technique instead.","title":"Schema evolution in action"},{"location":"/persistence-schema-evolution.html#add-fields","text":"Situation: You need to add a field to an existing message type. For example, a SeatReserved(letter:String, row:Int)SeatReserved(String letter, int row) now needs to have an associated code which indicates if it is a window or aisle seat.\nSolution: Adding fields is the most common change you’ll need to apply to your messages so make sure the serialization format you picked for your payloads can handle it appropriately, i.e. such changes should be binary compatible. This is achieved using the right serializer toolkit. In the following examples we will be using protobuf. See also how to add fields with Jackson.\nWhile being able to read messages with missing fields is half of the solution, you also need to deal with the missing values somehow. This is usually modeled as some kind of default value, or by representing the field as an Option[T]Optional<T> See below for an example how reading an optional field from a serialized protocol buffers message might look like.\nScala copysourcesealed abstract class SeatType { def code: String }\nobject SeatType {\n  def fromString(s: String) = s match {\n    case Window.code => Window\n    case Aisle.code  => Aisle\n    case Other.code  => Other\n    case _           => Unknown\n  }\n  case object Window extends SeatType { override val code = \"W\" }\n  case object Aisle extends SeatType { override val code = \"A\" }\n  case object Other extends SeatType { override val code = \"O\" }\n  case object Unknown extends SeatType { override val code = \"\" }\n\n}\n\ncase class SeatReserved(letter: String, row: Int, seatType: SeatType) Java copysourcestatic enum SeatType {\n  Window(\"W\"),\n  Aisle(\"A\"),\n  Other(\"O\"),\n  Unknown(\"\");\n\n  private final String code;\n\n  private SeatType(String code) {\n    this.code = code;\n  }\n\n  public static SeatType fromCode(String c) {\n    if (Window.code.equals(c)) return Window;\n    else if (Aisle.code.equals(c)) return Aisle;\n    else if (Other.code.equals(c)) return Other;\n    else return Unknown;\n  }\n}\nstatic class SeatReserved {\n  public final String letter;\n  public final int row;\n  public final SeatType seatType;\n\n  public SeatReserved(String letter, int row, SeatType seatType) {\n    this.letter = letter;\n    this.row = row;\n    this.seatType = seatType;\n  }\n}\nNext we prepare a protocol definition using the protobuf Interface Description Language, which we’ll use to generate the serializer code to be used on the Pekko Serialization layer (notice that the schema approach allows us to rename fields, as long as the numeric identifiers of the fields do not change):\ncopysource// FlightAppModels.proto\noption java_package = \"docs.persistence.proto\";\noption optimize_for = SPEED;\n\nmessage SeatReserved {\n  required string letter   = 1;\n  required uint32 row      = 2;\n  optional string seatType = 3; // the new field\n}\nThe serializer implementation uses the protobuf generated classes to marshall the payloads. Optional fields can be handled explicitly or missing values by calling the has... methods on the protobuf object, which we do for seatType in order to use a Unknown type in case the event was stored before we had introduced the field to this event type:\nScala copysource/**\n * Example serializer impl which uses protocol buffers generated classes (proto.*)\n * to perform the to/from binary marshalling.\n */\nclass AddedFieldsSerializerWithProtobuf extends SerializerWithStringManifest {\n  override def identifier = 67876\n\n  final val SeatReservedManifest = classOf[SeatReserved].getName\n\n  override def manifest(o: AnyRef): String = o.getClass.getName\n\n  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =\n    manifest match {\n      case SeatReservedManifest =>\n        // use generated protobuf serializer\n        seatReserved(FlightAppModels.SeatReserved.parseFrom(bytes))\n      case _ =>\n        throw new NotSerializableException(\"Unable to handle manifest: \" + manifest)\n    }\n\n  override def toBinary(o: AnyRef): Array[Byte] = o match {\n    case s: SeatReserved =>\n      FlightAppModels.SeatReserved.newBuilder\n        .setRow(s.row)\n        .setLetter(s.letter)\n        .setSeatType(s.seatType.code)\n        .build()\n        .toByteArray\n  }\n\n  // -- fromBinary helpers --\n\n  private def seatReserved(p: FlightAppModels.SeatReserved): SeatReserved =\n    SeatReserved(p.getLetter, p.getRow, seatType(p))\n\n  // handle missing field by assigning \"Unknown\" value\n  private def seatType(p: FlightAppModels.SeatReserved): SeatType =\n    if (p.hasSeatType) SeatType.fromString(p.getSeatType) else SeatType.Unknown\n\n} Java copysource/**\n * Example serializer impl which uses protocol buffers generated classes (proto.*) to perform the\n * to/from binary marshalling.\n */\nstatic class AddedFieldsSerializerWithProtobuf extends SerializerWithStringManifest {\n  @Override\n  public int identifier() {\n    return 67876;\n  }\n\n  private final String seatReservedManifest = SeatReserved.class.getName();\n\n  @Override\n  public String manifest(Object o) {\n    return o.getClass().getName();\n  }\n\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) throws NotSerializableException {\n    if (seatReservedManifest.equals(manifest)) {\n      // use generated protobuf serializer\n      try {\n        return seatReserved(FlightAppModels.SeatReserved.parseFrom(bytes));\n      } catch (InvalidProtocolBufferException e) {\n        throw new IllegalArgumentException(e.getMessage());\n      }\n    } else {\n      throw new NotSerializableException(\"Unable to handle manifest: \" + manifest);\n    }\n  }\n\n  @Override\n  public byte[] toBinary(Object o) {\n    if (o instanceof SeatReserved) {\n      SeatReserved s = (SeatReserved) o;\n      return FlightAppModels.SeatReserved.newBuilder()\n          .setRow(s.row)\n          .setLetter(s.letter)\n          .setSeatType(s.seatType.code)\n          .build()\n          .toByteArray();\n\n    } else {\n      throw new IllegalArgumentException(\"Unable to handle: \" + o);\n    }\n  }\n\n  // -- fromBinary helpers --\n\n  private SeatReserved seatReserved(FlightAppModels.SeatReserved p) {\n    return new SeatReserved(p.getLetter(), p.getRow(), seatType(p));\n  }\n\n  // handle missing field by assigning \"Unknown\" value\n  private SeatType seatType(FlightAppModels.SeatReserved p) {\n    if (p.hasSeatType()) return SeatType.fromCode(p.getSeatType());\n    else return SeatType.Unknown;\n  }\n}","title":"Add fields"},{"location":"/persistence-schema-evolution.html#rename-fields","text":"Situation: When first designing the system the SeatReserved event featured a code field. After some time you discover that what was originally called code actually means seatNr, thus the model should be changed to reflect this concept more accurately.\nSolution 1 - using IDL based serializers: First, we will discuss the most efficient way of dealing with such kinds of schema changes – IDL based serializers.\nIDL stands for Interface Description Language, and means that the schema of the messages that will be stored is based on this description. Most IDL based serializers also generate the serializer / deserializer code so that using them is not too hard. Examples of such serializers are protobuf or thrift.\nUsing these libraries rename operations are “free”, because the field name is never actually stored in the binary representation of the message. This is one of the advantages of schema based serializers, even though that they add the overhead of having to maintain the schema. When using serializers like this, no additional code change (except renaming the field and method used during serialization) is needed to perform such evolution:\nThis is how such a rename would look in protobuf:\ncopysource// protobuf message definition, BEFORE:\nmessage SeatReserved {\n  required string code = 1;\n}\n\n// protobuf message definition, AFTER:\nmessage SeatReserved {\n  required string seatNr = 1; // field renamed, id remains the same\n}\nIt is important to learn about the strengths and limitations of your serializers, in order to be able to move swiftly and refactor your models fearlessly as you go on with the project.\nNote Learn in-depth about the serialization engine you’re using as it will impact how you can approach schema evolution. Some operations are “free” in certain serialization formats (more often than not: removing/adding optional fields, sometimes renaming fields etc.), while some other operations are strictly not possible.\nSolution 2 - by manually handling the event versions: Another solution, in case your serialization format does not support renames like the above mentioned formats, is versioning your schema. For example, you could have made your events carry an additional field called _version which was set to 1 (because it was the initial schema), and once you change the schema you bump this number to 2, and write an adapter which can perform the rename.\nThis approach is popular when your serialization format is something like JSON, where renames can not be performed automatically by the serializer. See also how to rename fields with Jackson, which is using this kind of versioning approach.\nThe following snippet showcases how one could apply renames if working with plain JSON (using spray.json.JsObjecta JsObject as an example JSON representation):\nScala copysourceclass JsonRenamedFieldAdapter extends EventAdapter {\n  val marshaller = new ExampleJsonMarshaller\n\n  val V1 = \"v1\"\n  val V2 = \"v2\"\n\n  // this could be done independently for each event type\n  override def manifest(event: Any): String = V2\n\n  override def toJournal(event: Any): JsObject =\n    marshaller.toJson(event)\n\n  override def fromJournal(event: Any, manifest: String): EventSeq = event match {\n    case json: JsObject =>\n      EventSeq(marshaller.fromJson(manifest match {\n        case V1      => rename(json, \"code\", \"seatNr\")\n        case V2      => json // pass-through\n        case unknown => throw new IllegalArgumentException(s\"Unknown manifest: $unknown\")\n      }))\n    case _ =>\n      val c = event.getClass\n      throw new IllegalArgumentException(\"Can only work with JSON, was: %s\".format(c))\n  }\n\n  def rename(json: JsObject, from: String, to: String): JsObject = {\n    val value = json.fields(from)\n    val withoutOld = json.fields - from\n    JsObject(withoutOld + (to -> value))\n  }\n\n} Java copysourcestatic class JsonRenamedFieldAdapter implements EventAdapter {\n  // use your favorite json library\n  private final ExampleJsonMarshaller marshaller = new ExampleJsonMarshaller();\n\n  private final String V1 = \"v1\";\n  private final String V2 = \"v2\";\n\n  // this could be done independently for each event type\n  @Override\n  public String manifest(Object event) {\n    return V2;\n  }\n\n  @Override\n  public JsObject toJournal(Object event) {\n    return marshaller.toJson(event);\n  }\n\n  @Override\n  public EventSeq fromJournal(Object event, String manifest) {\n    if (event instanceof JsObject) {\n      JsObject json = (JsObject) event;\n      if (V1.equals(manifest)) json = rename(json, \"code\", \"seatNr\");\n      return EventSeq.single(json);\n    } else {\n      throw new IllegalArgumentException(\n          \"Can only work with JSON, was: \" + event.getClass().getName());\n    }\n  }\n\n  private JsObject rename(JsObject json, String from, String to) {\n    // use your favorite json library to rename the field\n    JsObject renamed = json;\n    return renamed;\n  }\n}\nAs you can see, manually handling renames induces some boilerplate onto the EventAdapter, however much of it you will find is common infrastructure code that can be either provided by an external library (for promotion management) or put together in a simple helper traitclass.\nNote The technique of versioning events and then promoting them to the latest version using JSON transformations can be applied to more than just field renames – it also applies to adding fields and all kinds of changes in the message format.","title":"Rename fields"},{"location":"/persistence-schema-evolution.html#remove-event-class-and-ignore-events","text":"Situation: While investigating app performance you notice that unreasonable amounts of CustomerBlinked events are being stored for every customer each time he/she blinks. Upon investigation, you decide that the event does not add any value and should be deleted. You still have to be able to replay from a journal which contains those old CustomerBlinked events though.\nNaive solution - drop events in EventAdapter:\nThe problem of removing an event type from the domain model is not as much its removal, as the implications for the recovery mechanisms that this entails. For example, a naive way of filtering out certain kinds of events from being delivered to a recovering PersistentActor is pretty simple, as one can filter them out in an EventAdapter:\nThe EventAdapterEventAdapter can drop old events (**O**) by emitting an empty EventSeqEventSeq. Other events can be passed through (**E**).\nThis however does not address the underlying cost of having to deserialize all the events during recovery, even those which will be filtered out by the adapter. In the next section we will improve the above explained mechanism to avoid deserializing events which would be filtered out by the adapter anyway, thus allowing to save precious time during a recovery containing lots of such events (without actually having to delete them).\nImproved solution - deserialize into tombstone:\nIn the just described technique we have saved the PersistentActor from receiving un-wanted events by filtering them out in the EventAdapterEventAdapter, however the event itself still was deserialized and loaded into memory. This has two notable downsides:\nfirst, that the deserialization was actually performed, so we spent some of our time budget on the deserialization, even though the event does not contribute anything to the persistent actors state. second, that we are unable to remove the event class from the system – since the serializer still needs to create the actual instance of it, as it does not know it will not be used.\nThe solution to these problems is to use a serializer that is aware of that event being no longer needed, and can notice this before starting to deserialize the object.\nThis approach allows us to remove the original class from our classpath, which makes for less “old” classes lying around in the project. This can for example be implemented by using an SerializerWithStringManifestSerializerWithStringManifest (documented in depth in Serializer with String Manifest). By looking at the string manifest, the serializer can notice that the type is no longer needed, and skip the deserialization all-together:\nThe serializer is aware of the old event types that need to be skipped (**O**), and can skip deserializing them altogether by returning a “tombstone” (**T**), which the EventAdapter converts into an empty EventSeq. Other events (**E**) can just be passed through.\nThe serializer detects that the string manifest points to a removed event type and skips attempting to deserialize it:\nScala copysourcecase object EventDeserializationSkipped\n\nclass RemovedEventsAwareSerializer extends SerializerWithStringManifest {\n  val utf8 = Charset.forName(\"UTF-8\")\n  override def identifier: Int = 8337\n\n  val SkipEventManifestsEvents = Set(\"docs.persistence.CustomerBlinked\" // ...\n  )\n\n  override def manifest(o: AnyRef): String = o.getClass.getName\n\n  override def toBinary(o: AnyRef): Array[Byte] = o match {\n    case _ => o.toString.getBytes(utf8) // example serialization\n  }\n\n  override def fromBinary(bytes: Array[Byte], manifest: String): AnyRef =\n    manifest match {\n      case m if SkipEventManifestsEvents.contains(m) =>\n        EventDeserializationSkipped\n\n      case _ => new String(bytes, utf8)\n    }\n} Java copysourcestatic class EventDeserializationSkipped {\n  public static EventDeserializationSkipped instance = new EventDeserializationSkipped();\n\n  private EventDeserializationSkipped() {}\n}\n\nstatic class RemovedEventsAwareSerializer extends SerializerWithStringManifest {\n  private final Charset utf8 = StandardCharsets.UTF_8;\n  private final String customerBlinkedManifest = \"blinked\";\n\n  // unique identifier of the serializer\n  @Override\n  public int identifier() {\n    return 8337;\n  }\n\n  // extract manifest to be stored together with serialized object\n  @Override\n  public String manifest(Object o) {\n    if (o instanceof CustomerBlinked) return customerBlinkedManifest;\n    else return o.getClass().getName();\n  }\n\n  @Override\n  public byte[] toBinary(Object o) {\n    return o.toString().getBytes(utf8); // example serialization\n  }\n\n  @Override\n  public Object fromBinary(byte[] bytes, String manifest) {\n    if (customerBlinkedManifest.equals(manifest)) return EventDeserializationSkipped.instance;\n    else return new String(bytes, utf8);\n  }\n}\nThe EventAdapter we implemented is aware of EventDeserializationSkipped events (our “Tombstones”), and emits and empty EventSeq whenever such object is encountered:\nScala copysourceclass SkippedEventsAwareAdapter extends EventAdapter {\n  override def manifest(event: Any) = \"\"\n  override def toJournal(event: Any) = event\n\n  override def fromJournal(event: Any, manifest: String) = event match {\n    case EventDeserializationSkipped => EventSeq.empty\n    case _                           => EventSeq(event)\n  }\n} Java copysourcestatic class SkippedEventsAwareAdapter implements EventAdapter {\n  @Override\n  public String manifest(Object event) {\n    return \"\";\n  }\n\n  @Override\n  public Object toJournal(Object event) {\n    return event;\n  }\n\n  @Override\n  public EventSeq fromJournal(Object event, String manifest) {\n    if (event == EventDeserializationSkipped.instance) return EventSeq.empty();\n    else return EventSeq.single(event);\n  }\n}","title":"Remove event class and ignore events"},{"location":"/persistence-schema-evolution.html#detach-domain-model-from-data-model","text":"Situation: You want to separate the application model (often called the “domain model”) completely from the models used to persist the corresponding events (the “data model”). For example because the data representation may change independently of the domain model.\nAnother situation where this technique may be useful is when your serialization tool of choice requires generated classes to be used for serialization and deserialization of objects, like for example Google Protocol Buffers do, yet you do not want to leak this implementation detail into the domain model itself, which you’d like to model as plain Scala caseJava classes.\nSolution: In order to detach the domain model, which is often represented using pure Scala (case)Java classes, from the data model classes which very often may be less user-friendly yet highly optimised for throughput and schema evolution (like the classes generated by protobuf for example), it is possible to use a simple EventAdapter which maps between these types in a 1:1 style as illustrated below:\nDomain events (**A**) are adapted to the data model events (**D**) by the EventAdapter. The data model can be a format natively understood by the journal, such that it can store it more efficiently or include additional data for the event (e.g. tags), for ease of later querying.\nWe will use the following domain and data models to showcase how the separation can be implemented by the adapter:\nScala copysource/** Domain model - highly optimised for domain language and maybe \"fluent\" usage */\nobject DomainModel {\n  final case class Customer(name: String)\n  final case class Seat(code: String) {\n    def bookFor(customer: Customer): SeatBooked = SeatBooked(code, customer)\n  }\n\n  final case class SeatBooked(code: String, customer: Customer)\n}\n\n/** Data model - highly optimised for schema evolution and persistence */\nobject DataModel {\n  final case class SeatBooked(code: String, customerName: String)\n} Java copysource// Domain model - highly optimised for domain language and maybe \"fluent\" usage\nstatic class Customer {\n  public final String name;\n\n  public Customer(String name) {\n    this.name = name;\n  }\n}\n\nstatic class Seat {\n  public final String code;\n\n  public Seat(String code) {\n    this.code = code;\n  }\n\n  public SeatBooked bookFor(Customer customer) {\n    return new SeatBooked(code, customer);\n  }\n}\n\nstatic class SeatBooked {\n  public final String code;\n  public final Customer customer;\n\n  public SeatBooked(String code, Customer customer) {\n    this.code = code;\n    this.customer = customer;\n  }\n}\n\n// Data model - highly optimised for schema evolution and persistence\nstatic class SeatBookedData {\n  public final String code;\n  public final String customerName;\n\n  public SeatBookedData(String code, String customerName) {\n    this.code = code;\n    this.customerName = customerName;\n  }\n}\nThe EventAdapterEventAdapter takes care of converting from one model to the other one (in both directions), allowing the models to be completely detached from each other, such that they can be optimised independently as long as the mapping logic is able to convert between them:\nScala copysourceclass DetachedModelsAdapter extends EventAdapter {\n  override def manifest(event: Any): String = \"\"\n\n  override def toJournal(event: Any): Any = event match {\n    case DomainModel.SeatBooked(code, customer) =>\n      DataModel.SeatBooked(code, customer.name)\n  }\n  override def fromJournal(event: Any, manifest: String): EventSeq = event match {\n    case DataModel.SeatBooked(code, customerName) =>\n      EventSeq(DomainModel.SeatBooked(code, DomainModel.Customer(customerName)))\n  }\n} Java copysourceclass DetachedModelsAdapter implements EventAdapter {\n  @Override\n  public String manifest(Object event) {\n    return \"\";\n  }\n\n  @Override\n  public Object toJournal(Object event) {\n    if (event instanceof SeatBooked) {\n      SeatBooked s = (SeatBooked) event;\n      return new SeatBookedData(s.code, s.customer.name);\n    } else {\n      throw new IllegalArgumentException(\"Unsupported: \" + event.getClass());\n    }\n  }\n\n  @Override\n  public EventSeq fromJournal(Object event, String manifest) {\n    if (event instanceof SeatBookedData) {\n      SeatBookedData d = (SeatBookedData) event;\n      return EventSeq.single(new SeatBooked(d.code, new Customer(d.customerName)));\n    } else {\n      throw new IllegalArgumentException(\"Unsupported: \" + event.getClass());\n    }\n  }\n}\nThe same technique could also be used directly in the Serializer if the end result of marshalling is bytes. Then the serializer can simply convert the bytes do the domain object by using the generated protobuf builders.","title":"Detach domain model from data model"},{"location":"/persistence-schema-evolution.html#store-events-as-human-readable-data-model","text":"Situation: You want to keep your persisted events in a human-readable format, for example JSON.\nSolution: This is a special case of the Detach domain model from data model pattern, and thus requires some co-operation from the Journal implementation to achieve this.\nAn example of a Journal which may implement this pattern is MongoDB, however other databases such as PostgreSQL and Cassandra could also do it because of their built-in JSON capabilities.\nIn this approach, the EventAdapterEventAdapter is used as the marshalling layer: it serializes the events to/from JSON. The journal plugin notices that the incoming event type is JSON (for example by performing a match on the incoming event) and stores the incoming object directly.\nScala copysourceclass JsonDataModelAdapter extends EventAdapter {\n  override def manifest(event: Any): String = \"\"\n\n  val marshaller = new ExampleJsonMarshaller\n\n  override def toJournal(event: Any): JsObject =\n    marshaller.toJson(event)\n\n  override def fromJournal(event: Any, manifest: String): EventSeq = event match {\n    case json: JsObject =>\n      EventSeq(marshaller.fromJson(json))\n    case _ =>\n      throw new IllegalArgumentException(\"Unable to fromJournal a non-JSON object! Was: \" + event.getClass)\n  }\n} Java copysourcestatic class JsonDataModelAdapter implements EventAdapter {\n\n  // use your favorite json library\n  private final ExampleJsonMarshaller marshaller = new ExampleJsonMarshaller();\n\n  @Override\n  public String manifest(Object event) {\n    return \"\";\n  }\n\n  @Override\n  public JsObject toJournal(Object event) {\n    return marshaller.toJson(event);\n  }\n\n  @Override\n  public EventSeq fromJournal(Object event, String manifest) {\n    if (event instanceof JsObject) {\n      JsObject json = (JsObject) event;\n      return EventSeq.single(marshaller.fromJson(json));\n    } else {\n      throw new IllegalArgumentException(\n          \"Unable to fromJournal a non-JSON object! Was: \" + event.getClass());\n    }\n  }\n}\nNote This technique only applies if the Pekko Persistence plugin you are using provides this capability. Check the documentation of your favourite plugin to see if it supports this style of persistence. If it doesn’t, you may want to skim the list of existing journal plugins, just in case some other plugin for your favourite datastore does provide this capability.\nAlternative solution:\nIn fact, an AsyncWriteJournal implementation could natively decide to not use binary serialization at all, and always serialize the incoming messages as JSON - in which case the toJournal implementation of the EventAdapterEventAdapter would be an identity function, and the fromJournal would need to de-serialize messages from JSON.\nNote If in need of human-readable events on the write-side of your application reconsider whether preparing materialized views using Persistence Query would not be an efficient way to go about this, without compromising the write-side’s throughput characteristics. If indeed you want to use a human-readable representation on the write-side, pick a Persistence plugin that provides that functionality, or – implement one yourself.","title":"Store events as human-readable data model"},{"location":"/persistence-schema-evolution.html#split-large-event-into-fine-grained-events","text":"Situation: While refactoring your domain events, you find that one of the events has become too large (coarse-grained) and needs to be split up into multiple fine-grained events.\nSolution: Let us consider a situation where an event represents “user details changed”. After some time we discover that this event is too coarse, and needs to be split into “user name changed” and “user address changed”, because somehow users keep changing their usernames a lot and we’d like to keep this as a separate event.\nThe write side change is very simple, we persist UserNameChanged or UserAddressChanged depending on what the user actually intended to change (instead of the composite UserDetailsChanged that we had in version 1 of our model).\nThe EventAdapter splits the incoming event into smaller more fine-grained events during recovery.\nDuring recovery however, we now need to convert the old V1 model into the V2 representation of the change. Depending if the old event contains a name change, we either emit the UserNameChanged or we don’t, and the address change is handled similarly:\nScala copysourcetrait Version1\ntrait Version2\n\n// V1 event:\nfinal case class UserDetailsChanged(name: String, address: String) extends Version1\n\n// corresponding V2 events:\nfinal case class UserNameChanged(name: String) extends Version2\nfinal case class UserAddressChanged(address: String) extends Version2\n\n// event splitting adapter:\nclass UserEventsAdapter extends EventAdapter {\n  override def manifest(event: Any): String = \"\"\n\n  override def fromJournal(event: Any, manifest: String): EventSeq = event match {\n    case UserDetailsChanged(null, address) => EventSeq(UserAddressChanged(address))\n    case UserDetailsChanged(name, null)    => EventSeq(UserNameChanged(name))\n    case UserDetailsChanged(name, address) =>\n      EventSeq(UserNameChanged(name), UserAddressChanged(address))\n    case event: Version2 => EventSeq(event)\n  }\n\n  override def toJournal(event: Any): Any = event\n} Java copysourceinterface Version1 {};\n\ninterface Version2 {}\n\n// V1 event:\nstatic class UserDetailsChanged implements Version1 {\n  public final String name;\n  public final String address;\n\n  public UserDetailsChanged(String name, String address) {\n    this.name = name;\n    this.address = address;\n  }\n}\n\n// corresponding V2 events:\nstatic class UserNameChanged implements Version2 {\n  public final String name;\n\n  public UserNameChanged(String name) {\n    this.name = name;\n  }\n}\nstatic class UserAddressChanged implements Version2 {\n  public final String address;\n\n  public UserAddressChanged(String address) {\n    this.address = address;\n  }\n}\n\n// event splitting adapter:\nstatic class UserEventsAdapter implements EventAdapter {\n  @Override\n  public String manifest(Object event) {\n    return \"\";\n  }\n\n  @Override\n  public EventSeq fromJournal(Object event, String manifest) {\n    if (event instanceof UserDetailsChanged) {\n      UserDetailsChanged c = (UserDetailsChanged) event;\n      if (c.name == null) return EventSeq.single(new UserAddressChanged(c.address));\n      else if (c.address == null) return EventSeq.single(new UserNameChanged(c.name));\n      else return EventSeq.create(new UserNameChanged(c.name), new UserAddressChanged(c.address));\n    } else {\n      return EventSeq.single(event);\n    }\n  }\n\n  @Override\n  public Object toJournal(Object event) {\n    return event;\n  }\n}\nBy returning an EventSeq from the event adapter, the recovered event can be converted to multiple events before being delivered to the persistent actor.","title":"Split large event into fine-grained events"},{"location":"/persistence-query.html","text":"","title":"Apache Persistence Query"},{"location":"/persistence-query.html#apache-persistence-query","text":"","title":"Apache Persistence Query"},{"location":"/persistence-query.html#dependency","text":"To use Persistence Query, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence-query\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-query_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-query_${versions.ScalaBinary}\"\n}\nThis will also add dependency on the Pekko Persistence module.","title":"Dependency"},{"location":"/persistence-query.html#introduction","text":"Pekko persistence query complements Event Sourcing by providing a universal asynchronous stream based query interface that various journal plugins can implement in order to expose their query capabilities.\nThe most typical use case of persistence query is implementing the so-called query side (also known as “read side”) in the popular CQRS architecture pattern - in which the writing side of the application (e.g. implemented using Pekko persistence) is completely separated from the “query side”. Pekko Persistence Query itself is not directly the query side of an application, however it can help to migrate data from the write side to the query side database. In very simple scenarios Persistence Query may be powerful enough to fulfill the query needs of your app, however we highly recommend (in the spirit of CQRS) of splitting up the write/read sides into separate datastores as the need arises.\nFor a similar implementation of query interface to Durable State Behaviors please refer to Persistence Query using Durable State.\nThe Microservices with Pekko tutorial explains how to implement an Event Sourced CQRS application with Pekko Persistence and Pekko Projections.","title":"Introduction"},{"location":"/persistence-query.html#design-overview","text":"Pekko persistence query is purposely designed to be a very loosely specified API. This is in order to keep the provided APIs general enough for each journal implementation to be able to expose its best features, e.g. a SQL journal can use complex SQL queries or if a journal is able to subscribe to a live event stream this should also be possible to expose the same API - a typed stream of events.\nEach read journal must explicitly document which types of queries it supports. Refer to your journal’s plugins documentation for details on which queries and semantics it supports.\nWhile Pekko Persistence Query does not provide actual implementations of ReadJournals, it defines a number of pre-defined query types for the most common query scenarios, that most journals are likely to implement (however they are not required to).","title":"Design overview"},{"location":"/persistence-query.html#read-journals","text":"In order to issue queries one has to first obtain an instance of a ReadJournalReadJournal. Read journals are implemented as Community plugins, each targeting a specific datastore (for example Cassandra or JDBC databases). For example, given a library that provides a pekko.persistence.query.my-read-journal obtaining the related journal is as simple as:\nScala copysource// obtain read journal by plugin id\nval readJournal =\n  PersistenceQuery(system).readJournalFor[MyScaladslReadJournal](\"pekko.persistence.query.my-read-journal\")\n\n// issue query to journal\nval source: Source[EventEnvelope, NotUsed] =\n  readJournal.eventsByPersistenceId(\"user-1337\", 0, Long.MaxValue)\n\n// materialize stream, consuming events\nsource.runForeach { event =>\n  println(\"Event: \" + event)\n} Java copysource// obtain read journal by plugin id\nfinal MyJavadslReadJournal readJournal =\n    PersistenceQuery.get(system)\n        .getReadJournalFor(\n            MyJavadslReadJournal.class, \"pekko.persistence.query.my-read-journal\");\n\n// issue query to journal\nSource<EventEnvelope, NotUsed> source =\n    readJournal.eventsByPersistenceId(\"user-1337\", 0, Long.MAX_VALUE);\n\n// materialize stream, consuming events\nsource.runForeach(event -> System.out.println(\"Event: \" + event), system);\nJournal implementers are encouraged to put this identifier in a variable known to the user, such that one can access it via readJournalFor[NoopJournal](NoopJournal.identifier)getJournalFor(NoopJournal.class, NoopJournal.identifier), however this is not enforced.\nRead journal implementations are available as Community plugins.","title":"Read Journals"},{"location":"/persistence-query.html#predefined-queries","text":"Pekko persistence query comes with a number of query interfaces built in and suggests Journal implementors to implement them according to the semantics described below. It is important to notice that while these query types are very common a journal is not obliged to implement all of them - for example because in a given journal such query would be significantly inefficient.\nNote Refer to the documentation of the ReadJournalReadJournal plugin you are using for a specific list of supported query types. For example, Journal plugins should document their stream completion strategies.\nThe predefined queries are:","title":"Predefined queries"},{"location":"/persistence-query.html#persistenceidsquery-and-currentpersistenceidsquery","text":"persistenceIdspersistenceIds which is designed to allow users to subscribe to a stream of all persistent ids in the system. By default this stream should be assumed to be a “live” stream, which means that the journal should keep emitting new persistence ids as they come into the system:\nScala copysourcereadJournal.persistenceIds() Java copysourcereadJournal.persistenceIds();\nIf your usage does not require a live stream, you can use the currentPersistenceIdscurrentPersistenceIds query:\nScala copysourcereadJournal.currentPersistenceIds() Java copysourcereadJournal.currentPersistenceIds();","title":"PersistenceIdsQuery and CurrentPersistenceIdsQuery"},{"location":"/persistence-query.html#eventsbypersistenceidquery-and-currenteventsbypersistenceidquery","text":"eventsByPersistenceIdeventsByPersistenceId is a query equivalent to replaying an event sourced actor, however, since it is a stream it is possible to keep it alive and watch for additional incoming events persisted by the persistent actor identified by the given persistenceId.\nScala copysourcereadJournal.eventsByPersistenceId(\"user-us-1337\", fromSequenceNr = 0L, toSequenceNr = Long.MaxValue)\n Java copysourcereadJournal.eventsByPersistenceId(\"user-us-1337\", 0L, Long.MAX_VALUE);\nMost journals will have to revert to polling in order to achieve this, which can typically be configured with a refresh-interval configuration property.\nIf your usage does not require a live stream, you can use the currentEventsByPersistenceIdcurrentEventsByPersistenceId query.","title":"EventsByPersistenceIdQuery and CurrentEventsByPersistenceIdQuery"},{"location":"/persistence-query.html#eventsbytag-and-currenteventsbytag","text":"eventsByTageventsByTag allows querying events regardless of which persistenceId they are associated with. This query is hard to implement in some journals or may need some additional preparation of the used data store to be executed efficiently. The goal of this query is to allow querying for all events which are “tagged” with a specific tag. That includes the use case to query all domain events of an Aggregate Root type. Please refer to your read journal plugin’s documentation to find out if and how it is supported.\nSome journals may support tagging of events or Event Adapters that wraps the events in a persistence.journal.Taggedpersistence.journal.Tagged with the given tags. The journal may support other ways of doing tagging - again, how exactly this is implemented depends on the used journal. Here is an example of such a tagging with an EventSourcedBehaviorEventSourcedBehavior:\nScala copysourceval NumberOfEntityGroups = 10\n\ndef tagEvent(entityId: String, event: Event): Set[String] = {\n  val entityGroup = s\"group-${math.abs(entityId.hashCode % NumberOfEntityGroups)}\"\n  event match {\n    case _: OrderCompleted => Set(entityGroup, \"order-completed\")\n    case _                 => Set(entityGroup)\n  }\n}\n\ndef apply(entityId: String): Behavior[Command] = {\n  EventSourcedBehavior[Command, Event, State](\n    persistenceId = PersistenceId(\"ShoppingCart\", entityId),\n    emptyState = State(),\n    commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"),\n    eventHandler = (state, evt) => throw new NotImplementedError(\"TODO: process the event return the next state\"))\n    .withTagger(event => tagEvent(entityId, event))\n} Java copysourceprivate final String entityId;\n\npublic static final int NUMBER_OF_ENTITY_GROUPS = 10;\n\n@Override\npublic Set<String> tagsFor(Event event) {\n  String entityGroup = \"group-\" + Math.abs(entityId.hashCode() % NUMBER_OF_ENTITY_GROUPS);\n  Set<String> tags = new HashSet<>();\n  tags.add(entityGroup);\n  if (event instanceof OrderCompleted) tags.add(\"order-completed\");\n  return tags;\n}\nNote A very important thing to keep in mind when using queries spanning multiple persistenceIds, such as EventsByTagEventsByTag is that the order of events at which the events appear in the stream rarely is guaranteed (or stable between materializations). Journals may choose to opt for strict ordering of the events, and should then document explicitly what kind of ordering guarantee they provide - for example “ordered by timestamp ascending, independently of persistenceId” is easy to achieve on relational databases, yet may be hard to implement efficiently on plain key-value datastores.\nIn the example below we query all events which have been tagged (we assume this was performed by the write-side using tagging of events or Event Adapters, or that the journal is smart enough that it can figure out what we mean by this tag - for example if the journal stored the events as json it may try to find those with the field tag set to this value etc.).\nScala copysource// assuming journal is able to work with numeric offsets we can:\n\nval completedOrders: Source[EventEnvelope, NotUsed] =\n  readJournal.eventsByTag(\"order-completed\", Offset.noOffset)\n\n// find first 10 completed orders:\nval firstCompleted: Future[Vector[OrderCompleted]] =\n  completedOrders\n    .map(_.event)\n    .collectType[OrderCompleted]\n    .take(10) // cancels the query stream after pulling 10 elements\n    .runFold(Vector.empty[OrderCompleted])(_ :+ _)\n\n// start another query, from the known offset\nval furtherOrders = readJournal.eventsByTag(\"order-completed\", offset = Sequence(10)) Java copysource// assuming journal is able to work with numeric offsets we can:\nfinal Source<EventEnvelope, NotUsed> completedOrders =\n    readJournal.eventsByTag(\"order-completed\", new Sequence(0L));\n\n// find first 10 completed orders:\nfinal CompletionStage<List<OrderCompleted>> firstCompleted =\n    completedOrders\n        .map(EventEnvelope::event)\n        .collectType(OrderCompleted.class)\n        .take(10) // cancels the query stream after pulling 10 elements\n        .runFold(\n            new ArrayList<>(10),\n            (acc, e) -> {\n              acc.add(e);\n              return acc;\n            },\n            system);\n\n// start another query, from the known offset\nSource<EventEnvelope, NotUsed> furtherOrders =\n    readJournal.eventsByTag(\"order-completed\", new Sequence(10));\nAs you can see, we can use all the usual stream operators available from Streams on the resulting query stream, including for example taking the first 10 and cancelling the stream. It is worth pointing out that the built-in EventsByTag query has an optionally supported offset parameter (of type Long) which the journals can use to implement resumable-streams. For example a journal may be able to use a WHERE clause to begin the read starting from a specific row, or in a datastore that is able to order events by insertion time it could treat the Long as a timestamp and select only older events.\nIf your usage does not require a live stream, you can use the currentEventsByTagcurrentEventsByTag query.","title":"EventsByTag and CurrentEventsByTag"},{"location":"/persistence-query.html#eventsbyslice-and-currenteventsbyslice","text":"Query events for given entity type and slices. A slice is deterministically defined based on the persistence id. The purpose is to evenly distribute all persistence ids over the slices.\nSee EventsBySliceQueryEventsBySliceQuery and CurrentEventsBySliceQueryCurrentEventsBySliceQuery.","title":"EventsBySlice and CurrentEventsBySlice"},{"location":"/persistence-query.html#materialized-values-of-queries","text":"Journals are able to provide additional information related to a query by exposing Materialized values, which are a feature of Streams that allows to expose additional values at stream materialization time.\nMore advanced query journals may use this technique to expose information about the character of the materialized stream, for example if it’s finite or infinite, strictly ordered or not ordered at all. The materialized value type is defined as the second type parameter of the returned SourceSource, which allows journals to provide users with their specialised query object, as demonstrated in the sample below:\nScala copysourcefinal case class RichEvent(tags: Set[String], payload: Any)\n\n// a plugin can provide:\ncase class QueryMetadata(deterministicOrder: Boolean, infinite: Boolean) Java copysourcestatic class RichEvent {\n  public final Set<String> tags;\n  public final Object payload;\n\n  public RichEvent(Set<String> tags, Object payload) {\n    this.tags = tags;\n    this.payload = payload;\n  }\n}\n// a plugin can provide:\nstatic final class QueryMetadata {\n  public final boolean deterministicOrder;\n  public final boolean infinite;\n\n  public QueryMetadata(boolean deterministicOrder, boolean infinite) {\n    this.deterministicOrder = deterministicOrder;\n    this.infinite = infinite;\n  }\n}\nScala copysourcedef byTagsWithMeta(tags: Set[String]): Source[RichEvent, QueryMetadata] = { Java copysourcepublic Source<RichEvent, QueryMetadata> byTagsWithMeta(Set<String> tags) {\nScala copysourceval query: Source[RichEvent, QueryMetadata] =\n  readJournal.byTagsWithMeta(Set(\"red\", \"blue\"))\n\nquery\n  .mapMaterializedValue { meta =>\n    println(\n      s\"The query is: \" +\n      s\"ordered deterministically: ${meta.deterministicOrder}, \" +\n      s\"infinite: ${meta.infinite}\")\n  }\n  .map { event =>\n    println(s\"Event payload: ${event.payload}\")\n  }\n  .runWith(Sink.ignore)\n Java copysource Set<String> tags = new HashSet<String>();\ntags.add(\"red\");\ntags.add(\"blue\");\nfinal Source<RichEvent, QueryMetadata> events =\n    readJournal\n        .byTagsWithMeta(tags)\n        .mapMaterializedValue(\n            meta -> {\n              System.out.println(\n                  \"The query is: \"\n                      + \"ordered deterministically: \"\n                      + meta.deterministicOrder\n                      + \" \"\n                      + \"infinite: \"\n                      + meta.infinite);\n              return meta;\n            });\n\nevents\n    .map(\n        event -> {\n          System.out.println(\"Event payload: \" + event.payload);\n          return event.payload;\n        })\n    .runWith(Sink.ignore(), system);","title":"Materialized values of queries"},{"location":"/persistence-query.html#performance-and-denormalization","text":"When building systems using Event Sourcing and CQRS (Command & Query Responsibility Segregation) techniques it is tremendously important to realise that the write-side has completely different needs from the read-side, and separating those concerns into datastores that are optimised for either side makes it possible to offer the best experience for the write and read sides independently.\nFor example, in a bidding system it is important to “take the write” and respond to the bidder that we have accepted the bid as soon as possible, which means that write-throughput is of highest importance for the write-side – often this means that data stores which are able to scale to accommodate these requirements have a less expressive query side.\nOn the other hand the same application may have some complex statistics view or we may have analysts working with the data to figure out best bidding strategies and trends – this often requires some kind of expressive query capabilities like for example SQL or writing Spark jobs to analyse the data. Therefore the data stored in the write-side needs to be projected into the other read-optimised datastore.\nNote When referring to Materialized Views in Pekko Persistence think of it as “some persistent storage of the result of a Query”. In other words, it means that the view is created once, in order to be afterwards queried multiple times, as in this format it may be more efficient or interesting to query it (instead of the source events directly).","title":"Performance and denormalization"},{"location":"/persistence-query.html#materialize-view-to-reactive-streams-compatible-datastore","text":"If the read datastore exposes a Reactive Streams interface then implementing a simple projection is as simple as, using the read-journal and feeding it into the databases driver interface, for example like so:\nScala copysourceimplicit val system: ActorSystem = ActorSystem()\n\nval readJournal =\n  PersistenceQuery(system).readJournalFor[MyScaladslReadJournal](JournalId)\nval dbBatchWriter: Subscriber[immutable.Seq[Any]] =\n  ReactiveStreamsCompatibleDBDriver.batchWriter\n\n// Using an example (Reactive Streams) Database driver\nreadJournal\n  .eventsByPersistenceId(\"user-1337\", fromSequenceNr = 0L, toSequenceNr = Long.MaxValue)\n  .map(envelope => envelope.event)\n  .map(convertToReadSideTypes) // convert to datatype\n  .grouped(20) // batch inserts into groups of 20\n  .runWith(Sink.fromSubscriber(dbBatchWriter)) // write batches to read-side database Java copysourcefinal ReactiveStreamsCompatibleDBDriver driver = new ReactiveStreamsCompatibleDBDriver();\nfinal Subscriber<List<Object>> dbBatchWriter = driver.batchWriter();\n\n// Using an example (Reactive Streams) Database driver\nreadJournal\n    .eventsByPersistenceId(\"user-1337\", 0L, Long.MAX_VALUE)\n    .map(envelope -> envelope.event())\n    .grouped(20) // batch inserts into groups of 20\n    .runWith(Sink.fromSubscriber(dbBatchWriter), system); // write batches to read-side database","title":"Materialize view to Reactive Streams compatible datastore"},{"location":"/persistence-query.html#materialize-view-using-mapasync","text":"If the target database does not provide a reactive streams Subscriber that can perform writes, you may have to implement the write logic using plain functions or Actors instead.\nIn case your write logic is state-less and you need to convert the events from one data type to another before writing into the alternative datastore, then the projection will look like this:\nScala copysourcetrait ExampleStore {\n  def save(event: Any): Future[Unit]\n} Java copysourcestatic class ExampleStore {\n  CompletionStage<Void> save(Object any) {\n    // ...\n  }\n}\nScala copysourceval store: ExampleStore = ???\n\nreadJournal\n  .eventsByTag(\"bid\", NoOffset)\n  .mapAsync(1) { e =>\n    store.save(e)\n  }\n  .runWith(Sink.ignore) Java copysourcefinal ExampleStore store = new ExampleStore();\n\nreadJournal\n    .eventsByTag(\"bid\", new Sequence(0L))\n    .mapAsync(1, store::save)\n    .runWith(Sink.ignore(), system);","title":"Materialize view using mapAsync"},{"location":"/persistence-query.html#resumable-projections","text":"Sometimes, you may need to use “resumable” projections, which will not start from the beginning of time each time when run. In such case, the sequence number (or offset) of the processed event will be stored and used the next time this projection is started. This pattern is implemented in the Pekko Projections module.","title":"Resumable projections"},{"location":"/persistence-query.html#query-plugins","text":"Query plugins are various (mostly community driven) ReadJournalReadJournal implementations for all kinds of available datastores. The complete list of available plugins is maintained on the Pekko Persistence Query Community Plugins page.\nThis section aims to provide tips and guide plugin developers through implementing a custom query plugin. Most users will not need to implement journals themselves, except if targeting a not yet supported datastore.\nNote Since different data stores provide different query capabilities journal plugins must extensively document their exposed semantics as well as handled query scenarios.","title":"Query plugins"},{"location":"/persistence-query.html#readjournal-plugin-api","text":"A read journal plugin must implement pekko.query.ReadJournalProviderpekko.query.ReadJournalProvider which creates instances of pekko.persistence.query.scaladsl.ReadJournal and persistence.query.javadsl.ReadJournal. The plugin must implement both the scaladsl and the javadsl traitsinterfaces because the pekko.stream.scaladsl.Source and stream.javadsl.Source are different types and even though those types can be converted to each other it is most convenient for the end user to get access to the Java or Scala Source directly. As illustrated below one of the implementations can delegate to the other.\nBelow is a simple journal implementation:\nScala copysourceimport org.apache.pekko\nclass MyReadJournalProvider(system: ExtendedActorSystem, config: Config) extends ReadJournalProvider {\n\n  private val readJournal: MyScaladslReadJournal =\n    new MyScaladslReadJournal(system, config)\n\n  override def scaladslReadJournal(): MyScaladslReadJournal =\n    readJournal\n\n  override def javadslReadJournal(): MyJavadslReadJournal =\n    new MyJavadslReadJournal(readJournal)\n}\n\nclass MyScaladslReadJournal(system: ExtendedActorSystem, config: Config)\n    extends pekko.persistence.query.scaladsl.ReadJournal\n    with pekko.persistence.query.scaladsl.EventsByTagQuery\n    with pekko.persistence.query.scaladsl.EventsByPersistenceIdQuery\n    with pekko.persistence.query.scaladsl.PersistenceIdsQuery\n    with pekko.persistence.query.scaladsl.CurrentPersistenceIdsQuery {\n\n  private val refreshInterval: FiniteDuration =\n    config.getDuration(\"refresh-interval\", MILLISECONDS).millis\n\n  /**\n   * You can use `NoOffset` to retrieve all events with a given tag or retrieve a subset of all\n   * events by specifying a `Sequence` `offset`. The `offset` corresponds to an ordered sequence number for\n   * the specific tag. Note that the corresponding offset of each event is provided in the\n   * [[pekko.persistence.query.EventEnvelope]], which makes it possible to resume the\n   * stream at a later point from a given offset.\n   *\n   * The `offset` is exclusive, i.e. the event with the exact same sequence number will not be included\n   * in the returned stream. This means that you can use the offset that is returned in `EventEnvelope`\n   * as the `offset` parameter in a subsequent query.\n   */\n  override def eventsByTag(tag: String, offset: Offset): Source[EventEnvelope, NotUsed] = offset match {\n    case Sequence(offsetValue) =>\n      Source.fromGraph(new MyEventsByTagSource(tag, offsetValue, refreshInterval))\n    case NoOffset => eventsByTag(tag, Sequence(0L)) // recursive\n    case _ =>\n      throw new IllegalArgumentException(\"MyJournal does not support \" + offset.getClass.getName + \" offsets\")\n  }\n\n  override def eventsByPersistenceId(\n      persistenceId: String,\n      fromSequenceNr: Long,\n      toSequenceNr: Long): Source[EventEnvelope, NotUsed] = {\n    // implement in a similar way as eventsByTag\n    ???\n  }\n\n  override def persistenceIds(): Source[String, NotUsed] = {\n    // implement in a similar way as eventsByTag\n    ???\n  }\n\n  override def currentPersistenceIds(): Source[String, NotUsed] = {\n    // implement in a similar way as eventsByTag\n    ???\n  }\n\n  // possibility to add more plugin specific queries\n\n  def byTagsWithMeta(tags: Set[String]): Source[RichEvent, QueryMetadata] = {\n    // implement in a similar way as eventsByTag\n    ???\n  }\n\n}\n\nclass MyJavadslReadJournal(scaladslReadJournal: MyScaladslReadJournal)\n    extends pekko.persistence.query.javadsl.ReadJournal\n    with pekko.persistence.query.javadsl.EventsByTagQuery\n    with pekko.persistence.query.javadsl.EventsByPersistenceIdQuery\n    with pekko.persistence.query.javadsl.PersistenceIdsQuery\n    with pekko.persistence.query.javadsl.CurrentPersistenceIdsQuery {\n\n  override def eventsByTag(tag: String, offset: Offset = Sequence(0L)): javadsl.Source[EventEnvelope, NotUsed] =\n    scaladslReadJournal.eventsByTag(tag, offset).asJava\n\n  override def eventsByPersistenceId(\n      persistenceId: String,\n      fromSequenceNr: Long = 0L,\n      toSequenceNr: Long = Long.MaxValue): javadsl.Source[EventEnvelope, NotUsed] =\n    scaladslReadJournal.eventsByPersistenceId(persistenceId, fromSequenceNr, toSequenceNr).asJava\n\n  override def persistenceIds(): javadsl.Source[String, NotUsed] =\n    scaladslReadJournal.persistenceIds().asJava\n\n  override def currentPersistenceIds(): javadsl.Source[String, NotUsed] =\n    scaladslReadJournal.currentPersistenceIds().asJava\n\n  // possibility to add more plugin specific queries\n\n  def byTagsWithMeta(tags: java.util.Set[String]): javadsl.Source[RichEvent, QueryMetadata] = {\n    import pekko.util.ccompat.JavaConverters._\n    scaladslReadJournal.byTagsWithMeta(tags.asScala.toSet).asJava\n  }\n}\n Java copysourcestatic class MyReadJournalProvider implements ReadJournalProvider {\n  private final MyJavadslReadJournal javadslReadJournal;\n\n  public MyReadJournalProvider(ExtendedActorSystem system, Config config) {\n    this.javadslReadJournal = new MyJavadslReadJournal(system, config);\n  }\n\n  @Override\n  public MyScaladslReadJournal scaladslReadJournal() {\n    return new MyScaladslReadJournal(javadslReadJournal);\n  }\n\n  @Override\n  public MyJavadslReadJournal javadslReadJournal() {\n    return this.javadslReadJournal;\n  }\n}\nstatic class MyJavadslReadJournal\n    implements org.apache.pekko.persistence.query.javadsl.ReadJournal,\n        org.apache.pekko.persistence.query.javadsl.EventsByTagQuery,\n        org.apache.pekko.persistence.query.javadsl.EventsByPersistenceIdQuery,\n        org.apache.pekko.persistence.query.javadsl.PersistenceIdsQuery,\n        org.apache.pekko.persistence.query.javadsl.CurrentPersistenceIdsQuery {\n\n  private final Duration refreshInterval;\n  private Connection conn;\n\n  public MyJavadslReadJournal(ExtendedActorSystem system, Config config) {\n    refreshInterval = config.getDuration(\"refresh-interval\");\n  }\n\n  /**\n   * You can use `NoOffset` to retrieve all events with a given tag or retrieve a subset of all\n   * events by specifying a `Sequence` `offset`. The `offset` corresponds to an ordered sequence\n   * number for the specific tag. Note that the corresponding offset of each event is provided in\n   * the [[pekko.persistence.query.EventEnvelope]], which makes it possible to resume the stream\n   * at a later point from a given offset.\n   *\n   * <p>The `offset` is exclusive, i.e. the event with the exact same sequence number will not be\n   * included in the returned stream. This means that you can use the offset that is returned in\n   * `EventEnvelope` as the `offset` parameter in a subsequent query.\n   */\n  @Override\n  public Source<EventEnvelope, NotUsed> eventsByTag(String tag, Offset offset) {\n    if (offset instanceof Sequence) {\n      Sequence sequenceOffset = (Sequence) offset;\n      return Source.fromGraph(\n          new MyEventsByTagSource(conn, tag, sequenceOffset.value(), refreshInterval));\n    } else if (offset == NoOffset.getInstance())\n      return eventsByTag(tag, Offset.sequence(0L)); // recursive\n    else\n      throw new IllegalArgumentException(\n          \"MyJavadslReadJournal does not support \" + offset.getClass().getName() + \" offsets\");\n  }\n\n  @Override\n  public Source<EventEnvelope, NotUsed> eventsByPersistenceId(\n      String persistenceId, long fromSequenceNr, long toSequenceNr) {\n    // implement in a similar way as eventsByTag\n    throw new UnsupportedOperationException(\"Not implemented yet\");\n  }\n\n  @Override\n  public Source<String, NotUsed> persistenceIds() {\n    // implement in a similar way as eventsByTag\n    throw new UnsupportedOperationException(\"Not implemented yet\");\n  }\n\n  @Override\n  public Source<String, NotUsed> currentPersistenceIds() {\n    // implement in a similar way as eventsByTag\n    throw new UnsupportedOperationException(\"Not implemented yet\");\n  }\n\n  // possibility to add more plugin specific queries\n\n  public Source<RichEvent, QueryMetadata> byTagsWithMeta(Set<String> tags) {\n    // implement in a similar way as eventsByTag\n    throw new UnsupportedOperationException(\"Not implemented yet\");\n  }\n}\nstatic class MyScaladslReadJournal\n    implements org.apache.pekko.persistence.query.scaladsl.ReadJournal,\n        org.apache.pekko.persistence.query.scaladsl.EventsByTagQuery,\n        org.apache.pekko.persistence.query.scaladsl.EventsByPersistenceIdQuery,\n        org.apache.pekko.persistence.query.scaladsl.PersistenceIdsQuery,\n        org.apache.pekko.persistence.query.scaladsl.CurrentPersistenceIdsQuery {\n\n  private final MyJavadslReadJournal javadslReadJournal;\n\n  public MyScaladslReadJournal(MyJavadslReadJournal javadslReadJournal) {\n    this.javadslReadJournal = javadslReadJournal;\n  }\n\n  @Override\n  public org.apache.pekko.stream.scaladsl.Source<EventEnvelope, NotUsed> eventsByTag(\n      String tag, org.apache.pekko.persistence.query.Offset offset) {\n    return javadslReadJournal.eventsByTag(tag, offset).asScala();\n  }\n\n  @Override\n  public org.apache.pekko.stream.scaladsl.Source<EventEnvelope, NotUsed> eventsByPersistenceId(\n      String persistenceId, long fromSequenceNr, long toSequenceNr) {\n    return javadslReadJournal\n        .eventsByPersistenceId(persistenceId, fromSequenceNr, toSequenceNr)\n        .asScala();\n  }\n\n  @Override\n  public org.apache.pekko.stream.scaladsl.Source<String, NotUsed> persistenceIds() {\n    return javadslReadJournal.persistenceIds().asScala();\n  }\n\n  @Override\n  public org.apache.pekko.stream.scaladsl.Source<String, NotUsed> currentPersistenceIds() {\n    return javadslReadJournal.currentPersistenceIds().asScala();\n  }\n\n  // possibility to add more plugin specific queries\n\n  public org.apache.pekko.stream.scaladsl.Source<RichEvent, QueryMetadata> byTagsWithMeta(\n      scala.collection.Set<String> tags) {\n    Set<String> jTags = scala.collection.JavaConverters.setAsJavaSetConverter(tags).asJava();\n    return javadslReadJournal.byTagsWithMeta(jTags).asScala();\n  }\n}\nAnd the eventsByTageventsByTag could be backed by a GraphStage for example:\nScala copysourceclass MyEventsByTagSource(tag: String, offset: Long, refreshInterval: FiniteDuration)\n    extends GraphStage[SourceShape[EventEnvelope]] {\n\n  private case object Continue\n  val out: Outlet[EventEnvelope] = Outlet(\"MyEventByTagSource.out\")\n  override def shape: SourceShape[EventEnvelope] = SourceShape(out)\n\n  override protected def initialAttributes: Attributes = Attributes(ActorAttributes.IODispatcher)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new TimerGraphStageLogic(shape) with OutHandler {\n      lazy val system = materializer.system\n      private val Limit = 1000\n      private val connection: java.sql.Connection = ???\n      private var currentOffset = offset\n      private var buf = Vector.empty[EventEnvelope]\n      private val serialization = SerializationExtension(system)\n\n      override def preStart(): Unit = {\n        scheduleWithFixedDelay(Continue, refreshInterval, refreshInterval)\n      }\n\n      override def onPull(): Unit = {\n        query()\n        tryPush()\n      }\n\n      override def onDownstreamFinish(cause: Throwable): Unit = {\n        // close connection if responsible for doing so\n      }\n\n      private def query(): Unit = {\n        if (buf.isEmpty) {\n          try {\n            buf = Select.run(tag, currentOffset, Limit)\n          } catch {\n            case NonFatal(e) =>\n              failStage(e)\n          }\n        }\n      }\n\n      private def tryPush(): Unit = {\n        if (buf.nonEmpty && isAvailable(out)) {\n          push(out, buf.head)\n          buf = buf.tail\n        }\n      }\n\n      override protected def onTimer(timerKey: Any): Unit = timerKey match {\n        case Continue =>\n          query()\n          tryPush()\n      }\n\n      object Select {\n        private def statement() =\n          connection.prepareStatement(\"\"\"\n            SELECT id, persistence_id, seq_nr, serializer_id, serializer_manifest, payload \n            FROM journal WHERE tag = ? AND id > ? \n            ORDER BY id LIMIT ?\n      \"\"\")\n\n        def run(tag: String, from: Long, limit: Int): Vector[EventEnvelope] = {\n          val s = statement()\n          try {\n            s.setString(1, tag)\n            s.setLong(2, from)\n            s.setLong(3, limit)\n            val rs = s.executeQuery()\n\n            val b = Vector.newBuilder[EventEnvelope]\n            while (rs.next()) {\n              val deserialized = serialization\n                .deserialize(rs.getBytes(\"payload\"), rs.getInt(\"serializer_id\"), rs.getString(\"serializer_manifest\"))\n                .get\n              currentOffset = rs.getLong(\"id\")\n              b += EventEnvelope(\n                Offset.sequence(currentOffset),\n                rs.getString(\"persistence_id\"),\n                rs.getLong(\"seq_nr\"),\n                deserialized,\n                System.currentTimeMillis())\n            }\n            b.result()\n          } finally s.close()\n        }\n      }\n    }\n\n} Java copysourcepublic class MyEventsByTagSource extends GraphStage<SourceShape<EventEnvelope>> {\n  public Outlet<EventEnvelope> out = Outlet.create(\"MyEventByTagSource.out\");\n  private static final String QUERY =\n      \"SELECT id, persistence_id, seq_nr, serializer_id, serializer_manifest, payload \"\n          + \"FROM journal WHERE tag = ? AND id > ? \"\n          + \"ORDER BY id LIMIT ?\";\n\n  enum Continue {\n    INSTANCE;\n  }\n\n  private static final int LIMIT = 1000;\n  private final Connection connection;\n  private final String tag;\n  private final long initialOffset;\n  private final Duration refreshInterval;\n\n  // assumes a shared connection, could also be a factory for creating connections/pool\n  public MyEventsByTagSource(\n      Connection connection, String tag, long initialOffset, Duration refreshInterval) {\n    this.connection = connection;\n    this.tag = tag;\n    this.initialOffset = initialOffset;\n    this.refreshInterval = refreshInterval;\n  }\n\n  @Override\n  public Attributes initialAttributes() {\n    return Attributes.apply(ActorAttributes.IODispatcher());\n  }\n\n  @Override\n  public SourceShape<EventEnvelope> shape() {\n    return SourceShape.of(out);\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new TimerGraphStageLogic(shape()) {\n      private ActorSystem system = materializer().system();\n      private long currentOffset = initialOffset;\n      private List<EventEnvelope> buf = new LinkedList<>();\n      private final Serialization serialization = SerializationExtension.get(system);\n\n      @Override\n      public void preStart() {\n        scheduleWithFixedDelay(Continue.INSTANCE, refreshInterval, refreshInterval);\n      }\n\n      @Override\n      public void onTimer(Object timerKey) {\n        query();\n        deliver();\n      }\n\n      private void deliver() {\n        if (isAvailable(out) && !buf.isEmpty()) {\n          push(out, buf.remove(0));\n        }\n      }\n\n      private void query() {\n        if (buf.isEmpty()) {\n\n          try (PreparedStatement s = connection.prepareStatement(QUERY)) {\n            s.setString(1, tag);\n            s.setLong(2, currentOffset);\n            s.setLong(3, LIMIT);\n            try (ResultSet rs = s.executeQuery()) {\n              final List<EventEnvelope> res = new ArrayList<>(LIMIT);\n              while (rs.next()) {\n                Object deserialized =\n                    serialization\n                        .deserialize(\n                            rs.getBytes(\"payload\"),\n                            rs.getInt(\"serializer_id\"),\n                            rs.getString(\"serializer_manifest\"))\n                        .get();\n                currentOffset = rs.getLong(\"id\");\n                res.add(\n                    new EventEnvelope(\n                        Offset.sequence(currentOffset),\n                        rs.getString(\"persistence_id\"),\n                        rs.getLong(\"seq_nr\"),\n                        deserialized,\n                        System.currentTimeMillis()));\n              }\n              buf = res;\n            }\n          } catch (Exception e) {\n            failStage(e);\n          }\n        }\n      }\n\n      {\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() {\n                query();\n                deliver();\n              }\n            });\n      }\n    };\n  }\n}\nThe ReadJournalProviderReadJournalProvider class must have a constructor with one of these signatures:\nconstructor with a ExtendedActorSystemExtendedActorSystem parameter, a com.typesafe.config.Config parameter, and a String parameter for the config path constructor with a ExtendedActorSystem parameter, and a com.typesafe.config.Config parameter constructor with one ExtendedActorSystem parameter constructor without parameters\nThe plugin section of the actor system’s config will be passed in the config constructor parameter. The config path of the plugin is passed in the String parameter.\nIf the underlying datastore only supports queries that are completed when they reach the end of the “result set”, the journal has to submit new queries after a while in order to support “infinite” event streams that include events stored after the initial query has completed. It is recommended that the plugin use a configuration property named refresh-interval for defining such a refresh interval.","title":"ReadJournal plugin API"},{"location":"/persistence-query.html#scaling-out","text":"In a use case where the number of events are very high, the work needed for each event is high or where resilience is important so that if a node crashes the persistent queries are quickly started on a new node and can resume operations Cluster Sharding together with event tagging is an excellent fit to shard events over a cluster.","title":"Scaling out"},{"location":"/persistence-query.html#example-project","text":"The Microservices with Pekko tutorial explains how to use Event Sourcing and Projections together. The events are tagged to be consumed by even processors to build other representations from the events, or publish the events to other services.","title":"Example project"},{"location":"/persistence-query-leveldb.html","text":"","title":"Persistence Query for LevelDB"},{"location":"/persistence-query-leveldb.html#persistence-query-for-leveldb","text":"The LevelDB journal and query plugin is deprecated and it is not advised to build new applications with it. As a replacement we recommend using Pekko Persistence JDBC.","title":"Persistence Query for LevelDB"},{"location":"/persistence-query-leveldb.html#dependency","text":"To use Persistence Query, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence-query\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-query_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-query_${versions.ScalaBinary}\"\n}\nThis will also add dependency on the pekko-persistence module.","title":"Dependency"},{"location":"/persistence-query-leveldb.html#introduction","text":"This is documentation for the LevelDB implementation of the Persistence Query API. Note that implementations for other journals may have different semantics.","title":"Introduction"},{"location":"/persistence-query-leveldb.html#how-to-get-the-readjournal","text":"The ReadJournal is retrieved via the org.apache.pekko.persistence.query.PersistenceQuery extension:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.query.PersistenceQuery\nimport pekko.persistence.query.journal.leveldb.scaladsl.LeveldbReadJournal\n\nval queries = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier) Java copysourceLeveldbReadJournal queries =\n    PersistenceQuery.get(system)\n        .getReadJournalFor(LeveldbReadJournal.class, LeveldbReadJournal.Identifier());","title":"How to get the ReadJournal"},{"location":"/persistence-query-leveldb.html#supported-queries","text":"","title":"Supported Queries"},{"location":"/persistence-query-leveldb.html#eventsbypersistenceidquery-and-currenteventsbypersistenceidquery","text":"eventsByPersistenceId is used for retrieving events for a specific PersistentActor identified by persistenceId.\nScala copysourceval queries = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier)\n\nval src: Source[EventEnvelope, NotUsed] =\n  queries.eventsByPersistenceId(\"some-persistence-id\", 0L, Long.MaxValue)\n\nval events: Source[Any, NotUsed] = src.map(_.event) Java copysourceLeveldbReadJournal queries =\n    PersistenceQuery.get(system)\n        .getReadJournalFor(LeveldbReadJournal.class, LeveldbReadJournal.Identifier());\n\nSource<EventEnvelope, NotUsed> source =\n    queries.eventsByPersistenceId(\"some-persistence-id\", 0, Long.MAX_VALUE);\nYou can retrieve a subset of all events by specifying fromSequenceNr and toSequenceNr or use 0L and Long.MaxValueLong.MAX_VALUE respectively to retrieve all events. Note that the corresponding sequence number of each event is provided in the EventEnvelope, which makes it possible to resume the stream at a later point from a given sequence number.\nThe returned event stream is ordered by sequence number, i.e. the same order as the PersistentActor persisted the events. The same prefix of stream elements (in same order) are returned for multiple executions of the query, except for when events have been deleted.\nThe stream is not completed when it reaches the end of the currently stored events, but it continues to push new events when new events are persisted. Corresponding query that is completed when it reaches the end of the currently stored events is provided by currentEventsByPersistenceId.\nThe LevelDB write journal is notifying the query side as soon as events are persisted, but for efficiency reasons the query side retrieves the events in batches that sometimes can be delayed up to the configured refresh-interval or given RefreshInterval hint.\nThe stream is completed with failure if there is a failure in executing the query in the backend journal.","title":"EventsByPersistenceIdQuery and CurrentEventsByPersistenceIdQuery"},{"location":"/persistence-query-leveldb.html#persistenceidsquery-and-currentpersistenceidsquery","text":"persistenceIds is used for retrieving all persistenceIds of all persistent actors.\nScala copysourceval queries = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier)\n\nval src: Source[String, NotUsed] = queries.persistenceIds() Java copysourceLeveldbReadJournal queries =\n    PersistenceQuery.get(system)\n        .getReadJournalFor(LeveldbReadJournal.class, LeveldbReadJournal.Identifier());\n\nSource<String, NotUsed> source = queries.persistenceIds();\nThe returned event stream is unordered and you can expect different order for multiple executions of the query.\nThe stream is not completed when it reaches the end of the currently used persistenceIds, but it continues to push new persistenceIds when new persistent actors are created. Corresponding query that is completed when it reaches the end of the currently used persistenceIds is provided by currentPersistenceIds.\nThe LevelDB write journal is notifying the query side as soon as new persistenceIds are created and there is no periodic polling or batching involved in this query.\nThe stream is completed with failure if there is a failure in executing the query in the backend journal.","title":"PersistenceIdsQuery and CurrentPersistenceIdsQuery"},{"location":"/persistence-query-leveldb.html#eventsbytag-and-currenteventsbytag","text":"eventsByTag is used for retrieving events that were marked with a given tag, e.g. all domain events of an Aggregate Root type.\nScala copysourceval queries = PersistenceQuery(system).readJournalFor[LeveldbReadJournal](LeveldbReadJournal.Identifier)\n\nval src: Source[EventEnvelope, NotUsed] =\n  queries.eventsByTag(tag = \"green\", offset = Sequence(0L)) Java copysourceLeveldbReadJournal queries =\n    PersistenceQuery.get(system)\n        .getReadJournalFor(LeveldbReadJournal.class, LeveldbReadJournal.Identifier());\n\nSource<EventEnvelope, NotUsed> source = queries.eventsByTag(\"green\", new Sequence(0L));\nTo tag events you create an Event Adapters that wraps the events in a org.apache.pekko.persistence.journal.Tagged with the given tags.\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.journal.WriteEventAdapter\nimport pekko.persistence.journal.Tagged\n\nclass MyTaggingEventAdapter extends WriteEventAdapter {\n  val colors = Set(\"green\", \"black\", \"blue\")\n  override def toJournal(event: Any): Any = event match {\n    case s: String =>\n      val tags = colors.foldLeft(Set.empty[String]) { (acc, c) =>\n        if (s.contains(c)) acc + c else acc\n      }\n      if (tags.isEmpty) event\n      else Tagged(event, tags)\n    case _ => event\n  }\n\n  override def manifest(event: Any): String = \"\"\n} Java copysourcestatic class MyTaggingEventAdapter implements WriteEventAdapter {\n\n  @Override\n  public Object toJournal(Object event) {\n    if (event instanceof String) {\n      String s = (String) event;\n      Set<String> tags = new HashSet<String>();\n      if (s.contains(\"green\")) tags.add(\"green\");\n      if (s.contains(\"black\")) tags.add(\"black\");\n      if (s.contains(\"blue\")) tags.add(\"blue\");\n      if (tags.isEmpty()) return event;\n      else return new Tagged(event, tags);\n    } else {\n      return event;\n    }\n  }\n\n  @Override\n  public String manifest(Object event) {\n    return \"\";\n  }\n}\nYou can use NoOffset to retrieve all events with a given tag or retrieve a subset of all events by specifying a Sequence offset. The offset corresponds to an ordered sequence number for the specific tag. Note that the corresponding offset of each event is provided in the EventEnvelope, which makes it possible to resume the stream at a later point from a given offset.\nThe offset is exclusive, i.e. the event with the exact same sequence number will not be included in the returned stream. This means that you can use the offset that is returned in EventEnvelope as the offset parameter in a subsequent query.\nIn addition to the offset the EventEnvelope also provides persistenceId and sequenceNr for each event. The sequenceNr is the sequence number for the persistent actor with the persistenceId that persisted the event. The persistenceId + sequenceNr is an unique identifier for the event.\nThe returned event stream is ordered by the offset (tag sequence number), which corresponds to the same order as the write journal stored the events. The same stream elements (in same order) are returned for multiple executions of the query. Deleted events are not deleted from the tagged event stream.\nNote Events deleted using deleteMessages(toSequenceNr) are not deleted from the “tagged stream”.\nThe stream is not completed when it reaches the end of the currently stored events, but it continues to push new events when new events are persisted. Corresponding query that is completed when it reaches the end of the currently stored events is provided by currentEventsByTag.\nThe LevelDB write journal is notifying the query side as soon as tagged events are persisted, but for efficiency reasons the query side retrieves the events in batches that sometimes can be delayed up to the configured refresh-interval or given RefreshInterval hint.\nThe stream is completed with failure if there is a failure in executing the query in the backend journal.","title":"EventsByTag and CurrentEventsByTag"},{"location":"/persistence-query-leveldb.html#configuration","text":"Configuration settings can be defined in the configuration section with the absolute path corresponding to the identifier, which is \"pekko.persistence.query.journal.leveldb\" for the default LeveldbReadJournal.Identifier.\nIt can be configured with the following properties:\ncopysource# Configuration for the LeveldbReadJournal\npekko.persistence.query.journal.leveldb {\n  # Implementation class of the LevelDB ReadJournalProvider\n  class = \"org.apache.pekko.persistence.query.journal.leveldb.LeveldbReadJournalProvider\"\n  \n  # Absolute path to the write journal plugin configuration entry that this \n  # query journal will connect to. That must be a LeveldbJournal or SharedLeveldbJournal.\n  # If undefined (or \"\") it will connect to the default journal as specified by the\n  # pekko.persistence.journal.plugin property.\n  write-plugin = \"\"\n  \n  # The LevelDB write journal is notifying the query side as soon as things\n  # are persisted, but for efficiency reasons the query side retrieves the events \n  # in batches that sometimes can be delayed up to the configured `refresh-interval`.\n  refresh-interval = 3s\n  \n  # How many events to fetch in one query (replay) and keep buffered until they\n  # are delivered downstreams.\n  max-buffer-size = 100\n}","title":"Configuration"},{"location":"/persistence-plugins.html","text":"","title":"Persistence Plugins"},{"location":"/persistence-plugins.html#persistence-plugins","text":"Storage backends for journals and snapshot stores are pluggable in the Pekko persistence extension.\nA directory of persistence journal and snapshot store plugins is available at the Pekko Community Projects page, see Community plugins\nPlugins maintained within the Pekko organization are:\npekko-persistence-cassandra (no Durable State support) pekko-persistence-jdbc pekko-persistence-r2dbc pekko-persistence-spanner\nPlugins can be selected either by “default” for all persistent actors, or “individually”, when a persistent actor defines its own set of plugins.\nWhen a persistent actor does NOT override the journalPluginId and snapshotPluginId methods, the persistence extension will use the “default” journal, snapshot-store and durable-state plugins configured in reference.conf:\npekko.persistence.journal.plugin = \"\"\npekko.persistence.snapshot-store.plugin = \"\"\npekko.persistence.state.plugin = \"\"\nHowever, these entries are provided as empty \"\", and require explicit user configuration via override in the user application.conf.\nFor an example of a journal plugin which writes messages to LevelDB see Local LevelDB journal. For an example of a snapshot store plugin which writes snapshots as individual files to the local filesystem see Local snapshot store. The state store is relatively new, one available implementation is the pekko-persistence-jdbc-plugin.","title":"Persistence Plugins"},{"location":"/persistence-plugins.html#eager-initialization-of-persistence-plugin","text":"By default, persistence plugins are started on-demand, as they are used. In some case, however, it might be beneficial to start a certain plugin eagerly. In order to do that, you should first add org.apache.pekko.persistence.Persistence under the pekko.extensions key. Then, specify the IDs of plugins you wish to start automatically under pekko.persistence.journal.auto-start-journals and pekko.persistence.snapshot-store.auto-start-snapshot-stores.\nFor example, if you want eager initialization for the leveldb journal plugin and the local snapshot store plugin, your configuration should look like this:\npekko {\n\n  extensions = [org.apache.pekko.persistence.Persistence]\n\n  persistence {\n\n    journal {\n      plugin = \"pekko.persistence.journal.leveldb\"\n      auto-start-journals = [\"org.apache.pekko.persistence.journal.leveldb\"]\n    }\n\n    snapshot-store {\n      plugin = \"pekko.persistence.snapshot-store.local\"\n      auto-start-snapshot-stores = [\"org.apache.pekko.persistence.snapshot-store.local\"]\n    }\n\n  }\n\n}","title":"Eager initialization of persistence plugin"},{"location":"/persistence-plugins.html#pre-packaged-plugins","text":"The Pekko Persistence module comes with few built-in persistence plugins, but none of these are suitable for production usage in a Pekko Cluster.","title":"Pre-packaged plugins"},{"location":"/persistence-plugins.html#local-leveldb-journal","text":"This plugin writes events to a local LevelDB instance.\nWarning The LevelDB plugin cannot be used in a Pekko Cluster since the storage is in a local file system.\nThe LevelDB journal is deprecated and it is not advised to build new applications with it. As a replacement we recommend using Pekko Persistence JDBC.\nThe LevelDB journal plugin config entry is pekko.persistence.journal.leveldb. Enable this plugin by defining config property:\ncopysource# Path to the journal plugin to be used\npekko.persistence.journal.plugin = \"pekko.persistence.journal.leveldb\"\nLevelDB based plugins will also require the following additional dependency declaration:\nsbt libraryDependencies += \"org.fusesource.leveldbjni\" % \"leveldbjni-all\" % \"1.8\" Maven <dependencies>\n  <dependency>\n    <groupId>org.fusesource.leveldbjni</groupId>\n    <artifactId>leveldbjni-all</artifactId>\n    <version>1.8</version>\n  </dependency>\n</dependencies> Gradle dependencies {\n  implementation \"org.fusesource.leveldbjni:leveldbjni-all:1.8\"\n}\nThe default location of LevelDB files is a directory named journal in the current working directory. This location can be changed by configuration where the specified path can be relative or absolute:\ncopysourcepekko.persistence.journal.leveldb.dir = \"target/journal\"\nWith this plugin, each actor system runs its own private LevelDB instance.\nOne peculiarity of LevelDB is that the deletion operation does not remove messages from the journal, but adds a “tombstone” for each deleted message instead. In the case of heavy journal usage, especially one including frequent deletes, this may be an issue as users may find themselves dealing with continuously increasing journal sizes. To this end, LevelDB offers a special journal compaction function that is exposed via the following configuration:\ncopysource# Number of deleted messages per persistence id that will trigger journal compaction\npekko.persistence.journal.leveldb.compaction-intervals {\n  persistence-id-1 = 100\n  persistence-id-2 = 200\n  # ...\n  persistence-id-N = 1000\n  # use wildcards to match unspecified persistence ids, if any\n  \"*\" = 250\n}","title":"Local LevelDB journal"},{"location":"/persistence-plugins.html#shared-leveldb-journal","text":"The LevelDB journal is deprecated and will be removed from a future Pekko version, it is not advised to build new applications with it. For testing in a multi node environment the “inmem” journal together with the proxy plugin can be used, but the actual journal used in production of applications is also a good choice.\nNote This plugin has been supplanted by Persistence Plugin Proxy.\nA shared LevelDB instance is started by instantiating the SharedLeveldbStore actor.\nScala copysourceimport org.apache.pekko.persistence.journal.leveldb.SharedLeveldbStore\n\nval store = system.actorOf(Props[SharedLeveldbStore](), \"store\") Java copysourcefinal ActorRef store = system.actorOf(Props.create(SharedLeveldbStore.class), \"store\");\nBy default, the shared instance writes journaled messages to a local directory named journal in the current working directory. The storage location can be changed by configuration:\ncopysourcepekko.persistence.journal.leveldb-shared.store.dir = \"target/shared\"\nActor systems that use a shared LevelDB store must activate the pekko.persistence.journal.leveldb-shared plugin.\ncopysourcepekko.persistence.journal.plugin = \"pekko.persistence.journal.leveldb-shared\"\nThis plugin must be initialized by injecting the (remote) SharedLeveldbStore actor reference. Injection is done by calling the SharedLeveldbJournal.setStore method with the actor reference as argument.\nScala copysourcetrait SharedStoreUsage extends Actor {\n  override def preStart(): Unit = {\n    context.actorSelection(\"pekko://example@127.0.0.1:2552/user/store\") ! Identify(1)\n  }\n\n  def receive = {\n    case ActorIdentity(1, Some(store)) =>\n      SharedLeveldbJournal.setStore(store, context.system)\n  }\n} Java copysourceclass SharedStorageUsage extends AbstractActor {\n  @Override\n  public void preStart() throws Exception {\n    String path = \"pekko://example@127.0.0.1:2552/user/store\";\n    ActorSelection selection = getContext().actorSelection(path);\n    selection.tell(new Identify(1), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            ActorIdentity.class,\n            ai -> {\n              if (ai.correlationId().equals(1)) {\n                Optional<ActorRef> store = ai.getActorRef();\n                if (store.isPresent()) {\n                  SharedLeveldbJournal.setStore(store.get(), getContext().getSystem());\n                } else {\n                  throw new RuntimeException(\"Couldn't identify store\");\n                }\n              }\n            })\n        .build();\n  }\n}\nInternal journal commands (sent by persistent actors) are buffered until injection completes. Injection is idempotent i.e. only the first injection is used.","title":"Shared LevelDB journal"},{"location":"/persistence-plugins.html#local-snapshot-store","text":"This plugin writes snapshot files to the local filesystem.\nWarning The local snapshot store plugin cannot be used in a Pekko Cluster since the storage is in a local file system.\nThe local snapshot store plugin config entry is pekko.persistence.snapshot-store.local. Enable this plugin by defining config property:\ncopysource# Path to the snapshot store plugin to be used\npekko.persistence.snapshot-store.plugin = \"pekko.persistence.snapshot-store.local\"\nThe default storage location is a directory named snapshots in the current working directory. This can be changed by configuration where the specified path can be relative or absolute:\ncopysourcepekko.persistence.snapshot-store.local.dir = \"target/snapshots\"\nNote that it is not mandatory to specify a snapshot store plugin. If you don’t use snapshots you don’t have to configure it.","title":"Local snapshot store"},{"location":"/persistence-plugins.html#persistence-plugin-proxy","text":"For testing purposes a persistence plugin proxy allows sharing of a journal and snapshot store on a single node across multiple actor systems (on the same or on different nodes). This, for example, allows persistent actors to failover to a backup node and continue using the shared journal instance from the backup node. The proxy works by forwarding all the journal/snapshot store messages to a single, shared, persistence plugin instance, and therefore supports any use case supported by the proxied plugin.\nWarning A shared journal/snapshot store is a single point of failure and should only be used for testing purposes.\nThe journal and snapshot store proxies are controlled via the pekko.persistence.journal.proxy and pekko.persistence.snapshot-store.proxy configuration entries, respectively. Set the target-journal-plugin or target-snapshot-store-plugin keys to the underlying plugin you wish to use (for example: pekko.persistence.journal.inmem). The start-target-journal and start-target-snapshot-store keys should be set to on in exactly one actor system - this is the system that will instantiate the shared persistence plugin. Next, the proxy needs to be told how to find the shared plugin. This can be done by setting the target-journal-address and target-snapshot-store-address configuration keys, or programmatically by calling the PersistencePluginProxy.setTargetLocation method.\nNote Pekko starts extensions lazily when they are required, and this includes the proxy. This means that in order for the proxy to work, the persistence plugin on the target node must be instantiated. This can be done by instantiating the PersistencePluginProxyExtension extension, or by calling the PersistencePluginProxy.start method.\nNote The proxied persistence plugin can (and should) be configured using its original configuration keys.","title":"Persistence Plugin Proxy"},{"location":"/persistence-journals.html","text":"","title":"Persistence - Building a storage backend"},{"location":"/persistence-journals.html#persistence-building-a-storage-backend","text":"Storage backends for journals and snapshot stores are pluggable in the Pekko persistence extension. A directory of persistence journal and snapshot store plugins is available at the Pekko Community Projects page, see Community plugins This documentation described how to build a new storage backend.\nApplications can provide their own plugins by implementing a plugin API and activating them by configuration. Plugin development requires the following imports:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence._\nimport pekko.persistence.journal._\nimport pekko.persistence.snapshot._\n Java copysourceimport org.apache.pekko.dispatch.Futures;\nimport org.apache.pekko.persistence.*;\nimport org.apache.pekko.persistence.journal.japi.*;\nimport org.apache.pekko.persistence.snapshot.japi.*;","title":"Persistence - Building a storage backend"},{"location":"/persistence-journals.html#journal-plugin-api","text":"A journal plugin extends AsyncWriteJournal.\nAsyncWriteJournal is an actor and the methods to be implemented are:\nScala copysource/**\n * Plugin API: asynchronously writes a batch (`Seq`) of persistent messages to the\n * journal.\n *\n * The batch is only for performance reasons, i.e. all messages don't have to be written\n * atomically. Higher throughput can typically be achieved by using batch inserts of many\n * records compared to inserting records one-by-one, but this aspect depends on the\n * underlying data store and a journal implementation can implement it as efficient as\n * possible. Journals should aim to persist events in-order for a given `persistenceId`\n * as otherwise in case of a failure, the persistent state may be end up being inconsistent.\n *\n * Each `AtomicWrite` message contains the single `PersistentRepr` that corresponds to\n * the event that was passed to the `persist` method of the `PersistentActor`, or it\n * contains several `PersistentRepr` that corresponds to the events that were passed\n * to the `persistAll` method of the `PersistentActor`. All `PersistentRepr` of the\n * `AtomicWrite` must be written to the data store atomically, i.e. all or none must\n * be stored. If the journal (data store) cannot support atomic writes of multiple\n * events it should reject such writes with a `Try` `Failure` with an\n * `UnsupportedOperationException` describing the issue. This limitation should\n * also be documented by the journal plugin.\n *\n * If there are failures when storing any of the messages in the batch the returned\n * `Future` must be completed with failure. The `Future` must only be completed with\n * success when all messages in the batch have been confirmed to be stored successfully,\n * i.e. they will be readable, and visible, in a subsequent replay. If there is\n * uncertainty about if the messages were stored or not the `Future` must be completed\n * with failure.\n *\n * Data store connection problems must be signaled by completing the `Future` with\n * failure.\n *\n * The journal can also signal that it rejects individual messages (`AtomicWrite`) by\n * the returned `immutable.Seq[Try[Unit]]`. It is possible but not mandatory to reduce\n * number of allocations by returning `Future.successful(Nil)` for the happy path,\n * i.e. when no messages are rejected. Otherwise the returned `Seq` must have as many elements\n * as the input `messages` `Seq`. Each `Try` element signals if the corresponding\n * `AtomicWrite` is rejected or not, with an exception describing the problem. Rejecting\n * a message means it was not stored, i.e. it must not be included in a later replay.\n * Rejecting a message is typically done before attempting to store it, e.g. because of\n * serialization error.\n *\n * Data store connection problems must not be signaled as rejections.\n *\n * It is possible but not mandatory to reduce number of allocations by returning\n * `Future.successful(Nil)` for the happy path, i.e. when no messages are rejected.\n *\n * Calls to this method are serialized by the enclosing journal actor. If you spawn\n * work in asynchronous tasks it is alright that they complete the futures in any order,\n * but the actual writes for a specific persistenceId should be serialized to avoid\n * issues such as events of a later write are visible to consumers (query side, or replay)\n * before the events of an earlier write are visible.\n * A PersistentActor will not send a new WriteMessages request before the previous one\n * has been completed.\n *\n * Please note that the `sender` field of the contained PersistentRepr objects has been\n * nulled out (i.e. set to `ActorRef.noSender`) in order to not use space in the journal\n * for a sender reference that will likely be obsolete during replay.\n *\n * Please also note that requests for the highest sequence number may be made concurrently\n * to this call executing for the same `persistenceId`, in particular it is possible that\n * a restarting actor tries to recover before its outstanding writes have completed. In\n * the latter case it is highly desirable to defer reading the highest sequence number\n * until all outstanding writes have completed, otherwise the PersistentActor may reuse\n * sequence numbers.\n *\n * This call is protected with a circuit-breaker.\n */\ndef asyncWriteMessages(messages: immutable.Seq[AtomicWrite]): Future[immutable.Seq[Try[Unit]]]\n\n/**\n * Plugin API: asynchronously deletes all persistent messages up to `toSequenceNr`\n * (inclusive).\n *\n * This call is protected with a circuit-breaker.\n * Message deletion doesn't affect the highest sequence number of messages,\n * journal must maintain the highest sequence number and never decrease it.\n */\ndef asyncDeleteMessagesTo(persistenceId: String, toSequenceNr: Long): Future[Unit]\n\n/**\n * Plugin API\n *\n * Allows plugin implementers to use `f pipeTo self` and\n * handle additional messages for implementing advanced features\n */\ndef receivePluginInternal: Actor.Receive = Actor.emptyBehavior Java copysource/**\n * Java API, Plugin API: asynchronously writes a batch (`Iterable`) of persistent messages to the\n * journal.\n *\n * <p>The batch is only for performance reasons, i.e. all messages don't have to be written\n * atomically. Higher throughput can typically be achieved by using batch inserts of many records\n * compared to inserting records one-by-one, but this aspect depends on the underlying data store\n * and a journal implementation can implement it as efficient as possible. Journals should aim to\n * persist events in-order for a given `persistenceId` as otherwise in case of a failure, the\n * persistent state may be end up being inconsistent.\n *\n * <p>Each `AtomicWrite` message contains the single `PersistentRepr` that corresponds to the\n * event that was passed to the `persist` method of the `PersistentActor`, or it contains several\n * `PersistentRepr` that corresponds to the events that were passed to the `persistAll` method of\n * the `PersistentActor`. All `PersistentRepr` of the `AtomicWrite` must be written to the data\n * store atomically, i.e. all or none must be stored. If the journal (data store) cannot support\n * atomic writes of multiple events it should reject such writes with an `Optional` with an\n * `UnsupportedOperationException` describing the issue. This limitation should also be documented\n * by the journal plugin.\n *\n * <p>If there are failures when storing any of the messages in the batch the returned `Future`\n * must be completed with failure. The `Future` must only be completed with success when all\n * messages in the batch have been confirmed to be stored successfully, i.e. they will be\n * readable, and visible, in a subsequent replay. If there is uncertainty about if the messages\n * were stored or not the `Future` must be completed with failure.\n *\n * <p>Data store connection problems must be signaled by completing the `Future` with failure.\n *\n * <p>The journal can also signal that it rejects individual messages (`AtomicWrite`) by the\n * returned `Iterable&lt;Optional&lt;Exception&gt;&gt;`. The returned `Iterable` must have as many\n * elements as the input `messages` `Iterable`. Each `Optional` element signals if the\n * corresponding `AtomicWrite` is rejected or not, with an exception describing the problem.\n * Rejecting a message means it was not stored, i.e. it must not be included in a later replay.\n * Rejecting a message is typically done before attempting to store it, e.g. because of\n * serialization error.\n *\n * <p>Data store connection problems must not be signaled as rejections.\n *\n * <p>Note that it is possible to reduce number of allocations by caching some result `Iterable`\n * for the happy path, i.e. when no messages are rejected.\n *\n * <p>Calls to this method are serialized by the enclosing journal actor. If you spawn work in\n * asynchronous tasks it is alright that they complete the futures in any order, but the actual\n * writes for a specific persistenceId should be serialized to avoid issues such as events of a\n * later write are visible to consumers (query side, or replay) before the events of an earlier\n * write are visible. This can also be done with consistent hashing if it is too fine grained to\n * do it on the persistenceId level. Normally a `PersistentActor` will only have one outstanding\n * write request to the journal but it may emit several write requests when `persistAsync` is used\n * and the max batch size is reached.\n *\n * <p>This call is protected with a circuit-breaker.\n */\nFuture<Iterable<Optional<Exception>>> doAsyncWriteMessages(Iterable<AtomicWrite> messages);\n\n/**\n * Java API, Plugin API: synchronously deletes all persistent messages up to `toSequenceNr`.\n *\n * <p>This call is protected with a circuit-breaker.\n *\n * @see AsyncRecoveryPlugin\n */\nFuture<Void> doAsyncDeleteMessagesTo(String persistenceId, long toSequenceNr);\nIf the storage backend API only supports synchronous, blocking writes, the methods should be implemented as:\nScala copysourcedef asyncWriteMessages(messages: immutable.Seq[AtomicWrite]): Future[immutable.Seq[Try[Unit]]] =\n  Future.fromTry(Try {\n    // blocking call here\n    ???\n  }) Java copysource@Override\npublic Future<Iterable<Optional<Exception>>> doAsyncWriteMessages(\n    Iterable<AtomicWrite> messages) {\n  try {\n    Iterable<Optional<Exception>> result = new ArrayList<Optional<Exception>>();\n    // blocking call here...\n    // result.add(..)\n    return Futures.successful(result);\n  } catch (Exception e) {\n    return Futures.failed(e);\n  }\n}\nA journal plugin must also implement the methods defined in AsyncRecovery for replays and sequence number recovery:\nScala copysource/**\n * Plugin API: asynchronously replays persistent messages. Implementations replay\n * a message by calling `replayCallback`. The returned future must be completed\n * when all messages (matching the sequence number bounds) have been replayed.\n * The future must be completed with a failure if any of the persistent messages\n * could not be replayed.\n *\n * The `replayCallback` must also be called with messages that have been marked\n * as deleted. In this case a replayed message's `deleted` method must return\n * `true`.\n *\n * The `toSequenceNr` is the lowest of what was returned by [[#asyncReadHighestSequenceNr]]\n * and what the user specified as recovery [[pekko.persistence.Recovery]] parameter.\n * This does imply that this call is always preceded by reading the highest sequence\n * number for the given `persistenceId`.\n *\n * This call is NOT protected with a circuit-breaker because it may take long time\n * to replay all events. The plugin implementation itself must protect against\n * an unresponsive backend store and make sure that the returned Future is\n * completed with success or failure within reasonable time. It is not allowed\n * to ignore completing the future.\n *\n * @param persistenceId persistent actor id.\n * @param fromSequenceNr sequence number where replay should start (inclusive).\n * @param toSequenceNr sequence number where replay should end (inclusive).\n * @param max maximum number of messages to be replayed.\n * @param recoveryCallback called to replay a single message. Can be called from any\n *                       thread.\n *\n * @see [[AsyncWriteJournal]]\n */\ndef asyncReplayMessages(persistenceId: String, fromSequenceNr: Long, toSequenceNr: Long, max: Long)(\n    recoveryCallback: PersistentRepr => Unit): Future[Unit]\n\n/**\n * Plugin API: asynchronously reads the highest stored sequence number for the\n * given `persistenceId`. The persistent actor will use the highest sequence\n * number after recovery as the starting point when persisting new events.\n * This sequence number is also used as `toSequenceNr` in subsequent call\n * to [[#asyncReplayMessages]] unless the user has specified a lower `toSequenceNr`.\n * Journal must maintain the highest sequence number and never decrease it.\n *\n * This call is protected with a circuit-breaker.\n *\n * Please also note that requests for the highest sequence number may be made concurrently\n * to writes executing for the same `persistenceId`, in particular it is possible that\n * a restarting actor tries to recover before its outstanding writes have completed.\n *\n * @param persistenceId persistent actor id.\n * @param fromSequenceNr hint where to start searching for the highest sequence\n *                       number. When a persistent actor is recovering this\n *                       `fromSequenceNr` will be the sequence number of the used\n *                       snapshot or `0L` if no snapshot is used.\n */\ndef asyncReadHighestSequenceNr(persistenceId: String, fromSequenceNr: Long): Future[Long] Java copysource/**\n * Java API, Plugin API: asynchronously replays persistent messages. Implementations replay a\n * message by calling `replayCallback`. The returned future must be completed when all messages\n * (matching the sequence number bounds) have been replayed. The future must be completed with a\n * failure if any of the persistent messages could not be replayed.\n *\n * <p>The `replayCallback` must also be called with messages that have been marked as deleted. In\n * this case a replayed message's `deleted` method must return `true`.\n *\n * <p>The `toSequenceNr` is the lowest of what was returned by {@link\n * #doAsyncReadHighestSequenceNr} and what the user specified as recovery {@link\n * org.apache.pekko.persistence.Recovery} parameter.\n *\n * @param persistenceId id of the persistent actor.\n * @param fromSequenceNr sequence number where replay should start (inclusive).\n * @param toSequenceNr sequence number where replay should end (inclusive).\n * @param max maximum number of messages to be replayed.\n * @param replayCallback called to replay a single message. Can be called from any thread.\n */\nFuture<Void> doAsyncReplayMessages(\n    String persistenceId,\n    long fromSequenceNr,\n    long toSequenceNr,\n    long max,\n    Consumer<PersistentRepr> replayCallback);\n\n/**\n * Java API, Plugin API: asynchronously reads the highest stored sequence number for the given\n * `persistenceId`. The persistent actor will use the highest sequence number after recovery as\n * the starting point when persisting new events. This sequence number is also used as\n * `toSequenceNr` in subsequent call to [[#asyncReplayMessages]] unless the user has specified a\n * lower `toSequenceNr`.\n *\n * @param persistenceId id of the persistent actor.\n * @param fromSequenceNr hint where to start searching for the highest sequence number.\n */\nFuture<Long> doAsyncReadHighestSequenceNr(String persistenceId, long fromSequenceNr);\nA journal plugin can be activated with the following minimal configuration:\ncopysource# Path to the journal plugin to be used\npekko.persistence.journal.plugin = \"my-journal\"\n\n# My custom journal plugin\nmy-journal {\n  # Class name of the plugin.\n  class = \"docs.persistence.MyJournal\"\n  # Dispatcher for the plugin actor.\n  plugin-dispatcher = \"pekko.actor.default-dispatcher\"\n}\nThe journal plugin instance is an actor so the methods corresponding to requests from persistent actors are executed sequentially. It may delegate to asynchronous libraries, spawn futures, or delegate to other actors to achieve parallelism.\nThe journal plugin class must have a constructor with one of these signatures:\nconstructor with one com.typesafe.config.Config parameter and a String parameter for the config path constructor with one com.typesafe.config.Config parameter constructor without parameters\nThe plugin section of the actor system’s config will be passed in the config constructor parameter. The config path of the plugin is passed in the String parameter.\nThe plugin-dispatcher is the dispatcher used for the plugin actor. If not specified, it defaults to pekko.persistence.dispatchers.default-plugin-dispatcher.\nDon’t run journal tasks/futures on the system default dispatcher, since that might starve other tasks.","title":"Journal plugin API"},{"location":"/persistence-journals.html#snapshot-store-plugin-api","text":"A snapshot store plugin must extend the SnapshotStore actor and implement the following methods:\nScala copysource /**\n * Plugin API: asynchronously loads a snapshot.\n *\n * If the future `Option` is `None` then all events will be replayed,\n * i.e. there was no snapshot. If snapshot could not be loaded the `Future`\n * should be completed with failure. That is important because events may\n * have been deleted and just replaying the events might not result in a valid\n * state.\n *\n * This call is protected with a circuit-breaker.\n *\n * @param persistenceId id of the persistent actor.\n * @param criteria selection criteria for loading.\n */\ndef loadAsync(persistenceId: String, criteria: SnapshotSelectionCriteria): Future[Option[SelectedSnapshot]]\n\n/**\n * Plugin API: asynchronously saves a snapshot.\n *\n * This call is protected with a circuit-breaker.\n *\n * @param metadata snapshot metadata.\n * @param snapshot snapshot.\n */\ndef saveAsync(metadata: SnapshotMetadata, snapshot: Any): Future[Unit]\n\n/**\n * Plugin API: deletes the snapshot identified by `metadata`.\n *\n * This call is protected with a circuit-breaker.\n *\n * @param metadata snapshot metadata.\n */\ndef deleteAsync(metadata: SnapshotMetadata): Future[Unit]\n\n/**\n * Plugin API: deletes all snapshots matching `criteria`.\n *\n * This call is protected with a circuit-breaker.\n *\n * @param persistenceId id of the persistent actor.\n * @param criteria selection criteria for deleting.\n */\ndef deleteAsync(persistenceId: String, criteria: SnapshotSelectionCriteria): Future[Unit]\n\n/**\n * Plugin API\n * Allows plugin implementers to use `f pipeTo self` and\n * handle additional messages for implementing advanced features\n */\ndef receivePluginInternal: Actor.Receive = Actor.emptyBehavior Java copysource/**\n * Java API, Plugin API: asynchronously loads a snapshot.\n *\n * @param persistenceId id of the persistent actor.\n * @param criteria selection criteria for loading.\n */\nFuture<Optional<SelectedSnapshot>> doLoadAsync(\n    String persistenceId, SnapshotSelectionCriteria criteria);\n\n/**\n * Java API, Plugin API: asynchronously saves a snapshot.\n *\n * @param metadata snapshot metadata.\n * @param snapshot snapshot.\n */\nFuture<Void> doSaveAsync(SnapshotMetadata metadata, Object snapshot);\n\n/**\n * Java API, Plugin API: deletes the snapshot identified by `metadata`.\n *\n * @param metadata snapshot metadata.\n */\nFuture<Void> doDeleteAsync(SnapshotMetadata metadata);\n\n/**\n * Java API, Plugin API: deletes all snapshots matching `criteria`.\n *\n * @param persistenceId id of the persistent actor.\n * @param criteria selection criteria for deleting.\n */\nFuture<Void> doDeleteAsync(String persistenceId, SnapshotSelectionCriteria criteria);\nA snapshot store plugin can be activated with the following minimal configuration:\ncopysource# Path to the snapshot store plugin to be used\npekko.persistence.snapshot-store.plugin = \"my-snapshot-store\"\n\n# My custom snapshot store plugin\nmy-snapshot-store {\n  # Class name of the plugin.\n  class = \"docs.persistence.MySnapshotStore\"\n  # Dispatcher for the plugin actor.\n  plugin-dispatcher = \"pekko.persistence.dispatchers.default-plugin-dispatcher\"\n}\nThe snapshot store instance is an actor so the methods corresponding to requests from persistent actors are executed sequentially. It may delegate to asynchronous libraries, spawn futures, or delegate to other actors to achieve parallelism.\nThe snapshot store plugin class must have a constructor with one of these signatures:\nconstructor with one com.typesafe.config.Config parameter and a String parameter for the config path constructor with one com.typesafe.config.Config parameter constructor without parameters\nThe plugin section of the actor system’s config will be passed in the config constructor parameter. The config path of the plugin is passed in the String parameter.\nThe plugin-dispatcher is the dispatcher used for the plugin actor. If not specified, it defaults to pekko.persistence.dispatchers.default-plugin-dispatcher.\nDon’t run snapshot store tasks/futures on the system default dispatcher, since that might starve other tasks.","title":"Snapshot store plugin API"},{"location":"/persistence-journals.html#plugin-tck","text":"In order to help developers build correct and high quality storage plugins, we provide a Technology Compatibility Kit (TCK for short).\nThe TCK is usable from Java as well as Scala projects. To test your implementation (independently of language) you need to include the pekko-persistence-tck dependency:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence-tck\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-tck_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-tck_${versions.ScalaBinary}\"\n}\nTo include the Journal TCK tests in your test suite simply extend the provided JournalSpecJavaJournalSpec:\nScala copysourceclass MyJournalSpec\n    extends JournalSpec(\n      config = ConfigFactory.parseString(\"\"\"pekko.persistence.journal.plugin = \"my.journal.plugin\"\"\"\")) {\n\n  override def supportsRejectingNonSerializableObjects: CapabilityFlag =\n    false // or CapabilityFlag.off\n\n  override def supportsSerialization: CapabilityFlag =\n    true // or CapabilityFlag.on\n} Java copysource@RunWith(JUnitRunner.class)\nclass MyJournalSpecTest extends JavaJournalSpec {\n\n  public MyJournalSpecTest() {\n    super(\n        ConfigFactory.parseString(\n            \"pekko.persistence.journal.plugin = \"\n                + \"\\\"pekko.persistence.journal.leveldb-shared\\\"\"));\n  }\n\n  @Override\n  public CapabilityFlag supportsRejectingNonSerializableObjects() {\n    return CapabilityFlag.off();\n  }\n}\nPlease note that some of the tests are optional, and by overriding the supports... methods you give the TCK the needed information about which tests to run. You can implement these methods using boolean values or the provided CapabilityFlag.on / CapabilityFlag.off values.\nWe also provide a simple benchmarking class JournalPerfSpecJavaJournalPerfSpec which includes all the tests that JournalSpecJavaJournalSpec has, and also performs some longer operations on the Journal while printing its performance stats. While it is NOT aimed to provide a proper benchmarking environment it can be used to get a rough feel about your journal’s performance in the most typical scenarios.\nIn order to include the SnapshotStore TCK tests in your test suite extend the SnapshotStoreSpec:\nScala copysourceclass MySnapshotStoreSpec\n    extends SnapshotStoreSpec(\n      config = ConfigFactory.parseString(\"\"\"\n    pekko.persistence.snapshot-store.plugin = \"my.snapshot-store.plugin\"\n    \"\"\")) {\n\n  override def supportsSerialization: CapabilityFlag =\n    true // or CapabilityFlag.on\n} Java copysource@RunWith(JUnitRunner.class)\nclass MySnapshotStoreTest extends JavaSnapshotStoreSpec {\n\n  public MySnapshotStoreTest() {\n    super(\n        ConfigFactory.parseString(\n            \"pekko.persistence.snapshot-store.plugin = \"\n                + \"\\\"pekko.persistence.snapshot-store.local\\\"\"));\n  }\n}\nIn case your plugin requires some setting up (starting a mock database, removing temporary files etc.) you can override the beforeAll and afterAll methods to hook into the tests lifecycle:\nScala copysourceclass MyJournalSpec\n    extends JournalSpec(config = ConfigFactory.parseString(\"\"\"\n    pekko.persistence.journal.plugin = \"my.journal.plugin\"\n    \"\"\")) {\n\n  override def supportsRejectingNonSerializableObjects: CapabilityFlag =\n    true // or CapabilityFlag.on\n\n  val storageLocations = List(\n    new File(system.settings.config.getString(\"pekko.persistence.journal.leveldb.dir\")),\n    new File(config.getString(\"pekko.persistence.snapshot-store.local.dir\")))\n\n  override def beforeAll(): Unit = {\n    super.beforeAll()\n    storageLocations.foreach(FileUtils.deleteRecursively)\n  }\n\n  override def afterAll(): Unit = {\n    storageLocations.foreach(FileUtils.deleteRecursively)\n    super.afterAll()\n  }\n\n} Java copysource@RunWith(JUnitRunner.class)\nclass MyJournalSpecTest extends JavaJournalSpec {\n\n  List<File> storageLocations = new ArrayList<File>();\n\n  public MyJournalSpecTest() {\n    super(\n        ConfigFactory.parseString(\n            \"persistence.journal.plugin = \"\n                + \"\\\"pekko.persistence.journal.leveldb-shared\\\"\"));\n\n    Config config = system().settings().config();\n    storageLocations.add(\n        new File(config.getString(\"pekko.persistence.journal.leveldb.dir\")));\n    storageLocations.add(\n        new File(config.getString(\"pekko.persistence.snapshot-store.local.dir\")));\n  }\n\n  @Override\n  public CapabilityFlag supportsRejectingNonSerializableObjects() {\n    return CapabilityFlag.on();\n  }\n\n  @Override\n  public void beforeAll() {\n    for (File storageLocation : storageLocations) {\n      FileUtils.deleteRecursively(storageLocation);\n    }\n    super.beforeAll();\n  }\n\n  @Override\n  public void afterAll() {\n    super.afterAll();\n    for (File storageLocation : storageLocations) {\n      FileUtils.deleteRecursively(storageLocation);\n    }\n  }\n}\nWe highly recommend including these specifications in your test suite, as they cover a broad range of cases you might have otherwise forgotten to test for when writing a plugin from scratch.","title":"Plugin TCK"},{"location":"/persistence-journals.html#corrupt-event-logs","text":"If a journal can’t prevent users from running persistent actors with the same persistenceId concurrently it is likely that an event log will be corrupted by having events with the same sequence number.\nIt is recommended that journals should still deliver these events during recovery so that a replay-filter can be used to decide what to do about it in a journal agnostic way.","title":"Corrupt event logs"},{"location":"/typed/replicated-eventsourcing-examples.html","text":"","title":"Replicated Event Sourcing Examples"},{"location":"/typed/replicated-eventsourcing-examples.html#replicated-event-sourcing-examples","text":"The following are more realistic examples of building systems with Replicated Event Sourcing.\nAuction example Shopping cart example","title":"Replicated Event Sourcing Examples"},{"location":"/typed/replicated-eventsourcing-auction.html","text":"","title":"Auction example"},{"location":"/typed/replicated-eventsourcing-auction.html#auction-example","text":"In this example we want to show that real-world applications can be implemented by designing events in a way that they don’t conflict. In the end, you will end up with a solution based on a custom CRDT.\nWe are building a small auction service. It has the following operations:\nPlace a bid Get the highest bid Finish the auction\nWe model those operations as commands to be sent to the auction actor:\nScala copysourcetype MoneyAmount = Int\n\ncase class Bid(bidder: String, offer: MoneyAmount, timestamp: Instant, originReplica: ReplicaId)\n\nsealed trait Command extends CborSerializable\ncase object Finish extends Command // A timer needs to schedule this event at each replica\nfinal case class OfferBid(bidder: String, offer: MoneyAmount) extends Command\nfinal case class GetHighestBid(replyTo: ActorRef[Bid]) extends Command\nfinal case class IsClosed(replyTo: ActorRef[Boolean]) extends Command\nprivate case object Close extends Command // Internal, should not be sent from the outside Java copysourcepublic static final class Bid {\n  public final String bidder;\n  public final int offer;\n  public final Instant timestamp;\n  public final ReplicaId originReplica;\n\n  public Bid(String bidder, int offer, Instant timestamp, ReplicaId originReplica) {\n    this.bidder = bidder;\n    this.offer = offer;\n    this.timestamp = timestamp;\n    this.originReplica = originReplica;\n  }\n}\n\ninterface Command extends CborSerializable {}\n\npublic enum Finish implements Command {\n  INSTANCE\n}\n\npublic static final class OfferBid implements Command {\n  public final String bidder;\n  public final int offer;\n\n  public OfferBid(String bidder, int offer) {\n    this.bidder = bidder;\n    this.offer = offer;\n  }\n}\n\npublic static final class GetHighestBid implements Command {\n  public final ActorRef<Bid> replyTo;\n\n  public GetHighestBid(ActorRef<Bid> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic static final class IsClosed implements Command {\n  public final ActorRef<Boolean> replyTo;\n\n  public IsClosed(ActorRef<Boolean> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\nprivate enum Close implements Command {\n  INSTANCE\n}\nThe events:\nScala copysourcesealed trait Event extends CborSerializable\nfinal case class BidRegistered(bid: Bid) extends Event\nfinal case class AuctionFinished(atReplica: ReplicaId) extends Event\nfinal case class WinnerDecided(atReplica: ReplicaId, winningBid: Bid, highestCounterOffer: MoneyAmount)\n    extends Event Java copysourceinterface Event extends CborSerializable {}\n\npublic static final class BidRegistered implements Event {\n  public final Bid bid;\n\n  @JsonCreator\n  public BidRegistered(Bid bid) {\n    this.bid = bid;\n  }\n}\n\npublic static final class AuctionFinished implements Event {\n  public final ReplicaId atReplica;\n\n  @JsonCreator\n  public AuctionFinished(ReplicaId atReplica) {\n    this.atReplica = atReplica;\n  }\n}\n\npublic static final class WinnerDecided implements Event {\n  public final ReplicaId atReplica;\n  public final Bid winningBid;\n  public final int amount;\n\n  public WinnerDecided(ReplicaId atReplica, Bid winningBid, int amount) {\n    this.atReplica = atReplica;\n    this.winningBid = winningBid;\n    this.amount = amount;\n  }\n}\nThe winner does not have to pay the highest bid but only enough to beat the second highest, so the highestCounterOffer is in the AuctionFinished event.\nLet’s have a look at the auction entity that will handle incoming commands:\nScala copysourcedef commandHandler(state: AuctionState, command: Command): Effect[Event, AuctionState] = {\n  state.phase match {\n    case Closing(_) | Closed =>\n      command match {\n        case GetHighestBid(replyTo) =>\n          replyTo ! state.highestBid.copy(offer = state.highestCounterOffer) // TODO this is not as described\n          Effect.none\n        case IsClosed(replyTo) =>\n          replyTo ! (state.phase == Closed)\n          Effect.none\n        case Finish =>\n          context.log.info(\"Finish\")\n          Effect.persist(AuctionFinished(replicationContext.replicaId))\n        case Close =>\n          context.log.info(\"Close\")\n          require(shouldClose(state))\n          // TODO send email (before or after persisting)\n          Effect.persist(WinnerDecided(replicationContext.replicaId, state.highestBid, state.highestCounterOffer))\n        case _: OfferBid =>\n          // auction finished, no more bids accepted\n          Effect.unhandled\n      }\n    case Running =>\n      command match {\n        case OfferBid(bidder, offer) =>\n          Effect.persist(\n            BidRegistered(\n              Bid(\n                bidder,\n                offer,\n                Instant.ofEpochMilli(replicationContext.currentTimeMillis()),\n                replicationContext.replicaId)))\n        case GetHighestBid(replyTo) =>\n          replyTo ! state.highestBid\n          Effect.none\n        case Finish =>\n          Effect.persist(AuctionFinished(replicationContext.replicaId))\n        case Close =>\n          context.log.warn(\"Premature close\")\n          // Close should only be triggered when we have already finished\n          Effect.unhandled\n        case IsClosed(replyTo) =>\n          replyTo ! false\n          Effect.none\n      }\n  }\n} Java copysource@Override\npublic CommandHandler<Command, Event, AuctionState> commandHandler() {\n\n  CommandHandlerBuilder<Command, Event, AuctionState> builder = newCommandHandlerBuilder();\n\n  // running\n  builder\n      .forState(state -> state.stillRunning)\n      .onCommand(\n          OfferBid.class,\n          (state, bid) ->\n              Effect()\n                  .persist(\n                      new BidRegistered(\n                          new Bid(\n                              bid.bidder,\n                              bid.offer,\n                              Instant.ofEpochMilli(\n                                  this.getReplicationContext().currentTimeMillis()),\n                              this.getReplicationContext().replicaId()))))\n      .onCommand(\n          GetHighestBid.class,\n          (state, get) -> {\n            get.replyTo.tell(state.highestBid);\n            return Effect().none();\n          })\n      .onCommand(\n          Finish.class,\n          (state, finish) ->\n              Effect().persist(new AuctionFinished(getReplicationContext().replicaId())))\n      .onCommand(Close.class, (state, close) -> Effect().unhandled())\n      .onCommand(\n          IsClosed.class,\n          (state, get) -> {\n            get.replyTo.tell(false);\n            return Effect().none();\n          });\n\n  // finished\n  builder\n      .forAnyState()\n      .onCommand(OfferBid.class, (state, bid) -> Effect().unhandled())\n      .onCommand(\n          GetHighestBid.class,\n          (state, get) -> {\n            get.replyTo.tell(state.highestBid);\n            return Effect().none();\n          })\n      .onCommand(\n          Finish.class,\n          (state, finish) ->\n              Effect().persist(new AuctionFinished(getReplicationContext().replicaId())))\n      .onCommand(\n          Close.class,\n          (state, close) ->\n              Effect()\n                  .persist(\n                      new WinnerDecided(\n                          getReplicationContext().replicaId(),\n                          state.highestBid,\n                          state.highestCounterOffer)))\n      .onCommand(\n          IsClosed.class,\n          (state, get) -> {\n            get.replyTo.tell(state.isClosed());\n            return Effect().none();\n          });\n\n  return builder.build();\n}\nThere is nothing specific to Replicated Event Sourcing about the command handler. It is the same as a command handler for a standard EventSourcedBehavior. For OfferBid and AuctionFinished we do nothing more than to emit events corresponding to the command. For GetHighestBid we respond with details from the state. Note, that we overwrite the actual offer of the highest bid here with the amount of the highestCounterOffer. This is done to follow the popular auction style where the actual highest bid is never publicly revealed.\nThe auction entity is started with the initial parameters for the auction. The minimum bid is modelled as an initialBid.\nScala copysourceobject AuctionEntity {\n\n  def apply(\n      replica: ReplicaId,\n      name: String,\n      initialBid: AuctionEntity.Bid, // the initial bid is basically the minimum price bidden at start time by the owner\n      closingAt: Instant,\n      responsibleForClosing: Boolean,\n      allReplicas: Set[ReplicaId]): Behavior[Command] = Behaviors.setup[Command] { ctx =>\n    Behaviors.withTimers { timers =>\n      ReplicatedEventSourcing.commonJournalConfig(\n        ReplicationId(\"auction\", name, replica),\n        allReplicas,\n        PersistenceTestKitReadJournal.Identifier) { replicationCtx =>\n        new AuctionEntity(ctx, replicationCtx, timers, closingAt, responsibleForClosing, allReplicas)\n          .behavior(initialBid)\n      }\n    }\n  }\n\n}\n\nclass AuctionEntity(\n    context: ActorContext[AuctionEntity.Command],\n    replicationContext: ReplicationContext,\n    timers: TimerScheduler[AuctionEntity.Command],\n    closingAt: Instant,\n    responsibleForClosing: Boolean,\n    allReplicas: Set[ReplicaId]) {\n  import AuctionEntity._\n\n  private def behavior(initialBid: AuctionEntity.Bid): EventSourcedBehavior[Command, Event, AuctionState] =\n    EventSourcedBehavior(\n      replicationContext.persistenceId,\n      AuctionState(phase = Running, highestBid = initialBid, highestCounterOffer = initialBid.offer),\n      commandHandler,\n      eventHandler).receiveSignal {\n      case (state, RecoveryCompleted) => recoveryCompleted(state)\n    }\n\n  private def recoveryCompleted(state: AuctionState): Unit = {\n    if (shouldClose(state))\n      context.self ! Close\n\n    val millisUntilClosing = closingAt.toEpochMilli - replicationContext.currentTimeMillis()\n    timers.startSingleTimer(Finish, millisUntilClosing.millis)\n  }\n} Java copysourceclass AuctionEntity\n    extends ReplicatedEventSourcedBehavior<\n        AuctionEntity.Command, AuctionEntity.Event, AuctionEntity.AuctionState> {\n\n  public static ReplicaId R1 = new ReplicaId(\"R1\");\n  public static ReplicaId R2 = new ReplicaId(\"R2\");\n\n  public static Set<ReplicaId> ALL_REPLICAS = new HashSet<>(Arrays.asList(R1, R2));\n\n  private final ActorContext<Command> context;\n  private final TimerScheduler<Command> timers;\n  private final Bid initialBid;\n  private final Instant closingAt;\n  private final boolean responsibleForClosing;\n\n  public static Behavior<Command> create(\n      ReplicaId replica,\n      String name,\n      Bid initialBid,\n      Instant closingAt,\n      boolean responsibleForClosing) {\n    return Behaviors.setup(\n        ctx ->\n            Behaviors.withTimers(\n                timers ->\n                    ReplicatedEventSourcing.commonJournalConfig(\n                        new ReplicationId(\"Auction\", name, replica),\n                        ALL_REPLICAS,\n                        PersistenceTestKitReadJournal.Identifier(),\n                        replicationCtx ->\n                            new AuctionEntity(\n                                ctx,\n                                replicationCtx,\n                                timers,\n                                initialBid,\n                                closingAt,\n                                responsibleForClosing))));\n  }\n\n  private AuctionEntity(\n      ActorContext<Command> context,\n      ReplicationContext replicationContext,\n      TimerScheduler<Command> timers,\n      Bid initialBid,\n      Instant closingAt,\n      boolean responsibleForClosing) {\n    super(replicationContext);\n    this.context = context;\n    this.timers = timers;\n    this.initialBid = initialBid;\n    this.closingAt = closingAt;\n    this.responsibleForClosing = responsibleForClosing;\n  }\n\n  @Override\n  public AuctionState emptyState() {\n    return new AuctionState(true, initialBid, initialBid.offer, Collections.emptySet());\n  }\n\n  @Override\n  public SignalHandler<AuctionState> signalHandler() {\n    return newSignalHandlerBuilder()\n        .onSignal(RecoveryCompleted.instance(), this::onRecoveryCompleted)\n        .build();\n  }\n\n  private void onRecoveryCompleted(AuctionState state) {\n    if (shouldClose(state)) {\n      context.getSelf().tell(Close.INSTANCE);\n    }\n\n    long millisUntilClosing =\n        closingAt.toEpochMilli() - getReplicationContext().currentTimeMillis();\n    timers.startSingleTimer(Finish.INSTANCE, Duration.ofMillis(millisUntilClosing));\n  }\n\n}\nThe auction moves through the following phases: Scala copysource/**\n * The auction passes through several workflow phases.\n * First, in `Running` `OfferBid` commands are accepted.\n *\n * `AuctionEntity` instances in all DCs schedule a `Finish` command\n * at a given time. That persists the `AuctionFinished` event and the\n * phase is in `Closing` until the auction is finished in all DCs.\n *\n * When the auction has been finished no more `OfferBid` commands are accepted.\n *\n * The auction is also finished immediately if `AuctionFinished` event from another\n * DC is seen before the scheduled `Finish` command. In that way the auction is finished\n * as quickly as possible in all DCs even though there might be some clock skew.\n *\n * One DC is responsible for finally deciding the winner and publishing the result.\n * All events must be collected from all DC before that can happen.\n * When the responsible DC has seen all `AuctionFinished` events from other DCs\n * all other events have also been propagated and it can persist `WinnerDecided` and\n * the auction is finally `Closed`.\n */\nsealed trait AuctionPhase\ncase object Running extends AuctionPhase\nfinal case class Closing(finishedAtReplica: Set[ReplicaId]) extends AuctionPhase\ncase object Closed extends AuctionPhase\nThe closing and closed states are to model waiting for all replicas to see the result of the auction before actually closing the action.\nLet’s have a look at our state class, AuctionState which also represents the CRDT in our example.\nScala copysourcecase class AuctionState(phase: AuctionPhase, highestBid: Bid, highestCounterOffer: MoneyAmount)\n    extends CborSerializable {\n\n  def applyEvent(event: Event): AuctionState =\n    event match {\n      case BidRegistered(b) =>\n        if (isHigherBid(b, highestBid))\n          withNewHighestBid(b)\n        else\n          withTooLowBid(b)\n      case AuctionFinished(atDc) =>\n        phase match {\n          case Running =>\n            copy(phase = Closing(Set(atDc)))\n          case Closing(alreadyFinishedDcs) =>\n            copy(phase = Closing(alreadyFinishedDcs + atDc))\n          case _ =>\n            this\n        }\n      case _: WinnerDecided =>\n        copy(phase = Closed)\n    }\n\n  def withNewHighestBid(bid: Bid): AuctionState = {\n    require(phase != Closed)\n    require(isHigherBid(bid, highestBid))\n    copy(highestBid = bid, highestCounterOffer = highestBid.offer // keep last highest bid around\n    )\n  }\n\n  def withTooLowBid(bid: Bid): AuctionState = {\n    require(phase != Closed)\n    require(isHigherBid(highestBid, bid))\n    copy(highestCounterOffer = highestCounterOffer.max(bid.offer)) // update highest counter offer\n  }\n\n  def isHigherBid(first: Bid, second: Bid): Boolean =\n    first.offer > second.offer ||\n    (first.offer == second.offer && first.timestamp.isBefore(second.timestamp)) || // if equal, first one wins\n    // If timestamps are equal, choose by dc where the offer was submitted\n    // In real auctions, this last comparison should be deterministic but unpredictable, so that submitting to a\n    // particular DC would not be an advantage.\n    (first.offer == second.offer && first.timestamp.equals(second.timestamp) && first.originReplica.id\n      .compareTo(second.originReplica.id) < 0)\n} Java copysourcestatic class AuctionState implements CborSerializable {\n\n  final boolean stillRunning;\n  final Bid highestBid;\n  final int highestCounterOffer;\n  final Set<String> finishedAtDc;\n\n  AuctionState(\n      boolean stillRunning, Bid highestBid, int highestCounterOffer, Set<String> finishedAtDc) {\n    this.stillRunning = stillRunning;\n    this.highestBid = highestBid;\n    this.highestCounterOffer = highestCounterOffer;\n    this.finishedAtDc = finishedAtDc;\n  }\n\n  AuctionState withNewHighestBid(Bid bid) {\n    assertTrue(stillRunning);\n    assertTrue(isHigherBid(bid, highestBid));\n    return new AuctionState(\n        stillRunning, bid, highestBid.offer, finishedAtDc); // keep last highest bid around\n  }\n\n  AuctionState withTooLowBid(Bid bid) {\n    assertTrue(stillRunning);\n    assertTrue(isHigherBid(highestBid, bid));\n    return new AuctionState(\n        stillRunning, highestBid, Math.max(highestCounterOffer, bid.offer), finishedAtDc);\n  }\n\n  static Boolean isHigherBid(Bid first, Bid second) {\n    return first.offer > second.offer\n        || (first.offer == second.offer && first.timestamp.isBefore(second.timestamp))\n        || // if equal, first one wins\n        // If timestamps are equal, choose by dc where the offer was submitted\n        // In real auctions, this last comparison should be deterministic but unpredictable, so\n        // that submitting to a\n        // particular DC would not be an advantage.\n        (first.offer == second.offer\n            && first.timestamp.equals(second.timestamp)\n            && first.originReplica.id().compareTo(second.originReplica.id()) < 0);\n  }\n\n  AuctionState addFinishedAtReplica(String replica) {\n    Set<String> s = new HashSet<>(finishedAtDc);\n    s.add(replica);\n    return new AuctionState(\n        false, highestBid, highestCounterOffer, Collections.unmodifiableSet(s));\n  }\n\n  public AuctionState close() {\n    return new AuctionState(false, highestBid, highestCounterOffer, Collections.emptySet());\n  }\n\n  public boolean isClosed() {\n    return !stillRunning && finishedAtDc.isEmpty();\n  }\n}\nThe state consists of a flag that keeps track of whether the auction is still active, the currently highest bid, and the highest counter offer so far.\nIn the eventHandler, we handle persisted events to drive the state change. When a new bid is registered,\nit needs to be decided whether the new bid is the winning bid or not the state needs to be updated accordingly\nThe point of CRDTs is that the state must be end up being the same regardless of the order the events have been processed. We can see how this works in the auction example: we are only interested in the highest bid, so, if we can define an ordering on all bids, it should suffice to compare the new bid with currently highest to eventually end up with the globally highest regardless of the order in which the events come in.\nThe ordering between bids is crucial, therefore. We need to ensure that it is deterministic and does not depend on local state outside of our state class so that all replicas come to the same result. We define the ordering as this:\nA higher bid wins. If there’s a tie between the two highest bids, the bid that was registered earlier wins. For that we keep track of the (local) timestamp the bid was registered. We need to make sure that no timestamp is used twice in the same replica (missing in this example). If there’s a tie between the timestamp, we define an arbitrary but deterministic ordering on the replicas, in our case we just compare the name strings of the replicas. That’s why we need to keep the identifier of the replica where a bid was registered for every Bid.\nIf the new bid was higher, we keep this one as the new highest and keep the amount of the former highest as the highestCounterOffer. If the new bid was lower, we just update the highestCounterOffer if necessary.\nUsing those rules, the order of incoming does not matter. Replicas will eventually converge to the same result.","title":"Auction example"},{"location":"/typed/replicated-eventsourcing-auction.html#triggering-closing","text":"In the auction we want to ensure that all bids are seen before declaring a winner. That means that an auction can only be closed once all replicas have seen all bids.\nIn the event handler above, when recovery is not running, it calls eventTriggers.\nScala copysourceprivate def eventTriggers(event: Event, newState: AuctionState): Unit = {\n  event match {\n    case finished: AuctionFinished =>\n      newState.phase match {\n        case Closing(alreadyFinishedAtDc) =>\n          context.log.infoN(\n            \"AuctionFinished at {}, already finished at [{}]\",\n            finished.atReplica,\n            alreadyFinishedAtDc.mkString(\", \"))\n          if (alreadyFinishedAtDc(replicationContext.replicaId)) {\n            if (shouldClose(newState)) context.self ! Close\n          } else {\n            context.log.info(\"Sending finish to self\")\n            context.self ! Finish\n          }\n\n        case _ => // no trigger for this state\n      }\n    case _ => // no trigger for this event\n  }\n}\n\nprivate def shouldClose(state: AuctionState): Boolean = {\n  responsibleForClosing && (state.phase match {\n    case Closing(alreadyFinishedAtDc) =>\n      val allDone = allReplicas.diff(alreadyFinishedAtDc).isEmpty\n      if (!allDone) {\n        context.log.info2(\n          s\"Not closing auction as not all DCs have reported finished. All DCs: {}. Reported finished {}\",\n          allReplicas,\n          alreadyFinishedAtDc)\n      }\n      allDone\n    case _ =>\n      false\n  })\n} Java copysourceprivate void eventTriggers(AuctionFinished event, AuctionState newState) {\n  if (newState.finishedAtDc.contains(getReplicationContext().replicaId().id())) {\n    if (shouldClose(newState)) {\n      context.getSelf().tell(Close.INSTANCE);\n    }\n  } else {\n    context.getSelf().tell(Finish.INSTANCE);\n  }\n}\n\nprivate boolean shouldClose(AuctionState state) {\n  return responsibleForClosing\n      && !state.isClosed()\n      && getReplicationContext().getAllReplicas().stream()\n          .map(ReplicaId::id)\n          .collect(Collectors.toSet())\n          .equals(state.finishedAtDc);\n}\nThe event trigger uses the ReplicationContext to decide when to trigger the Finish of the action. When a replica saves the AuctionFinished event it checks whether it should close the auction. For the close to happen the replica must be the one designated to close and all replicas must have reported that they have finished.","title":"Triggering closing"},{"location":"/typed/replicated-eventsourcing-cart.html","text":"","title":"Shopping cart example"},{"location":"/typed/replicated-eventsourcing-cart.html#shopping-cart-example","text":"The provided CRDT data structures can be used as the root state of a replicated EventSourcedBehavior but they can also be nested inside another data structure. This requires a bit more careful thinking about the eventual consistency.\nIn this sample we model a shopping cart as a map of product ids and the number of that product added or removed in the shopping cart. By using the CounterCounter CRDT and persisting its Update in our events we can be sure that an add or remove of items in any data center will eventually lead to all data centers ending up with the same number of each product.\nScala copysourceobject ShoppingCart {\n\n  type ProductId = String\n\n  sealed trait Command extends CborSerializable\n  final case class AddItem(id: ProductId, count: Int) extends Command\n  final case class RemoveItem(id: ProductId, count: Int) extends Command\n  final case class GetCartItems(replyTo: ActorRef[CartItems]) extends Command\n  final case class CartItems(items: Map[ProductId, Int]) extends CborSerializable\n\n  sealed trait Event extends CborSerializable\n  final case class ItemUpdated(id: ProductId, update: Counter.Updated) extends Event\n\n  final case class State(items: Map[ProductId, Counter])\n\n  def apply(entityId: String, replicaId: ReplicaId, allReplicaIds: Set[ReplicaId]): Behavior[Command] = {\n    ReplicatedEventSourcing.commonJournalConfig(\n      ReplicationId(\"blog\", entityId, replicaId),\n      allReplicaIds,\n      PersistenceTestKitReadJournal.Identifier) { replicationContext =>\n      EventSourcedBehavior[Command, Event, State](\n        replicationContext.persistenceId,\n        State(Map.empty),\n        (state, cmd) => commandHandler(state, cmd),\n        (state, event) => eventHandler(state, event))\n    }\n  }\n\n  private def commandHandler(state: State, cmd: Command): Effect[Event, State] = {\n    cmd match {\n      case AddItem(productId, count) =>\n        Effect.persist(ItemUpdated(productId, Counter.Updated(count)))\n      case RemoveItem(productId, count) =>\n        Effect.persist(ItemUpdated(productId, Counter.Updated(-count)))\n      case GetCartItems(replyTo) =>\n        val items = state.items.collect {\n          case (id, counter) if counter.value > 0 => id -> counter.value.toInt\n        }\n        replyTo ! CartItems(items)\n        Effect.none\n    }\n  }\n\n  private def eventHandler(state: State, event: Event): State = {\n    event match {\n      case ItemUpdated(id, update) =>\n        val newItems = state.items.get(id) match {\n          case Some(counter) => state.items + (id -> counter.applyOperation(update))\n          case None          => state.items + (id -> Counter.empty.applyOperation(update))\n        }\n        State(newItems)\n    }\n  }\n} Java copysourcepublic final class ShoppingCart\n    extends ReplicatedEventSourcedBehavior<\n        ShoppingCart.Command, ShoppingCart.Event, ShoppingCart.State> {\n\n  public interface Event {}\n\n  public static final class ItemUpdated implements Event {\n    public final String productId;\n    public final Counter.Updated update;\n\n    public ItemUpdated(String productId, Counter.Updated update) {\n      this.productId = productId;\n      this.update = update;\n    }\n  }\n\n  public interface Command {}\n\n  public static final class AddItem implements Command {\n    public final String productId;\n    public final int count;\n\n    public AddItem(String productId, int count) {\n      this.productId = productId;\n      this.count = count;\n    }\n  }\n\n  public static final class RemoveItem implements Command {\n    public final String productId;\n    public final int count;\n\n    public RemoveItem(String productId, int count) {\n      this.productId = productId;\n      this.count = count;\n    }\n  }\n\n  public static class GetCartItems implements Command {\n    public final ActorRef<CartItems> replyTo;\n\n    public GetCartItems(ActorRef<CartItems> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static final class CartItems {\n    public final Map<String, Integer> items;\n\n    public CartItems(Map<String, Integer> items) {\n      this.items = items;\n    }\n  }\n\n  public static final class State {\n    public final Map<String, Counter> items = new HashMap<>();\n  }\n\n  public static Behavior<Command> create(\n      String entityId, ReplicaId replicaId, Set<ReplicaId> allReplicas) {\n    return ReplicatedEventSourcing.commonJournalConfig(\n        new ReplicationId(\"blog\", entityId, replicaId),\n        allReplicas,\n        PersistenceTestKitReadJournal.Identifier(),\n        ShoppingCart::new);\n  }\n\n  private ShoppingCart(ReplicationContext replicationContext) {\n    super(replicationContext);\n  }\n\n  @Override\n  public State emptyState() {\n    return new State();\n  }\n\n  @Override\n  public CommandHandler<Command, Event, State> commandHandler() {\n    return newCommandHandlerBuilder()\n        .forAnyState()\n        .onCommand(AddItem.class, this::onAddItem)\n        .onCommand(RemoveItem.class, this::onRemoveItem)\n        .onCommand(GetCartItems.class, this::onGetCartItems)\n        .build();\n  }\n\n  private Effect<Event, State> onAddItem(State state, AddItem command) {\n    return Effect()\n        .persist(new ItemUpdated(command.productId, new Counter.Updated(command.count)));\n  }\n\n  private Effect<Event, State> onRemoveItem(State state, RemoveItem command) {\n    return Effect()\n        .persist(new ItemUpdated(command.productId, new Counter.Updated(-command.count)));\n  }\n\n  private Effect<Event, State> onGetCartItems(State state, GetCartItems command) {\n    command.replyTo.tell(new CartItems(filterEmptyAndNegative(state.items)));\n    return Effect().none();\n  }\n\n  private Map<String, Integer> filterEmptyAndNegative(Map<String, Counter> cart) {\n    Map<String, Integer> result = new HashMap<>();\n    for (Map.Entry<String, Counter> entry : cart.entrySet()) {\n      int count = entry.getValue().value().intValue();\n      if (count > 0) result.put(entry.getKey(), count);\n    }\n    return Collections.unmodifiableMap(result);\n  }\n\n  @Override\n  public EventHandler<State, Event> eventHandler() {\n    return newEventHandlerBuilder()\n        .forAnyState()\n        .onEvent(ItemUpdated.class, this::onItemUpdated)\n        .build();\n  }\n\n  private State onItemUpdated(State state, ItemUpdated event) {\n    final Counter counterForProduct;\n    if (state.items.containsKey(event.productId)) {\n      counterForProduct = state.items.get(event.productId);\n    } else {\n      counterForProduct = Counter.empty();\n    }\n    state.items.put(event.productId, counterForProduct.applyOperation(event.update));\n    return state;\n  }\n}\nWith this model we cannot have a ClearCart command as that could give different states in different data centers. It is quite easy to imagine such a scenario: commands arriving in the order ClearCart, AddItem('a', 5) in one data center and the order AddItem('a', 5), ClearCart in another.\nTo clear a cart a client would instead have to remove as many items of each product as it sees in the cart at the time of removal.","title":"Shopping cart example"},{"location":"/typed/index-persistence-durable-state.html","text":"","title":"Persistence (Durable State)"},{"location":"/typed/index-persistence-durable-state.html#persistence-durable-state-","text":"Durable State Module info Introduction Example and core API Effects and Side Effects Cluster Sharding and DurableStateBehavior Accessing the ActorContext Changing Behavior Replies Serialization Tagging Wrapping DurableStateBehavior Style Guide Command handlers in the state Optional initial state CQRS Persistence Query Dependency Introduction Using query with Pekko Projections","title":"Persistence (Durable State)"},{"location":"/typed/durable-state/persistence.html","text":"","title":"Durable State"},{"location":"/typed/durable-state/persistence.html#durable-state","text":"","title":"Durable State"},{"location":"/typed/durable-state/persistence.html#module-info","text":"To use Pekko Persistence, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-persistence-typed\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-persistence-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-typed_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-typed_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-persistence-testkit_${versions.ScalaBinary}\"\n}\nYou also have to select durable state store plugin, see Persistence Plugins.\nProject Info: Pekko Event Sourcing (typed) Artifact org.apache.pekko pekko-persistence-typed 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.persistence.typed License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/typed/durable-state/persistence.html#introduction","text":"This model of Pekko Persistence enables a stateful actor / entity to store the full state after processing each command instead of using event sourcing. This reduces the conceptual complexity and can be a handy tool for simple use cases. Very much like a CRUD based operation, the API is conceptually simple - a function from current state and incoming command to the next state which replaces the current state in the database.\n(State, Command) => State\nThe current state is always stored in the database. Since only the latest state is stored, we don’t have access to any of the history of changes, unlike event sourced storage. Pekko Persistence would read that state and store it in memory. After processing of the command is finished, the new state will be stored in the database. The processing of the next command will not start until the state has been successfully stored in the database.\nPekko Persistence also supports Event Sourcing based implementation, where only the events that are persisted by the actor are stored, but not the actual state of the actor. By storing all events, using this model, a stateful actor can be recovered by replaying the stored events to the actor, which allows it to rebuild its state.\nSince each entity lives on one node, consistency is guaranteed and reads can be served directly from memory. For details on how this guarantee is ensured, have a look at the Cluster Sharding and DurableStateBehavior section below.","title":"Introduction"},{"location":"/typed/durable-state/persistence.html#example-and-core-api","text":"Let’s start with a simple example that models a counter using a Pekko persistent actor. The minimum required for a DurableStateBehaviorDurableStateBehavior is:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.typed.state.scaladsl.DurableStateBehavior\nimport pekko.persistence.typed.PersistenceId\n\nobject MyPersistentCounter {\n  sealed trait Command[ReplyMessage] extends CborSerializable\n\n  final case class State(value: Int) extends CborSerializable\n\n  def counter(persistenceId: PersistenceId): DurableStateBehavior[Command[_], State] = {\n    DurableStateBehavior.apply[Command[_], State](\n      persistenceId,\n      emptyState = State(0),\n      commandHandler =\n        (state, command) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"))\n  }\n} Java copysourcepublic class MyPersistentCounter\n    extends DurableStateBehavior<MyPersistentCounter.Command<?>, MyPersistentCounter.State> {\n\n  interface Command<ReplyMessage> {}\n\n  public static class State {\n    private final int value;\n\n    public State(int value) {\n      this.value = value;\n    }\n\n    public int get() {\n      return value;\n    }\n  }\n\n  public static Behavior<Command<?>> create(PersistenceId persistenceId) {\n    return new MyPersistentCounter(persistenceId);\n  }\n\n  private MyPersistentCounter(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  @Override\n  public State emptyState() {\n    return new State(0);\n  }\n\n  @Override\n  public CommandHandler<Command<?>, State> commandHandler() {\n    return (state, command) -> {\n      throw new RuntimeException(\"TODO: process the command & return an Effect\");\n    };\n  }\n}\nThe first important thing to notice is the Behavior of a persistent actor is typed to the type of the Command because this is the type of message a persistent actor should receive. In Pekko, this is now enforced by the type system.\nThe components that make up a DurableStateBehavior are:\npersistenceId is the stable unique identifier for the persistent actor. emptyState defines the State when the entity is first created e.g. a Counter would start with 0 as state. commandHandler defines how to handle commands and map to appropriate effects e.g. persisting state and replying to actors.\nNext we’ll discuss each of these in detail.","title":"Example and core API"},{"location":"/typed/durable-state/persistence.html#persistenceid","text":"The PersistenceIdPersistenceId is the stable unique identifier for the persistent actor in the backend durabe state store.\nCluster Sharding is typically used together with DurableStateBehavior to ensure that there is only one active entity for each PersistenceId (entityId). There are techniques to ensure this uniqueness, an example of which can be found in the Persistence example in the Cluster Sharding documentation. This illustrates how to construct the PersistenceId from the entityTypeKey and entityId provided by the EntityContext.\nThe entityId in Cluster Sharding is the business domain identifier which uniquely identifies the instance of that specific EntityType. This means that across the cluster we have a unique combination of (EntityType, EntityId). Hence the entityId might not be unique enough to be used as the PersistenceId by itself. For example two different types of entities may have the same entityId. To create a unique PersistenceId the entityId should be prefixed with a stable name of the entity type, which typically is the same as the EntityTypeKey.name that is used in Cluster Sharding. There are PersistenceId.applyPersistenceId.of factory methods to help with constructing such PersistenceId from an entityTypeHint and entityId.\nThe default separator when concatenating the entityTypeHint and entityId is |, but a custom separator is supported.\nA custom identifier can be created with PersistenceId.ofUniqueId.","title":"PersistenceId"},{"location":"/typed/durable-state/persistence.html#command-handler","text":"The command handler is a function with 2 parameters, the current State and the incoming Command.\nA command handler returns an Effect directive that defines what state, if any, to persist. Effects are created using a factory that is returned via the Effect() method the Effect factory.\nThe two most commonly used effects are:\npersist will persist the latest value of the state. No history of state changes will be stored none no state to be persisted, for example a read-only command\nMore effects are explained in Effects and Side Effects.\nIn addition to returning the primary Effect for the command, DurableStateBehaviors can also chain side effects that are to be performed after successful persist which is achieved with the thenRun function e.g. Effect.persist(..).thenRunEffect().persist(..).thenRun.","title":"Command handler"},{"location":"/typed/durable-state/persistence.html#completing-the-example","text":"Let’s fill in the details of the example.\nCommands:\nScala copysourcesealed trait Command[ReplyMessage] extends CborSerializable\nfinal case object Increment extends Command[Nothing]\nfinal case class IncrementBy(value: Int) extends Command[Nothing]\nfinal case class GetValue(replyTo: ActorRef[State]) extends Command[State]\nfinal case object Delete extends Command[Nothing] Java copysourceinterface Command<ReplyMessage> {}\n\npublic enum Increment implements Command<Void> {\n  INSTANCE\n}\n\npublic static class IncrementBy implements Command<Void> {\n  public final int value;\n\n  public IncrementBy(int value) {\n    this.value = value;\n  }\n}\n\npublic static class GetValue implements Command<State> {\n  private final ActorRef<Integer> replyTo;\n\n  public GetValue(ActorRef<Integer> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\nState is a storage for the latest value of the counter.\nScala copysourcefinal case class State(value: Int) extends CborSerializable Java copysourcepublic static class State {\n  private final int value;\n\n  public State(int value) {\n    this.value = value;\n  }\n\n  public int get() {\n    return value;\n  }\n}\nThe command handler handles the commands Increment, IncrementBy and GetValue.\nIncrement increments the counter by 1 and persists the updated value as an effect in the State IncrementBy increments the counter by the value passed to it and persists the updated value as an effect in the State GetValue retrieves the value of the counter from the State and replies with it to the actor passed in\nScala copysourceimport pekko.persistence.typed.state.scaladsl.Effect\n\nval commandHandler: (State, Command[_]) => Effect[State] = (state, command) =>\n  command match {\n    case Increment         => Effect.persist(state.copy(value = state.value + 1))\n    case IncrementBy(by)   => Effect.persist(state.copy(value = state.value + by))\n    case GetValue(replyTo) => Effect.reply(replyTo)(state)\n    case Delete            => Effect.delete[State]()\n  } Java copysource@Override\npublic CommandHandler<Command<?>, State> commandHandler() {\n  return newCommandHandlerBuilder()\n      .forAnyState()\n      .onCommand(\n          Increment.class, (state, command) -> Effect().persist(new State(state.get() + 1)))\n      .onCommand(\n          IncrementBy.class,\n          (state, command) -> Effect().persist(new State(state.get() + command.value)))\n      .onCommand(\n          GetValue.class, (state, command) -> Effect().reply(command.replyTo, state.get()))\n      .build();\n}\nThese are used to create a DurableStateBehavior: These are defined in an DurableStateBehavior:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.typed.state.scaladsl.DurableStateBehavior\nimport pekko.persistence.typed.PersistenceId\n\ndef counter(id: String): DurableStateBehavior[Command[_], State] = {\n  DurableStateBehavior.apply[Command[_], State](\n    persistenceId = PersistenceId.ofUniqueId(id),\n    emptyState = State(0),\n    commandHandler = commandHandler)\n} Java copysourceimport org.apache.pekko.persistence.typed.state.javadsl.DurableStateBehavior;\nimport org.apache.pekko.persistence.typed.PersistenceId;\n\npublic class MyPersistentCounter\n    extends DurableStateBehavior<MyPersistentCounter.Command<?>, MyPersistentCounter.State> {\n\n  // commands, events and state defined here\n\n  public static Behavior<Command<?>> create(PersistenceId persistenceId) {\n    return new MyPersistentCounter(persistenceId);\n  }\n\n  private MyPersistentCounter(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  @Override\n  public State emptyState() {\n    return new State(0);\n  }\n\n  @Override\n  public CommandHandler<Command<?>, State> commandHandler() {\n    return newCommandHandlerBuilder()\n        .forAnyState()\n        .onCommand(\n            Increment.class, (state, command) -> Effect().persist(new State(state.get() + 1)))\n        .onCommand(\n            IncrementBy.class,\n            (state, command) -> Effect().persist(new State(state.get() + command.value)))\n        .onCommand(\n            GetValue.class, (state, command) -> Effect().reply(command.replyTo, state.get()))\n        .build();\n  }\n}","title":"Completing the example"},{"location":"/typed/durable-state/persistence.html#effects-and-side-effects","text":"A command handler returns an Effect directive that defines what state, if any, to persist. Effects are created using a factory that is returned via the Effect() method the Effect factory and can be one of:\npersist will persist the latest state. If it’s a new persistence id, the record will be inserted. In case of an existing persistence id, the record will be updated only if the revision number of the incoming record is 1 more than the already existing record. Otherwise persist will fail. delete will delete the state by setting it to the empty state and the revision number will be incremented by 1. none no state to be persisted, for example a read-only command unhandled the command is unhandled (not supported) in current state stop stop this actor stash the current command is stashed unstashAll process the commands that were stashed with Effect.stashEffect().stash reply send a reply message to the given ActorRef\nNote that only one of those can be chosen per incoming command. It is not possible to both persist and say none/unhandled.\nIn addition to returning the primary Effect for the command DurableStateBehaviors can also chain side effects that are to be performed after successful persist which is achieved with the thenRun function that runs the callback passed to it e.g. Effect.persist(..).thenRunEffect().persist(..).thenRun.\nAll thenRun registered callbacks are executed sequentially after successful execution of the persist statement (or immediately, in case of none and unhandled).\nIn addition to thenRun the following actions can also be performed after successful persist:\nthenStop the actor will be stopped thenUnstashAll process the commands that were stashed with Effect.stashEffect().stash thenReply send a reply message to the given ActorRef\nIn the example below, we use a different constructor of DurableStateBehavior.withEnforcedReplies, which creates a Behavior for a persistent actor that ensures that every command sends a reply back. Hence it will be a compilation error if the returned effect from a CommandHandler isn’t a ReplyEffect.\nInstead of Increment we will have a new command IncrementWithConfirmation that, along with persistence will also send an acknowledgement as a reply to the ActorRef passed in the command.\nExample of effects and side-effects:\nScala copysourcesealed trait Command[ReplyMessage] extends CborSerializable\nfinal case class IncrementWithConfirmation(replyTo: ActorRef[Done]) extends Command[Done]\nfinal case class GetValue(replyTo: ActorRef[State]) extends Command[State]\n\nfinal case class State(value: Int) extends CborSerializable\n\ndef counter(persistenceId: PersistenceId): DurableStateBehavior[Command[_], State] = {\n  DurableStateBehavior.withEnforcedReplies[Command[_], State](\n    persistenceId,\n    emptyState = State(0),\n    commandHandler = (state, command) =>\n      command match {\n\n        case IncrementWithConfirmation(replyTo) =>\n          Effect.persist(state.copy(value = state.value + 1)).thenReply(replyTo)(_ => Done)\n\n        case GetValue(replyTo) =>\n          Effect.reply(replyTo)(state)\n      })\n} Java copysourceimport org.apache.pekko.Done;\ninterface Command<ReplyMessage> {}\n\npublic static class IncrementWithConfirmation implements Command<Void> {\n  public final ActorRef<Done> replyTo;\n\n  public IncrementWithConfirmation(ActorRef<Done> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class GetValue implements Command<State> {\n  private final ActorRef<Integer> replyTo;\n\n  public GetValue(ActorRef<Integer> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class State {\n  private final int value;\n\n  public State(int value) {\n    this.value = value;\n  }\n\n  public int get() {\n    return value;\n  }\n}\n\npublic static Behavior<Command<?>> create(PersistenceId persistenceId) {\n  return new MyPersistentCounterWithReplies(persistenceId);\n}\n\nprivate MyPersistentCounterWithReplies(PersistenceId persistenceId) {\n  super(persistenceId);\n}\n\n@Override\npublic State emptyState() {\n  return new State(0);\n}\n\n@Override\npublic CommandHandler<Command<?>, State> commandHandler() {\n  return newCommandHandlerBuilder()\n      .forAnyState()\n      .onCommand(\n          IncrementWithConfirmation.class,\n          (state, command) ->\n              Effect()\n                  .persist(new State(state.get() + 1))\n                  .thenReply(command.replyTo, (st) -> Done.getInstance()))\n      .onCommand(\n          GetValue.class, (state, command) -> Effect().reply(command.replyTo, state.get()))\n      .build();\n}\nThe most common way to have a side-effect is to use the thenRun method on Effect. In case you have multiple side-effects that needs to be run for several commands, you can factor them out into functions and reuse for all the commands. For example:\nScala copysource// Example factoring out a chained effect to use in several places with `thenRun`\nval commonChainedEffects: Mood => Unit = _ => println(\"Command processed\")\n// Then in a command handler:\nEffect\n  .persist(Remembered(\"Yep\")) // persist event\n  .thenRun(commonChainedEffects) // add on common chained effect Java copysource// Example factoring out a chained effect to use in several places with `thenRun`\nstatic final Procedure<ExampleState> commonChainedEffect =\n    state -> System.out.println(\"Command handled!\");\n\n      @Override\n      public CommandHandler<MyCommand, MyEvent, ExampleState> commandHandler() {\n        return newCommandHandlerBuilder()\n            .forStateType(ExampleState.class)\n            .onCommand(\n                Cmd.class,\n                (state, cmd) ->\n                    Effect()\n                        .persist(new Evt(cmd.data))\n                        .thenRun(() -> cmd.replyTo.tell(new Ack()))\n                        .thenRun(commonChainedEffect))\n            .build();\n      }","title":"Effects and Side Effects"},{"location":"/typed/durable-state/persistence.html#side-effects-ordering-and-guarantees","text":"Any side effects are executed on an at-most-once basis and will not be executed if the persist fails.\nSide effects are not run when the actor is restarted or started again after being stopped.\nThe side effects are executed sequentially, it is not possible to execute side effects in parallel, unless they call out to something that is running concurrently (for example sending a message to another actor).\nIt’s possible to execute a side effect before persisting the state, but that can result in that the side effect is performed but that the state is not stored if the persist fails.","title":"Side effects ordering and guarantees"},{"location":"/typed/durable-state/persistence.html#cluster-sharding-and-durablestatebehavior","text":"Cluster Sharding is an excellent fit to spread persistent actors over a cluster, addressing them by id. It makes it possible to have more persistent actors exist in the cluster than what would fit in the memory of one node. Cluster sharding improves the resilience of the cluster. If a node crashes, the persistent actors are quickly started on a new node and can resume operations.\nThe DurableStateBehavior can then be run as any plain actor as described in actors documentation, but since Pekko Persistence is based on the single-writer principle, the persistent actors are typically used together with Cluster Sharding. For a particular persistenceId only one persistent actor instance should be active at one time. Cluster Sharding ensures that there is only one active entity (or actor instance) for each id.","title":"Cluster Sharding and DurableStateBehavior"},{"location":"/typed/durable-state/persistence.html#accessing-the-actorcontext","text":"If the DurableStateBehaviorDurableStateBehavior needs to use the ActorContextActorContext, for example to spawn child actors, it can be obtained by wrapping construction with Behaviors.setup:\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.typed.state.scaladsl.Effect\nimport pekko.persistence.typed.state.scaladsl.DurableStateBehavior.CommandHandler\n\ndef apply(): Behavior[String] =\n  Behaviors.setup { context =>\n    DurableStateBehavior[String, State](\n      persistenceId = PersistenceId.ofUniqueId(\"myPersistenceId\"),\n      emptyState = State(0),\n      commandHandler = CommandHandler.command { cmd =>\n        context.log.info(\"Got command {}\", cmd)\n        Effect.none\n      })\n  } Java copysourcepublic class MyPersistentBehavior\n    extends DurableStateBehavior<MyPersistentBehavior.Command, MyPersistentBehavior.State> {\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return Behaviors.setup(ctx -> new MyPersistentBehavior(persistenceId, ctx));\n  }\n\n  // this makes the context available to the command handler etc.\n  private final ActorContext<Command> context;\n\n  // optionally if you only need `ActorContext.getSelf()`\n  private final ActorRef<Command> self;\n\n  public MyPersistentBehavior(PersistenceId persistenceId, ActorContext<Command> ctx) {\n    super(persistenceId);\n    this.context = ctx;\n    this.self = ctx.getSelf();\n  }\n\n}","title":"Accessing the ActorContext"},{"location":"/typed/durable-state/persistence.html#changing-behavior","text":"After processing a message, actors are able to return the Behavior that is used for the next message.\nAs you can see in the above examples this is not supported by persistent actors. Instead, the state is persisted as an Effect by the commandHandler.\nThe reason a new behavior can’t be returned is that behavior is part of the actor’s state and must also carefully be reconstructed during recovery from the persisted state. This would imply that the state needs to be encoded such that the behavior can also be restored from it. That would be very prone to mistakes which is why it is not allowed in Pekko Persistence.\nFor basic actors you can use the same set of command handlers independent of what state the entity is in. For more complex actors it’s useful to be able to change the behavior in the sense that different functions for processing commands may be defined depending on what state the actor is in. This is useful when implementing finite state machine (FSM) like entities.\nThe next example demonstrates how to define different behavior based on the current State. It shows an actor that represents the state of a blog post. Before a post is started the only command it can process is to AddPost. Once it is started then one can look it up with GetPost, modify it with ChangeBody or publish it with Publish.\nThe state is captured by:\nScala copysourcesealed trait State\n\ncase object BlankState extends State\n\nfinal case class DraftState(content: PostContent) extends State {\n  def withBody(newBody: String): DraftState =\n    copy(content = content.copy(body = newBody))\n\n  def postId: String = content.postId\n}\n\nfinal case class PublishedState(content: PostContent) extends State {\n  def postId: String = content.postId\n} Java copysourceinterface State {}\n\nenum BlankState implements State {\n  INSTANCE\n}\n\nstatic class DraftState implements State {\n  final PostContent content;\n\n  DraftState(PostContent content) {\n    this.content = content;\n  }\n\n  DraftState withContent(PostContent newContent) {\n    return new DraftState(newContent);\n  }\n\n  DraftState withBody(String newBody) {\n    return withContent(new PostContent(postId(), content.title, newBody));\n  }\n\n  String postId() {\n    return content.postId;\n  }\n}\n\nstatic class PublishedState implements State {\n  final PostContent content;\n\n  PublishedState(PostContent content) {\n    this.content = content;\n  }\n\n  PublishedState withContent(PostContent newContent) {\n    return new PublishedState(newContent);\n  }\n\n  PublishedState withBody(String newBody) {\n    return withContent(new PostContent(postId(), content.title, newBody));\n  }\n\n  String postId() {\n    return content.postId;\n  }\n}\nThe commands, of which only a subset are valid depending on the state:\nScala copysourcesealed trait Command\nfinal case class AddPost(content: PostContent, replyTo: ActorRef[StatusReply[AddPostDone]]) extends Command\nfinal case class AddPostDone(postId: String)\nfinal case class GetPost(replyTo: ActorRef[PostContent]) extends Command\nfinal case class ChangeBody(newBody: String, replyTo: ActorRef[Done]) extends Command\nfinal case class Publish(replyTo: ActorRef[Done]) extends Command\nfinal case class PostContent(postId: String, title: String, body: String) Java copysourcepublic interface Command {}\npublic static class AddPost implements Command {\n  final PostContent content;\n  final ActorRef<AddPostDone> replyTo;\n\n  public AddPost(PostContent content, ActorRef<AddPostDone> replyTo) {\n    this.content = content;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class AddPostDone implements Command {\n  final String postId;\n\n  public AddPostDone(String postId) {\n    this.postId = postId;\n  }\n}\npublic static class GetPost implements Command {\n  final ActorRef<PostContent> replyTo;\n\n  public GetPost(ActorRef<PostContent> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class ChangeBody implements Command {\n  final String newBody;\n  final ActorRef<Done> replyTo;\n\n  public ChangeBody(String newBody, ActorRef<Done> replyTo) {\n    this.newBody = newBody;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class Publish implements Command {\n  final ActorRef<Done> replyTo;\n\n  public Publish(ActorRef<Done> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class PostContent implements Command {\n  final String postId;\n  final String title;\n  final String body;\n\n  public PostContent(String postId, String title, String body) {\n    this.postId = postId;\n    this.title = title;\n    this.body = body;\n  }\n}\nThe command handler to process each command is decided by the state class (or state predicate) that is given to the forStateType of the CommandHandlerBuilder and the match cases in the builders. The command handler to process each command is decided by first looking at the state and then the command. It typically becomes two levels of pattern matching, first on the state and then on the command. Delegating to methods like addPost, changeBody, publish etc. is a good practice because the one-line cases give a nice overview of the message dispatch.\nScala copysourceprivate val commandHandler: (State, Command) => Effect[State] = { (state, command) =>\n  state match {\n\n    case BlankState =>\n      command match {\n        case cmd: AddPost => addPost(cmd)\n        case _            => Effect.unhandled\n      }\n\n    case draftState: DraftState =>\n      command match {\n        case cmd: ChangeBody  => changeBody(draftState, cmd)\n        case Publish(replyTo) => publish(draftState, replyTo)\n        case GetPost(replyTo) => getPost(draftState, replyTo)\n        case AddPost(_, replyTo) =>\n          Effect.unhandled[State].thenRun(_ => replyTo ! StatusReply.Error(\"Cannot add post while in draft state\"))\n      }\n\n    case publishedState: PublishedState =>\n      command match {\n        case GetPost(replyTo) => getPost(publishedState, replyTo)\n        case AddPost(_, replyTo) =>\n          Effect.unhandled[State].thenRun(_ => replyTo ! StatusReply.Error(\"Cannot add post, already published\"))\n        case _ => Effect.unhandled\n      }\n  }\n}\n\nprivate def addPost(cmd: AddPost): Effect[State] = {\n  Effect.persist(DraftState(cmd.content)).thenRun { _ =>\n    // After persist is done additional side effects can be performed\n    cmd.replyTo ! StatusReply.Success(AddPostDone(cmd.content.postId))\n  }\n}\n\nprivate def changeBody(state: DraftState, cmd: ChangeBody): Effect[State] = {\n  Effect.persist(state.withBody(cmd.newBody)).thenRun { _ =>\n    cmd.replyTo ! Done\n  }\n}\n\nprivate def publish(state: DraftState, replyTo: ActorRef[Done]): Effect[State] = {\n  Effect.persist(PublishedState(state.content)).thenRun { _ =>\n    println(s\"Blog post ${state.postId} was published\")\n    replyTo ! Done\n  }\n}\n\nprivate def getPost(state: DraftState, replyTo: ActorRef[PostContent]): Effect[State] = {\n  replyTo ! state.content\n  Effect.none\n}\n\nprivate def getPost(state: PublishedState, replyTo: ActorRef[PostContent]): Effect[State] = {\n  replyTo ! state.content\n  Effect.none\n} Java copysource@Override\npublic CommandHandler<Command, State> commandHandler() {\n  CommandHandlerBuilder<Command, State> builder = newCommandHandlerBuilder();\n\n  builder.forStateType(BlankState.class).onCommand(AddPost.class, this::onAddPost);\n\n  builder\n      .forStateType(DraftState.class)\n      .onCommand(ChangeBody.class, this::onChangeBody)\n      .onCommand(Publish.class, this::onPublish)\n      .onCommand(GetPost.class, this::onGetPost);\n\n  builder\n      .forStateType(PublishedState.class)\n      .onCommand(ChangeBody.class, this::onChangeBody)\n      .onCommand(GetPost.class, this::onGetPost);\n\n  builder.forAnyState().onCommand(AddPost.class, (state, cmd) -> Effect().unhandled());\n\n  return builder.build();\n}\n\nprivate Effect<State> onAddPost(AddPost cmd) {\n  return Effect()\n      .persist(new DraftState(cmd.content))\n      .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));\n}\n\nprivate Effect<State> onChangeBody(DraftState state, ChangeBody cmd) {\n  return Effect()\n      .persist(state.withBody(cmd.newBody))\n      .thenRun(() -> cmd.replyTo.tell(Done.getInstance()));\n}\n\nprivate Effect<State> onChangeBody(PublishedState state, ChangeBody cmd) {\n  return Effect()\n      .persist(state.withBody(cmd.newBody))\n      .thenRun(() -> cmd.replyTo.tell(Done.getInstance()));\n}\n\nprivate Effect<State> onPublish(DraftState state, Publish cmd) {\n  return Effect()\n      .persist(new PublishedState(state.content))\n      .thenRun(\n          () -> {\n            System.out.println(\"Blog post published: \" + state.postId());\n            cmd.replyTo.tell(Done.getInstance());\n          });\n}\n\nprivate Effect<State> onGetPost(DraftState state, GetPost cmd) {\n  cmd.replyTo.tell(state.content);\n  return Effect().none();\n}\n\nprivate Effect<State> onGetPost(PublishedState state, GetPost cmd) {\n  cmd.replyTo.tell(state.content);\n  return Effect().none();\n}\nAnd finally the behavior is created from the DurableStateBehavior.apply:\nScala copysourceobject BlogPostEntityDurableState {\n  // commands, state defined here\n\n  def apply(entityId: String, persistenceId: PersistenceId): Behavior[Command] = {\n    Behaviors.setup { context =>\n      context.log.info(\"Starting BlogPostEntityDurableState {}\", entityId)\n      DurableStateBehavior[Command, State](persistenceId, emptyState = BlankState, commandHandler)\n    }\n  }\n\n  // commandHandler defined here\n} Java copysourcepublic class BlogPostEntityDurableState\n    extends DurableStateBehavior<\n        BlogPostEntityDurableState.Command, BlogPostEntityDurableState.State> {\n  // commands and state as in above snippets\n\n  public static Behavior<Command> create(String entityId, PersistenceId persistenceId) {\n    return Behaviors.setup(\n        context -> {\n          context.getLog().info(\"Starting BlogPostEntityDurableState {}\", entityId);\n          return new BlogPostEntityDurableState(persistenceId);\n        });\n  }\n\n  private BlogPostEntityDurableState(PersistenceId persistenceId) {\n    super(persistenceId);\n  }\n\n  @Override\n  public State emptyState() {\n    return BlankState.INSTANCE;\n  }\n\n  // commandHandler, eventHandler as in above snippets\n}\nThis can be refactored one or two steps further by defining the command handlers in the state class as illustrated in command handlers in the state.\nThere is also an example illustrating an optional initial state.","title":"Changing Behavior"},{"location":"/typed/durable-state/persistence.html#replies","text":"The Request-Response interaction pattern is very common for persistent actors, because you typically want to know if the command was rejected due to validation errors and when accepted you want a confirmation when the events have been successfully stored.\nTherefore you typically include a ActorRef[ReplyMessageType]ActorRef<ReplyMessageType>. If the command can either have a successful response or a validation error returned, the generic response type StatusReply[ReplyType]] StatusReply<ReplyType> can be used. If the successful reply does not contain a value but is more of an acknowledgement a pre defined StatusReply.AckStatusReply.ack() of type StatusReply[Done]StatusReply<Done> can be used.\nAfter validation errors or after persisting events, using a thenRun side effect, the reply message can be sent to the ActorRef.\nScala copysourcefinal case class AddPost(content: PostContent, replyTo: ActorRef[StatusReply[AddPostDone]]) extends Command\nfinal case class AddPostDone(postId: String) Java copysourcepublic static class AddPost implements Command {\n  final PostContent content;\n  final ActorRef<AddPostDone> replyTo;\n\n  public AddPost(PostContent content, ActorRef<AddPostDone> replyTo) {\n    this.content = content;\n    this.replyTo = replyTo;\n  }\n}\n\npublic static class AddPostDone implements Command {\n  final String postId;\n\n  public AddPostDone(String postId) {\n    this.postId = postId;\n  }\n}\nScala copysourceEffect.persist(DraftState(cmd.content)).thenRun { _ =>\n  // After persist is done additional side effects can be performed\n  cmd.replyTo ! StatusReply.Success(AddPostDone(cmd.content.postId))\n} Java copysourcereturn Effect()\n    .persist(new DraftState(cmd.content))\n    .thenRun(() -> cmd.replyTo.tell(new AddPostDone(cmd.content.postId)));\nSince this is such a common pattern there is a reply effect for this purpose. It has the nice property that it can be used to enforce that you do not forget to specify replies when implementing the DurableStateBehavior. If it’s defined with DurableStateBehavior.withEnforcedRepliesDurableStateBehaviorWithEnforcedReplies there will be compilation errors if the returned effect isn’t a ReplyEffect, which can be created with Effect.replyEffect().reply, Effect.noReplyEffect().noReply, Effect.thenReplyEffect().thenReply, or Effect.thenNoReplyEffect().thenNoReply.\nScala copysourcedef apply(persistenceId: PersistenceId): Behavior[Command] = {\n  DurableStateBehavior\n    .withEnforcedReplies[Command, Account](persistenceId, EmptyAccount, (state, cmd) => state.applyCommand(cmd))\n} Java copysourcepublic class AccountEntity\n    extends DurableStateBehaviorWithEnforcedReplies<\n        AccountEntity.Command, AccountEntity.Account> {\nThe commands must have a field of ActorRef[ReplyMessageType]ActorRef<ReplyMessageType> that can then be used to send a reply.\nScala copysourcesealed trait Command extends CborSerializable\nfinal case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command Java copysourceinterface Command extends CborSerializable {}\nThe ReplyEffect is created with Effect.replyEffect().reply, Effect.noReplyEffect().noReply, Effect.thenReplyEffect().thenReply, or Effect.thenNoReplyEffect().thenNoReply.\nNote that command handlers are defined with newCommandHandlerWithReplyBuilder when using EventSourcedBehaviorWithEnforcedReplies, as opposed to newCommandHandlerBuilder when using EventSourcedBehavior.\nScala copysourceprivate def deposit(cmd: Deposit) = {\n  Effect.persist(copy(balance = balance + cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n}\n\nprivate def withdraw(cmd: Withdraw) = {\n  if (canWithdraw(cmd.amount))\n    Effect.persist(copy(balance = balance - cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n  else\n    Effect.reply(cmd.replyTo)(\n      StatusReply.Error(s\"Insufficient balance ${balance} to be able to withdraw ${cmd.amount}\"))\n} Java copysourceprivate ReplyEffect<Account> withdraw(OpenedAccount account, Withdraw command) {\n  if (!account.canWithdraw(command.amount)) {\n    return Effect()\n        .reply(\n            command.replyTo,\n            StatusReply.error(\"not enough funds to withdraw \" + command.amount));\n  } else {\n    return Effect()\n        .persist(account.makeWithdraw(command.amount))\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n}\nThese effects will send the reply message even when DurableStateBehavior.withEnforcedRepliesDurableStateBehaviorWithEnforcedReplies is not used, but then there will be no compilation errors if the reply decision is left out.\nNote that the noReply is a way of making a conscious decision that a reply shouldn’t be sent for a specific command or that a reply will be sent later, perhaps after some asynchronous interaction with other actors or services.","title":"Replies"},{"location":"/typed/durable-state/persistence.html#serialization","text":"The same serialization mechanism as for actor messages is also used for persistent actors.\nYou need to enable serialization for your commands (messages) and state. Serialization with Jackson is a good choice in many cases and our recommendation if you don’t have other preference.","title":"Serialization"},{"location":"/typed/durable-state/persistence.html#tagging","text":"Persistence allows you to use tags in persistence query. Tagging allows you to identify a subset of states in the durable store and separately consume them as a stream through the DurableStateStoreQuery interface.\nScala copysourceDurableStateBehavior[Command[_], State](\n  persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n  emptyState = State(0),\n  commandHandler = (state, cmd) => throw new NotImplementedError(\"TODO: process the command & return an Effect\"))\n  .withTag(\"tag1\") Java copysourcepublic class MyPersistentBehavior\n    extends DurableStateBehavior<MyPersistentBehavior.Command, MyPersistentBehavior.State> {\n  @Override\n  public String tag() {\n    return \"tag1\";\n  }","title":"Tagging"},{"location":"/typed/durable-state/persistence.html#wrapping-durablestatebehavior","text":"When creating a DurableStateBehavior, it is possible to wrap DurableStateBehavior in other behaviors such as Behaviors.setup in order to access the ActorContext object. For instance to access the logger from within the ActorContext to log for debugging the commandHandler.\nScala copysourceBehaviors.setup[Command[_]] { context =>\n  DurableStateBehavior[Command[_], State](\n    persistenceId = PersistenceId.ofUniqueId(\"abc\"),\n    emptyState = State(0),\n    commandHandler = CommandHandler.command { cmd =>\n      context.log.info(\"Got command {}\", cmd)\n      Effect.none\n    })\n} Java copysourcepublic class MyPersistentBehavior\n    extends DurableStateBehavior<MyPersistentBehavior.Command, MyPersistentBehavior.State> {\n\n\n  public static Behavior<Command> create(PersistenceId persistenceId) {\n    return Behaviors.setup(context -> new MyPersistentBehavior(persistenceId, context));\n  }\n\n  private final ActorContext<Command> context;\n\n  private MyPersistentBehavior(PersistenceId persistenceId, ActorContext<Command> context) {\n    super(\n        persistenceId,\n        SupervisorStrategy.restartWithBackoff(\n            Duration.ofSeconds(10), Duration.ofSeconds(30), 0.2));\n    this.context = context;\n  }\n\n  @Override\n  public CommandHandler<Command, State> commandHandler() {\n    return (state, command) -> {\n      context.getLog().info(\"In command handler\");\n      return Effect().none();\n    };\n  }","title":"Wrapping DurableStateBehavior"},{"location":"/typed/durable-state/persistence-style.html","text":"","title":"Style Guide"},{"location":"/typed/durable-state/persistence-style.html#style-guide","text":"Command handlers in the state We can take the previous bank account example one step further by handling the commands within the state as well. Scala copysourceobject AccountEntity {\n  // Command\n  sealed trait Command extends CborSerializable\n  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command\n  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n\n  // Reply\n  final case class CurrentBalance(balance: BigDecimal)\n\n  val Zero = BigDecimal(0)\n\n  // type alias to reduce boilerplate\n  type ReplyEffect = pekko.persistence.typed.state.scaladsl.ReplyEffect[Account]\n\n  // State\n  sealed trait Account extends CborSerializable {\n    def applyCommand(cmd: Command): ReplyEffect\n  }\n  case object EmptyAccount extends Account {\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case CreateAccount(replyTo) =>\n          Effect.persist(OpenedAccount(Zero)).thenReply(replyTo)(_ => StatusReply.Ack)\n        case _ =>\n          // CreateAccount before handling any other commands\n          Effect.unhandled.thenNoReply()\n      }\n  }\n  case class OpenedAccount(balance: BigDecimal) extends Account {\n    require(balance >= Zero, \"Account balance can't be negative\")\n\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case cmd @ Deposit(_, _) => deposit(cmd)\n\n        case cmd @ Withdraw(_, _) => withdraw(cmd)\n\n        case GetBalance(replyTo) =>\n          Effect.reply(replyTo)(CurrentBalance(balance))\n\n        case CloseAccount(replyTo) =>\n          if (balance == Zero)\n            Effect.persist(ClosedAccount).thenReply(replyTo)(_ => StatusReply.Ack)\n          else\n            Effect.reply(replyTo)(StatusReply.Error(\"Can't close account with non-zero balance\"))\n\n        case CreateAccount(replyTo) =>\n          Effect.reply(replyTo)(StatusReply.Error(\"Account is already created\"))\n\n      }\n\n    private def canWithdraw(amount: BigDecimal): Boolean = {\n      balance - amount >= Zero\n    }\n\n    private def deposit(cmd: Deposit) = {\n      Effect.persist(copy(balance = balance + cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n    }\n\n    private def withdraw(cmd: Withdraw) = {\n      if (canWithdraw(cmd.amount))\n        Effect.persist(copy(balance = balance - cmd.amount)).thenReply(cmd.replyTo)(_ => StatusReply.Ack)\n      else\n        Effect.reply(cmd.replyTo)(\n          StatusReply.Error(s\"Insufficient balance ${balance} to be able to withdraw ${cmd.amount}\"))\n    }\n\n  }\n  case object ClosedAccount extends Account {\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case c: Deposit =>\n          replyClosed(c.replyTo)\n        case c: Withdraw =>\n          replyClosed(c.replyTo)\n        case GetBalance(replyTo) =>\n          Effect.reply(replyTo)(CurrentBalance(Zero))\n        case CloseAccount(replyTo) =>\n          replyClosed(replyTo)\n        case CreateAccount(replyTo) =>\n          replyClosed(replyTo)\n      }\n\n    private def replyClosed(replyTo: ActorRef[StatusReply[Done]]): ReplyEffect =\n      Effect.reply(replyTo)(StatusReply.Error(s\"Account is closed\"))\n  }\n\n  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:\n  val TypeKey: EntityTypeKey[Command] =\n    EntityTypeKey[Command](\"Account\")\n\n  def apply(persistenceId: PersistenceId): Behavior[Command] = {\n    DurableStateBehavior\n      .withEnforcedReplies[Command, Account](persistenceId, EmptyAccount, (state, cmd) => state.applyCommand(cmd))\n  }\n} Take note of how the command handler is delegating to applyCommand in the Account (state), which is implemented in the concrete EmptyAccount, OpenedAccount, and ClosedAccount.","title":"Style Guide"},{"location":"/typed/durable-state/persistence-style.html#optional-initial-state","text":"Sometimes, it’s not desirable to use a separate state class for the empty initial state, but rather act as if there is no state yet. You can use null as the emptyState, but be aware of that the state parameter will be null until the first non-null state has been persisted It’s possible to use Optional instead of null, but that requires extra boilerplate to unwrap the Optional state parameter. Therefore use of null is simpler. The following example illustrates using null as the emptyState. Option[State] can be used as the state type and None as the emptyState. Then pattern matching is used in command handlers at the outer layer before delegating to the state or other methods.\nScala copysourceobject AccountEntity {\n  // Command\n  sealed trait Command extends CborSerializable\n  final case class CreateAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Deposit(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class Withdraw(amount: BigDecimal, replyTo: ActorRef[StatusReply[Done]]) extends Command\n  final case class GetBalance(replyTo: ActorRef[CurrentBalance]) extends Command\n  final case class CloseAccount(replyTo: ActorRef[StatusReply[Done]]) extends Command\n\n  // Reply\n  final case class CurrentBalance(balance: BigDecimal) extends CborSerializable\n\n  val Zero = BigDecimal(0)\n\n  // type alias to reduce boilerplate\n  type ReplyEffect = pekko.persistence.typed.state.scaladsl.ReplyEffect[Option[Account]]\n\n  // State\n  sealed trait Account extends CborSerializable {\n    def applyCommand(cmd: Command): ReplyEffect\n  }\n  case class OpenedAccount(balance: BigDecimal) extends Account {\n    require(balance >= Zero, \"Account balance can't be negative\")\n\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case Deposit(amount, replyTo) =>\n          Effect.persist(Some(copy(balance = balance + amount))).thenReply(replyTo)(_ => StatusReply.Ack)\n\n        case Withdraw(amount, replyTo) =>\n          if (canWithdraw(amount))\n            Effect.persist(Some(copy(balance = balance - amount))).thenReply(replyTo)(_ => StatusReply.Ack)\n          else\n            Effect.reply(replyTo)(StatusReply.Error(s\"Insufficient balance $balance to be able to withdraw $amount\"))\n\n        case GetBalance(replyTo) =>\n          Effect.reply(replyTo)(CurrentBalance(balance))\n\n        case CloseAccount(replyTo) =>\n          if (balance == Zero)\n            Effect.persist(Some(ClosedAccount)).thenReply(replyTo)(_ => StatusReply.Ack)\n          else\n            Effect.reply(replyTo)(StatusReply.Error(\"Can't close account with non-zero balance\"))\n\n        case CreateAccount(replyTo) =>\n          Effect.reply(replyTo)(StatusReply.Error(\"Account is already created\"))\n\n      }\n\n    def canWithdraw(amount: BigDecimal): Boolean = {\n      balance - amount >= Zero\n    }\n  }\n\n  case object ClosedAccount extends Account {\n    override def applyCommand(cmd: Command): ReplyEffect =\n      cmd match {\n        case c: Deposit =>\n          replyClosed(c.replyTo)\n        case c: Withdraw =>\n          replyClosed(c.replyTo)\n        case GetBalance(replyTo) =>\n          Effect.reply(replyTo)(CurrentBalance(Zero))\n        case CloseAccount(replyTo) =>\n          replyClosed(replyTo)\n        case CreateAccount(replyTo) =>\n          replyClosed(replyTo)\n      }\n\n    private def replyClosed(replyTo: ActorRef[StatusReply[Done]]): ReplyEffect =\n      Effect.reply(replyTo)(StatusReply.Error(s\"Account is closed\"))\n  }\n\n  // when used with sharding, this TypeKey can be used in `sharding.init` and `sharding.entityRefFor`:\n  val TypeKey: EntityTypeKey[Command] =\n    EntityTypeKey[Command](\"Account\")\n\n  def apply(persistenceId: PersistenceId): Behavior[Command] = {\n    DurableStateBehavior.withEnforcedReplies[Command, Option[Account]](\n      persistenceId,\n      None,\n      (state, cmd) =>\n        state match {\n          case None          => onFirstCommand(cmd)\n          case Some(account) => account.applyCommand(cmd)\n        })\n  }\n\n  def onFirstCommand(cmd: Command): ReplyEffect = {\n    cmd match {\n      case CreateAccount(replyTo) =>\n        Effect.persist(Some(OpenedAccount(Zero))).thenReply(replyTo)(_ => StatusReply.Ack)\n      case _ =>\n        // CreateAccount before handling any other commands\n        Effect.unhandled.thenNoReply()\n    }\n  }\n} Java copysourcepublic class AccountEntity\n    extends DurableStateBehaviorWithEnforcedReplies<\n        AccountEntity.Command, AccountEntity.Account> {\n\n  public static final EntityTypeKey<Command> ENTITY_TYPE_KEY =\n      EntityTypeKey.create(Command.class, \"Account\");\n\n  // Command\n  interface Command extends CborSerializable {}\n\n  public static class CreateAccount implements Command {\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    @JsonCreator\n    public CreateAccount(ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class Deposit implements Command {\n    public final BigDecimal amount;\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    public Deposit(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n      this.amount = amount;\n    }\n  }\n\n  public static class Withdraw implements Command {\n    public final BigDecimal amount;\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    public Withdraw(BigDecimal amount, ActorRef<StatusReply<Done>> replyTo) {\n      this.amount = amount;\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class GetBalance implements Command {\n    public final ActorRef<CurrentBalance> replyTo;\n\n    @JsonCreator\n    public GetBalance(ActorRef<CurrentBalance> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  public static class CloseAccount implements Command {\n    public final ActorRef<StatusReply<Done>> replyTo;\n\n    @JsonCreator\n    public CloseAccount(ActorRef<StatusReply<Done>> replyTo) {\n      this.replyTo = replyTo;\n    }\n  }\n\n  // Reply\n  public static class CurrentBalance implements CborSerializable {\n    public final BigDecimal balance;\n\n    @JsonCreator\n    public CurrentBalance(BigDecimal balance) {\n      this.balance = balance;\n    }\n  }\n\n  // State\n  interface Account extends CborSerializable {}\n\n  public static class OpenedAccount implements Account {\n    public final BigDecimal balance;\n\n    public OpenedAccount() {\n      this.balance = BigDecimal.ZERO;\n    }\n\n    @JsonCreator\n    public OpenedAccount(BigDecimal balance) {\n      this.balance = balance;\n    }\n\n    OpenedAccount makeDeposit(BigDecimal amount) {\n      return new OpenedAccount(balance.add(amount));\n    }\n\n    boolean canWithdraw(BigDecimal amount) {\n      return (balance.subtract(amount).compareTo(BigDecimal.ZERO) >= 0);\n    }\n\n    OpenedAccount makeWithdraw(BigDecimal amount) {\n      if (!canWithdraw(amount))\n        throw new IllegalStateException(\"Account balance can't be negative\");\n      return new OpenedAccount(balance.subtract(amount));\n    }\n\n    ClosedAccount closedAccount() {\n      return new ClosedAccount();\n    }\n  }\n\n  public static class ClosedAccount implements Account {}\n\n  public static AccountEntity create(String accountNumber, PersistenceId persistenceId) {\n    return new AccountEntity(accountNumber, persistenceId);\n  }\n\n  private final String accountNumber;\n\n  private AccountEntity(String accountNumber, PersistenceId persistenceId) {\n    super(persistenceId);\n    this.accountNumber = accountNumber;\n  }\n\n  @Override\n  public Account emptyState() {\n    return null;\n  }\n\n  @Override\n  public CommandHandlerWithReply<Command, Account> commandHandler() {\n    CommandHandlerWithReplyBuilder<Command, Account> builder =\n        newCommandHandlerWithReplyBuilder();\n\n    builder.forNullState().onCommand(CreateAccount.class, this::createAccount);\n\n    builder\n        .forStateType(OpenedAccount.class)\n        .onCommand(Deposit.class, this::deposit)\n        .onCommand(Withdraw.class, this::withdraw)\n        .onCommand(GetBalance.class, this::getBalance)\n        .onCommand(CloseAccount.class, this::closeAccount);\n\n    builder\n        .forStateType(ClosedAccount.class)\n        .onAnyCommand(() -> Effect().unhandled().thenNoReply());\n\n    return builder.build();\n  }\n\n  private ReplyEffect<Account> createAccount(CreateAccount command) {\n    return Effect()\n        .persist(new OpenedAccount())\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n\n  private ReplyEffect<Account> deposit(OpenedAccount account, Deposit command) {\n    return Effect()\n        .persist(account.makeDeposit(command.amount))\n        .thenReply(command.replyTo, account2 -> StatusReply.ack());\n  }\n\n  private ReplyEffect<Account> withdraw(OpenedAccount account, Withdraw command) {\n    if (!account.canWithdraw(command.amount)) {\n      return Effect()\n          .reply(\n              command.replyTo,\n              StatusReply.error(\"not enough funds to withdraw \" + command.amount));\n    } else {\n      return Effect()\n          .persist(account.makeWithdraw(command.amount))\n          .thenReply(command.replyTo, account2 -> StatusReply.ack());\n    }\n  }\n\n  private ReplyEffect<Account> getBalance(OpenedAccount account, GetBalance command) {\n    return Effect().reply(command.replyTo, new CurrentBalance(account.balance));\n  }\n\n  private ReplyEffect<Account> closeAccount(OpenedAccount account, CloseAccount command) {\n    if (account.balance.equals(BigDecimal.ZERO)) {\n      return Effect()\n          .persist(account.closedAccount())\n          .thenReply(command.replyTo, account2 -> StatusReply.ack());\n    } else {\n      return Effect()\n          .reply(command.replyTo, StatusReply.error(\"balance must be zero for closing account\"));\n    }\n  }\n}","title":"Optional initial state"},{"location":"/typed/durable-state/cqrs.html","text":"","title":"CQRS"},{"location":"/typed/durable-state/cqrs.html#cqrs","text":"DurableStateBehaviors along with Pekko Projections can be used to implement Command Query Responsibility Segregation (CQRS). For implementing CQRS using EventSourcedBehavior, please take a look at the corresponding CQRS documentation.","title":"CQRS"},{"location":"/durable-state/persistence-query.html","text":"","title":"Persistence Query"},{"location":"/durable-state/persistence-query.html#persistence-query","text":"","title":"Persistence Query"},{"location":"/durable-state/persistence-query.html#dependency","text":"To use Apache Persistence Query, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence-query\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-query_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence-query_${versions.ScalaBinary}\"\n}\nThis will also add dependency on the Pekko Persistence module.","title":"Dependency"},{"location":"/durable-state/persistence-query.html#introduction","text":"Pekko persistence query provides a query interface to Durable State Behaviors. These queries are based on asynchronous streams. These streams are similar to the ones offered in the Event Sourcing based implementation. Various state store plugins can implement these interfaces to expose their query capabilities.\nOne of the rationales behind having a separate query module for Pekko Persistence is for implementing the so-called query side or read side in the popular CQRS architecture pattern - in which the writing side of the application implemented using Pekko persistence, is completely separated from the query side.","title":"Introduction"},{"location":"/durable-state/persistence-query.html#using-query-with-pekko-projections","text":"Pekko Persistence and Pekko Projections together can be used to develop a CQRS application. In the application the durable state is stored in a database and fetched as an asynchronous stream to the user. Currently queries on durable state, provided by the DurableStateStoreQuery interface, is used to implement tag based searches in Pekko Projections.\nAt present the query is based on tags. So if you have not tagged your objects, this query cannot be used.\nThe example below shows how to get the DurableStateStoreQuery from the DurableStateStoreRegistry extension.\nScala copysourceimport org.apache.pekko\nimport pekko.persistence.state.DurableStateStoreRegistry\nimport pekko.persistence.query.scaladsl.DurableStateStoreQuery\nimport pekko.persistence.query.DurableStateChange\nimport pekko.persistence.query.UpdatedDurableState\n\nval durableStateStoreQuery =\n  DurableStateStoreRegistry(system).durableStateStoreFor[DurableStateStoreQuery[Record]](pluginId)\nval source: Source[DurableStateChange[Record], NotUsed] = durableStateStoreQuery.changes(\"tag\", offset)\nsource.map {\n  case UpdatedDurableState(persistenceId, revision, value, offset, timestamp) => Some(value)\n  case _: DeletedDurableState[_]                                              => None\n} Java copysourceimport org.apache.pekko.persistence.state.DurableStateStoreRegistry;\nimport org.apache.pekko.persistence.query.javadsl.DurableStateStoreQuery;\nimport org.apache.pekko.persistence.query.DurableStateChange;\nimport org.apache.pekko.persistence.query.UpdatedDurableState;\n\nDurableStateStoreQuery<Record> durableStateStoreQuery =\n    DurableStateStoreRegistry.get(system)\n        .getDurableStateStoreFor(DurableStateStoreQuery.class, pluginId);\nSource<DurableStateChange<Record>, NotUsed> source =\n    durableStateStoreQuery.changes(\"tag\", offset);\nsource.map(\n    chg -> {\n      if (chg instanceof UpdatedDurableState) {\n        UpdatedDurableState<Record> upd = (UpdatedDurableState<Record>) chg;\n        return upd.value();\n      } else {\n        throw new IllegalArgumentException(\"Unexpected DurableStateChange \" + chg.getClass());\n      }\n    });\nThe DurableStateChangeDurableStateChange elements can be UpdatedDurableState or DeletedDurableState.","title":"Using query with Pekko Projections"},{"location":"/stream/index.html","text":"","title":"Streams"},{"location":"/stream/index.html#streams","text":"","title":"Streams"},{"location":"/stream/index.html#module-info","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-stream-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-stream-testkit_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Streams Artifact org.apache.pekko pekko-stream 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.stream License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko\nIntroduction Motivation How to read these docs Streams Quickstart Guide Dependency First steps Reusable Pieces Time-Based Processing Reactive Tweets Design Principles behind Apache Pekko Streams What shall users of Pekko Streams expect? Interoperation with other Reactive Streams implementations What shall users of streaming libraries expect? The difference between Error and Failure Basics and working with Flows Dependency Introduction Core concepts Defining and running streams Back-pressure explained Stream Materialization Stream ordering Actor Materializer Lifecycle Working with Graphs Dependency Introduction Constructing Graphs Constructing and combining Partial Graphs Constructing Sources, Sinks and Flows from Partial Graphs Combining Sources and Sinks with simplified API Building reusable Graph components Predefined shapes Bidirectional Flows Accessing the materialized value inside the Graph Graph cycles, liveness and deadlocks Modularity, Composition and Hierarchy Dependency Introduction Basics of composition and modularity Composing complex systems Materialized values Attributes Buffers and working with rate Dependency Introduction Buffers for asynchronous operators Buffers in Pekko Streams Rate transformation Context Propagation Restrictions Creation Composition Dynamic stream handling Dependency Introduction Controlling stream completion with KillSwitch Dynamic fan-in and fan-out with MergeHub, BroadcastHub and PartitionHub Custom stream processing Dependency Introduction Custom processing with GraphStage Thread safety of custom operators Resources and the operator lifecycle Extending Flow Operators with Custom Operators Futures interop Dependency Overview Actors interop Dependency Overview Reactive Streams Interop Dependency Overview Other implementations Error Handling in Streams Dependency Introduction Logging errors Recover Recover with retries Delayed restarts with a backoff operator Supervision Strategies Working with streaming IO Dependency Introduction Streaming TCP Streaming File IO StreamRefs - Reactive Streams over the network Dependency Introduction Stream References Bulk Stream References Serialization of SourceRef and SinkRef Configuration Pipelining and Parallelism Dependency Introduction Pipelining Parallel processing Combining pipelining and parallel processing Testing streams Dependency Introduction Built-in sources, sinks and operators TestKit Streams TestKit Fuzzing Mode Substreams Dependency Introduction Nesting operators Flattening operators Streams Cookbook Dependency Introduction Working with Flows Working with Operators Working with rate Working with IO Configuration Operators Source operators Sink operators Additional Sink and Source converters File IO Sinks and Sources Simple operators Flow operators composed of Sinks and Sources Asynchronous operators Timer driven operators Backpressure aware operators Nesting and flattening operators Time aware operators Fan-in operators Fan-out operators Watching status operators Actor interop operators Compression operators Error handling Source.actorRef Sink.actorRef ActorSource.actorRef ActorSink.actorRef Source.actorRefWithBackpressure Sink.actorRefWithBackpressure ActorSource.actorRefWithBackpressure ActorSink.actorRefWithBackpressure aggregateWithBoundary alsoTo alsoToAll Flow.asFlowWithContext StreamConverters.asInputStream StreamConverters.asJavaStream ask ActorFlow.ask ActorFlow.askWithContext ActorFlow.askWithStatus ActorFlow.askWithContext StreamConverters.asOutputStream Sink.asPublisher Source.asSourceWithContext Source.asSubscriber backpressureTimeout Balance batch batchWeighted Broadcast buffer Sink.cancelled collect Sink.collect Sink.collection collectType Source.combine Sink.combine Source.completionStage Flow.completionStageFlow Sink.completionStageSink Source.completionStageSource completionTimeout concat concatAllLazy concatLazy conflate conflateWithSeed Source.cycle Compression.deflate delay delayWith detach divertTo drop dropWhile dropWithin Source.empty expand extrapolate Source.failed filter filterNot flatMapConcat flatMapMerge flatMapPrefix Flow.flattenOptional fold Sink.fold foldAsync Sink.foreach Sink.foreachAsync Sink.foreachParallel Source.applySource.from Source.fromCompletionStage FileIO.fromFile Source.fromFuture Source.fromFutureSource StreamConverters.fromInputStream Source.fromIterator fromJavaStream StreamConverters.fromJavaStream fromMaterializer Sink.fromMaterializer StreamConverters.fromOutputStream FileIO.fromPath Source.fromPublisher Flow.fromSinkAndSource Flow.fromSinkAndSourceCoupled Source.fromSourceCompletionStage Sink.fromSubscriber Source.future Flow.futureFlow Sink.futureSink Source.futureSource groupBy grouped groupedWeighted groupedWeightedWithin groupedWithin Compression.gunzip Compression.gzip Sink.head Sink.headOption idleTimeout Sink.ignore Compression.inflate initialDelay initialTimeout interleave interleaveAll intersperse StreamConverters.javaCollector StreamConverters.javaCollectorParallelUnordered keepAlive Sink.last Sink.lastOption Source.lazily Source.lazilyAsync Source.lazyCompletionStage Flow.lazyCompletionStageFlow Sink.lazyCompletionStageSink Source.lazyCompletionStageSource Flow.lazyFlow Source.lazyFuture Flow.lazyFutureFlow Sink.lazyFutureSink Source.lazyFutureSource Flow.lazyInitAsync Sink.lazyInitAsync Source.lazySingle Sink.lazySink Source.lazySource limit limitWeighted log logWithMarker map mapAsync mapAsyncUnordered mapConcat mapError Source.maybe merge mergeAll mergeLatest mergePreferred mergePrioritized mergePrioritizedN MergeSequence mergeSorted monitor never Sink.never Sink.onComplete RestartSource.onFailuresWithBackoff RestartFlow.onFailuresWithBackoff orElse Partition prefixAndTail preMaterialize Sink.preMaterialize prepend prependLazy Source.queue Sink.queue Source.range recover recoverWith recoverWithRetries reduce Sink.reduce Source.repeat scan scanAsync Sink.seq setup Sink.setup Source.single PubSub.sink sliding PubSub.source splitAfter splitWhen statefulMap statefulMapConcat take Sink.takeLast takeWhile takeWithin throttle Source.tick FileIO.toFile FileIO.toPath Source.unfold Source.unfoldAsync Source.unfoldResource Source.unfoldResourceAsync Unzip UnzipWith watch watchTermination wireTap RestartSource.withBackoff RestartFlow.withBackoff RestartSink.withBackoff RetryFlow.withBackoff RetryFlow.withBackoffAndContext zip zipAll zipLatest zipLatestWith Source.zipN zipWith zipWithIndex Source.zipWithN","title":"Module info"},{"location":"/stream/stream-introduction.html","text":"","title":"Introduction"},{"location":"/stream/stream-introduction.html#introduction","text":"","title":"Introduction"},{"location":"/stream/stream-introduction.html#motivation","text":"The way we consume services from the Internet today includes many instances of streaming data, both downloading from a service as well as uploading to it or peer-to-peer data transfers. Regarding data as a stream of elements instead of in its entirety is very useful because it matches the way computers send and receive them (for example via TCP), but it is often also a necessity because data sets frequently become too large to be handled as a whole. We spread computations or analyses over large clusters and call it “big data”, where the whole principle of processing them is by feeding those data sequentially—as a stream—through some CPUs.\nActors can be seen as dealing with streams as well: they send and receive series of messages in order to transfer knowledge (or data) from one place to another. We have found it tedious and error-prone to implement all the proper measures in order to achieve stable streaming between actors, since in addition to sending and receiving we also need to take care to not overflow any buffers or mailboxes in the process. Another pitfall is that Actor messages can be lost and must be retransmitted in that case. Failure to do so would lead to holes at the receiving side.\nFor these reasons we decided to bundle up a solution to these problems as an Pekko Streams API. The purpose is to offer an intuitive and safe way to formulate stream processing setups such that we can then execute them efficiently and with bounded resource usage—no more OutOfMemoryErrors. In order to achieve this our streams need to be able to limit the buffering that they employ, they need to be able to slow down producers if the consumers cannot keep up. This feature is called back-pressure and is at the core of the Reactive Streams initiative of which Pekko is a founding member. For you this means that the hard problem of propagating and reacting to back-pressure has been incorporated in the design of Pekko Streams already, so you have one less thing to worry about; it also means that Pekko Streams interoperate seamlessly with all other Reactive Streams implementations (where Reactive Streams interfaces define the interoperability SPI while implementations like Pekko Streams offer a nice user API).","title":"Motivation"},{"location":"/stream/stream-introduction.html#relationship-with-reactive-streams","text":"The Pekko Streams API is completely decoupled from the Reactive Streams interfaces. While Pekko Streams focus on the formulation of transformations on data streams the scope of Reactive Streams is to define a common mechanism of how to move data across an asynchronous boundary without losses, buffering or resource exhaustion.\nThe relationship between these two is that the Pekko Streams API is geared towards end-users while the Pekko Streams implementation uses the Reactive Streams interfaces internally to pass data between the different operators. For this reason you will not find any resemblance between the Reactive Streams interfaces and the Pekko Streams API. This is in line with the expectations of the Reactive Streams project, whose primary purpose is to define interfaces such that different streaming implementation can interoperate; it is not the purpose of Reactive Streams to describe an end-user API.","title":"Relationship with Reactive Streams"},{"location":"/stream/stream-introduction.html#how-to-read-these-docs","text":"Stream processing is a different paradigm to the Actor Model or to Future composition, therefore it may take some careful study of this subject until you feel familiar with the tools and techniques. The documentation is here to help and for best results we recommend the following approach:\nRead the Quick Start Guide to get a feel for how streams look like and what they can do. The top-down learners may want to peruse the Design Principles behind Pekko Streams at this point. The bottom-up learners may feel more at home rummaging through the Streams Cookbook. For a complete overview of the built-in processing operators you can look at the operator index The other sections can be read sequentially or as needed during the previous steps, each digging deeper into specific topics.","title":"How to read these docs"},{"location":"/stream/stream-quickstart.html","text":"","title":"Streams Quickstart Guide"},{"location":"/stream/stream-quickstart.html#streams-quickstart-guide","text":"","title":"Streams Quickstart Guide"},{"location":"/stream/stream-quickstart.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}\nNote Both the Java and Scala DSLs of Pekko Streams are bundled in the same JAR. For a smooth development experience, when using an IDE such as Eclipse or IntelliJ, you can disable the auto-importer from suggesting javadsl imports when working in Scala, or viceversa. See IDE Tips.","title":"Dependency"},{"location":"/stream/stream-quickstart.html#first-steps","text":"A stream usually begins at a source, so this is also how we start a Pekko Stream. Before we create one, we import the full complement of streaming tools:\nScala copysourceimport org.apache.pekko\nimport pekko.stream._\nimport pekko.stream.scaladsl._ Java copysourceimport org.apache.pekko.stream.*;\nimport org.apache.pekko.stream.javadsl.*;\nIf you want to execute the code samples while you read through the quick start guide, you will also need the following imports:\nScala copysourceimport pekko.{ Done, NotUsed }\nimport pekko.actor.ActorSystem\nimport pekko.util.ByteString\nimport scala.concurrent._\nimport scala.concurrent.duration._\nimport java.nio.file.Paths Java copysourceimport org.apache.pekko.Done;\nimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.util.ByteString;\n\nimport java.nio.file.Paths;\nimport java.math.BigInteger;\nimport java.time.Duration;\nimport java.util.concurrent.CompletionStage;\nimport java.util.concurrent.ExecutionException;\nAnd an objecta class to start a Pekko ActorSystemActorSystem and hold your code . Making the ActorSystem implicit makes it available to the streams without manually passing it when running them:\nScala copysourceobject Main extends App {\n  implicit val system: ActorSystem = ActorSystem(\"QuickStart\")\n  // Code here\n} Java copysourcepublic class Main {\n  public static void main(String[] argv) {\n    final ActorSystem system = ActorSystem.create(\"QuickStart\");\n    // Code here\n  }\n}\nNow we will start with a rather simple source, emitting the integers 1 to 100:\nScala copysourceval source: Source[Int, NotUsed] = Source(1 to 100) Java copysourcefinal Source<Integer, NotUsed> source = Source.range(1, 100);\nThe SourceSource type is parameterized with two types: the first one is the type of element that this source emits and the second one, the “materialized value”, allows running the source to produce some auxiliary value (e.g. a network source may provide information about the bound port or the peer’s address). Where no auxiliary information is produced, the type NotUsedNotUsed is used. A simple range of integers falls into this category - running our stream produces a NotUsed.\nHaving created this source means that we have a description of how to emit the first 100 natural numbers, but this source is not yet active. In order to get those numbers out we have to run it:\nScala copysourcesource.runForeach(i => println(i)) Java copysourcesource.runForeach(i -> System.out.println(i), system);\nThis line will complement the source with a consumer function—in this example we print out the numbers to the console—and pass this little stream setup to an Actor that runs it. This activation is signaled by having “run” be part of the method name; there are other methods that run Pekko Streams, and they all follow this pattern.\nWhen running this source in a scala.Appprogram you might notice it does not terminate, because the ActorSystemActorSystem is never terminated. Luckily runForeachrunForeach returns a Future[DoneDone]CompletionStage<DoneDone> which resolves when the stream finishes:\nScala copysourceval done: Future[Done] = source.runForeach(i => println(i))\n\nimplicit val ec = system.dispatcher\ndone.onComplete(_ => system.terminate()) Java copysourcefinal CompletionStage<Done> done = source.runForeach(i -> System.out.println(i), system);\n\ndone.thenRun(() -> system.terminate());\nThe nice thing about Pekko Streams is that the Source is a description of what you want to run, and like an architect’s blueprint it can be reused, incorporated into a larger design. We may choose to transform the source of integers and write it to a file instead:\nScala copysourceval factorials = source.scan(BigInt(1))((acc, next) => acc * next)\n\nval result: Future[IOResult] =\n  factorials.map(num => ByteString(s\"$num\\n\")).runWith(FileIO.toPath(Paths.get(\"factorials.txt\"))) Java copysourcefinal Source<BigInteger, NotUsed> factorials =\n    source.scan(BigInteger.ONE, (acc, next) -> acc.multiply(BigInteger.valueOf(next)));\n\nfinal CompletionStage<IOResult> result =\n    factorials\n        .map(num -> ByteString.fromString(num.toString() + \"\\n\"))\n        .runWith(FileIO.toPath(Paths.get(\"factorials.txt\")), system);\nFirst we use the scanscan operator to run a computation over the whole stream: starting with the number 1 (BigInt(1)BigInteger.ONE) we multiply by each of the incoming numbers, one after the other; the scan operation emits the initial value and then every calculation result. This yields the series of factorial numbers which we stash away as a Source for later reuse—it is important to keep in mind that nothing is actually computed yet, this is a description of what we want to have computed once we run the stream. Then we convert the resulting series of numbers into a stream of ByteStringByteString objects describing lines in a text file. This stream is then run by attaching a file as the receiver of the data. In the terminology of Pekko Streams this is called a SinkSink. IOResultIOResult is a type that IO operations return in Pekko Streams in order to tell you how many bytes or elements were consumed and whether the stream terminated normally or exceptionally.","title":"First steps"},{"location":"/stream/stream-quickstart.html#browser-embedded-example","text":"Here is another example that you can edit and run in the browser:\nimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.actor.ActorSystem\nimport pekko.stream.scaladsl._\n\nfinal case class Author(handle: String)\n\nfinal case class Hashtag(name: String)\n\nfinal case class Tweet(author: Author, timestamp: Long, body: String) {\n  def hashtags: Set[Hashtag] =\n    body\n      .split(\" \")\n      .collect {\n        case t if t.startsWith(\"#\") => Hashtag(t.replaceAll(\"[^#\\\\w]\", \"\"))\n      }\n      .toSet\n}\n\nval pekkoTag = Hashtag(\"#pekko\")\n\nval tweets: Source[Tweet, NotUsed] = Source(\n  Tweet(Author(\"rolandkuhn\"), System.currentTimeMillis, \"#pekko rocks!\") ::\n  Tweet(Author(\"patriknw\"), System.currentTimeMillis, \"#pekko !\") ::\n  Tweet(Author(\"bantonsson\"), System.currentTimeMillis, \"#pekko !\") ::\n  Tweet(Author(\"drewhk\"), System.currentTimeMillis, \"#pekko !\") ::\n  Tweet(Author(\"ktosopl\"), System.currentTimeMillis, \"#pekko on the rocks!\") ::\n  Tweet(Author(\"mmartynas\"), System.currentTimeMillis, \"wow #pekko !\") ::\n  Tweet(Author(\"pekkoteam\"), System.currentTimeMillis, \"#pekko rocks!\") ::\n  Tweet(Author(\"bananaman\"), System.currentTimeMillis, \"#bananas rock!\") ::\n  Tweet(Author(\"appleman\"), System.currentTimeMillis, \"#apples rock!\") ::\n  Tweet(Author(\"drama\"), System.currentTimeMillis, \"we compared #apples to #oranges!\") ::\n  Nil)\n\n  implicit val system: ActorSystem = ActorSystem(\"reactive-tweets\")\n\n  tweets\n    .filterNot(_.hashtags.contains(pekkoTag)) // Remove all tweets containing #pekko hashtag\n    .map(_.hashtags) // Get all sets of hashtags ...\n    .reduce(_ ++ _) // ... and reduce them to a single set, removing duplicates across all tweets\n    .mapConcat(identity) // Flatten the set of hashtags to a stream of hashtags\n    .map(_.name.toUpperCase) // Convert all hashtags to upper case\n    .runWith(Sink.foreach(println)) // Attach the Flow to a Sink that will finally print the hashtags","title":"Browser-embedded example"},{"location":"/stream/stream-quickstart.html#reusable-pieces","text":"One of the nice parts of Pekko Streams—and something that other stream libraries do not offer—is that not only sources can be reused like blueprints, all other elements can be as well. We can take the file-writing SinkSink, prepend the processing steps necessary to get the ByteStringByteString elements from incoming strings and package that up as a reusable piece as well. Since the language for writing these streams always flows from left to right (just like plain English), we need a starting point that is like a source but with an “open” input. In Pekko Streams, this is called a FlowFlow:\nScala copysourcedef lineSink(filename: String): Sink[String, Future[IOResult]] =\n  Flow[String].map(s => ByteString(s + \"\\n\")).toMat(FileIO.toPath(Paths.get(filename)))(Keep.right) Java copysourcepublic Sink<String, CompletionStage<IOResult>> lineSink(String filename) {\n  return Flow.of(String.class)\n      .map(s -> ByteString.fromString(s.toString() + \"\\n\"))\n      .toMat(FileIO.toPath(Paths.get(filename)), Keep.right());\n}\nStarting from a flow of strings we convert each to ByteString and then feed to the already known file-writing Sink. The resulting blueprint is a Sink[String, Future[IOResult]]Sink<String, CompletionStage<IOResult>>, which means that it accepts strings as its input and when materialized it will create auxiliary information of type Future[IOResultIOResult]CompletionStage<IOResultIOResult> (when chaining operations on a Source or Flow the type of the auxiliary information—called the “materialized value”—is given by the leftmost starting point; since we want to retain what the FileIO.toPathFileIO.toPath sink has to offer, we need to say Keep.rightKeep.right().\nWe can use the new and shiny SinkSink we just created by attaching it to our factorials source—after a small adaptation to turn the numbers into strings:\nScala copysourcefactorials.map(_.toString).runWith(lineSink(\"factorial2.txt\")) Java copysourcefactorials.map(BigInteger::toString).runWith(lineSink(\"factorial2.txt\"), system);","title":"Reusable Pieces"},{"location":"/stream/stream-quickstart.html#time-based-processing","text":"Before we start looking at a more involved example we explore the streaming nature of what Pekko Streams can do. Starting from the factorials source we transform the stream by zipping it together with another stream, represented by a SourceSource that emits the number 0 to 100: the first number emitted by the factorials source is the factorial of zero, the second is the factorial of one, and so on. We combine these two by forming strings like \"3! = 6\".\nScala copysourcefactorials\n  .zipWith(Source(0 to 100))((num, idx) => s\"$idx! = $num\")\n  .throttle(1, 1.second)\n  .runForeach(println) Java copysourcefactorials\n    .zipWith(Source.range(0, 99), (num, idx) -> String.format(\"%d! = %s\", idx, num))\n    .throttle(1, Duration.ofSeconds(1))\n    .runForeach(s -> System.out.println(s), system);\nAll operations so far have been time-independent and could have been performed in the same fashion on strict collections of elements. The next line demonstrates that we are in fact dealing with streams that can flow at a certain speed: we use the throttlethrottle operator to slow down the stream to 1 element per second.\nIf you run this program you will see one line printed per second. One aspect that is not immediately visible deserves mention, though: if you try and set the streams to produce a billion numbers each then you will notice that your JVM does not crash with an OutOfMemoryError, even though you will also notice that running the streams happens in the background, asynchronously (this is the reason for the auxiliary information to be provided as a FutureCompletionStage, in the future). The secret that makes this work is that Pekko Streams implicitly implement pervasive flow control, all operators respect back-pressure. This allows the throttle operator to signal to all its upstream sources of data that it can only accept elements at a certain rate—when the incoming rate is higher than one per second the throttle operator will assert back-pressure upstream.\nThis is all there is to Pekko Streams in a nutshell—glossing over the fact that there are dozens of sources and sinks and many more stream transformation operators to choose from, see also operator index.","title":"Time-Based Processing"},{"location":"/stream/stream-quickstart.html#reactive-tweets","text":"A typical use case for stream processing is consuming a live stream of data that we want to extract or aggregate some other data from. In this example we’ll consider consuming a stream of tweets and extracting information concerning Pekko from them.\nWe will also consider the problem inherent to all non-blocking streaming solutions: “What if the subscriber is too slow to consume the live stream of data?”. Traditionally the solution is often to buffer the elements, but this can—and usually will—cause eventual buffer overflows and instability of such systems. Instead Pekko Streams depend on internal backpressure signals that allow to control what should happen in such scenarios.\nHere’s the data model we’ll be working with throughout the quickstart examples:\nScala copysourcefinal case class Author(handle: String)\n\nfinal case class Hashtag(name: String)\n\nfinal case class Tweet(author: Author, timestamp: Long, body: String) {\n  def hashtags: Set[Hashtag] =\n    body\n      .split(\" \")\n      .collect {\n        case t if t.startsWith(\"#\") => Hashtag(t.replaceAll(\"[^#\\\\w]\", \"\"))\n      }\n      .toSet\n}\n\nval pekkoTag = Hashtag(\"#pekko\") Java copysourcepublic static class Author {\n  public final String handle;\n\n  public Author(String handle) {\n    this.handle = handle;\n  }\n\n  // ...\n\n}\n\npublic static class Hashtag {\n  public final String name;\n\n  public Hashtag(String name) {\n    this.name = name;\n  }\n\n  // ...\n}\n\npublic static class Tweet {\n  public final Author author;\n  public final long timestamp;\n  public final String body;\n\n  public Tweet(Author author, long timestamp, String body) {\n    this.author = author;\n    this.timestamp = timestamp;\n    this.body = body;\n  }\n\n  public Set<Hashtag> hashtags() {\n    return Arrays.asList(body.split(\" \")).stream()\n        .filter(a -> a.startsWith(\"#\"))\n        .map(a -> new Hashtag(a))\n        .collect(Collectors.toSet());\n  }\n\n  // ...\n}\n\npublic static final Hashtag PEKKO = new Hashtag(\"#pekko\");\nNote If you would like to get an overview of the used vocabulary first instead of diving head-first into an actual example you can have a look at the Core concepts and Defining and running streams sections of the docs, and then come back to this quickstart to see it all pieced together into a simple example application.","title":"Reactive Tweets"},{"location":"/stream/stream-quickstart.html#transforming-and-consuming-simple-streams","text":"The example application we will be looking at is a simple Twitter feed stream from which we’ll want to extract certain information, like for example finding all twitter handles of users who tweet about #pekko.\nIn order to prepare our environment by creating an ActorSystemActorSystem which will be responsible for running the streams we are about to create:\nScala copysourceimplicit val system: ActorSystem = ActorSystem(\"reactive-tweets\") Java copysourcefinal ActorSystem system = ActorSystem.create(\"reactive-tweets\");\nLet’s assume we have a stream of tweets readily available. In Pekko, this is expressed as a Source[Out, M]Source<Out, M>:\nScala copysourceval tweets: Source[Tweet, NotUsed] Java copysourceSource<Tweet, NotUsed> tweets;\nStreams always start flowing from a Source[Out,M1]Source<Out,M1> then can continue through Flow[In,Out,M2]Flow<In,Out,M2> elements or more advanced operators to finally be consumed by a Sink[In,M3]Sink<In,M3> (ignore the type parameters M1, M2 and M3 for now, they are not relevant to the types of the elements produced/consumed by these classes – they are “materialized types”, which we’ll talk about below). The first type parameter—Tweet in this case—designates the kind of elements produced by the source while the M type parameters describe the object that is created during materialization (see below)—NotUsed (from the scala.runtime package) means that no value is produced, it is the generic equivalent of void.\nThe operations should look familiar to anyone who has used the Scala Collections library, however they operate on streams and not collections of data (which is a very important distinction, as some operations only make sense in streaming and vice versa):\nScala copysourceval authors: Source[Author, NotUsed] =\n  tweets.filter(_.hashtags.contains(pekkoTag)).map(_.author) Java copysourcefinal Source<Author, NotUsed> authors =\n    tweets.filter(t -> t.hashtags().contains(PEKKO)).map(t -> t.author);\nFinally in order to materialize and run the stream computation we need to attach the Flow to a SinkSink<T, M> that will get the Flow running. The simplest way to do this is to call runWith(sink)runWith(sink) on a SourceSource<Out, M>. For convenience a number of common Sinks are predefined and collected as static methods on the Sink companion objectSink class. For now let’s print each author:\nScala copysourceauthors.runWith(Sink.foreach(println)) Java copysourceauthors.runWith(Sink.foreach(a -> System.out.println(a)), system);\nor by using the shorthand version (which are defined only for the most popular Sinks such as Sink.foldSink.fold and Sink.foreachSink.foreach):\nScala copysourceauthors.runForeach(println) Java copysourceauthors.runForeach(a -> System.out.println(a), system);\nMaterializing and running a stream always requires an ActorSystem to be in implicit scope (or passed in explicitly, like this: .runWith(sink)(system))passed in explicitly, like this: runWith(sink, system).\nThe complete snippet looks like this:\nScala copysourceimplicit val system: ActorSystem = ActorSystem(\"reactive-tweets\")\n\nval authors: Source[Author, NotUsed] =\n  tweets.filter(_.hashtags.contains(pekkoTag)).map(_.author)\n\nauthors.runWith(Sink.foreach(println)) Java copysourcefinal ActorSystem system = ActorSystem.create(\"reactive-tweets\");\n\nfinal Source<Author, NotUsed> authors =\n    tweets.filter(t -> t.hashtags().contains(PEKKO)).map(t -> t.author);\n\nauthors.runWith(Sink.foreach(a -> System.out.println(a)), system);","title":"Transforming and consuming simple streams"},{"location":"/stream/stream-quickstart.html#flattening-sequences-in-streams","text":"In the previous section we were working on 1:1 relationships of elements which is the most common case, but sometimes we might want to map from one element to a number of elements and receive a “flattened” stream, similarly like flatMap works on Scala Collections. In order to get a flattened stream of hashtags from our stream of tweets we can use the mapConcatmapConcat operator:\nScala copysourceval hashtags: Source[Hashtag, NotUsed] = tweets.mapConcat(_.hashtags.toList) Java copysourcefinal Source<Hashtag, NotUsed> hashtags =\n    tweets.mapConcat(t -> new ArrayList<Hashtag>(t.hashtags()));\nNote The name flatMap was consciously avoided due to its proximity with for-comprehensions and monadic composition. It is problematic for two reasons: firstfirstly, flattening by concatenation is often undesirable in bounded stream processing due to the risk of deadlock (with merge being the preferred strategy), and secondsecondly, the monad laws would not hold for our implementation of flatMap (due to the liveness issues). Please note that the mapConcatmapConcat requires the supplied function to return an iterable (f: Out => immutable.Iterable[T]a strict collection (Out f -> java.util.List<T>), whereas flatMap would have to operate on streams all the way through.","title":"Flattening sequences in streams"},{"location":"/stream/stream-quickstart.html#broadcasting-a-stream","text":"Now let’s say we want to persist all hashtags, as well as all author names from this one live stream. For example we’d like to write all author handles into one file, and all hashtags into another file on disk. This means we have to split the source stream into two streams which will handle the writing to these different files.\nElements that can be used to form such “fan-out” (or “fan-in”) structures are referred to as “junctions” in Pekko Streams. One of these that we’ll be using in this example is called BroadcastBroadcast, and it emits elements from its input port to all of its output ports.\nPekko Streams intentionally separate the linear stream structures (Flows) from the non-linear, branching ones (Graphs) in order to offer the most convenient API for both of these cases. Graphs can express arbitrarily complex stream setups at the expense of not reading as familiarly as collection transformations.\nGraphs are constructed using GraphDSLGraphDSL like this:\nScala copysourceval writeAuthors: Sink[Author, NotUsed] = ???\nval writeHashtags: Sink[Hashtag, NotUsed] = ???\nval g = RunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  val bcast = b.add(Broadcast[Tweet](2))\n  tweets ~> bcast.in\n  bcast.out(0) ~> Flow[Tweet].map(_.author) ~> writeAuthors\n  bcast.out(1) ~> Flow[Tweet].mapConcat(_.hashtags.toList) ~> writeHashtags\n  ClosedShape\n})\ng.run() Java copysourceSink<Author, NotUsed> writeAuthors;\nSink<Hashtag, NotUsed> writeHashtags;\nRunnableGraph.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final UniformFanOutShape<Tweet, Tweet> bcast = b.add(Broadcast.create(2));\n              final FlowShape<Tweet, Author> toAuthor =\n                  b.add(Flow.of(Tweet.class).map(t -> t.author));\n              final FlowShape<Tweet, Hashtag> toTags =\n                  b.add(\n                      Flow.of(Tweet.class)\n                          .mapConcat(t -> new ArrayList<Hashtag>(t.hashtags())));\n              final SinkShape<Author> authors = b.add(writeAuthors);\n              final SinkShape<Hashtag> hashtags = b.add(writeHashtags);\n\n              b.from(b.add(tweets)).viaFanOut(bcast).via(toAuthor).to(authors);\n              b.from(bcast).via(toTags).to(hashtags);\n              return ClosedShape.getInstance();\n            }))\n    .run(system);\nAs you can see, inside the GraphDSL we use an implicit graph builder b to mutably construct the graph using the ~> “edge operator” (also read as “connect” or “via” or “to”). The operator is provided implicitly by importing GraphDSL.Implicits._we use graph builder b to construct the graph using UniformFanOutShapeUniformFanOutShape and FlowFlow s.\nGraphDSL.createGraphDSL.create returns a GraphGraph, in this example a Graph[ClosedShape, NotUsed]Graph<ClosedShape,NotUsed> where ClosedShapeClosedShape means that it is a fully connected graph or “closed” - there are no unconnected inputs or outputs. Since it is closed it is possible to transform the graph into a RunnableGraphRunnableGraph using RunnableGraph.fromGraphRunnableGraph.fromGraph. The RunnableGraph can then be run()run() to materialize a stream out of it.\nBoth Graph and RunnableGraph are immutable, thread-safe, and freely shareable.\nA graph can also have one of several other shapes, with one or more unconnected ports. Having unconnected ports expresses a graph that is a partial graph. Concepts around composing and nesting graphs in large structures are explained in detail in Modularity, Composition and Hierarchy. It is also possible to wrap complex computation graphs as Flows, Sinks or Sources, which will be explained in detail in Constructing Sources, Sinks and Flows from Partial GraphsConstructing and combining Partial Graphs.","title":"Broadcasting a stream"},{"location":"/stream/stream-quickstart.html#back-pressure-in-action","text":"One of the main advantages of Pekko Streams is that they always propagate back-pressure information from stream Sinks (Subscribers) to their Sources (Publishers). It is not an optional feature, and is enabled at all times. To learn more about the back-pressure protocol used by Pekko Streams and all other Reactive Streams compatible implementations read Back-pressure explained.\nA typical problem applications (not using Pekko Streams) like this often face is that they are unable to process the incoming data fast enough, either temporarily or by design, and will start buffering incoming data until there’s no more space to buffer, resulting in either OutOfMemoryError s or other severe degradations of service responsiveness. With Pekko Streams buffering can and must be handled explicitly. For example, if we are only interested in the “most recent tweets, with a buffer of 10 elements” this can be expressed using the bufferbuffer element:\nScala copysourcetweets.buffer(10, OverflowStrategy.dropHead).map(slowComputation).runWith(Sink.ignore) Java copysourcetweets\n    .buffer(10, OverflowStrategy.dropHead())\n    .map(t -> slowComputation(t))\n    .runWith(Sink.ignore(), system);\nThe buffer element takes an explicit and required OverflowStrategyOverflowStrategy, which defines how the buffer should react when it receives another element while it is full. Strategies provided include dropping the oldest element (dropHead), dropping the entire buffer, signalling errorsfailures etc. Be sure to pick and choose the strategy that fits your use case best.","title":"Back-pressure in action"},{"location":"/stream/stream-quickstart.html#materialized-values","text":"So far we’ve been only processing data using Flows and consuming it into some kind of external Sink - be it by printing values or storing them in some external system. However sometimes we may be interested in some value that can be obtained from the materialized processing pipeline. For example, we want to know how many tweets we have processed. While this question is not as obvious to give an answer to in case of an infinite stream of tweets (one way to answer this question in a streaming setting would be to create a stream of counts described as “up until now, we’ve processed N tweets”), but in general it is possible to deal with finite streams and come up with a nice result such as a total count of elements.\nFirst, let’s write such an element counter using Sink.fold andFlow.of(Class) and Sink.fold to see how the types look like:\nScala copysourceval count: Flow[Tweet, Int, NotUsed] = Flow[Tweet].map(_ => 1)\n\nval sumSink: Sink[Int, Future[Int]] = Sink.fold[Int, Int](0)(_ + _)\n\nval counterGraph: RunnableGraph[Future[Int]] =\n  tweets.via(count).toMat(sumSink)(Keep.right)\n\nval sum: Future[Int] = counterGraph.run()\n\nsum.foreach(c => println(s\"Total tweets processed: $c\")) Java copysourcefinal Sink<Integer, CompletionStage<Integer>> sumSink =\n    Sink.<Integer, Integer>fold(0, (acc, elem) -> acc + elem);\n\nfinal RunnableGraph<CompletionStage<Integer>> counter =\n    tweets.map(t -> 1).toMat(sumSink, Keep.right());\n\nfinal CompletionStage<Integer> sum = counter.run(system);\n\nsum.thenAcceptAsync(\n    c -> System.out.println(\"Total tweets processed: \" + c), system.dispatcher());\nFirst we prepare a reusable Flow that will change each incoming tweet into an integer of value 1. We’ll use this in order to combine those with a Sink.fold that will sum all Int elements of the stream and make its result available as a Future[Int]. Next we connect the tweets stream to count with via. Finally we connect the Flow to the previously prepared Sink using toMatSink.fold will sum all Integer elements of the stream and make its result available as a CompletionStage<Integer>. Next we use the map method of tweets Source which will change each incoming tweet into an integer value 1. Finally we connect the Flow to the previously prepared Sink using toMat.\nRemember those mysterious Mat type parameters on Source[+Out, +Mat], Flow[-In, +Out, +Mat] and Sink[-In, +Mat]Source<Out, Mat>, Flow<In, Out, Mat> and Sink<In, Mat>? They represent the type of values these processing parts return when materialized. When you chain these together, you can explicitly combine their materialized values. In our example we used the Keep.rightKeep.right() predefined function, which tells the implementation to only care about the materialized type of the operator currently appended to the right. The materialized type of sumSink is Future[Int]CompletionStage and because of using Keep.rightKeep.right(), the resulting RunnableGraphRunnableGraph has also a type parameter of Future[Int]CompletionStage<Integer>.\nThis step does not yet materialize the processing pipeline, it merely prepares the description of the Flow, which is now connected to a Sink, and therefore can be run(), as indicated by its type: RunnableGraph[Future[Int]]RunnableGraph<CompletionStage<Integer>>. Next we call run()run() which materializes and runs the Flow. The value returned by calling run() on a RunnableGraph[T]RunnableGraph<T> is of type T. In our case this type is Future[Int]CompletionStage<Integer> which, when completed, will contain the total length of our tweets stream. In case of the stream failing, this future would complete with a Failure.\nA RunnableGraphRunnableGraph may be reused and materialized multiple times, because it is only the “blueprint” of the stream. This means that if we materialize a stream, for example one that consumes a live stream of tweets within a minute, the materialized values for those two materializations will be different, as illustrated by this example:\nScala copysourceval sumSink = Sink.fold[Int, Int](0)(_ + _)\nval counterRunnableGraph: RunnableGraph[Future[Int]] =\n  tweetsInMinuteFromNow.filter(_.hashtags contains pekkoTag).map(t => 1).toMat(sumSink)(Keep.right)\n\n// materialize the stream once in the morning\nval morningTweetsCount: Future[Int] = counterRunnableGraph.run()\n// and once in the evening, reusing the flow\nval eveningTweetsCount: Future[Int] = counterRunnableGraph.run()\n Java copysourcefinal Sink<Integer, CompletionStage<Integer>> sumSink =\n    Sink.<Integer, Integer>fold(0, (acc, elem) -> acc + elem);\nfinal RunnableGraph<CompletionStage<Integer>> counterRunnableGraph =\n    tweetsInMinuteFromNow\n        .filter(t -> t.hashtags().contains(PEKKO))\n        .map(t -> 1)\n        .toMat(sumSink, Keep.right());\n\n// materialize the stream once in the morning\nfinal CompletionStage<Integer> morningTweetsCount = counterRunnableGraph.run(system);\n// and once in the evening, reusing the blueprint\nfinal CompletionStage<Integer> eveningTweetsCount = counterRunnableGraph.run(system);\nMany elements in Pekko Streams provide materialized values which can be used for obtaining either results of computation or steering these elements which will be discussed in detail in Stream Materialization. Summing up this section, now we know what happens behind the scenes when we run this one-liner, which is equivalent to the multi line version above:\nScala copysourceval sum: Future[Int] = tweets.map(t => 1).runWith(sumSink) Java copysourcefinal CompletionStage<Integer> sum = tweets.map(t -> 1).runWith(sumSink, system);\nNote runWith() is a convenience method that automatically ignores the materialized value of any other operators except those appended by the runWith() itself. In the above example it translates to using Keep.rightKeep.right() as the combiner for materialized values.","title":"Materialized values"},{"location":"/general/stream/stream-design.html","text":"","title":"Design Principles behind Apache Pekko Streams"},{"location":"/general/stream/stream-design.html#design-principles-behind-apache-pekko-streams","text":"It took quite a while until we were reasonably happy with the look and feel of the API and the architecture of the implementation, and while being guided by intuition the design phase was very much exploratory research. This section details the findings and codifies them into a set of principles that have emerged during the process.\nNote As detailed in the introduction, keep in mind that the Pekko Streams API is completely decoupled from the Reactive Streams interfaces which are an implementation detail for how to pass stream data between individual operators.","title":"Design Principles behind Apache Pekko Streams"},{"location":"/general/stream/stream-design.html#what-shall-users-of-pekko-streams-expect-","text":"Pekko is built upon a conscious decision to offer APIs that are minimal and consistent—as opposed to easy or intuitive. The credo is that we favor explicitness over magic, and if we provide a feature then it must work always, no exceptions. Another way to say this is that we minimize the number of rules a user has to learn instead of trying to keep the rules close to what we think users might expect.\nFrom this follows that the principles implemented by Pekko Streams are:\nall features are explicit in the API, no magic supreme compositionality: combined pieces retain the function of each part exhaustive model of the domain of distributed bounded stream processing\nThis means that we provide all the tools necessary to express any stream processing topology, that we model all the essential aspects of this domain (back-pressure, buffering, transformations, failure recovery, etc.) and that whatever the user builds is reusable in a larger context.","title":"What shall users of Pekko Streams expect?"},{"location":"/general/stream/stream-design.html#pekko-streams-does-not-send-dropped-stream-elements-to-the-dead-letter-office","text":"One important consequence of offering only features that can be relied upon is the restriction that Pekko Streams cannot ensure that all objects sent through a processing topology will be processed. Elements can be dropped for a number of reasons:\nplain user code can consume one element in a map(…) operator and produce an entirely different one as its result common stream operators drop elements intentionally, e.g. take/drop/filter/conflate/buffer/… stream failure will tear down the stream without waiting for processing to finish, all elements that are in flight will be discarded stream cancellation will propagate upstream (e.g. from a take operator) leading to upstream processing steps being terminated without having processed all of their inputs\nThis means that sending JVM objects into a stream that need to be cleaned up will require the user to ensure that this happens outside of the Pekko Streams facilities (e.g. by cleaning them up after a timeout or when their results are observed on the stream output, or by using other means like finalizers etc.).","title":"Pekko Streams does not send dropped stream elements to the dead letter office"},{"location":"/general/stream/stream-design.html#resulting-implementation-considerations","text":"Compositionality entails reusability of partial stream topologies, which led us to the lifted approach of describing data flows as (partial) graphs that can act as composite sources, flows (a.k.a. pipes) and sinks of data. These building blocks shall then be freely shareable, with the ability to combine them freely to form larger graphs. The representation of these pieces must therefore be an immutable blueprint that is materialized in an explicit step in order to start the stream processing. The resulting stream processing engine is then also immutable in the sense of having a fixed topology that is prescribed by the blueprint. Dynamic networks need to be modeled by explicitly using the Reactive Streams interfaces for plugging different engines together.\nThe process of materialization will often create specific objects that are useful to interact with the processing engine once it is running, for example for shutting it down or for extracting metrics. This means that the materialization function produces a result termed the materialized value of a graph.","title":"Resulting Implementation Considerations"},{"location":"/general/stream/stream-design.html#interoperation-with-other-reactive-streams-implementations","text":"Pekko Streams fully implement the Reactive Streams specification and interoperate with all other conformant implementations. We chose to completely separate the Reactive Streams interfaces from the user-level API because we regard them to be an SPI that is not targeted at endusers. In order to obtain a Publisher or Subscriber from a Pekko Stream topology, a corresponding Sink.asPublisherSink.asPublisher or Source.asSubscriberSource.asSubscriber element must be used.\nAll stream Processors produced by the default materialization of Pekko Streams are restricted to having a single Subscriber, additional Subscribers will be rejected. The reason for this is that the stream topologies described using our DSL never require fan-out behavior from the Publisher sides of the elements, all fan-out is done using explicit elements like Broadcast<T>Broadcast[T].\nThis means that Sink.asPublisher(true)Sink.asPublisher(WITH_FANOUT) must be used where broadcast behavior is needed for interoperation with other Reactive Streams implementations.","title":"Interoperation with other Reactive Streams implementations"},{"location":"/general/stream/stream-design.html#rationale-and-benefits-from-sink-source-flow-not-directly-extending-reactive-streams-interfaces","text":"A sometimes overlooked crucial piece of information about Reactive Streams is that they are a Service Provider Interface, as explained in depth in one of the early discussions about the specification. Pekko Streams was designed during the development of Reactive Streams, so they both heavily influenced one another.\nIt may be enlightening to learn that even within the Reactive Specification the types had initially attempted to hide Publisher, Subscriber and the other SPI types from users of the API. Though since those internal SPI types would end up surfacing to end users of the standard in some cases, it was decided to remove the API types, and only keep the SPI types which are the Publisher, Subscriber et al.\nWith this historical knowledge and context about the purpose of the standard – being an internal detail of interoperable libraries - we can with certainty say that it can’t be really said that a direct inheritance relationship with these types can be considered some form of advantage or meaningful differentiator between libraries. Rather, it could be seen that APIs which expose those SPI types to end-users are leaking internal implementation details accidentally.\nThe SourceSource, SinkSink and FlowFlow types which are part of Pekko Streams have the purpose of providing the fluent DSL, as well as to be “factories” for running those streams. Their direct counterparts in Reactive Streams are, respectively, Publisher, Subscriber` and Processor. In other words, Pekko Streams operate on a lifted representation of the computing graph, which then is materialized and executed in accordance to Reactive Streams rules. This also allows Pekko Streams to perform optimizations like fusing and dispatcher configuration during the materialization step.\nAnother not obvious gain from hiding the Reactive Streams interfaces comes from the fact that org.reactivestreams.Subscriber (et al) have now been included in Java 9+, and thus become part of Java itself, so libraries should migrate to using the java.util.concurrent.Flow.Subscriber instead of org.reactivestreams.Subscriber. Libraries which selected to expose and directly extend the Reactive Streams types will now have a tougher time to adapt the JDK9+ types – all their classes that extend Subscriber and friends will need to be copied or changed to extend the exact same interface, but from a different package. In Pekko, we simply expose the new type when asked to – already supporting JDK9 types, from the day JDK9 was released.\nThe other, and perhaps more important reason for hiding the Reactive Streams interfaces comes back to the first points of this explanation: the fact of Reactive Streams being an SPI, and as such is hard to “get right” in ad-hoc implementations. Thus Pekko Streams discourages the use of the hard to implement pieces of the underlying infrastructure, and offers simpler, more type-safe, yet more powerful abstractions for users to work with: GraphStageGraphStages and operators. It is of course still (and easily) possible to accept or obtain Reactive Streams (or JDK+ Flow) representations of the stream operators by using methods like Sink.asPublisherSink.asPublisher or fromSubscriberfromSubscriber.","title":"Rationale and benefits from Sink/Source/Flow not directly extending Reactive Streams interfaces"},{"location":"/general/stream/stream-design.html#what-shall-users-of-streaming-libraries-expect-","text":"We expect libraries to be built on top of Pekko Streams, in fact Pekko HTTP is one such example that lives within the Pekko project itself. In order to allow users to profit from the principles that are described for Pekko Streams above, the following rules are established:\nlibraries shall provide their users with reusable pieces, i.e. expose factories that return operators, allowing full compositionality libraries may optionally and additionally provide facilities that consume and materialize operators\nThe reasoning behind the first rule is that compositionality would be destroyed if different libraries only accepted operators and expected to materialize them: using two of these together would be impossible because materialization can only happen once. As a consequence, the functionality of a library must be expressed such that materialization can be done by the user, outside of the library’s control.\nThe second rule allows a library to additionally provide nice sugar for the common case, an example of which is the Pekko HTTP API that provides a handleWith method for convenient materialization.\nNote One important consequence of this is that a reusable flow description cannot be bound to “live” resources, any connection to or allocation of such resources must be deferred until materialization time. Examples of “live” resources are already existing TCP connections, a multicast Publisher, etc.; a TickSource does not fall into this category if its timer is created only upon materialization (as is the case for our implementation). Exceptions from this need to be well-justified and carefully documented.","title":"What shall users of streaming libraries expect?"},{"location":"/general/stream/stream-design.html#resulting-implementation-constraints","text":"Pekko Streams must enable a library to express any stream processing utility in terms of immutable blueprints. The most common building blocks are\nSourceSource: something with exactly one output stream SinkSink: something with exactly one input stream FlowFlow: something with exactly one input and one output stream BidiFlowBidiFlow: something with exactly two input streams and two output streams that conceptually behave like two Flows of opposite direction GraphGraph: a packaged stream processing topology that exposes a certain set of input and output ports, characterized by an object of type ShapeShape.\nNote A source that emits a stream of streams is still a normal Source, the kind of elements that are produced does not play a role in the static stream topology that is being expressed.","title":"Resulting Implementation Constraints"},{"location":"/general/stream/stream-design.html#the-difference-between-error-and-failure","text":"The starting point for this discussion is the definition given by the Reactive Manifesto. Translated to streams this means that an error is accessible within the stream as a normal data element, while a failure means that the stream itself has failed and is collapsing. In concrete terms, on the Reactive Streams interface level data elements (including errors) are signaled via onNext while failures raise the onError signal.\nNote Unfortunately the method name for signaling failure to a Subscriber is called onError for historical reasons. Always keep in mind that the Reactive Streams interfaces (Publisher/Subscription/Subscriber) are modeling the low-level infrastructure for passing streams between execution units, and errors on this level are precisely the failures that we are talking about on the higher level that is modeled by Pekko Streams.\nThere is only limited support for treating onError in Pekko Streams compared to the operators that are available for the transformation of data elements, which is intentional in the spirit of the previous paragraph. Since onError signals that the stream is collapsing, its ordering semantics are not the same as for stream completion: transformation operators of any kind will collapse with the stream, possibly still holding elements in implicit or explicit buffers. This means that data elements emitted before a failure can still be lost if the onError overtakes them.\nThe ability for failures to propagate faster than data elements is essential for tearing down streams that are back-pressured—especially since back-pressure can be the failure mode (e.g. by tripping upstream buffers which then abort because they cannot do anything else; or if a dead-lock occurred).","title":"The difference between Error and Failure"},{"location":"/general/stream/stream-design.html#the-semantics-of-stream-recovery","text":"A recovery element (i.e. any transformation that absorbs an onError signal and turns that into possibly more data elements followed normal stream completion) acts as a bulkhead that confines a stream collapse to a given region of the stream topology. Within the collapsed region buffered elements may be lost, but the outside is not affected by the failure.\nThis works in the same fashion as a try–catch expression: it marks a region in which exceptions are caught, but the exact amount of code that was skipped within this region in case of a failure might not be known precisely—the placement of statements matters.","title":"The semantics of stream recovery"},{"location":"/stream/stream-flows-and-basics.html","text":"","title":"Basics and working with Flows"},{"location":"/stream/stream-flows-and-basics.html#basics-and-working-with-flows","text":"","title":"Basics and working with Flows"},{"location":"/stream/stream-flows-and-basics.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-flows-and-basics.html#introduction","text":"","title":"Introduction"},{"location":"/stream/stream-flows-and-basics.html#core-concepts","text":"Pekko Streams is a library to process and transfer a sequence of elements using bounded buffer space. This latter property is what we refer to as boundedness, and it is the defining feature of Pekko Streams. Translated to everyday terms, it is possible to express a chain (or as we see later, graphs) of processing entities. Each of these entities executes independently (and possibly concurrently) from the others while only buffering a limited number of elements at any given time. This property of bounded buffers is one of the differences from the actor model, where each actor usually has an unbounded, or a bounded, but dropping mailbox. Pekko Stream processing entities have bounded “mailboxes” that do not drop.\nBefore we move on, let’s define some basic terminology which will be used throughout the entire documentation:\nStream An active process that involves moving and transforming data. Element An element is the processing unit of streams. All operations transform and transfer elements from upstream to downstream. Buffer sizes are always expressed as number of elements independently from the actual size of the elements. Back-pressure A means of flow-control, a way for consumers of data to notify a producer about their current availability, effectively slowing down the upstream producer to match their consumption speeds. In the context of Pekko Streams back-pressure is always understood as non-blocking and asynchronous. Non-Blocking Means that a certain operation does not hinder the progress of the calling thread, even if it takes a long time to finish the requested operation. Graph A description of a stream processing topology, defining the pathways through which elements shall flow when the stream is running. Operator The common name for all building blocks that build up a Graph. Examples of operators are map(), filter(), custom ones extending GraphStages and graph junctions like Merge or Broadcast. For the full list of built-in operators see the operator index\nWhen we talk about asynchronous, non-blocking backpressure, we mean that the operators available in Pekko Streams will not use blocking calls but asynchronous message passing to exchange messages between each other. This way they can slow down a fast producer without blocking its thread. This is a thread-pool friendly design, since entities that need to wait (a fast producer waiting on a slow consumer) will not block the thread but can hand it back for further use to an underlying thread-pool.","title":"Core concepts"},{"location":"/stream/stream-flows-and-basics.html#defining-and-running-streams","text":"Linear processing pipelines can be expressed in Pekko Streams using the following core abstractions:\nSource An operator with exactly one output, emitting data elements whenever downstream operators are ready to receive them. Sink An operator with exactly one input, requesting and accepting data elements, possibly slowing down the upstream producer of elements. Flow An operator which has exactly one input and output, which connects its upstream and downstream by transforming the data elements flowing through it. RunnableGraph A Flow that has both ends “attached” to a Source and Sink respectively, and is ready to be run().\nIt is possible to attach a FlowFlow to a SourceSource resulting in a composite source, and it is also possible to prepend a Flow to a SinkSink to get a new sink. After a stream is properly constructed by having both a source and a sink, it will be represented by the RunnableGraphRunnableGraph type, indicating that it is ready to be executed.\nIt is important to remember that even after constructing the RunnableGraph by connecting all the source, sink and different operators, no data will flow through it until it is materialized. Materialization is the process of allocating all resources needed to run the computation described by a Graph (in Pekko Streams this will often involve starting up Actors). Thanks to Flows being a description of the processing pipeline they are immutable, thread-safe, and freely shareable, which means that it is for example safe to share and send them between actors, to have one actor prepare the work, and then have it be materialized at some completely different place in the code.\nScala copysourceval source = Source(1 to 10)\nval sink = Sink.fold[Int, Int](0)(_ + _)\n\n// connect the Source to the Sink, obtaining a RunnableGraph\nval runnable: RunnableGraph[Future[Int]] = source.toMat(sink)(Keep.right)\n\n// materialize the flow and get the value of the sink\nval sum: Future[Int] = runnable.run()\n Java copysourcefinal Source<Integer, NotUsed> source =\n    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));\n// note that the Future is scala.concurrent.Future\nfinal Sink<Integer, CompletionStage<Integer>> sink = Sink.fold(0, Integer::sum);\n\n// connect the Source to the Sink, obtaining a RunnableFlow\nfinal RunnableGraph<CompletionStage<Integer>> runnable = source.toMat(sink, Keep.right());\n\n// materialize the flow\nfinal CompletionStage<Integer> sum = runnable.run(system);\nAfter running (materializing) the RunnableGraph[T] we get back the materialized value of type T. Every stream operator can produce a materialized value, and it is the responsibility of the user to combine them to a new type. In the above example, we used toMat to indicate that we want to transform the materialized value of the source and sink, and we used the convenience function Keep.right to say that we are only interested in the materialized value of the sink. In our example, the Sink.fold materializes a value of type Future which will represent the result of the folding process over the stream. In general, a stream can expose multiple materialized values, but it is quite common to be interested in only the value of the Source or the Sink in the stream. For this reason there is a convenience method called runWith() available for Sink, Source or Flow requiring, respectively, a supplied Source (in order to run a Sink), a Sink (in order to run a Source) or both a Source and a Sink (in order to run a Flow, since it has neither attached yet).\nAfter running (materializing) the RunnableGraph we get a special container object, the MaterializedMap. Both sources and sinks are able to put specific objects into this map. Whether they put something in or not is implementation dependent. For example, a Sink.fold will make a CompletionStage available in this map which will represent the result of the folding process over the stream. In general, a stream can expose multiple materialized values, but it is quite common to be interested in only the value of the Source or the Sink in the stream. For this reason there is a convenience method called runWith() available for Sink, Source or Flow requiring, respectively, a supplied Source (in order to run a Sink), a Sink (in order to run a Source) or both a Source and a Sink (in order to run a Flow, since it has neither attached yet).\nScala copysourceval source = Source(1 to 10)\nval sink = Sink.fold[Int, Int](0)(_ + _)\n\n// materialize the flow, getting the Sink's materialized value\nval sum: Future[Int] = source.runWith(sink) Java copysourcefinal Source<Integer, NotUsed> source =\n    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));\nfinal Sink<Integer, CompletionStage<Integer>> sink = Sink.fold(0, Integer::sum);\n\n// materialize the flow, getting the Sink's materialized value\nfinal CompletionStage<Integer> sum = source.runWith(sink, system);\nIt is worth pointing out that since operators are immutable, connecting them returns a new operator, instead of modifying the existing instance, so while constructing long flows, remember to assign the new value to a variable or run it:\nScala copysourceval source = Source(1 to 10)\nsource.map(_ => 0) // has no effect on source, since it's immutable\nsource.runWith(Sink.fold(0)(_ + _)) // 55\n\nval zeroes = source.map(_ => 0) // returns new Source[Int], with `map()` appended\nzeroes.runWith(Sink.fold(0)(_ + _)) // 0 Java copysourcefinal Source<Integer, NotUsed> source =\n    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));\nsource.map(x -> 0); // has no effect on source, since it's immutable\nsource.runWith(Sink.fold(0, Integer::sum), system); // 55\n\n// returns new Source<Integer>, with `map()` appended\nfinal Source<Integer, NotUsed> zeroes = source.map(x -> 0);\nfinal Sink<Integer, CompletionStage<Integer>> fold = Sink.fold(0, Integer::sum);\nzeroes.runWith(fold, system); // 0\nNote By default, Pekko Streams elements support exactly one downstream operator. Making fan-out (supporting multiple downstream operators) an explicit opt-in feature allows default stream elements to be less complex and more efficient. Also, it allows for greater flexibility on how exactly to handle the multicast scenarios, by providing named fan-out elements such as broadcast (signals all down-stream elements) or balance (signals one of available down-stream elements).\nIn the above example we used the runWith method, which both materializes the stream and returns the materialized value of the given sink or source.\nSince a stream can be materialized multiple times, the materialized value will also be calculated anew MaterializedMap returned is different for each such materialization, usually leading to different values being returned each time. In the example below, we create two running materialized instances of the stream that we described in the runnable variable. Both materializations give us a different FutureCompletionStage from the map even though we used the same sink to refer to the future:\nScala copysource// connect the Source to the Sink, obtaining a RunnableGraph\nval sink = Sink.fold[Int, Int](0)(_ + _)\nval runnable: RunnableGraph[Future[Int]] =\n  Source(1 to 10).toMat(sink)(Keep.right)\n\n// get the materialized value of the sink\nval sum1: Future[Int] = runnable.run()\nval sum2: Future[Int] = runnable.run()\n\n// sum1 and sum2 are different Futures! Java copysource// connect the Source to the Sink, obtaining a RunnableGraph\nfinal Sink<Integer, CompletionStage<Integer>> sink = Sink.fold(0, Integer::sum);\nfinal RunnableGraph<CompletionStage<Integer>> runnable =\n    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).toMat(sink, Keep.right());\n\n// get the materialized value of the FoldSink\nfinal CompletionStage<Integer> sum1 = runnable.run(system);\nfinal CompletionStage<Integer> sum2 = runnable.run(system);\n\n// sum1 and sum2 are different Futures!","title":"Defining and running streams"},{"location":"/stream/stream-flows-and-basics.html#defining-sources-sinks-and-flows","text":"The objects SourceSource and SinkSink define various ways to create sources and sinks of elements. The following examples show some of the most useful constructs (refer to the API documentation for more details):\nScala copysource// Create a source from an Iterable\nSource(List(1, 2, 3))\n\n// Create a source from a Future\nSource.future(Future.successful(\"Hello Streams!\"))\n\n// Create a source from a single element\nSource.single(\"only one element\")\n\n// an empty source\nSource.empty\n\n// Sink that folds over the stream and returns a Future\n// of the final result as its materialized value\nSink.fold[Int, Int](0)(_ + _)\n\n// Sink that returns a Future as its materialized value,\n// containing the first element of the stream\nSink.head\n\n// A Sink that consumes a stream without doing anything with the elements\nSink.ignore\n\n// A Sink that executes a side-effecting call for every element of the stream\nSink.foreach[String](println(_)) Java copysource// Create a source from an Iterable\nList<Integer> list = new LinkedList<>();\nlist.add(1);\nlist.add(2);\nlist.add(3);\nSource.from(list);\n\n// Create a source form a Future\nSource.future(Futures.successful(\"Hello Streams!\"));\n\n// Create a source from a single element\nSource.single(\"only one element\");\n\n// an empty source\nSource.empty();\n\n// Sink that folds over the stream and returns a Future\n// of the final result in the MaterializedMap\nSink.fold(0, Integer::sum);\n\n// Sink that returns a Future in the MaterializedMap,\n// containing the first element of the stream\nSink.head();\n\n// A Sink that consumes a stream without doing anything with the elements\nSink.ignore();\n\n// A Sink that executes a side-effecting call for every element of the stream\nSink.foreach(System.out::println);\nThere are various ways to wire up different parts of a stream, the following examples show some of the available options:\nScala copysource// Explicitly creating and wiring up a Source, Sink and Flow\nSource(1 to 6).via(Flow[Int].map(_ * 2)).to(Sink.foreach(println(_)))\n\n// Starting from a Source\nval source = Source(1 to 6).map(_ * 2)\nsource.to(Sink.foreach(println(_)))\n\n// Starting from a Sink\nval sink: Sink[Int, NotUsed] = Flow[Int].map(_ * 2).to(Sink.foreach(println(_)))\nSource(1 to 6).to(sink)\n\n// Broadcast to a sink inline\nval otherSink: Sink[Int, NotUsed] =\n  Flow[Int].alsoTo(Sink.foreach(println(_))).to(Sink.ignore)\nSource(1 to 6).to(otherSink)\n Java copysource// Explicitly creating and wiring up a Source, Sink and Flow\nSource.from(Arrays.asList(1, 2, 3, 4))\n    .via(Flow.of(Integer.class).map(elem -> elem * 2))\n    .to(Sink.foreach(System.out::println));\n\n// Starting from a Source\nfinal Source<Integer, NotUsed> source =\n    Source.from(Arrays.asList(1, 2, 3, 4)).map(elem -> elem * 2);\nsource.to(Sink.foreach(System.out::println));\n\n// Starting from a Sink\nfinal Sink<Integer, NotUsed> sink =\n    Flow.of(Integer.class).map(elem -> elem * 2).to(Sink.foreach(System.out::println));\nSource.from(Arrays.asList(1, 2, 3, 4)).to(sink);","title":"Defining sources, sinks and flows"},{"location":"/stream/stream-flows-and-basics.html#illegal-stream-elements","text":"In accordance to the Reactive Streams specification (Rule 2.13) Pekko Streams do not allow null to be passed through the stream as an element. In case you want to model the concept of absence of a value we recommend using scala.Option or scala.util.Eitherjava.util.Optional which is available since Java 8.","title":"Illegal stream elements"},{"location":"/stream/stream-flows-and-basics.html#back-pressure-explained","text":"Pekko Streams implement an asynchronous non-blocking back-pressure protocol standardised by the Reactive Streams specification, which Pekko is a founding member of.\nThe user of the library does not have to write any explicit back-pressure handling code — it is built in and dealt with automatically by all of the provided Pekko Streams operators. It is possible however to add explicit buffer operators with overflow strategies that can influence the behavior of the stream. This is especially important in complex processing graphs which may even contain loops (which must be treated with very special care, as explained in Graph cycles, liveness and deadlocks).\nThe back pressure protocol is defined in terms of the number of elements a downstream Subscriber is able to receive and buffer, referred to as demand. The source of data, referred to as Publisher in Reactive Streams terminology and implemented as SourceSource in Pekko Streams, guarantees that it will never emit more elements than the received total demand for any given Subscriber.\nNote The Reactive Streams specification defines its protocol in terms of Publisher and Subscriber. These types are not meant to be user facing API, instead they serve as the low-level building blocks for different Reactive Streams implementations. Pekko Streams implements these concepts as SourceSource, FlowFlow (referred to as Processor in Reactive Streams) and SinkSink without exposing the Reactive Streams interfaces directly. If you need to integrate with other Reactive Stream libraries, read Integrating with Reactive Streams.\nThe mode in which Reactive Streams back-pressure works can be colloquially described as “dynamic push / pull mode”, since it will switch between push and pull based back-pressure models depending on the downstream being able to cope with the upstream production rate or not.\nTo illustrate this further let us consider both problem situations and how the back-pressure protocol handles them:","title":"Back-pressure explained"},{"location":"/stream/stream-flows-and-basics.html#slow-publisher-fast-subscriber","text":"This is the happy case – we do not need to slow down the Publisher in this case. However signalling rates are rarely constant and could change at any point in time, suddenly ending up in a situation where the Subscriber is now slower than the Publisher. In order to safeguard from these situations, the back-pressure protocol must still be enabled during such situations, however we do not want to pay a high penalty for this safety net being enabled.\nThe Reactive Streams protocol solves this by asynchronously signalling from the Subscriber to the Publisher Request(n:Int) Request(int n) signals. The protocol guarantees that the Publisher will never signal more elements than the signalled demand. Since the Subscriber however is currently faster, it will be signalling these Request messages at a higher rate (and possibly also batching together the demand - requesting multiple elements in one Request signal). This means that the Publisher should not ever have to wait (be back-pressured) with publishing its incoming elements.\nAs we can see, in this scenario we effectively operate in so called push-mode since the Publisher can continue producing elements as fast as it can, since the pending demand will be recovered just-in-time while it is emitting elements.","title":"Slow Publisher, fast Subscriber"},{"location":"/stream/stream-flows-and-basics.html#fast-publisher-slow-subscriber","text":"This is the case when back-pressuring the Publisher is required, because the Subscriber is not able to cope with the rate at which its upstream would like to emit data elements.\nSince the Publisher is not allowed to signal more elements than the pending demand signalled by the Subscriber, it will have to abide to this back-pressure by applying one of the below strategies:\nnot generate elements, if it is able to control their production rate, try buffering the elements in a bounded manner until more demand is signalled, drop elements until more demand is signalled, tear down the stream if unable to apply any of the above strategies.\nAs we can see, this scenario effectively means that the Subscriber will pull the elements from the Publisher – this mode of operation is referred to as pull-based back-pressure.","title":"Fast Publisher, slow Subscriber"},{"location":"/stream/stream-flows-and-basics.html#stream-materialization","text":"When constructing flows and graphs in Pekko Streams think of them as preparing a blueprint, an execution plan. Stream materialization is the process of taking a stream description (RunnableGraphRunnableGraph) and allocating all the necessary resources it needs in order to run. In the case of Pekko Streams this often means starting up Actors which power the processing, but is not restricted to that—it could also mean opening files or socket connections etc.—depending on what the stream needs.\nMaterialization is triggered at so called “terminal operations”. Most notably this includes the various forms of the run() and runWith() methods defined on SourceSource and FlowFlow elements as well as a small number of special syntactic sugars for running with well-known sinks, such as runForeach(el => ...)runForeach(el -> ...) (being an alias to runWith(Sink.foreach(el => ...))runWith(Sink.foreach(el -> ...))).\nMaterialization is performed synchronously on the materializing thread by an ActorSystemActorSystem global MaterializerMaterializer. The actual stream processing is handled by actors started up during the streams materialization, which will be running on the thread pools they have been configured to run on - which defaults to the dispatcher set in the ActorSystem config or provided as attributes on the stream that is getting materialized.\nNote Reusing instances of linear computation operators (Source, Sink, Flow) inside composite Graphs is legal, yet will materialize that operator multiple times.","title":"Stream Materialization"},{"location":"/stream/stream-flows-and-basics.html#operator-fusion","text":"By default, Pekko Streams will fuse the stream operators. This means that the processing steps of a flow or stream can be executed within the same Actor and has two consequences:\npassing elements from one operator to the next is a lot faster between fused operators due to avoiding the asynchronous messaging overhead fused stream operators do not run in parallel to each other, meaning that only up to one CPU core is used for each fused part\nTo allow for parallel processing you will have to insert asynchronous boundaries manually into your flows and operators by way of adding Attributes.asyncBoundaryAttributes.asyncBoundary using the method async on SourceSource, SinkSink and FlowFlow to operators that shall communicate with the downstream of the graph in an asynchronous fashion.\nScala copysourceSource(List(1, 2, 3)).map(_ + 1).async.map(_ * 2).to(Sink.ignore) Java copysourceSource.range(1, 3).map(x -> x + 1).async().map(x -> x * 2).to(Sink.ignore());\nIn this example we create two regions within the flow which will be executed in one Actor each—assuming that adding and multiplying integers is an extremely costly operation this will lead to a performance gain since two CPUs can work on the tasks in parallel. It is important to note that asynchronous boundaries are not singular places within a flow where elements are passed asynchronously (as in other streaming libraries), but instead attributes always work by adding information to the flow graph that has been constructed up to this point:\nThis means that everything that is inside the red bubble will be executed by one actor and everything outside of it by another. This scheme can be applied successively, always having one such boundary enclose the previous ones plus all operators that have been added since then.\nWarning Without fusing (i.e. up to version 2.0-M2) each stream operator had an implicit input buffer that holds a few elements for efficiency reasons. If your flow graphs contain cycles then these buffers may have been crucial in order to avoid deadlocks. With fusing these implicit buffers are no longer there, data elements are passed without buffering between fused operators. In those cases where buffering is needed in order to allow the stream to run at all, you will have to insert explicit buffers with the .buffer() operator—typically a buffer of size 2 is enough to allow a feedback loop to function.","title":"Operator Fusion"},{"location":"/stream/stream-flows-and-basics.html#combining-materialized-values","text":"Since every operator in Pekko Streams can provide a materialized value after being materialized, it is necessary to somehow express how these values should be composed to a final value when we plug these operators together. For this, many operator methods have variants that take an additional argument, a function, that will be used to combine the resulting values. Some examples of using these combiners are illustrated in the example below.\nScala copysource// A source that can be signalled explicitly from the outside\nval source: Source[Int, Promise[Option[Int]]] = Source.maybe[Int]\n\n// A flow that internally throttles elements to 1/second, and returns a Cancellable\n// which can be used to shut down the stream\nval flow: Flow[Int, Int, Cancellable] = throttler\n\n// A sink that returns the first element of a stream in the returned Future\nval sink: Sink[Int, Future[Int]] = Sink.head[Int]\n\n// By default, the materialized value of the leftmost stage is preserved\nval r1: RunnableGraph[Promise[Option[Int]]] = source.via(flow).to(sink)\n\n// Simple selection of materialized values by using Keep.right\nval r2: RunnableGraph[Cancellable] = source.viaMat(flow)(Keep.right).to(sink)\nval r3: RunnableGraph[Future[Int]] = source.via(flow).toMat(sink)(Keep.right)\n\n// Using runWith will always give the materialized values of the stages added\n// by runWith() itself\nval r4: Future[Int] = source.via(flow).runWith(sink)\nval r5: Promise[Option[Int]] = flow.to(sink).runWith(source)\nval r6: (Promise[Option[Int]], Future[Int]) = flow.runWith(source, sink)\n\n// Using more complex combinations\nval r7: RunnableGraph[(Promise[Option[Int]], Cancellable)] =\n  source.viaMat(flow)(Keep.both).to(sink)\n\nval r8: RunnableGraph[(Promise[Option[Int]], Future[Int])] =\n  source.via(flow).toMat(sink)(Keep.both)\n\nval r9: RunnableGraph[((Promise[Option[Int]], Cancellable), Future[Int])] =\n  source.viaMat(flow)(Keep.both).toMat(sink)(Keep.both)\n\nval r10: RunnableGraph[(Cancellable, Future[Int])] =\n  source.viaMat(flow)(Keep.right).toMat(sink)(Keep.both)\n\n// It is also possible to map over the materialized values. In r9 we had a\n// doubly nested pair, but we want to flatten it out\nval r11: RunnableGraph[(Promise[Option[Int]], Cancellable, Future[Int])] =\n  r9.mapMaterializedValue {\n    case ((promise, cancellable), future) =>\n      (promise, cancellable, future)\n  }\n\n// Now we can use pattern matching to get the resulting materialized values\nval (promise, cancellable, future) = r11.run()\n\n// Type inference works as expected\npromise.success(None)\ncancellable.cancel()\nfuture.map(_ + 3)\n\n// The result of r11 can be also achieved by using the Graph API\nval r12: RunnableGraph[(Promise[Option[Int]], Cancellable, Future[Int])] =\n  RunnableGraph.fromGraph(GraphDSL.createGraph(source, flow, sink)((_, _, _)) { implicit builder => (src, f, dst) =>\n    import GraphDSL.Implicits._\n    src ~> f ~> dst\n    ClosedShape\n  })\n Java copysource // An empty source that can be shut down explicitly from the outside\nSource<Integer, CompletableFuture<Optional<Integer>>> source = Source.<Integer>maybe();\n\n// A flow that internally throttles elements to 1/second, and returns a Cancellable\n// which can be used to shut down the stream\nFlow<Integer, Integer, Cancellable> flow = throttler;\n\n// A sink that returns the first element of a stream in the returned Future\nSink<Integer, CompletionStage<Integer>> sink = Sink.head();\n\n// By default, the materialized value of the leftmost stage is preserved\nRunnableGraph<CompletableFuture<Optional<Integer>>> r1 = source.via(flow).to(sink);\n\n// Simple selection of materialized values by using Keep.right\nRunnableGraph<Cancellable> r2 = source.viaMat(flow, Keep.right()).to(sink);\nRunnableGraph<CompletionStage<Integer>> r3 = source.via(flow).toMat(sink, Keep.right());\n\n// Using runWith will always give the materialized values of the stages added\n// by runWith() itself\nCompletionStage<Integer> r4 = source.via(flow).runWith(sink, system);\nCompletableFuture<Optional<Integer>> r5 = flow.to(sink).runWith(source, system);\nPair<CompletableFuture<Optional<Integer>>, CompletionStage<Integer>> r6 =\n    flow.runWith(source, sink, system);\n\n// Using more complex combinations\nRunnableGraph<Pair<CompletableFuture<Optional<Integer>>, Cancellable>> r7 =\n    source.viaMat(flow, Keep.both()).to(sink);\n\nRunnableGraph<Pair<CompletableFuture<Optional<Integer>>, CompletionStage<Integer>>> r8 =\n    source.via(flow).toMat(sink, Keep.both());\n\nRunnableGraph<\n        Pair<Pair<CompletableFuture<Optional<Integer>>, Cancellable>, CompletionStage<Integer>>>\n    r9 = source.viaMat(flow, Keep.both()).toMat(sink, Keep.both());\n\nRunnableGraph<Pair<Cancellable, CompletionStage<Integer>>> r10 =\n    source.viaMat(flow, Keep.right()).toMat(sink, Keep.both());\n\n// It is also possible to map over the materialized values. In r9 we had a\n// doubly nested pair, but we want to flatten it out\n\nRunnableGraph<Cancellable> r11 =\n    r9.mapMaterializedValue(\n        (nestedTuple) -> {\n          CompletableFuture<Optional<Integer>> p = nestedTuple.first().first();\n          Cancellable c = nestedTuple.first().second();\n          CompletionStage<Integer> f = nestedTuple.second();\n\n          // Picking the Cancellable, but we could  also construct a domain class here\n          return c;\n        });\nNote In Graphs it is possible to access the materialized value from inside the stream. For details see Accessing the materialized value inside the Graph.","title":"Combining materialized values"},{"location":"/stream/stream-flows-and-basics.html#source-pre-materialization","text":"There are situations in which you require a SourceSource materialized value before the Source gets hooked up to the rest of the graph. This is particularly useful in the case of “materialized value powered” Sources, like Source.queue, Source.actorRef or Source.maybe.\nBy using the preMaterializepreMaterialize operator on a Source, you can obtain its materialized value and another Source. The latter can be used to consume messages from the original Source. Note that this can be materialized multiple times.\nScala copysourceval completeWithDone: PartialFunction[Any, CompletionStrategy] = { case Done => CompletionStrategy.immediately }\nval matValuePoweredSource =\n  Source.actorRef[String](\n    completionMatcher = completeWithDone,\n    failureMatcher = PartialFunction.empty,\n    bufferSize = 100,\n    overflowStrategy = OverflowStrategy.fail)\n\nval (actorRef, source) = matValuePoweredSource.preMaterialize()\n\nactorRef ! \"Hello!\"\n\n// pass source around for materialization\nsource.runWith(Sink.foreach(println)) Java copysourceSource<String, ActorRef> matValuePoweredSource =\n    Source.actorRef(\n        elem -> {\n          // complete stream immediately if we send it Done\n          if (elem == Done.done()) return Optional.of(CompletionStrategy.immediately());\n          else return Optional.empty();\n        },\n        // never fail the stream because of a message\n        elem -> Optional.empty(),\n        100,\n        OverflowStrategy.fail());\n\nPair<ActorRef, Source<String, NotUsed>> actorRefSourcePair =\n    matValuePoweredSource.preMaterialize(system);\n\nactorRefSourcePair.first().tell(\"Hello!\", ActorRef.noSender());\n\n// pass source around for materialization\nactorRefSourcePair.second().runWith(Sink.foreach(System.out::println), system);","title":"Source pre-materialization"},{"location":"/stream/stream-flows-and-basics.html#stream-ordering","text":"In Pekko Streams, almost all computation operators preserve input order of elements. This means that if inputs {IA1,IA2,...,IAn} “cause” outputs {OA1,OA2,...,OAk} and inputs {IB1,IB2,...,IBm} “cause” outputs {OB1,OB2,...,OBl} and all of IAi happened before all IBi then OAi happens before OBi.\nThis property is even upheld by async operations such as mapAsyncmapAsync, however an unordered version exists called mapAsyncUnorderedmapAsyncUnordered which does not preserve this ordering.\nHowever, in the case of Junctions which handle multiple input streams (e.g. Merge) the output order is, in general, not defined for elements arriving on different input ports. That is a merge-like operation may emit Ai before emitting Bi, and it is up to its internal logic to decide the order of emitted elements. Specialized elements such as Zip however do guarantee their outputs order, as each output element depends on all upstream elements having been signalled already – thus the ordering in the case of zipping is defined by this property.\nIf you find yourself in need of fine grained control over order of emitted elements in fan-in scenarios consider using MergePreferred, MergePrioritized or GraphStage – which gives you full control over how the merge is performed.","title":"Stream ordering"},{"location":"/stream/stream-flows-and-basics.html#actor-materializer-lifecycle","text":"The MaterializerMaterializer is a component that is responsible for turning the stream blueprint into a running stream and emitting the “materialized value”. An ActorSystemActorSystem wide Materializer is provided by the Pekko Extension SystemMaterializerSystemMaterializer by having an implicit ActorSystem in scopepassing the ActorSystem to the various run methods this way there is no need to worry about the Materializer unless there are special requirements.\nThe use case that may require a custom instance of Materializer is when all streams materialized in an actor should be tied to the Actor lifecycle and stop if the Actor stops or crashes.\nAn important aspect of working with streams and actors is understanding a Materializer’s life-cycle. The materializer is bound to the lifecycle of the ActorRefFactoryActorRefFactory it is created from, which in practice will be either an ActorSystemActorSystem or ActorContextActorContext (when the materializer is created within an ActorActor).\nTying it to the ActorSystem should be replaced with using the system materializer from Pekko 2.6 and on.\nWhen run by the system materializer the streams will run until the ActorSystem is shut down. When the materializer is shut down before the streams have run to completion, they will be terminated abruptly. This is a little different than the usual way to terminate streams, which is by cancelling/completing them. The stream lifecycles are bound to the materializer like this to prevent leaks, and in normal operations you should not rely on the mechanism and rather use KillSwitchKillSwitch or normal completion signals to manage the lifecycles of your streams.\nIf we look at the following example, where we create the Materializer within an Actor:\nScala copysourcefinal class RunWithMyself extends Actor {\n  implicit val mat: Materializer = Materializer(context)\n\n  Source.maybe.runWith(Sink.onComplete {\n    case Success(done) => println(s\"Completed: $done\")\n    case Failure(ex)   => println(s\"Failed: ${ex.getMessage}\")\n  })\n\n  def receive = {\n    case \"boom\" =>\n      context.stop(self) // will also terminate the stream\n  }\n} Java copysourcefinal class RunWithMyself extends AbstractActor {\n\n  Materializer mat = Materializer.createMaterializer(context());\n\n  @Override\n  public void preStart() throws Exception {\n    Source.repeat(\"hello\")\n        .runWith(\n            Sink.onComplete(\n                tryDone -> {\n                  System.out.println(\"Terminated stream: \" + tryDone);\n                }),\n            mat);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            p -> {\n              // this WILL terminate the above stream as well\n              context().stop(self());\n            })\n        .build();\n  }\n}\nIn the above example we used the ActorContextActorContext to create the materializer. This binds its lifecycle to the surrounding ActorActor. In other words, while the stream we started there would under normal circumstances run forever, if we stop the Actor it would terminate the stream as well. We have bound the stream’s lifecycle to the surrounding actor’s lifecycle. This is a very useful technique if the stream is closely related to the actor, e.g. when the actor represents a user or other entity, that we continuously query using the created stream – and it would not make sense to keep the stream alive when the actor has terminated already. The streams termination will be signalled by an “Abrupt termination exception” signaled by the stream.\nYou may also cause a Materializer to shut down by explicitly calling shutdown()shutdown() on it, resulting in abruptly terminating all of the streams it has been running then.\nSometimes, however, you may want to explicitly create a stream that will out-last the actor’s life. For example, you are using a Pekko stream to push some large stream of data to an external service. You may want to eagerly stop the Actor since it has performed all of its duties already:\nScala copysourcefinal class RunForever(implicit val mat: Materializer) extends Actor {\n\n  Source.maybe.runWith(Sink.onComplete {\n    case Success(done) => println(s\"Completed: $done\")\n    case Failure(ex)   => println(s\"Failed: ${ex.getMessage}\")\n  })\n\n  def receive = {\n    case \"boom\" =>\n      context.stop(self) // will NOT terminate the stream (it's bound to the system!)\n  }\n} Java copysourcefinal class RunForever extends AbstractActor {\n\n  private final Materializer materializer;\n\n  public RunForever(Materializer materializer) {\n    this.materializer = materializer;\n  }\n\n  @Override\n  public void preStart() throws Exception {\n    Source.repeat(\"hello\")\n        .runWith(\n            Sink.onComplete(\n                tryDone -> {\n                  System.out.println(\"Terminated stream: \" + tryDone);\n                }),\n            materializer);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            p -> {\n              // will NOT terminate the stream (it's bound to the system!)\n              context().stop(self());\n            })\n        .build();\n  }\nIn the above example we pass in a materializer to the Actor, which results in binding its lifecycle to the entire ActorSystemActorSystem rather than the single enclosing actor. This can be useful if you want to share a materializer or group streams into specific materializers, for example because of the materializer’s settings etc.\nWarning Do not create new actor materializers inside actors by passing the context.systemcontext.system() to it. This will cause a new MaterializerMaterializer to be created and potentially leaked (unless you shut it down explicitly) for each such actor. It is instead recommended to either pass-in the Materializer or create one using the actor’s context.","title":"Actor Materializer Lifecycle"},{"location":"/stream/stream-graphs.html","text":"","title":"Working with Graphs"},{"location":"/stream/stream-graphs.html#working-with-graphs","text":"","title":"Working with Graphs"},{"location":"/stream/stream-graphs.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-graphs.html#introduction","text":"In Pekko Streams, computation graphs are not expressed using a fluent DSL like linear computations are, instead they are written in a more graph-resembling DSL which aims to make translating graph drawings (e.g. from notes taken from design discussions, or illustrations in protocol specifications) to and from code simpler. In this section we’ll dive into the multiple ways of constructing and re-using graphs, as well as explain common pitfalls and how to avoid them.\nGraphs are needed whenever you want to perform any kind of fan-in (“multiple inputs”) or fan-out (“multiple outputs”) operations. Considering linear Flows to be like roads, we can picture graph operations as junctions: multiple flows being connected at a single point. Some operators which are common enough and fit the linear style of Flows, such as concat (which concatenates two streams, such that the second one is consumed after the first one has completed), may have shorthand methods defined on Flow or Source themselves, however you should keep in mind that those are also implemented as graph junctions.","title":"Introduction"},{"location":"/stream/stream-graphs.html#constructing-graphs","text":"Graphs are built from simple Flows which serve as the linear connections within the graphs as well as junctions which serve as fan-in and fan-out points for Flows. Thanks to the junctions having meaningful types based on their behavior and making them explicit elements these elements should be rather straightforward to use.\nPekko Streams currently provide these junctions (for a detailed list see the operator index):\nFan-out Broadcast[T]Broadcast<T> – (1 input, N outputs) given an input element emits to each output Balance[T]Balance<T> – (1 input, N outputs) given an input element emits to one of its output ports UnzipWith[In,A,B,...]UnzipWith<In,A,B,...> – (1 input, N outputs) takes a function of 1 input that given a value for each input emits N output elements (where N <= 20) UnZip[A,B]UnZip<A,B> – (1 input, 2 outputs) splits a stream of (A,B)Pair<A,B> tuples into two streams, one of type A and one of type B Fan-in Merge[In]Merge<In> – (N inputs , 1 output) picks randomly from inputs pushing them one by one to its output MergePreferred[In]MergePreferred<In> – like Merge but if elements are available on preferred port, it picks from it, otherwise randomly from others MergePrioritized[In]MergePrioritized<In> – like Merge but if elements are available on all input ports, it picks from them randomly based on their priority MergeLatest[In]MergeLatest<In> – (N inputs, 1 output) emits List[In], when i-th input stream emits element, then i-th element in emitted list is updated MergeSequence[In]MergeSequence<In> – (N inputs, 1 output) emits List[In], where the input streams must represent a partitioned sequence that must be merged back together in order ZipWith[A,B,...,Out]ZipWith<A,B,...,Out> – (N inputs, 1 output) which takes a function of N inputs that given a value for each input emits 1 output element Zip[A,B]Zip<A,B> – (2 inputs, 1 output) is a ZipWith specialised to zipping input streams of A and B into a (A,B)Pair(A,B) tuple stream Concat[A]Concat<A> – (2 inputs, 1 output) concatenates two streams (first consume one, then the second one)\nOne of the goals of the GraphDSL DSL is to look similar to how one would draw a graph on a whiteboard, so that it is simple to translate a design from whiteboard to code and be able to relate those two. Let’s illustrate this by translating the below hand drawn graph into Pekko Streams:\nSuch a graph is simple to translate to the Graph DSL since each linear element corresponds to a Flow, and each circle corresponds to either a Junction or a Source or Sink if it is beginning or ending a Flow. Junctions must always be created with defined type parameters, as otherwise the Nothing type will be inferred.\nScala copysourceval g = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder: GraphDSL.Builder[NotUsed] =>\n  import GraphDSL.Implicits._\n  val in = Source(1 to 10)\n  val out = Sink.ignore\n\n  val bcast = builder.add(Broadcast[Int](2))\n  val merge = builder.add(Merge[Int](2))\n\n  val f1, f2, f3, f4 = Flow[Int].map(_ + 10)\n\n  in ~> f1 ~> bcast ~> f2 ~> merge ~> f3 ~> out\n  bcast ~> f4 ~> merge\n  ClosedShape\n}) Java copysourcefinal Source<Integer, NotUsed> in = Source.from(Arrays.asList(1, 2, 3, 4, 5));\nfinal Sink<List<String>, CompletionStage<List<String>>> sink = Sink.head();\nfinal Flow<Integer, Integer, NotUsed> f1 = Flow.of(Integer.class).map(elem -> elem + 10);\nfinal Flow<Integer, Integer, NotUsed> f2 = Flow.of(Integer.class).map(elem -> elem + 20);\nfinal Flow<Integer, String, NotUsed> f3 = Flow.of(Integer.class).map(elem -> elem.toString());\nfinal Flow<Integer, Integer, NotUsed> f4 = Flow.of(Integer.class).map(elem -> elem + 30);\n\nfinal RunnableGraph<CompletionStage<List<String>>> result =\n    RunnableGraph.fromGraph(\n        GraphDSL // create() function binds sink, out which is sink's out port and builder DSL\n            .create( // we need to reference out's shape in the builder DSL below (in to()\n            // function)\n            sink, // previously created sink (Sink)\n            (builder, out) -> { // variables: builder (GraphDSL.Builder) and out (SinkShape)\n              final UniformFanOutShape<Integer, Integer> bcast =\n                  builder.add(Broadcast.create(2));\n              final UniformFanInShape<Integer, Integer> merge = builder.add(Merge.create(2));\n\n              final Outlet<Integer> source = builder.add(in).out();\n              builder\n                  .from(source)\n                  .via(builder.add(f1))\n                  .viaFanOut(bcast)\n                  .via(builder.add(f2))\n                  .viaFanIn(merge)\n                  .via(builder.add(f3.grouped(1000)))\n                  .to(out); // to() expects a SinkShape\n              builder.from(bcast).via(builder.add(f4)).toFanIn(merge);\n              return ClosedShape.getInstance();\n            }));\nNote Junction reference equality defines graph node equality (i.e. the same merge instance used in a GraphDSL refers to the same location in the resulting graph).\nNotice the import GraphDSL.Implicits._ which brings into scope the ~> operator (read as “edge”, “via” or “to”) and its inverted counterpart <~ (for noting down flows in the opposite direction where appropriate).\nBy looking at the snippets above, it should be apparent that the GraphDSL.Builderbuilder object is mutable. It is used (implicitly) by the ~> operator, also making it a mutable operation as well. The reason for this design choice is to enable simpler creation of complex graphs, which may even contain cycles. Once the GraphDSL has been constructed though, the GraphDSLRunnableGraph instance is immutable, thread-safe, and freely shareable. The same is true of all operators—sources, sinks, and flows—once they are constructed. This means that you can safely re-use one given Flow or junction in multiple places in a processing graph.\nWe have seen examples of such re-use already above: the merge and broadcast junctions were imported into the graph using builder.add(...), an operation that will make a copy of the blueprint that is passed to it and return the inlets and outlets of the resulting copy so that they can be wired up. Another alternative is to pass existing graphs—of any shape—into the factory method that produces a new graph. The difference between these approaches is that importing using builder.add(...) ignores the materialized value of the imported graph while importing via the factory method allows its inclusion; for more details see Stream Materialization.\nIn the example below, we prepare a graph that consists of two parallel streams, in which we re-use the same instance of Flow, yet it will properly be materialized as two connections between the corresponding Sources and Sinks:\nScala copysource val topHeadSink = Sink.head[Int]\nval bottomHeadSink = Sink.head[Int]\nval sharedDoubler = Flow[Int].map(_ * 2)\n\nRunnableGraph.fromGraph(GraphDSL.createGraph(topHeadSink, bottomHeadSink)((_, _)) { implicit builder =>\n  (topHS, bottomHS) =>\n  import GraphDSL.Implicits._\n  val broadcast = builder.add(Broadcast[Int](2))\n  Source.single(1) ~> broadcast.in\n\n  broadcast ~> sharedDoubler ~> topHS.in\n  broadcast ~> sharedDoubler ~> bottomHS.in\n  ClosedShape\n}) Java copysourcefinal Sink<Integer, CompletionStage<Integer>> topHeadSink = Sink.head();\nfinal Sink<Integer, CompletionStage<Integer>> bottomHeadSink = Sink.head();\nfinal Flow<Integer, Integer, NotUsed> sharedDoubler =\n    Flow.of(Integer.class).map(elem -> elem * 2);\n\nfinal RunnableGraph<Pair<CompletionStage<Integer>, CompletionStage<Integer>>> g =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            topHeadSink, // import this sink into the graph\n            bottomHeadSink, // and this as well\n            Keep.both(),\n            (b, top, bottom) -> {\n              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n\n              b.from(b.add(Source.single(1)))\n                  .viaFanOut(bcast)\n                  .via(b.add(sharedDoubler))\n                  .to(top);\n              b.from(bcast).via(b.add(sharedDoubler)).to(bottom);\n              return ClosedShape.getInstance();\n            }));\nIn some cases we may have a list of graph elements, for example, if they are dynamically created. If these graphs have similar signatures, we can construct a graph collecting all their materialized values as a collection:\nScala copysourceval sinks = immutable\n  .Seq(\"a\", \"b\", \"c\")\n  .map(prefix => Flow[String].filter(str => str.startsWith(prefix)).toMat(Sink.head[String])(Keep.right))\n\nval g: RunnableGraph[Seq[Future[String]]] = RunnableGraph.fromGraph(GraphDSL.create(sinks) {\n  implicit b => sinkList =>\n    import GraphDSL.Implicits._\n    val broadcast = b.add(Broadcast[String](sinkList.size))\n\n    Source(List(\"ax\", \"bx\", \"cx\")) ~> broadcast\n    sinkList.foreach(sink => broadcast ~> sink)\n\n    ClosedShape\n})\n\nval matList: Seq[Future[String]] = g.run() Java copysource// create the source\nfinal Source<String, NotUsed> in = Source.from(Arrays.asList(\"ax\", \"bx\", \"cx\"));\n// generate the sinks from code\nList<String> prefixes = Arrays.asList(\"a\", \"b\", \"c\");\nfinal List<Sink<String, CompletionStage<String>>> list = new ArrayList<>();\nfor (String prefix : prefixes) {\n  final Sink<String, CompletionStage<String>> sink =\n      Flow.of(String.class)\n          .filter(str -> str.startsWith(prefix))\n          .toMat(Sink.head(), Keep.right());\n  list.add(sink);\n}\n\nfinal RunnableGraph<List<CompletionStage<String>>> g =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            list,\n            (GraphDSL.Builder<List<CompletionStage<String>>> builder,\n                List<SinkShape<String>> outs) -> {\n              final UniformFanOutShape<String, String> bcast =\n                  builder.add(Broadcast.create(outs.size()));\n\n              final Outlet<String> source = builder.add(in).out();\n              builder.from(source).viaFanOut(bcast);\n\n              for (SinkShape<String> sink : outs) {\n                builder.from(bcast).to(sink);\n              }\n\n              return ClosedShape.getInstance();\n            }));\nList<CompletionStage<String>> result = g.run(system);","title":"Constructing Graphs"},{"location":"/stream/stream-graphs.html#constructing-and-combining-partial-graphs","text":"Sometimes, it is not possible (or needed) to construct the entire computation graph in one place, but instead construct all of its different phases in different places and in the end connect them all into a complete graph and run it.\nThis can be achieved by returning a different Shape than ClosedShape, for example FlowShape(in, out), from the function given to GraphDSL.create. See Predefined shapes for a list of such predefined shapes. Making a Graph a RunnableGraphusing the returned Graph from GraphDSL.create() rather than passing it to RunnableGraph.fromGraph() to wrap it in a RunnableGraph.The reason of representing it as a different type is that a RunnableGraph requires all ports to be connected, and if they are not it will throw an exception at construction time, which helps to avoid simple wiring errors while working with graphs. A partial graph however allows you to return the set of yet to be connected ports from the code block that performs the internal wiring.\nLet’s imagine we want to provide users with a specialized element that given 3 inputs will pick the greatest int value of each zipped triple. We’ll want to expose 3 input ports (unconnected sources) and one output port (unconnected sink).\nScala copysourceval pickMaxOfThree = GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  val zip1 = b.add(ZipWith[Int, Int, Int](math.max _))\n  val zip2 = b.add(ZipWith[Int, Int, Int](math.max _))\n  zip1.out ~> zip2.in0\n\n  UniformFanInShape(zip2.out, zip1.in0, zip1.in1, zip2.in1)\n}\n\nval resultSink = Sink.head[Int]\n\nval g = RunnableGraph.fromGraph(GraphDSL.createGraph(resultSink) { implicit b => sink =>\n  import GraphDSL.Implicits._\n\n  // importing the partial graph will return its shape (inlets & outlets)\n  val pm3 = b.add(pickMaxOfThree)\n\n  Source.single(1) ~> pm3.in(0)\n  Source.single(2) ~> pm3.in(1)\n  Source.single(3) ~> pm3.in(2)\n  pm3.out          ~> sink.in\n  ClosedShape\n})\n\nval max: Future[Int] = g.run()\nAwait.result(max, 300.millis) should equal(3) Java copysourcefinal Graph<FanInShape2<Integer, Integer, Integer>, NotUsed> zip =\n    ZipWith.create((Integer left, Integer right) -> Math.max(left, right));\n\nfinal Graph<UniformFanInShape<Integer, Integer>, NotUsed> pickMaxOfThree =\n    GraphDSL.create(\n        builder -> {\n          final FanInShape2<Integer, Integer, Integer> zip1 = builder.add(zip);\n          final FanInShape2<Integer, Integer, Integer> zip2 = builder.add(zip);\n\n          builder.from(zip1.out()).toInlet(zip2.in0());\n          // return the shape, which has three inputs and one output\n          return UniformFanInShape.<Integer, Integer>create(\n              zip2.out(), Arrays.asList(zip1.in0(), zip1.in1(), zip2.in1()));\n        });\n\nfinal Sink<Integer, CompletionStage<Integer>> resultSink = Sink.<Integer>head();\n\nfinal RunnableGraph<CompletionStage<Integer>> g =\n    RunnableGraph.<CompletionStage<Integer>>fromGraph(\n        GraphDSL.create(\n            resultSink,\n            (builder, sink) -> {\n              // import the partial graph explicitly\n              final UniformFanInShape<Integer, Integer> pm = builder.add(pickMaxOfThree);\n\n              builder.from(builder.add(Source.single(1))).toInlet(pm.in(0));\n              builder.from(builder.add(Source.single(2))).toInlet(pm.in(1));\n              builder.from(builder.add(Source.single(3))).toInlet(pm.in(2));\n              builder.from(pm.out()).to(sink);\n              return ClosedShape.getInstance();\n            }));\n\nfinal CompletionStage<Integer> max = g.run(system);\nNote While the above example shows composing two 2-input ZipWiths, in reality ZipWith already provides numerous overloads including a 3 (and many more) parameter versions. So this could be implemented using one ZipWith using the 3 parameter version, like this: ZipWith((a, b, c) => out)ZipWith.create((a, b, c) -> out). (The ZipWith with N input has N+1 type parameter; the last type param is the output type.)\nAs you can see, first we construct the partial graph that contains all the zipping and comparing of stream elements. This partial graph will have three inputs and one output, wherefore we use the UniformFanInShapedescribes how to compute the maximum of two input streams. then we reuse that twice while constructing the partial graph that extends this to three input streams. Then we import it (all of its nodes and connections) explicitly into the closed graph built in the second steplast graph in which all the undefined elements are rewired to real sources and sinks. The graph can then be run and yields the expected result.\nWarning Please note that GraphDSL is not able to provide compile time type-safety about whether or not all elements have been properly connected—this validation is performed as a runtime check during the graph’s instantiation. A partial graph also verifies that all ports are either connected or part of the returned Shape.","title":"Constructing and combining Partial Graphs"},{"location":"/stream/stream-graphs.html#constructing-sources-sinks-and-flows-from-partial-graphs","text":"Instead of treating a partial graphGraph as a collection of flows and junctions which may not yet all be connected it is sometimes useful to expose such a complex graph as a simpler structure, such as a Source, Sink or Flow.\nIn fact, these concepts can be expressed as special cases of a partially connected graph:\nSource is a partial graph with exactly one output, that is it returns a SourceShape. Sink is a partial graph with exactly one input, that is it returns a SinkShape. Flow is a partial graph with exactly one input and exactly one output, that is it returns a FlowShape.\nBeing able to hide complex graphs inside of simple elements such as Sink / Source / Flow enables you to create one complex element and from there on treat it as simple compound operator for linear computations.\nIn order to create a Source from a graph the method Source.fromGraph is used, to use it we must have a Graph[SourceShape, T]Graph with a SourceShape. This is constructed using GraphDSL.create and returning a SourceShape from the function passed inGraphDSL.create and providing building a SourceShape graph. The single outlet must be provided to the SourceShape.of method and will become “the sink that must be attached before this Source can run”.\nRefer to the example below, in which we create a Source that zips together two numbers, to see this graph construction in action:\nScala copysourceval pairs = Source.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  // prepare graph elements\n  val zip = b.add(Zip[Int, Int]())\n  def ints = Source.fromIterator(() => Iterator.from(1))\n\n  // connect the graph\n  ints.filter(_ % 2 != 0) ~> zip.in0\n  ints.filter(_ % 2 == 0) ~> zip.in1\n\n  // expose port\n  SourceShape(zip.out)\n})\n\nval firstPair: Future[(Int, Int)] = pairs.runWith(Sink.head) Java copysource// first create an indefinite source of integer numbers\nclass Ints implements Iterator<Integer> {\n  private int next = 0;\n\n  @Override\n  public boolean hasNext() {\n    return true;\n  }\n\n  @Override\n  public Integer next() {\n    return next++;\n  }\n}\n  final Source<Integer, NotUsed> ints = Source.fromIterator(() -> new Ints());\n\n  final Source<Pair<Integer, Integer>, NotUsed> pairs =\n      Source.fromGraph(\n          GraphDSL.create(\n              builder -> {\n                final FanInShape2<Integer, Integer, Pair<Integer, Integer>> zip =\n                    builder.add(Zip.create());\n\n                builder.from(builder.add(ints.filter(i -> i % 2 == 0))).toInlet(zip.in0());\n                builder.from(builder.add(ints.filter(i -> i % 2 == 1))).toInlet(zip.in1());\n\n                return SourceShape.of(zip.out());\n              }));\n\n  final CompletionStage<Pair<Integer, Integer>> firstPair =\n      pairs.runWith(Sink.<Pair<Integer, Integer>>head(), system);\nSimilarly the same can be done for a Sink[T]Sink<T>, using SinkShape.of in which case the provided value must be an Inlet[T]Inlet<T>. For defining a Flow[T]Flow<T> we need to expose both an inlet and an outletundefined source and sink:\nScala copysourceval pairUpWithToString =\n  Flow.fromGraph(GraphDSL.create() { implicit b =>\n    import GraphDSL.Implicits._\n\n    // prepare graph elements\n    val broadcast = b.add(Broadcast[Int](2))\n    val zip = b.add(Zip[Int, String]())\n\n    // connect the graph\n    broadcast.out(0).map(identity)   ~> zip.in0\n    broadcast.out(1).map(_.toString) ~> zip.in1\n\n    // expose ports\n    FlowShape(broadcast.in, zip.out)\n  })\n\npairUpWithToString.runWith(Source(List(1)), Sink.head) Java copysourcefinal Flow<Integer, Pair<Integer, String>, NotUsed> pairs =\n    Flow.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n              final FanInShape2<Integer, String, Pair<Integer, String>> zip =\n                  b.add(Zip.create());\n\n              b.from(bcast).toInlet(zip.in0());\n              b.from(bcast)\n                  .via(b.add(Flow.of(Integer.class).map(i -> i.toString())))\n                  .toInlet(zip.in1());\n\n              return FlowShape.of(bcast.in(), zip.out());\n            }));\n\n    Source.single(1).via(pairs).runWith(Sink.<Pair<Integer, String>>head(), system);","title":"Constructing Sources, Sinks and Flows from Partial Graphs"},{"location":"/stream/stream-graphs.html#combining-sources-and-sinks-with-simplified-api","text":"There is a simplified API you can use to combine sources and sinks with junctions like: Broadcast[T], Balance[T], Merge[In] and Concat[A]Broadcast<T>, Balance<T>, Merge<In> and Concat<A> without the need for using the Graph DSL. The combine method takes care of constructing the necessary graph underneath. In following example we combine two sources into one (fan-in):\nScala copysourceval sourceOne = Source(List(1))\nval sourceTwo = Source(List(2))\nval merged = Source.combine(sourceOne, sourceTwo)(Merge(_))\n\nval mergedResult: Future[Int] = merged.runWith(Sink.fold(0)(_ + _)) Java copysourceSource<Integer, NotUsed> source1 = Source.single(1);\nSource<Integer, NotUsed> source2 = Source.single(2);\n\nfinal Source<Integer, NotUsed> sources =\n    Source.combine(source1, source2, new ArrayList<>(), i -> Merge.<Integer>create(i));\n    sources.runWith(Sink.<Integer, Integer>fold(0, (a, b) -> a + b), system);\nThe same can be done for a Sink[T]Sink but in this case it will be fan-out:\nScala copysourceval sendRemotely = Sink.actorRef(actorRef, \"Done\", _ => \"Failed\")\nval localProcessing = Sink.foreach[Int](_ => /* do something useful */ ())\n\nval sink = Sink.combine(sendRemotely, localProcessing)(Broadcast[Int](_))\n\nSource(List(0, 1, 2)).runWith(sink) Java copysourceSink<Integer, NotUsed> sendRemotely = Sink.actorRef(actorRef, \"Done\");\nSink<Integer, CompletionStage<Done>> localProcessing =\n    Sink.<Integer>foreach(\n        a -> {\n          /*do something useful*/\n        });\nSink<Integer, NotUsed> sinks =\n    Sink.combine(sendRemotely, localProcessing, new ArrayList<>(), a -> Broadcast.create(a));\n\nSource.<Integer>from(Arrays.asList(new Integer[] {0, 1, 2})).runWith(sinks, system);","title":"Combining Sources and Sinks with simplified API"},{"location":"/stream/stream-graphs.html#building-reusable-graph-components","text":"It is possible to build reusable, encapsulated components of arbitrary input and output ports using the graph DSL.\nAs an example, we will build a graph junction that represents a pool of workers, where a worker is expressed as a Flow[I,O,_]Flow<I,O,M>, i.e. a simple transformation of jobs of type I to results of type O (as you have seen already, this flow can actually contain a complex graph inside). Our reusable worker pool junction will not preserve the order of the incoming jobs (they are assumed to have a proper ID field) and it will use a Balance junction to schedule jobs to available workers. On top of this, our junction will feature a “fastlane”, a dedicated port where jobs of higher priority can be sent.\nAltogether, our junction will have two input ports of type I (for the normal and priority jobs) and an output port of type O. To represent this interface, we need to define a custom Shape. The following lines show how to do that.\nScala copysource// A shape represents the input and output ports of a reusable\n// processing module\ncase class PriorityWorkerPoolShape[In, Out](jobsIn: Inlet[In], priorityJobsIn: Inlet[In], resultsOut: Outlet[Out])\n    extends Shape {\n\n  // It is important to provide the list of all input and output\n  // ports with a stable order. Duplicates are not allowed.\n  override val inlets: immutable.Seq[Inlet[_]] =\n    jobsIn :: priorityJobsIn :: Nil\n  override val outlets: immutable.Seq[Outlet[_]] =\n    resultsOut :: Nil\n\n  // A Shape must be able to create a copy of itself. Basically\n  // it means a new instance with copies of the ports\n  override def deepCopy() =\n    PriorityWorkerPoolShape(jobsIn.carbonCopy(), priorityJobsIn.carbonCopy(), resultsOut.carbonCopy())\n\n}","title":"Building reusable Graph components"},{"location":"/stream/stream-graphs.html#predefined-shapes","text":"In general a custom Shape needs to be able to provide all its input and output ports, be able to copy itself, and also be able to create a new instance from given ports. There are some predefined shapes provided to avoid unnecessary boilerplate:\nSourceShape, SinkShape, FlowShape for simpler shapes, UniformFanInShape and UniformFanOutShape for junctions with multiple input (or output) ports of the same type, FanInShape1, FanInShape2, …, FanOutShape1, FanOutShape2, … for junctions with multiple input (or output) ports of different types.\nSince our shape has two input ports and one output port, we can use the FanInShape DSL to define our custom shape:\nScala copysourceimport FanInShape.{ Init, Name }\n\nclass PriorityWorkerPoolShape2[In, Out](_init: Init[Out] = Name(\"PriorityWorkerPool\"))\n    extends FanInShape[Out](_init) {\n  protected override def construct(i: Init[Out]) = new PriorityWorkerPoolShape2(i)\n\n  val jobsIn = newInlet[In](\"jobsIn\")\n  val priorityJobsIn = newInlet[In](\"priorityJobsIn\")\n  // Outlet[Out] with name \"out\" is automatically created\n}\nNow that we have a Shape we can wire up a Graph that represents our worker pool. First, we will merge incoming normal and priority jobs using MergePreferred, then we will send the jobs to a Balance junction which will fan-out to a configurable number of workers (flows), finally we merge all these results together and send them out through our only output port. This is expressed by the following code:\nScala copysourceobject PriorityWorkerPool {\n  def apply[In, Out](\n      worker: Flow[In, Out, Any],\n      workerCount: Int): Graph[PriorityWorkerPoolShape[In, Out], NotUsed] = {\n\n    GraphDSL.create() { implicit b =>\n      import GraphDSL.Implicits._\n\n      val priorityMerge = b.add(MergePreferred[In](1))\n      val balance = b.add(Balance[In](workerCount))\n      val resultsMerge = b.add(Merge[Out](workerCount))\n\n      // After merging priority and ordinary jobs, we feed them to the balancer\n      priorityMerge ~> balance\n\n      // Wire up each of the outputs of the balancer to a worker flow\n      // then merge them back\n      for (i <- 0 until workerCount)\n        balance.out(i) ~> worker ~> resultsMerge.in(i)\n\n      // We now expose the input ports of the priorityMerge and the output\n      // of the resultsMerge as our PriorityWorkerPool ports\n      // -- all neatly wrapped in our domain specific Shape\n      PriorityWorkerPoolShape(\n        jobsIn = priorityMerge.in(0),\n        priorityJobsIn = priorityMerge.preferred,\n        resultsOut = resultsMerge.out)\n    }\n\n  }\n\n}\nAll we need to do now is to use our custom junction in a graph. The following code simulates some simple workers and jobs using plain strings and prints out the results. Actually we used two instances of our worker pool junction using add() twice.\nScala copysourceval worker1 = Flow[String].map(\"step 1 \" + _)\nval worker2 = Flow[String].map(\"step 2 \" + _)\n\nRunnableGraph\n  .fromGraph(GraphDSL.create() { implicit b =>\n    import GraphDSL.Implicits._\n\n    val priorityPool1 = b.add(PriorityWorkerPool(worker1, 4))\n    val priorityPool2 = b.add(PriorityWorkerPool(worker2, 2))\n\n    Source(1 to 100).map(\"job: \" + _)          ~> priorityPool1.jobsIn\n    Source(1 to 100).map(\"priority job: \" + _) ~> priorityPool1.priorityJobsIn\n\n    priorityPool1.resultsOut                        ~> priorityPool2.jobsIn\n    Source(1 to 100).map(\"one-step, priority \" + _) ~> priorityPool2.priorityJobsIn\n\n    priorityPool2.resultsOut ~> Sink.foreach(println)\n    ClosedShape\n  })\n  .run()","title":"Predefined shapes"},{"location":"/stream/stream-graphs.html#bidirectional-flows","text":"A graph topology that is often useful is that of two flows going in opposite directions. Take for example a codec operator that serializes outgoing messages and deserializes incoming octet streams. Another such operator could add a framing protocol that attaches a length header to outgoing data and parses incoming frames back into the original octet stream chunks. These two operators are meant to be composed, applying one atop the other as part of a protocol stack. For this purpose exists the special type BidiFlow which is a graph that has exactly two open inlets and two open outlets. The corresponding shape is called BidiShape and is defined like this:\ncopysource/**\n * A bidirectional flow of elements that consequently has two inputs and two\n * outputs, arranged like this:\n *\n * {{{\n *        +------+\n *  In1 ~>|      |~> Out1\n *        | bidi |\n * Out2 <~|      |<~ In2\n *        +------+\n * }}}\n */\nfinal case class BidiShape[-In1, +Out1, -In2, +Out2](\n    in1: Inlet[In1 @uncheckedVariance],\n    out1: Outlet[Out1 @uncheckedVariance],\n    in2: Inlet[In2 @uncheckedVariance],\n    out2: Outlet[Out2 @uncheckedVariance])\n    extends Shape {\n  override val inlets: immutable.Seq[Inlet[_]] = in1 :: in2 :: Nil\n  override val outlets: immutable.Seq[Outlet[_]] = out1 :: out2 :: Nil\n\n  /**\n   * Java API for creating from a pair of unidirectional flows.\n   */\n  def this(top: FlowShape[In1, Out1], bottom: FlowShape[In2, Out2]) = this(top.in, top.out, bottom.in, bottom.out)\n\n  override def deepCopy(): BidiShape[In1, Out1, In2, Out2] =\n    BidiShape(in1.carbonCopy(), out1.carbonCopy(), in2.carbonCopy(), out2.carbonCopy())\n\n}\nA bidirectional flow is defined just like a unidirectional Flow as demonstrated for the codec mentioned above:\nScala copysourcetrait Message\ncase class Ping(id: Int) extends Message\ncase class Pong(id: Int) extends Message\n\ndef toBytes(msg: Message): ByteString = {\n  implicit val order = ByteOrder.LITTLE_ENDIAN\n  msg match {\n    case Ping(id) => ByteString.newBuilder.putByte(1).putInt(id).result()\n    case Pong(id) => ByteString.newBuilder.putByte(2).putInt(id).result()\n  }\n}\n\ndef fromBytes(bytes: ByteString): Message = {\n  implicit val order = ByteOrder.LITTLE_ENDIAN\n  val it = bytes.iterator\n  it.getByte match {\n    case 1     => Ping(it.getInt)\n    case 2     => Pong(it.getInt)\n    case other => throw new RuntimeException(s\"parse error: expected 1|2 got $other\")\n  }\n}\n\nval codecVerbose = BidiFlow.fromGraph(GraphDSL.create() { b =>\n  // construct and add the top flow, going outbound\n  val outbound = b.add(Flow[Message].map(toBytes))\n  // construct and add the bottom flow, going inbound\n  val inbound = b.add(Flow[ByteString].map(fromBytes))\n  // fuse them together into a BidiShape\n  BidiShape.fromFlows(outbound, inbound)\n})\n\n// this is the same as the above\nval codec = BidiFlow.fromFunctions(toBytes _, fromBytes _) Java copysourcestatic interface Message {}\n\nstatic class Ping implements Message {\n  final int id;\n\n  public Ping(int id) {\n    this.id = id;\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (o instanceof Ping) {\n      return ((Ping) o).id == id;\n    } else return false;\n  }\n\n  @Override\n  public int hashCode() {\n    return id;\n  }\n}\n\nstatic class Pong implements Message {\n  final int id;\n\n  public Pong(int id) {\n    this.id = id;\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (o instanceof Pong) {\n      return ((Pong) o).id == id;\n    } else return false;\n  }\n\n  @Override\n  public int hashCode() {\n    return id;\n  }\n}\n\npublic static ByteString toBytes(Message msg) {\n  if (msg instanceof Ping) {\n    final int id = ((Ping) msg).id;\n    return new ByteStringBuilder().putByte((byte) 1).putInt(id, ByteOrder.LITTLE_ENDIAN).result();\n  } else {\n    final int id = ((Pong) msg).id;\n    return new ByteStringBuilder().putByte((byte) 2).putInt(id, ByteOrder.LITTLE_ENDIAN).result();\n  }\n}\n\npublic static Message fromBytes(ByteString bytes) {\n  final ByteIterator it = bytes.iterator();\n  switch (it.getByte()) {\n    case 1:\n      return new Ping(it.getInt(ByteOrder.LITTLE_ENDIAN));\n    case 2:\n      return new Pong(it.getInt(ByteOrder.LITTLE_ENDIAN));\n    default:\n      throw new RuntimeException(\"message format error\");\n  }\n}\n\npublic final BidiFlow<Message, ByteString, ByteString, Message, NotUsed> codecVerbose =\n    BidiFlow.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final FlowShape<Message, ByteString> top =\n                  b.add(Flow.of(Message.class).map(BidiFlowDocTest::toBytes));\n              final FlowShape<ByteString, Message> bottom =\n                  b.add(Flow.of(ByteString.class).map(BidiFlowDocTest::fromBytes));\n              return BidiShape.fromFlows(top, bottom);\n            }));\n\npublic final BidiFlow<Message, ByteString, ByteString, Message, NotUsed> codec =\n    BidiFlow.fromFunctions(BidiFlowDocTest::toBytes, BidiFlowDocTest::fromBytes);\nThe first version resembles the partial graph constructor, while for the simple case of a functional 1:1 transformation there is a concise convenience method as shown on the last line. The implementation of the two functions is not difficult either:\nScala copysourcedef toBytes(msg: Message): ByteString = {\n  implicit val order = ByteOrder.LITTLE_ENDIAN\n  msg match {\n    case Ping(id) => ByteString.newBuilder.putByte(1).putInt(id).result()\n    case Pong(id) => ByteString.newBuilder.putByte(2).putInt(id).result()\n  }\n}\n\ndef fromBytes(bytes: ByteString): Message = {\n  implicit val order = ByteOrder.LITTLE_ENDIAN\n  val it = bytes.iterator\n  it.getByte match {\n    case 1     => Ping(it.getInt)\n    case 2     => Pong(it.getInt)\n    case other => throw new RuntimeException(s\"parse error: expected 1|2 got $other\")\n  }\n} Java copysourcepublic static ByteString toBytes(Message msg) {\n  if (msg instanceof Ping) {\n    final int id = ((Ping) msg).id;\n    return new ByteStringBuilder().putByte((byte) 1).putInt(id, ByteOrder.LITTLE_ENDIAN).result();\n  } else {\n    final int id = ((Pong) msg).id;\n    return new ByteStringBuilder().putByte((byte) 2).putInt(id, ByteOrder.LITTLE_ENDIAN).result();\n  }\n}\n\npublic static Message fromBytes(ByteString bytes) {\n  final ByteIterator it = bytes.iterator();\n  switch (it.getByte()) {\n    case 1:\n      return new Ping(it.getInt(ByteOrder.LITTLE_ENDIAN));\n    case 2:\n      return new Pong(it.getInt(ByteOrder.LITTLE_ENDIAN));\n    default:\n      throw new RuntimeException(\"message format error\");\n  }\n}\nIn this way you can integrate any other serialization library that turns an object into a sequence of bytes.\nThe other operator that we talked about is a little more involved since reversing a framing protocol means that any received chunk of bytes may correspond to zero or more messages. This is best implemented using GraphStage (see also Custom processing with GraphStage).\nScala copysourceval framing = BidiFlow.fromGraph(GraphDSL.create() { b =>\n  implicit val order = ByteOrder.LITTLE_ENDIAN\n\n  def addLengthHeader(bytes: ByteString) = {\n    val len = bytes.length\n    ByteString.newBuilder.putInt(len).append(bytes).result()\n  }\n\n  class FrameParser extends GraphStage[FlowShape[ByteString, ByteString]] {\n\n    val in = Inlet[ByteString](\"FrameParser.in\")\n    val out = Outlet[ByteString](\"FrameParser.out\")\n    override val shape = FlowShape.of(in, out)\n\n    override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {\n\n      // this holds the received but not yet parsed bytes\n      var stash = ByteString.empty\n      // this holds the current message length or -1 if at a boundary\n      var needed = -1\n\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            if (isClosed(in)) run()\n            else pull(in)\n          }\n        })\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = {\n            val bytes = grab(in)\n            stash = stash ++ bytes\n            run()\n          }\n\n          override def onUpstreamFinish(): Unit = {\n            // either we are done\n            if (stash.isEmpty) completeStage()\n            // or we still have bytes to emit\n            // wait with completion and let run() complete when the\n            // rest of the stash has been sent downstream\n            else if (isAvailable(out)) run()\n          }\n        })\n\n      private def run(): Unit = {\n        if (needed == -1) {\n          // are we at a boundary? then figure out next length\n          if (stash.length < 4) {\n            if (isClosed(in)) completeStage()\n            else pull(in)\n          } else {\n            needed = stash.iterator.getInt\n            stash = stash.drop(4)\n            run() // cycle back to possibly already emit the next chunk\n          }\n        } else if (stash.length < needed) {\n          // we are in the middle of a message, need more bytes,\n          // or have to stop if input closed\n          if (isClosed(in)) completeStage()\n          else pull(in)\n        } else {\n          // we have enough to emit at least one message, so do it\n          val emit = stash.take(needed)\n          stash = stash.drop(needed)\n          needed = -1\n          push(out, emit)\n        }\n      }\n    }\n  }\n\n  val outbound = b.add(Flow[ByteString].map(addLengthHeader))\n  val inbound = b.add(Flow[ByteString].via(new FrameParser))\n  BidiShape.fromFlows(outbound, inbound)\n}) Java copysourcepublic static ByteString addLengthHeader(ByteString bytes) {\n  final int len = bytes.size();\n  return new ByteStringBuilder().putInt(len, ByteOrder.LITTLE_ENDIAN).append(bytes).result();\n}\n\npublic static class FrameParser extends GraphStage<FlowShape<ByteString, ByteString>> {\n  public Inlet<ByteString> in = Inlet.create(\"FrameParser.in\");\n  public Outlet<ByteString> out = Outlet.create(\"FrameParser.out\");\n  private FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<ByteString, ByteString> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n\n      // this holds the received but not yet parsed bytes\n      private ByteString stash = emptyByteString();\n      // this holds the current message length or -1 if at a boundary\n      private int needed = -1;\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() throws Exception {\n                ByteString bytes = grab(in);\n                stash = stash.concat(bytes);\n                run();\n              }\n\n              @Override\n              public void onUpstreamFinish() throws Exception {\n                // either we are done\n                if (stash.isEmpty()) completeStage();\n                // or we still have bytes to emit\n                // wait with completion and let run() complete when the\n                // rest of the stash has been sent downstream\n                else if (isAvailable(out)) run();\n              }\n            });\n\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                if (isClosed(in)) run();\n                else pull(in);\n              }\n            });\n      }\n\n      private void run() {\n        if (needed == -1) {\n          // are we at a boundary? then figure out next length\n          if (stash.size() < 4) {\n            if (isClosed(in)) completeStage();\n            else pull(in);\n          } else {\n            needed = stash.iterator().getInt(ByteOrder.LITTLE_ENDIAN);\n            stash = stash.drop(4);\n            run(); // cycle back to possibly already emit the next chunk\n          }\n        } else if (stash.size() < needed) {\n          // we are in the middle of a message, need more bytes\n          // or in is already closed and we cannot pull any more\n          if (isClosed(in)) completeStage();\n          else pull(in);\n        } else {\n          // we have enough to emit at least one message, so do it\n          final ByteString emit = stash.take(needed);\n          stash = stash.drop(needed);\n          needed = -1;\n          push(out, emit);\n        }\n      }\n    };\n  }\n}\n\npublic final BidiFlow<ByteString, ByteString, ByteString, ByteString, NotUsed> framing =\n    BidiFlow.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final FlowShape<ByteString, ByteString> top =\n                  b.add(Flow.of(ByteString.class).map(BidiFlowDocTest::addLengthHeader));\n              final FlowShape<ByteString, ByteString> bottom =\n                  b.add(Flow.of(ByteString.class).via(new FrameParser()));\n              return BidiShape.fromFlows(top, bottom);\n            }));\nWith these implementations we can build a protocol stack and test it:\nScala copysource/* construct protocol stack\n *         +------------------------------------+\n *         | stack                              |\n *         |                                    |\n *         |  +-------+            +---------+  |\n *    ~>   O~~o       |     ~>     |         o~~O    ~>\n * Message |  | codec | ByteString | framing |  | ByteString\n *    <~   O~~o       |     <~     |         o~~O    <~\n *         |  +-------+            +---------+  |\n *         +------------------------------------+\n */\nval stack = codec.atop(framing)\n\n// test it by plugging it into its own inverse and closing the right end\nval pingpong = Flow[Message].collect { case Ping(id) => Pong(id) }\nval flow = stack.atop(stack.reversed).join(pingpong)\nval result = Source((0 to 9).map(Ping(_))).via(flow).limit(20).runWith(Sink.seq)\nAwait.result(result, 1.second) should ===((0 to 9).map(Pong(_))) Java copysource/* construct protocol stack\n *         +------------------------------------+\n *         | stack                              |\n *         |                                    |\n *         |  +-------+            +---------+  |\n *    ~>   O~~o       |     ~>     |         o~~O    ~>\n * Message |  | codec | ByteString | framing |  | ByteString\n *    <~   O~~o       |     <~     |         o~~O    <~\n *         |  +-------+            +---------+  |\n *         +------------------------------------+\n */\nfinal BidiFlow<Message, ByteString, ByteString, Message, NotUsed> stack = codec.atop(framing);\n\n// test it by plugging it into its own inverse and closing the right end\nfinal Flow<Message, Message, NotUsed> pingpong =\n    Flow.of(Message.class)\n        .collect(\n            new PFBuilder<Message, Message>().match(Ping.class, p -> new Pong(p.id)).build());\nfinal Flow<Message, Message, NotUsed> flow = stack.atop(stack.reversed()).join(pingpong);\nfinal CompletionStage<List<Message>> result =\n    Source.from(Arrays.asList(0, 1, 2))\n        .<Message>map(id -> new Ping(id))\n        .via(flow)\n        .grouped(10)\n        .runWith(Sink.<List<Message>>head(), system);\nassertArrayEquals(\n    new Message[] {new Pong(0), new Pong(1), new Pong(2)},\n    result.toCompletableFuture().get(1, TimeUnit.SECONDS).toArray(new Message[0]));\nThis example demonstrates how BidiFlow subgraphs can be hooked together and also turned around with the .reversed.reversed() method. The test simulates both parties of a network communication protocol without actually having to open a network connection—the flows can be connected directly.","title":"Bidirectional Flows"},{"location":"/stream/stream-graphs.html#accessing-the-materialized-value-inside-the-graph","text":"In certain cases it might be necessary to feed back the materialized value of a Graph (partial, closed or backing a Source, Sink, Flow or BidiFlow). This is possible by using builder.materializedValue which gives an Outlet that can be used in the graph as an ordinary source or outlet, and which will eventually emit the materialized value. If the materialized value is needed at more than one place, it is possible to call materializedValue any number of times to acquire the necessary number of outlets.\nScala copysourceimport GraphDSL.Implicits._\nval foldFlow: Flow[Int, Int, Future[Int]] = Flow.fromGraph(GraphDSL.createGraph(Sink.fold[Int, Int](0)(_ + _)) {\n  implicit builder => fold =>\n    FlowShape(fold.in, builder.materializedValue.mapAsync(4)(identity).outlet)\n}) Java copysourcefinal Sink<Integer, CompletionStage<Integer>> foldSink =\n    Sink.fold(\n        0,\n        (a, b) -> {\n          return a + b;\n        });\n\nfinal Flow<CompletionStage<Integer>, Integer, NotUsed> flatten =\n    Flow.<CompletionStage<Integer>>create().mapAsync(4, x -> x);\n\nfinal Flow<Integer, Integer, CompletionStage<Integer>> foldingFlow =\n    Flow.fromGraph(\n        GraphDSL.create(\n            foldSink,\n            (b, fold) -> {\n              return FlowShape.of(\n                  fold.in(), b.from(b.materializedValue()).via(b.add(flatten)).out());\n            }));\nBe careful not to introduce a cycle where the materialized value actually contributes to the materialized value. The following example demonstrates a case where the materialized FutureCompletionStage of a fold is fed back to the fold itself.\nScala copysourceimport GraphDSL.Implicits._\n// This cannot produce any value:\nval cyclicFold: Source[Int, Future[Int]] =\n  Source.fromGraph(GraphDSL.createGraph(Sink.fold[Int, Int](0)(_ + _)) { implicit builder => fold =>\n    // - Fold cannot complete until its upstream mapAsync completes\n    // - mapAsync cannot complete until the materialized Future produced by\n    //   fold completes\n    // As a result this Source will never emit anything, and its materialited\n    // Future will never complete\n    builder.materializedValue.mapAsync(4)(identity) ~> fold\n    SourceShape(builder.materializedValue.mapAsync(4)(identity).outlet)\n  }) Java copysource// This cannot produce any value:\nfinal Source<Integer, CompletionStage<Integer>> cyclicSource =\n    Source.fromGraph(\n        GraphDSL.create(\n            foldSink,\n            (b, fold) -> {\n              // - Fold cannot complete until its upstream mapAsync completes\n              // - mapAsync cannot complete until the materialized Future produced by\n              //   fold completes\n              // As a result this Source will never emit anything, and its materialited\n              // Future will never complete\n              b.from(b.materializedValue()).via(b.add(flatten)).to(fold);\n              return SourceShape.of(b.from(b.materializedValue()).via(b.add(flatten)).out());\n            }));","title":"Accessing the materialized value inside the Graph"},{"location":"/stream/stream-graphs.html#graph-cycles-liveness-and-deadlocks","text":"Cycles in bounded stream topologies need special considerations to avoid potential deadlocks and other liveness issues. This section shows several examples of problems that can arise from the presence of feedback arcs in stream processing graphs.\nIn the following examples runnable graphs are created but do not run because each have some issue and will deadlock after start. Source variable is not defined as the nature and number of element does not matter for described problems.\nThe first example demonstrates a graph that contains a naïve cycle. The graph takes elements from the source, prints them, then broadcasts those elements to a consumer (we just used Sink.ignore for now) and to a feedback arc that is merged back into the main stream via a Merge junction.\nNote The graph DSL allows the connection arrows to be reversed, which is particularly handy when writing cycles—as we will see there are cases where this is very helpful.\nScala copysource// WARNING! The graph below deadlocks!\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  val merge = b.add(Merge[Int](2))\n  val bcast = b.add(Broadcast[Int](2))\n\n  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore\n            merge                    <~                      bcast\n  ClosedShape\n}) Java copysource// WARNING! The graph below deadlocks!\nfinal Flow<Integer, Integer, NotUsed> printFlow =\n    Flow.of(Integer.class)\n        .map(\n            s -> {\n              System.out.println(s);\n              return s;\n            });\n\nRunnableGraph.fromGraph(\n    GraphDSL.create(\n        b -> {\n          final UniformFanInShape<Integer, Integer> merge = b.add(Merge.create(2));\n          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n          final Outlet<Integer> src = b.add(source).out();\n          final FlowShape<Integer, Integer> printer = b.add(printFlow);\n          final SinkShape<Integer> ignore = b.add(Sink.ignore());\n\n          b.from(src).viaFanIn(merge).via(printer).viaFanOut(bcast).to(ignore);\n          b.to(merge).fromFanOut(bcast);\n          return ClosedShape.getInstance();\n        }));\nRunning this we observe that after a few numbers have been printed, no more elements are logged to the console - all processing stops after some time. After some investigation we observe that:\nthrough merging from source we increase the number of elements flowing in the cycle by broadcasting back to the cycle we do not decrease the number of elements in the cycle\nSince Pekko Streams (and Reactive Streams in general) guarantee bounded processing (see the “Buffering” section for more details) it means that only a bounded number of elements are buffered over any time span. Since our cycle gains more and more elements, eventually all of its internal buffers become full, backpressuring source forever. To be able to process more elements from source elements would need to leave the cycle somehow.\nIf we modify our feedback loop by replacing the Merge junction with a MergePreferred we can avoid the deadlock. MergePreferred is unfair as it always tries to consume from a preferred input port if there are elements available before trying the other lower priority input ports. Since we feed back through the preferred port it is always guaranteed that the elements in the cycles can flow.\nScala copysource// WARNING! The graph below stops consuming from \"source\" after a few steps\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  val merge = b.add(MergePreferred[Int](1))\n  val bcast = b.add(Broadcast[Int](2))\n\n  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore\n            merge.preferred              <~                  bcast\n  ClosedShape\n}) Java copysource// WARNING! The graph below stops consuming from \"source\" after a few steps\nRunnableGraph.fromGraph(\n    GraphDSL.create(\n        b -> {\n          final MergePreferredShape<Integer> merge = b.add(MergePreferred.create(1));\n          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n          final Outlet<Integer> src = b.add(source).out();\n          final FlowShape<Integer, Integer> printer = b.add(printFlow);\n          final SinkShape<Integer> ignore = b.add(Sink.ignore());\n\n          b.from(src).viaFanIn(merge).via(printer).viaFanOut(bcast).to(ignore);\n          b.to(merge.preferred()).fromFanOut(bcast);\n          return ClosedShape.getInstance();\n        }));\nIf we run the example we see that the same sequence of numbers are printed over and over again, but the processing does not stop. Hence, we avoided the deadlock, but source is still back-pressured forever, because buffer space is never recovered: the only action we see is the circulation of a couple of initial elements from source.\nNote What we see here is that in certain cases we need to choose between boundedness and liveness. Our first example would not deadlock if there were an infinite buffer in the loop, or vice versa, if the elements in the cycle were balanced (as many elements are removed as many are injected) then there would be no deadlock.\nTo make our cycle both live (not deadlocking) and fair we can introduce a dropping element on the feedback arc. In this case we chose the buffer() operation giving it a dropping strategy OverflowStrategy.dropHead.\nScala copysourceRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  val merge = b.add(Merge[Int](2))\n  val bcast = b.add(Broadcast[Int](2))\n\n  source ~> merge ~> Flow[Int].map { s => println(s); s } ~> bcast ~> Sink.ignore\n      merge <~ Flow[Int].buffer(10, OverflowStrategy.dropHead) <~ bcast\n  ClosedShape\n}) Java copysourceRunnableGraph.fromGraph(\n    GraphDSL.create(\n        b -> {\n          final UniformFanInShape<Integer, Integer> merge = b.add(Merge.create(2));\n          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n          final FlowShape<Integer, Integer> droppyFlow =\n              b.add(Flow.of(Integer.class).buffer(10, OverflowStrategy.dropHead()));\n          final Outlet<Integer> src = b.add(source).out();\n          final FlowShape<Integer, Integer> printer = b.add(printFlow);\n          final SinkShape<Integer> ignore = b.add(Sink.ignore());\n\n          b.from(src).viaFanIn(merge).via(printer).viaFanOut(bcast).to(ignore);\n          b.to(merge).via(droppyFlow).fromFanOut(bcast);\n          return ClosedShape.getInstance();\n        }));\nIf we run this example we see that\nThe flow of elements does not stop, there are always elements printed We see that some of the numbers are printed several times over time (due to the feedback loop) but on average the numbers are increasing in the long term\nThis example highlights that one solution to avoid deadlocks in the presence of potentially unbalanced cycles (cycles where the number of circulating elements are unbounded) is to drop elements. An alternative would be to define a larger buffer with OverflowStrategy.fail which would fail the stream instead of deadlocking it after all buffer space has been consumed.\nAs we discovered in the previous examples, the core problem was the unbalanced nature of the feedback loop. We circumvented this issue by adding a dropping element, but now we want to build a cycle that is balanced from the beginning instead. To achieve this we modify our first graph by replacing the Merge junction with a ZipWith. Since ZipWith takes one element from source and from the feedback arc to inject one element into the cycle, we maintain the balance of elements.\nScala copysource// WARNING! The graph below never processes any elements\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  val zip = b.add(ZipWith[Int, Int, Int]((left, right) => right))\n  val bcast = b.add(Broadcast[Int](2))\n\n  source ~> zip.in0\n  zip.out.map { s => println(s); s } ~> bcast ~> Sink.ignore\n  zip.in1             <~                bcast\n  ClosedShape\n}) Java copysource// WARNING! The graph below never processes any elements\nRunnableGraph.fromGraph(\n    GraphDSL.create(\n        b -> {\n          final FanInShape2<Integer, Integer, Integer> zip =\n              b.add(ZipWith.create((Integer left, Integer right) -> left));\n          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n          final FlowShape<Integer, Integer> printer = b.add(printFlow);\n          final SinkShape<Integer> ignore = b.add(Sink.ignore());\n\n          b.from(b.add(source)).toInlet(zip.in0());\n          b.from(zip.out()).via(printer).viaFanOut(bcast).to(ignore);\n          b.to(zip.in1()).fromFanOut(bcast);\n          return ClosedShape.getInstance();\n        }));\nStill, when we try to run the example it turns out that no element is printed at all! After some investigation we realize that:\nIn order to get the first element from source into the cycle we need an already existing element in the cycle In order to get an initial element in the cycle we need an element from source\nThese two conditions are a typical “chicken-and-egg” problem. The solution is to inject an initial element into the cycle that is independent from source. We do this by using a Concat junction on the backwards arc that injects a single element using Source.single.\nScala copysourceRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  val zip = b.add(ZipWith((left: Int, right: Int) => left))\n  val bcast = b.add(Broadcast[Int](2))\n  val concat = b.add(Concat[Int]())\n  val start = Source.single(0)\n\n  source ~> zip.in0\n  zip.out.map { s => println(s); s } ~> bcast ~> Sink.ignore\n  zip.in1 <~ concat <~ start\n             concat         <~          bcast\n  ClosedShape\n}) Java copysourceRunnableGraph.fromGraph(\n    GraphDSL.create(\n        b -> {\n          final FanInShape2<Integer, Integer, Integer> zip =\n              b.add(ZipWith.create((Integer left, Integer right) -> left));\n          final UniformFanOutShape<Integer, Integer> bcast = b.add(Broadcast.create(2));\n          final UniformFanInShape<Integer, Integer> concat = b.add(Concat.create());\n          final FlowShape<Integer, Integer> printer = b.add(printFlow);\n          final SinkShape<Integer> ignore = b.add(Sink.ignore());\n\n          b.from(b.add(source)).toInlet(zip.in0());\n          b.from(zip.out()).via(printer).viaFanOut(bcast).to(ignore);\n          b.to(zip.in1()).viaFanIn(concat).from(b.add(Source.single(1)));\n          b.to(concat).fromFanOut(bcast);\n          return ClosedShape.getInstance();\n        }));\nWhen we run the above example we see that processing starts and never stops. The important takeaway from this example is that balanced cycles often need an initial “kick-off” element to be injected into the cycle.","title":"Graph cycles, liveness and deadlocks"},{"location":"/stream/stream-composition.html","text":"","title":"Modularity, Composition and Hierarchy"},{"location":"/stream/stream-composition.html#modularity-composition-and-hierarchy","text":"","title":"Modularity, Composition and Hierarchy"},{"location":"/stream/stream-composition.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-composition.html#introduction","text":"Pekko Streams provide a uniform model of stream processing graphs, which allows flexible composition of reusable components. In this chapter we show how these look like from the conceptual and API perspective, demonstrating the modularity aspects of the library.","title":"Introduction"},{"location":"/stream/stream-composition.html#basics-of-composition-and-modularity","text":"Every operator used in Pekko Streams can be imagined as a “box” with input and output ports where elements to be processed arrive and leave the operator. In this view, a Source is nothing else than a “box” with a single output port, or, a BidiFlow is a “box” with exactly two input and two output ports. In the figure below we illustrate the most commonly used operators viewed as “boxes”.\nThe linear operators are Source, Sink and Flow, as these can be used to compose strict chains of operators. Fan-in and Fan-out operators have usually multiple input or multiple output ports, therefore they allow to build more complex graph layouts, not only chains. BidiFlow operators are usually useful in IO related tasks, where there are input and output channels to be handled. Due to the specific shape of BidiFlow it is easy to stack them on top of each other to build a layered protocol for example. The TLS support in Pekko is for example implemented as a BidiFlow.\nThese reusable components already allow the creation of complex processing networks. What we have seen so far does not implement modularity though. It is desirable for example to package up a larger graph entity into a reusable component which hides its internals only exposing the ports that are meant to the users of the module to interact with. One good example is the Http server component, which is encoded internally as a BidiFlow which interfaces with the client TCP connection using an input-output port pair accepting and sending ByteString s, while its upper ports emit and receive HttpRequest and HttpResponse instances.\nThe following figure demonstrates various composite operators, that contain various other type of operators internally, but hiding them behind a shape that looks like a Source, Flow, etc.\nOne interesting example above is a Flow which is composed of a disconnected Sink and Source. This can be achieved by using the fromSinkAndSource() constructor method on Flow which takes the two parts as parameters.\nPlease note that when combining a Flow using that method, the termination signals are not carried “through” as the Sink and Source are assumed to be fully independent. If however you want to construct a Flow like this but need the termination events to trigger “the other side” of the composite flow, you can use Flow.fromSinkAndSourceCoupled or Flow.fromSinkAndSourceCoupledMat which does just that. For example the cancellation of the composite flows source-side will then lead to completion of its sink-side. Read FlowFlow’s API documentation for a detailed explanation how this works.\nThe example BidiFlow demonstrates that internally a module can be of arbitrary complexity, and the exposed ports can be wired in flexible ways. The only constraint is that all the ports of enclosed modules must be either connected to each other, or exposed as interface ports, and the number of such ports needs to match the requirement of the shape, for example a Source allows only one exposed output port, the rest of the internal ports must be properly connected.\nThese mechanics allow arbitrary nesting of modules. For example the following figure demonstrates a RunnableGraph that is built from a composite Source and a composite Sink (which in turn contains a composite Flow).\nThe above diagram contains one more shape that we have not seen yet, which is called RunnableGraph. It turns out, that if we wire all exposed ports together, so that no more open ports remain, we get a module that is closed. This is what the RunnableGraph class represents. This is the shape that a Materializer can take and turn into a network of running entities that perform the task described. In fact, a RunnableGraph is a module itself, and (maybe somewhat surprisingly) it can be used as part of larger graphs. It is rarely useful to embed a closed graph shape in a larger graph (since it becomes an isolated island as there are no open port for communication with the rest of the graph), but this demonstrates the uniform underlying model.\nIf we try to build a code snippet that corresponds to the above diagram, our first try might look like this:\nScala copysourceSource.single(0).map(_ + 1).filter(_ != 0).map(_ - 2).to(Sink.fold(0)(_ + _))\n\n// ... where is the nesting? Java copysourceSource.single(0)\n    .map(i -> i + 1)\n    .filter(i -> i != 0)\n    .map(i -> i - 2)\n    .to(Sink.fold(0, (acc, i) -> acc + i));\n\n// ... where is the nesting?\nIt is clear however that there is no nesting present in our first attempt. Since the library cannot figure out where we intended to put composite module boundaries, it is our responsibility to do that. If we are using the DSL provided by the Flow, Source, Sink classes then nesting can be achieved by calling one of the methods withAttributes() or named() (where the latter is a shorthand for adding a name attribute).\nThe following code demonstrates how to achieve the desired nesting:\nScala copysourceval nestedSource =\n  Source\n    .single(0) // An atomic source\n    .map(_ + 1) // an atomic processing stage\n    .named(\"nestedSource\") // wraps up the current Source and gives it a name\n\nval nestedFlow =\n  Flow[Int]\n    .filter(_ != 0) // an atomic processing stage\n    .map(_ - 2) // another atomic processing stage\n    .named(\"nestedFlow\") // wraps up the Flow, and gives it a name\n\nval nestedSink =\n  nestedFlow\n    .to(Sink.fold(0)(_ + _)) // wire an atomic sink to the nestedFlow\n    .named(\"nestedSink\") // wrap it up\n\n// Create a RunnableGraph\nval runnableGraph = nestedSource.to(nestedSink) Java copysourcefinal Source<Integer, NotUsed> nestedSource =\n    Source.single(0) // An atomic source\n        .map(i -> i + 1) // an atomic processing stage\n        .named(\"nestedSource\"); // wraps up the current Source and gives it a name\n\nfinal Flow<Integer, Integer, NotUsed> nestedFlow =\n    Flow.of(Integer.class)\n        .filter(i -> i != 0) // an atomic processing stage\n        .map(i -> i - 2) // another atomic processing stage\n        .named(\"nestedFlow\"); // wraps up the Flow, and gives it a name\n\nfinal Sink<Integer, NotUsed> nestedSink =\n    nestedFlow\n        .to(Sink.fold(0, (acc, i) -> acc + i)) // wire an atomic sink to the nestedFlow\n        .named(\"nestedSink\"); // wrap it up\n\n// Create a RunnableGraph\nfinal RunnableGraph<NotUsed> runnableGraph = nestedSource.to(nestedSink);\nOnce we have hidden the internals of our components, they act like any other built-in component of similar shape. If we hide some of the internals of our composites, the result looks just like if any other predefine component has been used:\nIf we look at usage of built-in components, and our custom components, there is no difference in usage as the code snippet below demonstrates.\nScala copysource// Create a RunnableGraph from our components\nval runnableGraph = nestedSource.to(nestedSink)\n\n// Usage is uniform, no matter if modules are composite or atomic\nval runnableGraph2 = Source.single(0).to(Sink.fold(0)(_ + _)) Java copysource// Create a RunnableGraph from our components\nfinal RunnableGraph<NotUsed> runnableGraph = nestedSource.to(nestedSink);\n\n// Usage is uniform, no matter if modules are composite or atomic\nfinal RunnableGraph<NotUsed> runnableGraph2 =\n    Source.single(0).to(Sink.fold(0, (acc, i) -> acc + i));","title":"Basics of composition and modularity"},{"location":"/stream/stream-composition.html#composing-complex-systems","text":"In the previous section we explored the possibility of composition, and hierarchy, but we stayed away from non-linear, generalized operators. There is nothing in Pekko Streams though that enforces that stream processing layouts can only be linear. The DSL for Source and friends is optimized for creating such linear chains, as they are the most common in practice. There is a more advanced DSL for building complex graphs, that can be used if more flexibility is needed. We will see that the difference between the two DSLs is only on the surface: the concepts they operate on are uniform across all DSLs and fit together nicely.\nAs a first example, let’s look at a more complex layout:\nThe diagram shows a RunnableGraph (remember, if there are no unwired ports, the graph is closed, and therefore can be materialized) that encapsulates a non-trivial stream processing network. It contains fan-in, fan-out operators, directed and non-directed cycles. The runnable() method of the GraphDSL object allows the creation of a general, closed, and runnable graph. For example the network on the diagram can be realized like this:\nScala copysourceimport GraphDSL.Implicits._\nRunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n  val A: Outlet[Int]                  = builder.add(Source.single(0)).out\n  val B: UniformFanOutShape[Int, Int] = builder.add(Broadcast[Int](2))\n  val C: UniformFanInShape[Int, Int]  = builder.add(Merge[Int](2))\n  val D: FlowShape[Int, Int]          = builder.add(Flow[Int].map(_ + 1))\n  val E: UniformFanOutShape[Int, Int] = builder.add(Balance[Int](2))\n  val F: UniformFanInShape[Int, Int]  = builder.add(Merge[Int](2))\n  val G: Inlet[Any]                   = builder.add(Sink.foreach(println)).in\n\n                C     <~      F\n  A  ~>  B  ~>  C     ~>      F\n         B  ~>  D  ~>  E  ~>  F\n                       E  ~>  G\n\n  ClosedShape\n}) Java copysourceRunnableGraph.fromGraph(\n    GraphDSL.create(\n        builder -> {\n          final Outlet<Integer> A = builder.add(Source.single(0)).out();\n          final UniformFanOutShape<Integer, Integer> B = builder.add(Broadcast.create(2));\n          final UniformFanInShape<Integer, Integer> C = builder.add(Merge.create(2));\n          final FlowShape<Integer, Integer> D =\n              builder.add(Flow.of(Integer.class).map(i -> i + 1));\n          final UniformFanOutShape<Integer, Integer> E = builder.add(Balance.create(2));\n          final UniformFanInShape<Integer, Integer> F = builder.add(Merge.create(2));\n          final Inlet<Integer> G = builder.add(Sink.<Integer>foreach(System.out::println)).in();\n\n          builder.from(F).toFanIn(C);\n          builder.from(A).viaFanOut(B).viaFanIn(C).toFanIn(F);\n          builder.from(B).via(D).viaFanOut(E).toFanIn(F);\n          builder.from(E).toInlet(G);\n          return ClosedShape.getInstance();\n        }));\nIn the code above we used the implicit port numbering feature (to make the graph more readable and similar to the diagram) and we imported Source s, Sink s and Flow s explicitly. It is possible to refer to the ports explicitly, and it is not necessary to import our linear operators via add(), so another version might look like this:\nScala copysourceimport GraphDSL.Implicits._\nRunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n  val B = builder.add(Broadcast[Int](2))\n  val C = builder.add(Merge[Int](2))\n  val E = builder.add(Balance[Int](2))\n  val F = builder.add(Merge[Int](2))\n\n  Source.single(0) ~> B.in; B.out(0) ~> C.in(1); C.out ~> F.in(0)\n  C.in(0) <~ F.out\n\n  B.out(1).map(_ + 1) ~> E.in; E.out(0) ~> F.in(1)\n  E.out(1) ~> Sink.foreach(println)\n  ClosedShape\n}) Java copysourceRunnableGraph.fromGraph(\n    GraphDSL.create(\n        builder -> {\n          final SourceShape<Integer> A = builder.add(Source.single(0));\n          final UniformFanOutShape<Integer, Integer> B = builder.add(Broadcast.create(2));\n          final UniformFanInShape<Integer, Integer> C = builder.add(Merge.create(2));\n          final FlowShape<Integer, Integer> D =\n              builder.add(Flow.of(Integer.class).map(i -> i + 1));\n          final UniformFanOutShape<Integer, Integer> E = builder.add(Balance.create(2));\n          final UniformFanInShape<Integer, Integer> F = builder.add(Merge.create(2));\n          final SinkShape<Integer> G = builder.add(Sink.foreach(System.out::println));\n\n          builder.from(F.out()).toInlet(C.in(0));\n          builder.from(A).toInlet(B.in());\n          builder.from(B.out(0)).toInlet(C.in(1));\n          builder.from(C.out()).toInlet(F.in(0));\n          builder.from(B.out(1)).via(D).toInlet(E.in());\n          builder.from(E.out(0)).toInlet(F.in(1));\n          builder.from(E.out(1)).to(G);\n          return ClosedShape.getInstance();\n        }));\nSimilar to the case in the first section, so far we have not considered modularity. We created a complex graph, but the layout is flat, not modularized. We will modify our example, and create a reusable component with the graph DSL. The way to do it is to use the create() factory method on GraphDSL. If we remove the sources and sinks from the previous example, what remains is a partial graph:\nWe can recreate a similar graph in code, using the DSL in a similar way than before:\nScala copysourceimport GraphDSL.Implicits._\nval partial = GraphDSL.create() { implicit builder =>\n  val B = builder.add(Broadcast[Int](2))\n  val C = builder.add(Merge[Int](2))\n  val E = builder.add(Balance[Int](2))\n  val F = builder.add(Merge[Int](2))\n\n                                   C  <~  F\n  B  ~>                            C  ~>  F\n  B  ~>  Flow[Int].map(_ + 1)  ~>  E  ~>  F\n  FlowShape(B.in, E.out(1))\n}.named(\"partial\") Java copysourcefinal Graph<FlowShape<Integer, Integer>, NotUsed> partial =\n    GraphDSL.create(\n        builder -> {\n          final UniformFanOutShape<Integer, Integer> B = builder.add(Broadcast.create(2));\n          final UniformFanInShape<Integer, Integer> C = builder.add(Merge.create(2));\n          final UniformFanOutShape<Integer, Integer> E = builder.add(Balance.create(2));\n          final UniformFanInShape<Integer, Integer> F = builder.add(Merge.create(2));\n\n          builder.from(F.out()).toInlet(C.in(0));\n          builder.from(B).viaFanIn(C).toFanIn(F);\n          builder\n              .from(B)\n              .via(builder.add(Flow.of(Integer.class).map(i -> i + 1)))\n              .viaFanOut(E)\n              .toFanIn(F);\n\n          return new FlowShape<Integer, Integer>(B.in(), E.out(1));\n        });\nThe only new addition is the return value of the builder block, which is a Shape. All operators (including Source, BidiFlow, etc.) have a shape, which encodes the typed ports of the module. In our example there is exactly one input and output port left, so we can declare it to have a FlowShape by returning an instance of it. While it is possible to create new Shape types, it is usually recommended to use one of the matching built-in ones.\nThe resulting graph is already a properly wrapped module, so there is no need to call named() to encapsulate the graph, but it is a good practice to give names to modules to help debugging.\nSince our partial graph has the right shape, it can be already used in the simpler, linear DSL:\nScala copysourceSource.single(0).via(partial).to(Sink.ignore) Java copysourceSource.single(0).via(partial).to(Sink.ignore());\nIt is not possible to use it as a Flow yet, though (i.e. we cannot call .filter() on it), but Flow has a fromGraph() method that adds the DSL to a FlowShape. There are similar methods on Source, Sink and BidiShape, so it is easy to get back to the simpler DSL if an operator has the right shape. For convenience, it is also possible to skip the partial graph creation, and use one of the convenience creator methods. To demonstrate this, we will create the following graph:\nThe code version of the above closed graph might look like this:\nScala copysource// Convert the partial graph of FlowShape to a Flow to get\n// access to the fluid DSL (for example to be able to call .filter())\nval flow = Flow.fromGraph(partial)\n\n// Simple way to create a graph backed Source\nval source = Source.fromGraph( GraphDSL.create() { implicit builder =>\n  val merge = builder.add(Merge[Int](2))\n  Source.single(0)      ~> merge\n  Source(List(2, 3, 4)) ~> merge\n\n  // Exposing exactly one output port\n  SourceShape(merge.out)\n})\n\n// Building a Sink with a nested Flow, using the fluid DSL\nval sink = {\n  val nestedFlow = Flow[Int].map(_ * 2).drop(10).named(\"nestedFlow\")\n  nestedFlow.to(Sink.head)\n}\n\n// Putting all together\nval closed = source.via(flow.filter(_ > 1)).to(sink) Java copysource// Convert the partial graph of FlowShape to a Flow to get\n// access to the fluid DSL (for example to be able to call .filter())\nfinal Flow<Integer, Integer, NotUsed> flow = Flow.fromGraph(partial);\n\n// Simple way to create a graph backed Source\nfinal Source<Integer, NotUsed> source =\n    Source.fromGraph(\n        GraphDSL.create(\n            builder -> {\n              final UniformFanInShape<Integer, Integer> merge = builder.add(Merge.create(2));\n              builder.from(builder.add(Source.single(0))).toFanIn(merge);\n              builder.from(builder.add(Source.from(Arrays.asList(2, 3, 4)))).toFanIn(merge);\n              // Exposing exactly one output port\n              return new SourceShape<Integer>(merge.out());\n            }));\n\n// Building a Sink with a nested Flow, using the fluid DSL\nfinal Sink<Integer, NotUsed> sink =\n    Flow.of(Integer.class).map(i -> i * 2).drop(10).named(\"nestedFlow\").to(Sink.head());\n\n// Putting all together\nfinal RunnableGraph<NotUsed> closed = source.via(flow.filter(i -> i > 1)).to(sink);\nNote All graph builder sections check if the resulting graph has all ports connected except the exposed ones and will throw an exception if this is violated.\nWe are still in debt of demonstrating that RunnableGraph is a component like any other, which can be embedded in graphs. In the following snippet we embed one closed graph in another:\nScala copysourceval closed1 = Source.single(0).to(Sink.foreach(println))\nval closed2 = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n  val embeddedClosed: ClosedShape = builder.add(closed1)\n  // …\n  embeddedClosed\n}) Java copysourcefinal RunnableGraph<NotUsed> closed1 = Source.single(0).to(Sink.foreach(System.out::println));\nfinal RunnableGraph<NotUsed> closed2 =\n    RunnableGraph.fromGraph(\n        GraphDSL.create(\n            builder -> {\n              final ClosedShape embeddedClosed = builder.add(closed1);\n              return embeddedClosed; // Could return ClosedShape.getInstance()\n            }));\nThe type of the imported module indicates that the imported module has a ClosedShape, and so we are not able to wire it to anything else inside the enclosing closed graph. Nevertheless, this “island” is embedded properly, and will be materialized just like any other module that is part of the graph.\nAs we have demonstrated, the two DSLs are fully interoperable, as they encode a similar nested structure of “boxes with ports”, it is only the DSLs that differ to be as much powerful as possible on the given abstraction level. It is possible to embed complex graphs in the fluid DSL, and it is just as easy to import and embed a Flow, etc, in a larger, complex structure.\nWe have also seen, that every module has a Shape (for example a Sink has a SinkShape) independently which DSL was used to create it. This uniform representation enables the rich composability of various stream processing entities in a convenient way.","title":"Composing complex systems"},{"location":"/stream/stream-composition.html#materialized-values","text":"After realizing that RunnableGraph is nothing more than a module with no unused ports (it is an island), it becomes clear that after materialization the only way to communicate with the running stream processing logic is via some side-channel. This side channel is represented as a materialized value. The situation is similar to Actor s, where the Props instance describes the actor logic, but it is the call to actorOf() that creates an actually running actor, and returns an ActorRef that can be used to communicate with the running actor itself. Since the Props can be reused, each call will return a different reference.\nWhen it comes to streams, each materialization creates a new running network corresponding to the blueprint that was encoded in the provided RunnableGraph. To be able to interact with the running network, each materialization needs to return a different object that provides the necessary interaction capabilities. In other words, the RunnableGraph can be seen as a factory, which creates:\na network of running processing entities, inaccessible from the outside a materialized value, optionally providing a controlled interaction capability with the network\nUnlike actors though, each of the operators might provide a materialized value, so when we compose multiple operators or modules, we need to combine the materialized value as well (there are default rules which make this easier, for example to() and via() takes care of the most common case of taking the materialized value to the left. See Combining materialized values for details). We demonstrate how this works by a code example and a diagram which graphically demonstrates what is happening.\nThe propagation of the individual materialized values from the enclosed modules towards the top will look like this:\nTo implement the above, first, we create a composite Source, where the enclosed Source have a materialized type of Promise[[Option[Int]] CompletableFuture<Optional<Integer>>>. By using the combiner function Keep.left, the resulting materialized type is of the nested module (indicated by the color red on the diagram):\nScala copysource// Materializes to Promise[Option[Int]]                                   (red)\nval source: Source[Int, Promise[Option[Int]]] = Source.maybe[Int]\n\n// Materializes to NotUsed                                               (black)\nval flow1: Flow[Int, Int, NotUsed] = Flow[Int].take(100)\n\n// Materializes to Promise[Int]                                          (red)\nval nestedSource: Source[Int, Promise[Option[Int]]] =\n  source.viaMat(flow1)(Keep.left).named(\"nestedSource\") Java copysource// Materializes to CompletableFuture<Optional<Integer>>                   (red)\nfinal Source<Integer, CompletableFuture<Optional<Integer>>> source = Source.<Integer>maybe();\n\n// Materializes to NotUsed                                                (black)\nfinal Flow<Integer, Integer, NotUsed> flow1 = Flow.of(Integer.class).take(100);\n\n// Materializes to CompletableFuture<Optional<Integer>>                  (red)\nfinal Source<Integer, CompletableFuture<Optional<Integer>>> nestedSource =\n    source.viaMat(flow1, Keep.left()).named(\"nestedSource\");\nNext, we create a composite Flow from two smaller components. Here, the second enclosed Flow has a materialized type of Future[OutgoingConnection] CompletionStage<OutgoingConnection>, and we propagate this to the parent by using Keep.right as the combiner function (indicated by the color yellow on the diagram):\nScala copysource// Materializes to NotUsed                                                (orange)\nval flow2: Flow[Int, ByteString, NotUsed] = Flow[Int].map { i =>\n  ByteString(i.toString)\n}\n\n// Materializes to Future[OutgoingConnection]                             (yellow)\nval flow3: Flow[ByteString, ByteString, Future[OutgoingConnection]] =\n  Tcp(system).outgoingConnection(\"localhost\", 8080)\n\n// Materializes to Future[OutgoingConnection]                             (yellow)\nval nestedFlow: Flow[Int, ByteString, Future[OutgoingConnection]] =\n  flow2.viaMat(flow3)(Keep.right).named(\"nestedFlow\") Java copysource// Materializes to NotUsed                                                (orange)\nfinal Flow<Integer, ByteString, NotUsed> flow2 =\n    Flow.of(Integer.class).map(i -> ByteString.fromString(i.toString()));\n\n// Materializes to Future<OutgoingConnection>                             (yellow)\nfinal Flow<ByteString, ByteString, CompletionStage<OutgoingConnection>> flow3 =\n    Tcp.get(system).outgoingConnection(\"localhost\", 8080);\n\n// Materializes to Future<OutgoingConnection>                             (yellow)\nfinal Flow<Integer, ByteString, CompletionStage<OutgoingConnection>> nestedFlow =\n    flow2.viaMat(flow3, Keep.right()).named(\"nestedFlow\");\nAs a third step, we create a composite Sink, using our nestedFlow as a building block. In this snippet, both the enclosed Flow and the folding Sink has a materialized value that is interesting for us, so we use Keep.both to get a Pair of them as the materialized type of nestedSink (indicated by the color blue on the diagram)\nScala copysource// Materializes to Future[String]                                         (green)\nval sink: Sink[ByteString, Future[String]] = Sink.fold(\"\")(_ + _.utf8String)\n\n// Materializes to (Future[OutgoingConnection], Future[String])           (blue)\nval nestedSink: Sink[Int, (Future[OutgoingConnection], Future[String])] =\n  nestedFlow.toMat(sink)(Keep.both) Java copysource// Materializes to Future<String>                                         (green)\nfinal Sink<ByteString, CompletionStage<String>> sink =\n    Sink.<String, ByteString>fold(\"\", (acc, i) -> acc + i.utf8String());\n\n// Materializes to Pair<Future<OutgoingConnection>, Future<String>>       (blue)\nfinal Sink<Integer, Pair<CompletionStage<OutgoingConnection>, CompletionStage<String>>>\n    nestedSink = nestedFlow.toMat(sink, Keep.both());\nAs the last example, we wire together nestedSource and nestedSink and we use a custom combiner function to create a yet another materialized type of the resulting RunnableGraph. This combiner function ignores the Future[String] CompletionStage<String> part, and wraps the other two values in a custom case class MyClass (indicated by color purple on the diagram):\nScala copysourcecase class MyClass(private val p: Promise[Option[Int]], conn: OutgoingConnection) {\n  def close() = p.trySuccess(None)\n}\n\ndef f(p: Promise[Option[Int]], rest: (Future[OutgoingConnection], Future[String])): Future[MyClass] = {\n\n  val connFuture = rest._1\n  connFuture.map(MyClass(p, _))\n}\n\n// Materializes to Future[MyClass]                                        (purple)\nval runnableGraph: RunnableGraph[Future[MyClass]] =\n  nestedSource.toMat(nestedSink)(f) Java copysourcestatic class MyClass {\n  private CompletableFuture<Optional<Integer>> p;\n  private OutgoingConnection conn;\n\n  public MyClass(CompletableFuture<Optional<Integer>> p, OutgoingConnection conn) {\n    this.p = p;\n    this.conn = conn;\n  }\n\n  public void close() {\n    p.complete(Optional.empty());\n  }\n}\n\nstatic class Combiner {\n  static CompletionStage<MyClass> f(\n      CompletableFuture<Optional<Integer>> p,\n      Pair<CompletionStage<OutgoingConnection>, CompletionStage<String>> rest) {\n    return rest.first().thenApply(c -> new MyClass(p, c));\n  }\n} copysource// Materializes to Future<MyClass>                                        (purple)\nfinal RunnableGraph<CompletionStage<MyClass>> runnableGraph =\n    nestedSource.toMat(nestedSink, Combiner::f);\nNote The nested structure in the above example is not necessary for combining the materialized values, it demonstrates how the two features work together. See Combining materialized values for further examples of combining materialized values without nesting and hierarchy involved.","title":"Materialized values"},{"location":"/stream/stream-composition.html#attributes","text":"We have seen that we can use named() to introduce a nesting level in the fluid DSL and also explicit nesting by using create() from GraphDSL. Apart from having the effect of adding a nesting level, named() is actually a shorthand for calling addAttributes(Attributes.name(\"someName\")), adding the name attribute to the graph.\nAttributes provide a way to fine-tune certain aspects of the materialized running entity. Attributes are inherited by nested modules, unless they override them with a custom value. This means the attribute specified closest to the operator in the graph will be the one that is in effect for that operator.\nAnother example of an attribute is the inputBuffer attribute which has the main purpose to provide control over buffer sizes for asynchronous boundaries (see Buffers for asynchronous operators).\nThe code below, a modification of an earlier example sets the inputBuffer attribute on certain modules, but not on others. Note that this is only to show how attributes inheritance works, the actual inputBuffer attribute does not have any specific effect when running these streams:\nScala copysourceimport Attributes._\nval nestedSource =\n  Source.single(0).map(_ + 1).named(\"nestedSource\") // Wrap, no inputBuffer set\n\nval nestedFlow =\n  Flow[Int]\n    .filter(_ != 0)\n    .via(Flow[Int].map(_ - 2).withAttributes(inputBuffer(4, 4))) // override\n    .named(\"nestedFlow\") // Wrap, no inputBuffer set\n\nval nestedSink =\n  nestedFlow\n    .to(Sink.fold(0)(_ + _)) // wire an atomic sink to the nestedFlow\n    .withAttributes(name(\"nestedSink\") and inputBuffer(3, 3)) // override Java copysourcefinal Source<Integer, NotUsed> nestedSource =\n    Source.single(0).map(i -> i + 1).named(\"nestedSource\"); // Wrap, no inputBuffer set\n\nfinal Flow<Integer, Integer, NotUsed> nestedFlow =\n    Flow.of(Integer.class)\n        .filter(i -> i != 0)\n        .via(\n            Flow.of(Integer.class)\n                .map(i -> i - 2)\n                .withAttributes(Attributes.inputBuffer(4, 4))) // override\n        .named(\"nestedFlow\"); // Wrap, no inputBuffer set\n\nfinal Sink<Integer, NotUsed> nestedSink =\n    nestedFlow\n        .to(Sink.fold(0, (acc, i) -> acc + i)) // wire an atomic sink to the nestedFlow\n        .withAttributes(\n            Attributes.name(\"nestedSink\").and(Attributes.inputBuffer(3, 3))); // override\nThe effect is, that each module inherits the inputBuffer attribute from its enclosing parent, unless it has the same attribute explicitly set. nestedSource gets the default attributes from the materializer itself. nestedSink on the other hand has this attribute set, so it will be used by all nested modules. nestedFlow will inherit from nestedSink except the map operator which has again an explicitly provided attribute overriding the inherited one.\nThis diagram illustrates the inheritance process for the example code (representing the materializer default attributes as the color red, the attributes set on nestedSink as blue and the attributes set on nestedFlow as green).","title":"Attributes"},{"location":"/stream/stream-rate.html","text":"","title":"Buffers and working with rate"},{"location":"/stream/stream-rate.html#buffers-and-working-with-rate","text":"","title":"Buffers and working with rate"},{"location":"/stream/stream-rate.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-rate.html#introduction","text":"When upstream and downstream rates differ, especially when the throughput has spikes, it can be useful to introduce buffers in a stream. In this chapter we cover how buffers are used in Pekko Streams.","title":"Introduction"},{"location":"/stream/stream-rate.html#buffers-for-asynchronous-operators","text":"In this section we will discuss internal buffers that are introduced as an optimization when using asynchronous operators.\nTo run an operator asynchronously it has to be marked explicitly as such using the `.async``.async()` method. Being run asynchronously means that an operator, after handing out an element to its downstream consumer is able to immediately process the next message. To demonstrate what we mean by this, let’s take a look at the following example:\nScala copysourceSource(1 to 3)\n  .map { i =>\n    println(s\"A: $i\"); i\n  }\n  .async\n  .map { i =>\n    println(s\"B: $i\"); i\n  }\n  .async\n  .map { i =>\n    println(s\"C: $i\"); i\n  }\n  .async\n  .runWith(Sink.ignore) Java copysourceSource.from(Arrays.asList(1, 2, 3))\n    .map(\n        i -> {\n          System.out.println(\"A: \" + i);\n          return i;\n        })\n    .async()\n    .map(\n        i -> {\n          System.out.println(\"B: \" + i);\n          return i;\n        })\n    .async()\n    .map(\n        i -> {\n          System.out.println(\"C: \" + i);\n          return i;\n        })\n    .async()\n    .runWith(Sink.ignore(), system);\nRunning the above example, one of the possible outputs looks like this:\nA: 1\nA: 2\nB: 1\nA: 3\nB: 2\nC: 1\nB: 3\nC: 2\nC: 3\nNote that the order is not A:1, B:1, C:1, A:2, B:2, C:2, which would correspond to the normal fused synchronous execution model of flows where an element completely passes through the processing pipeline before the next element enters the flow. The next element is processed by an asynchronous operator as soon as it has emitted the previous one.\nWhile pipelining in general increases throughput, in practice there is a cost of passing an element through the asynchronous (and therefore thread crossing) boundary which is significant. To amortize this cost Pekko Streams uses a windowed, batching backpressure strategy internally. It is windowed because as opposed to a Stop-And-Wait protocol multiple elements might be “in-flight” concurrently with requests for elements. It is also batching because a new element is not immediately requested once an element has been drained from the window-buffer but multiple elements are requested after multiple elements have been drained. This batching strategy reduces the communication cost of propagating the backpressure signal through the asynchronous boundary.\nWhile this internal protocol is mostly invisible to the user (apart from its throughput increasing effects) there are situations when these details get exposed. In all of our previous examples we always assumed that the rate of the processing chain is strictly coordinated through the backpressure signal causing all operators to process no faster than the throughput of the connected chain. There are tools in Pekko Streams however that enable the rates of different segments of a processing chain to be “detached” or to define the maximum throughput of the stream through external timing sources. These situations are exactly those where the internal batching buffering strategy suddenly becomes non-transparent.","title":"Buffers for asynchronous operators"},{"location":"/stream/stream-rate.html#internal-buffers-and-their-effect","text":"As we have explained, for performance reasons Pekko Streams introduces a buffer for every asynchronous operator. The purpose of these buffers is solely optimization, in fact the size of 1 would be the most natural choice if there would be no need for throughput improvements. Therefore it is recommended to keep these buffer sizes small, and increase them only to a level suitable for the throughput requirements of the application. Default buffer sizes can be set through configuration:\npekko.stream.materializer.max-input-buffer-size = 16\nAlternatively they can be set per stream by adding an attribute to the complete RunnableGraph or on smaller segments of the stream it is possible by defining a separate `Flow``Flow` with these attributes:\nScala copysourceval section = Flow[Int].map(_ * 2).async.addAttributes(Attributes.inputBuffer(initial = 1, max = 1)) // the buffer size of this map is 1\nval flow = section.via(Flow[Int].map(_ / 2)).async // the buffer size of this map is the default\nval runnableGraph =\n  Source(1 to 10).via(flow).to(Sink.foreach(elem => println(elem)))\n\nval withOverriddenDefaults = runnableGraph.withAttributes(Attributes.inputBuffer(initial = 64, max = 64)) Java copysourcefinal Flow<Integer, Integer, NotUsed> flow1 =\n    Flow.of(Integer.class)\n        .map(elem -> elem * 2)\n        .async()\n        .addAttributes(Attributes.inputBuffer(1, 1)); // the buffer size of this map is 1\nfinal Flow<Integer, Integer, NotUsed> flow2 =\n    flow1\n        .via(Flow.of(Integer.class).map(elem -> elem / 2))\n        .async(); // the buffer size of this map is the value from the surrounding graph it is\n// used in\nfinal RunnableGraph<NotUsed> runnableGraph =\n    Source.range(1, 10).via(flow1).to(Sink.foreach(elem -> System.out.println(elem)));\n\nfinal RunnableGraph<NotUsed> withOverridenDefaults =\n    runnableGraph.withAttributes(Attributes.inputBuffer(64, 64));\nHere is an example of a code that demonstrate some of the issues caused by internal buffers:\nScala copysourceimport scala.concurrent.duration._\ncase class Tick()\n\nRunnableGraph.fromGraph(GraphDSL.create() { implicit b =>\n  import GraphDSL.Implicits._\n\n  // this is the asynchronous stage in this graph\n  val zipper = b.add(ZipWith[Tick, Int, Int]((tick, count) => count).async)\n\n  Source.tick(initialDelay = 3.second, interval = 3.second, Tick()) ~> zipper.in0\n\n  Source\n    .tick(initialDelay = 1.second, interval = 1.second, \"message!\")\n    .conflateWithSeed(seed = _ => 1)((count, _) => count + 1) ~> zipper.in1\n\n  zipper.out ~> Sink.foreach(println)\n  ClosedShape\n}) Java copysourcefinal Duration oneSecond = Duration.ofSeconds(1);\nfinal Source<String, Cancellable> msgSource = Source.tick(oneSecond, oneSecond, \"message!\");\nfinal Source<String, Cancellable> tickSource =\n    Source.tick(oneSecond.multipliedBy(3), oneSecond.multipliedBy(3), \"tick\");\nfinal Flow<String, Integer, NotUsed> conflate =\n    Flow.of(String.class).conflateWithSeed(first -> 1, (count, elem) -> count + 1);\n\nRunnableGraph.fromGraph(\n        GraphDSL.create(\n            b -> {\n              // this is the asynchronous stage in this graph\n              final FanInShape2<String, Integer, Integer> zipper =\n                  b.add(ZipWith.create((String tick, Integer count) -> count).async());\n              b.from(b.add(msgSource)).via(b.add(conflate)).toInlet(zipper.in1());\n              b.from(b.add(tickSource)).toInlet(zipper.in0());\n              b.from(zipper.out()).to(b.add(Sink.foreach(elem -> System.out.println(elem))));\n              return ClosedShape.getInstance();\n            }))\n    .run(system);\nRunning the above example one would expect the number 3 to be printed in every 3 seconds (the conflateWithSeed step here is configured so that it counts the number of elements received before the downstream `ZipWith``ZipWith` consumes them). What is being printed is different though, we will see the number 1. The reason for this is the internal buffer which is by default 16 elements large, and prefetches elements before the `ZipWith``ZipWith` starts consuming them. It is possible to fix this issue by changing the buffer size of `ZipWith``ZipWith` to 1. We will still see a leading 1 though which is caused by an initial prefetch of the `ZipWith``ZipWith` element.\nNote In general, when time or rate driven operators exhibit strange behavior, one of the first solutions to try should be to decrease the input buffer of the affected elements to 1.","title":"Internal buffers and their effect"},{"location":"/stream/stream-rate.html#buffers-in-pekko-streams","text":"In this section we will discuss explicit user defined buffers that are part of the domain logic of the stream processing pipeline of an application.\nThe example below will ensure that 1000 jobs (but not more) are dequeued from an external (imaginary) system and stored locally in memory - relieving the external system:\nScala copysource// Getting a stream of jobs from an imaginary external system as a Source\nval jobs: Source[Job, NotUsed] = inboundJobsConnector()\njobs.buffer(1000, OverflowStrategy.backpressure) Java copysource// Getting a stream of jobs from an imaginary external system as a Source\nfinal Source<Job, NotUsed> jobs = inboundJobsConnector;\njobs.buffer(1000, OverflowStrategy.backpressure());\nThe next example will also queue up 1000 jobs locally, but if there are more jobs waiting in the imaginary external systems, it makes space for the new element by dropping one element from the tail of the buffer. Dropping from the tail is a very common strategy but it must be noted that this will drop the youngest waiting job. If some “fairness” is desired in the sense that we want to be nice to jobs that has been waiting for long, then this option can be useful.\nScala copysourcejobs.buffer(1000, OverflowStrategy.dropTail) Java copysourcejobs.buffer(1000, OverflowStrategy.dropTail());\nInstead of dropping the youngest element from the tail of the buffer a new element can be dropped without enqueueing it to the buffer at all.\nScala copysourcejobs.buffer(1000, OverflowStrategy.dropNew) Java copysourcejobs.buffer(1000, OverflowStrategy.dropNew());\nHere is another example with a queue of 1000 jobs, but it makes space for the new element by dropping one element from the head of the buffer. This is the oldest waiting job. This is the preferred strategy if jobs are expected to be resent if not processed in a certain period. The oldest element will be retransmitted soon, (in fact a retransmitted duplicate might be already in the queue!) so it makes sense to drop it first.\nScala copysourcejobs.buffer(1000, OverflowStrategy.dropHead) Java copysourcejobs.buffer(1000, OverflowStrategy.dropHead());\nCompared to the dropping strategies above, dropBuffer drops all the 1000 jobs it has enqueued once the buffer gets full. This aggressive strategy is useful when dropping jobs is preferred to delaying jobs.\nScala copysourcejobs.buffer(1000, OverflowStrategy.dropBuffer) Java copysourcejobs.buffer(1000, OverflowStrategy.dropBuffer());\nIf our imaginary external job provider is a client using our API, we might want to enforce that the client cannot have more than 1000 queued jobs otherwise we consider it flooding and terminate the connection. This is achievable by the error strategy which fails the stream once the buffer gets full.\nScala copysourcejobs.buffer(1000, OverflowStrategy.fail) Java copysourcejobs.buffer(1000, OverflowStrategy.fail());","title":"Buffers in Pekko Streams"},{"location":"/stream/stream-rate.html#rate-transformation","text":"","title":"Rate transformation"},{"location":"/stream/stream-rate.html#understanding-conflate","text":"When a fast producer can not be informed to slow down by backpressure or some other signal, conflate might be useful to combine elements from a producer until a demand signal comes from a consumer.\nBelow is an example snippet that summarizes fast stream of elements to a standard deviation, mean and count of elements that have arrived while the stats have been calculated.\nScala copysourceval statsFlow = Flow[Double].conflateWithSeed(immutable.Seq(_))(_ :+ _).map { s =>\n  val μ = s.sum / s.size\n  val se = s.map(x => pow(x - μ, 2))\n  val σ = sqrt(se.sum / se.size)\n  (σ, μ, s.size)\n} Java copysourcefinal Flow<Double, Tuple3<Double, Double, Integer>, NotUsed> statsFlow =\n    Flow.of(Double.class)\n        .conflateWithSeed(\n            elem -> Collections.singletonList(elem),\n            (acc, elem) -> {\n              return Stream.concat(acc.stream(), Collections.singletonList(elem).stream())\n                  .collect(Collectors.toList());\n            })\n        .map(\n            s -> {\n              final Double mean = s.stream().mapToDouble(d -> d).sum() / s.size();\n              final DoubleStream se = s.stream().mapToDouble(x -> Math.pow(x - mean, 2));\n              final Double stdDev = Math.sqrt(se.sum() / s.size());\n              return new Tuple3<>(stdDev, mean, s.size());\n            });\nThis example demonstrates that such flow’s rate is decoupled. The element rate at the start of the flow can be much higher than the element rate at the end of the flow.\nAnother possible use of conflate is to not consider all elements for summary when the producer starts getting too fast. The example below demonstrates how conflate can be used to randomly drop elements when the consumer is not able to keep up with the producer.\nScala copysourceval p = 0.01\nval sampleFlow = Flow[Double]\n  .conflateWithSeed(immutable.Seq(_)) {\n    case (acc, elem) if Random.nextDouble() < p => acc :+ elem\n    case (acc, _)                               => acc\n  }\n  .mapConcat(identity) Java copysourcefinal Double p = 0.01;\nfinal Flow<Double, Double, NotUsed> sampleFlow =\n    Flow.of(Double.class)\n        .conflateWithSeed(\n            elem -> Collections.singletonList(elem),\n            (acc, elem) -> {\n              if (r.nextDouble() < p) {\n                return Stream.concat(acc.stream(), Collections.singletonList(elem).stream())\n                    .collect(Collectors.toList());\n              }\n              return acc;\n            })\n        .mapConcat(d -> d);\nSee also conflate and conflateWithSeed` for more information and examples.","title":"Understanding conflate"},{"location":"/stream/stream-rate.html#understanding-extrapolate-and-expand","text":"Now we will discuss two operators, extrapolate and expand, helping to deal with slow producers that are unable to keep up with the demand coming from consumers. They allow for additional values to be sent as elements to a consumer.\nAs a simple use case of extrapolate, here is a flow that repeats the last emitted element to a consumer, whenever the consumer signals demand and the producer cannot supply new elements yet.\nScala copysourceval lastFlow = Flow[Double].extrapolate(Iterator.continually(_)) Java copysourcefinal Flow<Double, Double, NotUsed> lastFlow =\n    Flow.of(Double.class).extrapolate(in -> Stream.iterate(in, i -> i).iterator());\nFor situations where there may be downstream demand before any element is emitted from upstream, you can use the initial parameter of extrapolate to “seed” the stream.\nScala copysourceval initial = 2.0\nval seedFlow = Flow[Double].extrapolate(Iterator.continually(_), Some(initial)) Java copysourceDouble initial = 2.0;\nfinal Flow<Double, Double, NotUsed> lastFlow =\n    Flow.of(Double.class).extrapolate(in -> Stream.iterate(in, i -> i).iterator(), initial);\nextrapolate and expand also allow to produce meta-information based on demand signalled from the downstream. Leveraging this, here is a flow that tracks and reports a drift between a fast consumer and a slow producer.\nScala copysourceval driftFlow = Flow[Double].map(_ -> 0).extrapolate[(Double, Int)] { case (i, _) => Iterator.from(1).map(i -> _) } Java copysourcefinal Flow<Double, Pair<Double, Integer>, NotUsed> driftFlow =\n    Flow.of(Double.class)\n        .map(d -> new Pair<>(d, 0))\n        .extrapolate(\n            d -> Stream.iterate(1, i -> i + 1).map(i -> new Pair<>(d.first(), i)).iterator());\nAnd here’s a more concise representation with expand.\nScala copysourceval driftFlow = Flow[Double].expand(i => Iterator.from(0).map(i -> _)) Java copysourcefinal Flow<Double, Pair<Double, Integer>, NotUsed> driftFlow =\n    Flow.of(Double.class)\n        .expand(d -> Stream.iterate(0, i -> i + 1).map(i -> new Pair<>(d, i)).iterator());\nThe difference is due to the different handling of the Iterator-generating argument.\nWhile extrapolate uses an Iterator only when there is unmet downstream demand, expand always creates an Iterator and emits elements downstream from it.\nThis makes expand able to transform or even filter out (by providing an empty Iterator) the “original” elements.\nRegardless, since we provide a non-empty Iterator in both examples, this means that the output of this flow is going to report a drift of zero if the producer is fast enough - or a larger drift otherwise.\nSee also extrapolate and expand for more information and examples.","title":"Understanding extrapolate and expand"},{"location":"/stream/stream-context.html","text":"","title":"Context Propagation"},{"location":"/stream/stream-context.html#context-propagation","text":"It can be convenient to attach metadata to each element in the stream.\nFor example, when reading from an external data source it can be useful to keep track of the read offset, so it can be marked as processed when the element reaches the SinkSink.\nFor this use case we provide the SourceWithContextSourceWithContext and FlowWithContextFlowWithContext variations on SourceSource and FlowFlow.\nEssentially, a FlowWithContextFlowWithContext is just a FlowFlow that contains tuplespairs of element and context, but the advantage is in the operators: most operators on FlowWithContextFlowWithContext will work on the element rather than on the tuplepair, allowing you to focus on your application logic rather without worrying about the context.","title":"Context Propagation"},{"location":"/stream/stream-context.html#restrictions","text":"Not all operations that are available on FlowFlow are also available on FlowWithContextFlowWithContext. This is intentional: in the use case of keeping track of a read offset, if the FlowWithContextFlowWithContext was allowed to arbitrarily filter and reorder the stream, the SinkSink would have no way to determine whether an element was skipped or merely reordered and still in flight.\nFor this reason, FlowWithContextFlowWithContext allows filtering operations (such as filter, filterNot, collect, etc.) and grouping operations (such as grouped, sliding, etc.) but not reordering operations (such as mapAsyncUnordered and statefulMapConcat). Finally, also ‘one-to-n’ operations such as mapConcat are allowed.\nFiltering operations will drop the context along with dropped elements, while grouping operations will keep all contexts from the elements in the group. Streaming one-to-many operations such as mapConcat associate the original context with each of the produced elements.\nAs an escape hatch, there is a via operator that allows you to insert an arbitrary FlowFlow that can process the tuplespairs of elements and context in any way desired. When using this operator, it is the responsibility of the implementor to make sure this FlowFlow does not perform any operations (such as reordering) that might break assumptions made by the SinkSink consuming the context elements.","title":"Restrictions"},{"location":"/stream/stream-context.html#creation","text":"The simplest way to create a SourceWithContextSourceWithContext is to first create a regular SourceSource with elements from which the context can be extracted, and then use Source.asSourceWithContext.","title":"Creation"},{"location":"/stream/stream-context.html#composition","text":"When you have a SourceWithContextSourceWithContext source that produces elements of type Foo with a context of type Ctx, and a FlowFlow flow from Foo to Bar, you cannot simply source.via(flow) to arrive at a SourceWithContextSourceWithContext that produces elements of type Bar with contexts of type Ctx. The reason for this is that flow might reorder the elements flowing through it, making via challenging to implement.\nDue to this there is a unsafeDataVia that can be used instead however no protection is offered to prevent reordering or dropping/duplicating elements from stream so use this operation with great care.\nThere is also a Flow.asFlowWithContext which can be used when the types used in the inner FlowFlow have room to hold the context. If this is not the case, a better solution is usually to build the flow from the ground up as a FlowWithContextFlowWithContext, instead of first building a FlowFlow and trying to convert it to FlowWithContextFlowWithContext after-the-fact.","title":"Composition"},{"location":"/stream/stream-dynamic.html","text":"","title":"Dynamic stream handling"},{"location":"/stream/stream-dynamic.html#dynamic-stream-handling","text":"","title":"Dynamic stream handling"},{"location":"/stream/stream-dynamic.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-dynamic.html#introduction","text":"","title":"Introduction"},{"location":"/stream/stream-dynamic.html#controlling-stream-completion-with-killswitch","text":"A KillSwitchKillSwitch allows the completion of operators of FlowShapeFlowShape from the outside. It consists of a flow element that can be linked to an operator of FlowShape needing completion control. The KillSwitch trait interface allows to:\ncomplete the stream(s) via shutdown()shutdown() fail the stream(s) via abort(Throwable error)abort(Throwable error)\nScala copysourcetrait KillSwitch {\n\n  /**\n   * After calling [[KillSwitch#shutdown]] the linked [[Graph]]s of [[FlowShape]] are completed normally.\n   */\n  def shutdown(): Unit\n\n  /**\n   * After calling [[KillSwitch#abort]] the linked [[Graph]]s of [[FlowShape]] are failed.\n   */\n  def abort(ex: Throwable): Unit\n}\nAfter the first call to either shutdown or abort, all subsequent calls to any of these methods will be ignored. Stream completion is performed by both\ncancelling its upstream. completing (in case of shutdown) or failing (in case of abort) its downstream\nA KillSwitch can control the completion of one or multiple streams, and therefore comes in two different flavours.","title":"Controlling stream completion with KillSwitch"},{"location":"/stream/stream-dynamic.html#uniquekillswitch","text":"UniqueKillSwitchUniqueKillSwitch allows to control the completion of one materialized GraphGraph of FlowShapeFlowShape. Refer to the below for usage examples.\nShutdown\nScala copysourceval countingSrc = Source(Stream.from(1)).delay(1.second, DelayOverflowStrategy.backpressure)\nval lastSnk = Sink.last[Int]\n\nval (killSwitch, last) = countingSrc\n  .viaMat(KillSwitches.single)(Keep.right)\n  .toMat(lastSnk)(Keep.both)\n  .run()\n\ndoSomethingElse()\n\nkillSwitch.shutdown()\n\nAwait.result(last, 1.second) shouldBe 2 Java copysourcefinal Source<Integer, NotUsed> countingSrc =\n    Source.from(new ArrayList<>(Arrays.asList(1, 2, 3, 4)))\n        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure());\nfinal Sink<Integer, CompletionStage<Integer>> lastSnk = Sink.last();\n\nfinal Pair<UniqueKillSwitch, CompletionStage<Integer>> stream =\n    countingSrc\n        .viaMat(KillSwitches.single(), Keep.right())\n        .toMat(lastSnk, Keep.both())\n        .run(system);\n\nfinal UniqueKillSwitch killSwitch = stream.first();\nfinal CompletionStage<Integer> completionStage = stream.second();\n\ndoSomethingElse();\nkillSwitch.shutdown();\n\nfinal int finalCount = completionStage.toCompletableFuture().get(1, TimeUnit.SECONDS);\nassertEquals(2, finalCount);\nAbort\nScala copysourceval countingSrc = Source(Stream.from(1)).delay(1.second, DelayOverflowStrategy.backpressure)\nval lastSnk = Sink.last[Int]\n\nval (killSwitch, last) = countingSrc\n  .viaMat(KillSwitches.single)(Keep.right)\n  .toMat(lastSnk)(Keep.both).run()\n\nval error = new RuntimeException(\"boom!\")\nkillSwitch.abort(error)\n\nAwait.result(last.failed, 1.second) shouldBe error Java copysourcefinal Source<Integer, NotUsed> countingSrc =\n    Source.from(new ArrayList<>(Arrays.asList(1, 2, 3, 4)))\n        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure());\nfinal Sink<Integer, CompletionStage<Integer>> lastSnk = Sink.last();\n\nfinal Pair<UniqueKillSwitch, CompletionStage<Integer>> stream =\n    countingSrc\n        .viaMat(KillSwitches.single(), Keep.right())\n        .toMat(lastSnk, Keep.both())\n        .run(system);\n\nfinal UniqueKillSwitch killSwitch = stream.first();\nfinal CompletionStage<Integer> completionStage = stream.second();\n\nfinal Exception error = new Exception(\"boom!\");\nkillSwitch.abort(error);\n\nfinal int result =\n    completionStage.toCompletableFuture().exceptionally(e -> -1).get(1, TimeUnit.SECONDS);\nassertEquals(-1, result);","title":"UniqueKillSwitch"},{"location":"/stream/stream-dynamic.html#sharedkillswitch","text":"A SharedKillSwitchSharedKillSwitch allows to control the completion of an arbitrary number operators of FlowShapeFlowShape. It can be materialized multiple times via its flowflow method, and all materialized operators linked to it are controlled by the switch. Refer to the below for usage examples.\nShutdown\nScala copysourceval countingSrc = Source(Stream.from(1)).delay(1.second, DelayOverflowStrategy.backpressure)\nval lastSnk = Sink.last[Int]\nval sharedKillSwitch = KillSwitches.shared(\"my-kill-switch\")\n\nval last = countingSrc\n  .via(sharedKillSwitch.flow)\n  .runWith(lastSnk)\n\nval delayedLast = countingSrc\n  .delay(1.second, DelayOverflowStrategy.backpressure)\n  .via(sharedKillSwitch.flow)\n  .runWith(lastSnk)\n\ndoSomethingElse()\n\nsharedKillSwitch.shutdown()\n\nAwait.result(last, 1.second) shouldBe 2\nAwait.result(delayedLast, 1.second) shouldBe 1 Java copysourcefinal Source<Integer, NotUsed> countingSrc =\n    Source.from(new ArrayList<>(Arrays.asList(1, 2, 3, 4)))\n        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure());\nfinal Sink<Integer, CompletionStage<Integer>> lastSnk = Sink.last();\nfinal SharedKillSwitch killSwitch = KillSwitches.shared(\"my-kill-switch\");\n\nfinal CompletionStage<Integer> completionStage =\n    countingSrc\n        .viaMat(killSwitch.flow(), Keep.right())\n        .toMat(lastSnk, Keep.right())\n        .run(system);\nfinal CompletionStage<Integer> completionStageDelayed =\n    countingSrc\n        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure())\n        .viaMat(killSwitch.flow(), Keep.right())\n        .toMat(lastSnk, Keep.right())\n        .run(system);\n\ndoSomethingElse();\nkillSwitch.shutdown();\n\nfinal int finalCount = completionStage.toCompletableFuture().get(1, TimeUnit.SECONDS);\nfinal int finalCountDelayed =\n    completionStageDelayed.toCompletableFuture().get(1, TimeUnit.SECONDS);\n\nassertEquals(2, finalCount);\nassertEquals(1, finalCountDelayed);\nAbort\nScala copysourceval countingSrc = Source(Stream.from(1)).delay(1.second)\nval lastSnk = Sink.last[Int]\nval sharedKillSwitch = KillSwitches.shared(\"my-kill-switch\")\n\nval last1 = countingSrc.via(sharedKillSwitch.flow).runWith(lastSnk)\nval last2 = countingSrc.via(sharedKillSwitch.flow).runWith(lastSnk)\n\nval error = new RuntimeException(\"boom!\")\nsharedKillSwitch.abort(error)\n\nAwait.result(last1.failed, 1.second) shouldBe error\nAwait.result(last2.failed, 1.second) shouldBe error Java copysourcefinal Source<Integer, NotUsed> countingSrc =\n    Source.from(new ArrayList<>(Arrays.asList(1, 2, 3, 4)))\n        .delay(Duration.ofSeconds(1), DelayOverflowStrategy.backpressure());\nfinal Sink<Integer, CompletionStage<Integer>> lastSnk = Sink.last();\nfinal SharedKillSwitch killSwitch = KillSwitches.shared(\"my-kill-switch\");\n\nfinal CompletionStage<Integer> completionStage1 =\n    countingSrc\n        .viaMat(killSwitch.flow(), Keep.right())\n        .toMat(lastSnk, Keep.right())\n        .run(system);\nfinal CompletionStage<Integer> completionStage2 =\n    countingSrc\n        .viaMat(killSwitch.flow(), Keep.right())\n        .toMat(lastSnk, Keep.right())\n        .run(system);\n\nfinal Exception error = new Exception(\"boom!\");\nkillSwitch.abort(error);\n\nfinal int result1 =\n    completionStage1.toCompletableFuture().exceptionally(e -> -1).get(1, TimeUnit.SECONDS);\nfinal int result2 =\n    completionStage2.toCompletableFuture().exceptionally(e -> -1).get(1, TimeUnit.SECONDS);\n\nassertEquals(-1, result1);\nassertEquals(-1, result2);\nNote A UniqueKillSwitchUniqueKillSwitch is always a result of a materialization, whilst SharedKillSwitchSharedKillSwitch needs to be constructed before any materialization takes place.","title":"SharedKillSwitch"},{"location":"/stream/stream-dynamic.html#dynamic-fan-in-and-fan-out-with-mergehub-broadcasthub-and-partitionhub","text":"There are many cases when consumers or producers of a certain service (represented as a Sink, Source, or possibly Flow) are dynamic and not known in advance. The Graph DSL does not allow to represent this, all connections of the graph must be known in advance and must be connected upfront. To allow dynamic fan-in and fan-out streaming, the Hubs should be used. They provide means to construct SinkSink and SourceSource pairs that are “attached” to each other, but one of them can be materialized multiple times to implement dynamic fan-in or fan-out.","title":"Dynamic fan-in and fan-out with MergeHub, BroadcastHub and PartitionHub"},{"location":"/stream/stream-dynamic.html#using-the-mergehub","text":"A MergeHubMergeHub allows to implement a dynamic fan-in junction point in a graph where elements coming from different producers are emitted in a First-Comes-First-Served fashion. If the consumer cannot keep up then all of the producers are backpressured. The hub itself comes as a SourceSource to which the single consumer can be attached. It is not possible to attach any producers until this Source has been materialized (started). This is ensured by the fact that we only get the corresponding SinkSink as a materialized value. Usage might look like this:\nScala copysource// A simple consumer that will print to the console for now\nval consumer = Sink.foreach(println)\n\n// Attach a MergeHub Source to the consumer. This will materialize to a\n// corresponding Sink.\nval runnableGraph: RunnableGraph[Sink[String, NotUsed]] =\n  MergeHub.source[String](perProducerBufferSize = 16).to(consumer)\n\n// By running/materializing the consumer we get back a Sink, and hence\n// now have access to feed elements into it. This Sink can be materialized\n// any number of times, and every element that enters the Sink will\n// be consumed by our consumer.\nval toConsumer: Sink[String, NotUsed] = runnableGraph.run()\n\n// Feeding two independent sources into the hub.\nSource.single(\"Hello!\").runWith(toConsumer)\nSource.single(\"Hub!\").runWith(toConsumer) Java copysource// A simple consumer that will print to the console for now\nSink<String, CompletionStage<Done>> consumer = Sink.foreach(System.out::println);\n\n// Attach a MergeHub Source to the consumer. This will materialize to a\n// corresponding Sink.\nRunnableGraph<Sink<String, NotUsed>> runnableGraph = MergeHub.of(String.class, 16).to(consumer);\n\n// By running/materializing the consumer we get back a Sink, and hence\n// now have access to feed elements into it. This Sink can be materialized\n// any number of times, and every element that enters the Sink will\n// be consumed by our consumer.\nSink<String, NotUsed> toConsumer = runnableGraph.run(system);\n\nSource.single(\"Hello!\").runWith(toConsumer, system);\nSource.single(\"Hub!\").runWith(toConsumer, system);\nThis sequence, while might look odd at first, ensures proper startup order. Once we get the Sink, we can use it as many times as wanted. Everything that is fed to it will be delivered to the consumer we attached previously until it cancels.","title":"Using the MergeHub"},{"location":"/stream/stream-dynamic.html#using-the-broadcasthub","text":"A BroadcastHubBroadcastHub can be used to consume elements from a common producer by a dynamic set of consumers. The rate of the producer will be automatically adapted to the slowest consumer. In this case, the hub is a SinkSink to which the single producer must be attached first. Consumers can only be attached once the Sink has been materialized (i.e. the producer has been started). One example of using the BroadcastHub:\nScala copysource// A simple producer that publishes a new \"message\" every second\nval producer = Source.tick(1.second, 1.second, \"New message\")\n\n// Attach a BroadcastHub Sink to the producer. This will materialize to a\n// corresponding Source.\n// (We need to use toMat and Keep.right since by default the materialized\n// value to the left is used)\nval runnableGraph: RunnableGraph[Source[String, NotUsed]] =\n  producer.toMat(BroadcastHub.sink(bufferSize = 256))(Keep.right)\n\n// By running/materializing the producer, we get back a Source, which\n// gives us access to the elements published by the producer.\nval fromProducer: Source[String, NotUsed] = runnableGraph.run()\n\n// Print out messages from the producer in two independent consumers\nfromProducer.runForeach(msg => println(\"consumer1: \" + msg))\nfromProducer.runForeach(msg => println(\"consumer2: \" + msg)) Java copysource// A simple producer that publishes a new \"message\" every second\nSource<String, Cancellable> producer =\n    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), \"New message\");\n\n// Attach a BroadcastHub Sink to the producer. This will materialize to a\n// corresponding Source.\n// (We need to use toMat and Keep.right since by default the materialized\n// value to the left is used)\nRunnableGraph<Source<String, NotUsed>> runnableGraph =\n    producer.toMat(BroadcastHub.of(String.class, 256), Keep.right());\n\n// By running/materializing the producer, we get back a Source, which\n// gives us access to the elements published by the producer.\nSource<String, NotUsed> fromProducer = runnableGraph.run(materializer);\n\n// Print out messages from the producer in two independent consumers\nfromProducer.runForeach(msg -> System.out.println(\"consumer1: \" + msg), materializer);\nfromProducer.runForeach(msg -> System.out.println(\"consumer2: \" + msg), materializer);\nThe resulting SourceSource can be materialized any number of times, each materialization effectively attaching a new subscriber. If there are no subscribers attached to this hub then it will not drop any elements but instead backpressure the upstream producer until subscribers arrive. This behavior can be tweaked by using the operators .buffer.buffer for example with a drop strategy, or attaching a subscriber that drops all messages. If there are no other subscribers, this will ensure that the producer is kept drained (dropping all elements) and once a new subscriber arrives it will adaptively slow down, ensuring no more messages are dropped.","title":"Using the BroadcastHub"},{"location":"/stream/stream-dynamic.html#combining-dynamic-operators-to-build-a-simple-publish-subscribe-service","text":"The features provided by the Hub implementations are limited by default. This is by design, as various combinations can be used to express additional features like unsubscribing producers or consumers externally. We show here an example that builds a FlowFlow representing a publish-subscribe channel. The input of the Flow is published to all subscribers while the output streams all the elements published.\nFirst, we connect a MergeHubMergeHub and a BroadcastHubBroadcastHub together to form a publish-subscribe channel. Once we materialize this small stream, we get back a pair of SourceSource and SinkSink that together define the publish and subscribe sides of our channel.\nScala copysource// Obtain a Sink and Source which will publish and receive from the \"bus\" respectively.\nval (sink, source) =\n  MergeHub.source[String](perProducerBufferSize = 16).toMat(BroadcastHub.sink(bufferSize = 256))(Keep.both).run() Java copysource// Obtain a Sink and Source which will publish and receive from the \"bus\" respectively.\nPair<Sink<String, NotUsed>, Source<String, NotUsed>> sinkAndSource =\n    MergeHub.of(String.class, 16)\n        .toMat(BroadcastHub.of(String.class, 256), Keep.both())\n        .run(system);\n\nSink<String, NotUsed> sink = sinkAndSource.first();\nSource<String, NotUsed> source = sinkAndSource.second();\nWe now use a few tricks to add more features. First of all, we attach a Sink.ignoreSink.ignore at the broadcast side of the channel to keep it drained when there are no subscribers. If this behavior is not the desired one this line can be dropped.\nScala copysource// Ensure that the Broadcast output is dropped if there are no listening parties.\n// If this dropping Sink is not attached, then the broadcast hub will not drop any\n// elements itself when there are no subscribers, backpressuring the producer instead.\nsource.runWith(Sink.ignore) Java copysource// Ensure that the Broadcast output is dropped if there are no listening parties.\n// If this dropping Sink is not attached, then the broadcast hub will not drop any\n// elements itself when there are no subscribers, backpressuring the producer instead.\nsource.runWith(Sink.ignore(), system);\nWe now wrap the SinkSink and SourceSource in a FlowFlow using Flow.fromSinkAndSourceFlow.fromSinkAndSource. This bundles up the two sides of the channel into one and forces users of it to always define a publisher and subscriber side (even if the subscriber side is dropping). It also allows us to attach a KillSwitchKillSwitch as a BidiStage which in turn makes it possible to close both the original Sink and Source at the same time. Finally, we add backpressureTimeout on the consumer side to ensure that subscribers that block the channel for more than 3 seconds are forcefully removed (and their stream failed).\nScala copysource// We create now a Flow that represents a publish-subscribe channel using the above\n// started stream as its \"topic\". We add two more features, external cancellation of\n// the registration and automatic cleanup for very slow subscribers.\nval busFlow: Flow[String, String, UniqueKillSwitch] =\n  Flow\n    .fromSinkAndSource(sink, source)\n    .joinMat(KillSwitches.singleBidi[String, String])(Keep.right)\n    .backpressureTimeout(3.seconds) Java copysource// We create now a Flow that represents a publish-subscribe channel using the above\n// started stream as its \"topic\". We add two more features, external cancellation of\n// the registration and automatic cleanup for very slow subscribers.\nFlow<String, String, UniqueKillSwitch> busFlow =\n    Flow.fromSinkAndSource(sink, source)\n        .joinMat(KillSwitches.singleBidi(), Keep.right())\n        .backpressureTimeout(Duration.ofSeconds(1));\nThe resulting Flow now has a type of Flow[String, String, UniqueKillSwitch] representing a publish-subscribe channel which can be used any number of times to attach new producers or consumers. In addition, it materializes to a UniqueKillSwitch (see UniqueKillSwitch) that can be used to deregister a single user externally:\nScala copysourceval switch: UniqueKillSwitch =\n  Source.repeat(\"Hello world!\").viaMat(busFlow)(Keep.right).to(Sink.foreach(println)).run()\n\n// Shut down externally\nswitch.shutdown() Java copysourceUniqueKillSwitch killSwitch =\n    Source.repeat(\"Hello World!\")\n        .viaMat(busFlow, Keep.right())\n        .to(Sink.foreach(System.out::println))\n        .run(system);\n\n// Shut down externally\nkillSwitch.shutdown();","title":"Combining dynamic operators to build a simple Publish-Subscribe service"},{"location":"/stream/stream-dynamic.html#using-the-partitionhub","text":"This is a may change feature*\nA PartitionHubPartitionHub can be used to route elements from a common producer to a dynamic set of consumers. The selection of consumer is done with a function. Each element can be routed to only one consumer.\nThe rate of the producer will be automatically adapted to the slowest consumer. In this case, the hub is a SinkSink to which the single producer must be attached first. Consumers can only be attached once the Sink has been materialized (i.e. the producer has been started). One example of using the PartitionHub:\nScala copysource// A simple producer that publishes a new \"message-\" every second\nval producer = Source.tick(1.second, 1.second, \"message\").zipWith(Source(1 to 100))((a, b) => s\"$a-$b\")\n\n// Attach a PartitionHub Sink to the producer. This will materialize to a\n// corresponding Source.\n// (We need to use toMat and Keep.right since by default the materialized\n// value to the left is used)\nval runnableGraph: RunnableGraph[Source[String, NotUsed]] =\n  producer.toMat(\n    PartitionHub.sink(\n      (size, elem) => math.abs(elem.hashCode % size),\n      startAfterNrOfConsumers = 2,\n      bufferSize = 256))(Keep.right)\n\n// By running/materializing the producer, we get back a Source, which\n// gives us access to the elements published by the producer.\nval fromProducer: Source[String, NotUsed] = runnableGraph.run()\n\n// Print out messages from the producer in two independent consumers\nfromProducer.runForeach(msg => println(\"consumer1: \" + msg))\nfromProducer.runForeach(msg => println(\"consumer2: \" + msg)) Java copysource// A simple producer that publishes a new \"message-n\" every second\nSource<String, Cancellable> producer =\n    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), \"message\")\n        .zipWith(Source.range(0, 100), (a, b) -> a + \"-\" + b);\n\n// Attach a PartitionHub Sink to the producer. This will materialize to a\n// corresponding Source.\n// (We need to use toMat and Keep.right since by default the materialized\n// value to the left is used)\nRunnableGraph<Source<String, NotUsed>> runnableGraph =\n    producer.toMat(\n        PartitionHub.of(String.class, (size, elem) -> Math.abs(elem.hashCode() % size), 2, 256),\n        Keep.right());\n\n// By running/materializing the producer, we get back a Source, which\n// gives us access to the elements published by the producer.\nSource<String, NotUsed> fromProducer = runnableGraph.run(materializer);\n\n// Print out messages from the producer in two independent consumers\nfromProducer.runForeach(msg -> System.out.println(\"consumer1: \" + msg), materializer);\nfromProducer.runForeach(msg -> System.out.println(\"consumer2: \" + msg), materializer);\nThe partitioner function takes two parameters; the first is the number of active consumers and the second is the stream element. The function should return the index of the selected consumer for the given element, i.e. int greater than or equal to 0 and less than number of consumers.\nThe resulting SourceSource can be materialized any number of times, each materialization effectively attaching a new consumer. If there are no consumers attached to this hub then it will not drop any elements but instead backpressure the upstream producer until consumers arrive. This behavior can be tweaked by using an operator, for example .buffer.buffer with a drop strategy, or attaching a consumer that drops all messages. If there are no other consumers, this will ensure that the producer is kept drained (dropping all elements) and once a new consumer arrives and messages are routed to the new consumer it will adaptively slow down, ensuring no more messages are dropped.\nIt is possible to define how many initial consumers that are required before it starts emitting any messages to the attached consumers. While not enough consumers have been attached messages are buffered and when the buffer is full the upstream producer is backpressured. No messages are dropped.\nThe above example illustrate a stateless partition function. For more advanced stateful routing the ofStateful statefulSink can be used. Here is an example of a stateful round-robin function:\nScala copysource// A simple producer that publishes a new \"message-\" every second\nval producer = Source.tick(1.second, 1.second, \"message\").zipWith(Source(1 to 100))((a, b) => s\"$a-$b\")\n\n// New instance of the partitioner function and its state is created\n// for each materialization of the PartitionHub.\ndef roundRobin(): (PartitionHub.ConsumerInfo, String) => Long = {\n  var i = -1L\n\n  (info, elem) => {\n    i += 1\n    info.consumerIdByIdx((i % info.size).toInt)\n  }\n}\n\n// Attach a PartitionHub Sink to the producer. This will materialize to a\n// corresponding Source.\n// (We need to use toMat and Keep.right since by default the materialized\n// value to the left is used)\nval runnableGraph: RunnableGraph[Source[String, NotUsed]] =\n  producer.toMat(PartitionHub.statefulSink(() => roundRobin(), startAfterNrOfConsumers = 2, bufferSize = 256))(\n    Keep.right)\n\n// By running/materializing the producer, we get back a Source, which\n// gives us access to the elements published by the producer.\nval fromProducer: Source[String, NotUsed] = runnableGraph.run()\n\n// Print out messages from the producer in two independent consumers\nfromProducer.runForeach(msg => println(\"consumer1: \" + msg))\nfromProducer.runForeach(msg => println(\"consumer2: \" + msg)) Java copysource// A simple producer that publishes a new \"message-n\" every second\nSource<String, Cancellable> producer =\n    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), \"message\")\n        .zipWith(Source.range(0, 100), (a, b) -> a + \"-\" + b);\n\n// Attach a PartitionHub Sink to the producer. This will materialize to a\n// corresponding Source.\n// (We need to use toMat and Keep.right since by default the materialized\n// value to the left is used)\nRunnableGraph<Source<String, NotUsed>> runnableGraph =\n    producer.toMat(\n        PartitionHub.ofStateful(String.class, () -> new RoundRobin<String>(), 2, 256),\n        Keep.right());\n\n// By running/materializing the producer, we get back a Source, which\n// gives us access to the elements published by the producer.\nSource<String, NotUsed> fromProducer = runnableGraph.run(materializer);\n\n// Print out messages from the producer in two independent consumers\nfromProducer.runForeach(msg -> System.out.println(\"consumer1: \" + msg), materializer);\nfromProducer.runForeach(msg -> System.out.println(\"consumer2: \" + msg), materializer);\nNote that it is a factory of a function to be able to hold stateful variables that are unique for each materialization. In this example the partitioner function is implemented as a class to be able to hold the mutable variable. A new instance of RoundRobin is created for each materialization of the hub.\ncopysource// Using a class since variable must otherwise be final.\n// New instance is created for each materialization of the PartitionHub.\nstatic class RoundRobin<T> implements ToLongBiFunction<ConsumerInfo, T> {\n\n  private long i = -1;\n\n  @Override\n  public long applyAsLong(ConsumerInfo info, T elem) {\n    i++;\n    return info.consumerIdByIdx((int) (i % info.size()));\n  }\n}\nThe function takes two parameters; the first is information about active consumers, including an array of consumer identifiers and the second is the stream element. The function should return the selected consumer identifier for the given element. The function will never be called when there are no active consumers, i.e. there is always at least one element in the array of identifiers.\nAnother interesting type of routing is to prefer routing to the fastest consumers. The ConsumerInfoConsumerInfo has an accessor queueSize that is approximate number of buffered elements for a consumer. Larger value than other consumers could be an indication of that the consumer is slow. Note that this is a moving target since the elements are consumed concurrently. Here is an example of a hub that routes to the consumer with least buffered elements:\nScala copysourceval producer = Source(0 until 100)\n\n// ConsumerInfo.queueSize is the approximate number of buffered elements for a consumer.\n// Note that this is a moving target since the elements are consumed concurrently.\nval runnableGraph: RunnableGraph[Source[Int, NotUsed]] =\n  producer.toMat(\n    PartitionHub.statefulSink(\n      () => (info, elem) => info.consumerIds.minBy(id => info.queueSize(id)),\n      startAfterNrOfConsumers = 2,\n      bufferSize = 16))(Keep.right)\n\nval fromProducer: Source[Int, NotUsed] = runnableGraph.run()\n\nfromProducer.runForeach(msg => println(\"consumer1: \" + msg))\nfromProducer.throttle(10, 100.millis).runForeach(msg => println(\"consumer2: \" + msg)) Java copysourceSource<Integer, NotUsed> producer = Source.range(0, 100);\n\n// ConsumerInfo.queueSize is the approximate number of buffered elements for a consumer.\n// Note that this is a moving target since the elements are consumed concurrently.\nRunnableGraph<Source<Integer, NotUsed>> runnableGraph =\n    producer.toMat(\n        PartitionHub.ofStateful(\n            Integer.class,\n            () ->\n                (info, elem) -> {\n                  final List<Object> ids = info.getConsumerIds();\n                  int minValue = info.queueSize(0);\n                  long fastest = info.consumerIdByIdx(0);\n                  for (int i = 1; i < ids.size(); i++) {\n                    int value = info.queueSize(i);\n                    if (value < minValue) {\n                      minValue = value;\n                      fastest = info.consumerIdByIdx(i);\n                    }\n                  }\n                  return fastest;\n                },\n            2,\n            8),\n        Keep.right());\n\nSource<Integer, NotUsed> fromProducer = runnableGraph.run(materializer);\n\nfromProducer.runForeach(msg -> System.out.println(\"consumer1: \" + msg), materializer);\nfromProducer\n    .throttle(10, Duration.ofMillis(100))\n    .runForeach(msg -> System.out.println(\"consumer2: \" + msg), materializer);","title":"Using the PartitionHub"},{"location":"/stream/stream-customize.html","text":"","title":"Custom stream processing"},{"location":"/stream/stream-customize.html#custom-stream-processing","text":"","title":"Custom stream processing"},{"location":"/stream/stream-customize.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-customize.html#introduction","text":"While the processing vocabulary of Pekko Streams is quite rich (see the Streams Cookbook for examples) it is sometimes necessary to define new transformation operators either because some functionality is missing from the stock operations, or for performance reasons. In this part we show how to build custom operators and graph junctions of various kinds.\nNote A custom operator should not be the first tool you reach for, defining operators using flows and the graph DSL is in general easier and does to a larger extent protect you from mistakes that might be easy to make with a custom GraphStage","title":"Introduction"},{"location":"/stream/stream-customize.html#custom-processing-with-graphstage","text":"The GraphStageGraphStage abstraction can be used to create arbitrary operators with any number of input or output ports. It is a counterpart of the GraphDSL.create()GraphDSL.create() method which creates new stream processing operators by composing others. Where GraphStage differs is that it creates an operator that is itself not divisible into smaller ones, and allows state to be maintained inside it in a safe way.\nAs a first motivating example, we will build a new SourceSource that will emit numbers from 1 until it is cancelled. To start, we need to define the “interface” of our operator, which is called shape in Pekko Streams terminology (this is explained in more detail in the section Modularity, Composition and Hierarchy). This is how it looks:\nScala copysourceimport org.apache.pekko\nimport pekko.stream.stage.GraphStage\n\nclass NumbersSource extends GraphStage[SourceShape[Int]] {\n  // Define the (sole) output port of this stage\n  val out: Outlet[Int] = Outlet(\"NumbersSource\")\n  // Define the shape of this stage, which is SourceShape with the port we defined above\n  override val shape: SourceShape[Int] = SourceShape(out)\n\n  // This is where the actual (possibly stateful) logic will live\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = ???\n} Java copysourcepublic class NumbersSource extends GraphStage<SourceShape<Integer>> {\n  // Define the (sole) output port of this stage\n  public final Outlet<Integer> out = Outlet.create(\"NumbersSource.out\");\n\n  // Define the shape of this stage, which is SourceShape with the port we defined above\n  private final SourceShape<Integer> shape = SourceShape.of(out);\n\n  @Override\n  public SourceShape<Integer> shape() {\n    return shape;\n  }\n\n  // This is where the actual (possibly stateful) logic is created\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape()) {\n      // All state MUST be inside the GraphStageLogic,\n      // never inside the enclosing GraphStage.\n      // This state is safe to access and modify from all the\n      // callbacks that are provided by GraphStageLogic and the\n      // registered handlers.\n      private int counter = 1;\n\n      {\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                push(out, counter);\n                counter += 1;\n              }\n            });\n      }\n    };\n  }\n}\nAs you see, in itself the GraphStage only defines the ports of this operator and a shape that contains the ports. It also has, a currently unimplemented method called createLogiccreateLogic. If you recall, operators are reusable in multiple materializations, each resulting in a different executing entity. In the case of GraphStage the actual running logic is modeled as an instance of a GraphStageLogicGraphStageLogic which will be created by the materializer by calling the createLogic method. In other words, all we need to do is to create a suitable logic that will emit the numbers we want.\nNote It is very important to keep the GraphStage object itself immutable and reusable. All mutable state needs to be confined to the GraphStageLogic that is created for every materialization.\nIn order to emit from a SourceSource in a backpressured stream one needs first to have demand from downstream. To receive the necessary events one needs to register a subclass of OutHandler AbstractOutHandler with the output port (OutletOutlet). This handler will receive events related to the lifecycle of the port. In our case we need to override onPull() which indicates that we are free to emit a single element. There is another callback, onDownstreamFinish() which is called if the downstream cancelled. Since the default behavior of that callback is to stop the operator, we don’t need to override it. In the onPull callback we will emit the next number. This is how it looks like in the end:\nScala copysourceimport org.apache.pekko\nimport pekko.stream.Attributes\nimport pekko.stream.Outlet\nimport pekko.stream.SourceShape\nimport pekko.stream.stage.GraphStage\nimport pekko.stream.stage.GraphStageLogic\nimport pekko.stream.stage.OutHandler\n\nclass NumbersSource extends GraphStage[SourceShape[Int]] {\n  val out: Outlet[Int] = Outlet(\"NumbersSource\")\n  override val shape: SourceShape[Int] = SourceShape(out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      // All state MUST be inside the GraphStageLogic,\n      // never inside the enclosing GraphStage.\n      // This state is safe to access and modify from all the\n      // callbacks that are provided by GraphStageLogic and the\n      // registered handlers.\n      private var counter = 1\n\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            push(out, counter)\n            counter += 1\n          }\n        })\n    }\n}\nInstances of the above GraphStageGraphStage are subclasses of Graph[SourceShape[Int],NotUsed] Graph<SourceShape<Integer>,NotUsed> which means that they are already usable in many situations, but do not provide the DSL methods we usually have for other Source s. In order to convert this Graph to a proper Source we need to wrap it using Source.fromGraph (see Modularity, Composition and Hierarchy for more details about operators and DSLs). Now we can use the source as any other built-in one:\nScala copysource// A GraphStage is a proper Graph, just like what GraphDSL.create would return\nval sourceGraph: Graph[SourceShape[Int], NotUsed] = new NumbersSource\n\n// Create a Source from the Graph to access the DSL\nval mySource: Source[Int, NotUsed] = Source.fromGraph(sourceGraph)\n\n// Returns 55\nval result1: Future[Int] = mySource.take(10).runFold(0)(_ + _)\n\n// The source is reusable. This returns 5050\nval result2: Future[Int] = mySource.take(100).runFold(0)(_ + _) Java copysource// A GraphStage is a proper Graph, just like what GraphDSL.create would return\nGraph<SourceShape<Integer>, NotUsed> sourceGraph = new NumbersSource();\n\n// Create a Source from the Graph to access the DSL\nSource<Integer, NotUsed> mySource = Source.fromGraph(sourceGraph);\n\n// Returns 55\nCompletionStage<Integer> result1 =\n    mySource.take(10).runFold(0, (sum, next) -> sum + next, system);\n\n// The source is reusable. This returns 5050\nCompletionStage<Integer> result2 =\n    mySource.take(100).runFold(0, (sum, next) -> sum + next, system);\nSimilarly, to create a custom SinkSink one can register a subclass InHandlerInHandler with the operator InletInlet. The onPush()onPush() callback is used to signal the handler a new element has been pushed to the operator, and can hence be grabbed and used. onPush() can be overridden to provide custom behavior. Please note, most Sinks would need to request upstream elements as soon as they are created: this can be done by calling pull(inlet)pull(inlet) in the preStart()preStart() callback.\nScala copysourceimport org.apache.pekko\nimport pekko.stream.Attributes\nimport pekko.stream.Inlet\nimport pekko.stream.SinkShape\nimport pekko.stream.stage.GraphStage\nimport pekko.stream.stage.GraphStageLogic\nimport pekko.stream.stage.InHandler\n\nclass StdoutSink extends GraphStage[SinkShape[Int]] {\n  val in: Inlet[Int] = Inlet(\"StdoutSink\")\n  override val shape: SinkShape[Int] = SinkShape(in)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n\n      // This requests one element at the Sink startup.\n      override def preStart(): Unit = pull(in)\n\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = {\n            println(grab(in))\n            pull(in)\n          }\n        })\n    }\n} Java copysourcepublic class StdoutSink extends GraphStage<SinkShape<Integer>> {\n  public final Inlet<Integer> in = Inlet.create(\"StdoutSink.in\");\n\n  private final SinkShape<Integer> shape = SinkShape.of(in);\n\n  @Override\n  public SinkShape<Integer> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape()) {\n\n      // This requests one element at the Sink startup.\n      @Override\n      public void preStart() {\n        pull(in);\n      }\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() throws Exception {\n                Integer element = grab(in);\n                System.out.println(element);\n                pull(in);\n              }\n            });\n      }\n    };\n  }\n}","title":"Custom processing with GraphStage"},{"location":"/stream/stream-customize.html#port-states-and-","text":"In order to interact with a port (InletInlet or OutletOutlet) of the operator we need to be able to receive events and generate new events belonging to the port.","title":"Port states, InHandler AbstractInHandler and OutHandler AbstractOutHandler"},{"location":"/stream/stream-customize.html#output-port","text":"From the GraphStageLogicGraphStageLogic the following operations are available on an output port:\npush(out,elem) pushes an element to the output port. Only possible after the port has been pulled by downstream. complete(out) closes the output port normally. fail(out,exception) closes the port with a failure signal.\nThe events corresponding to an output port can be received in an OutHandlerAbstractOutHandler instance registered to the output port using setHandler(out,handler). This handler has two callbacks:\nonPull()onPull() {scala=“#onPull():Unit” java=“#onPull()”] is called when the output port is ready to emit the next element, push(out, elem) is now allowed to be called on this port. onDownstreamFinish()onDownstreamFinish() is called once the downstream has cancelled and no longer allows messages to be pushed to it. No more onPull() will arrive after this event. If not overridden this will default to stopping the operator.\nAlso, there are two query methods available for output ports:\nisAvailable(out)isAvailable(out) returns true if the port can be pushed isClosed(out) returns true if the port is closed. At this point the port can not be pushed and will not be pulled anymore.\nThe relationship of the above operations, events and queries are summarized in the state machine below. Green shows the initial state while orange indicates the end state. If an operation is not listed for a state, then it is invalid to call it while the port is in that state. If an event is not listed for a state, then that event cannot happen in that state.","title":"Output port"},{"location":"/stream/stream-customize.html#input-port","text":"The following operations are available for input ports:\npull(in) requests a new element from an input port. This is only possible after the port has been pushed by upstream. grab(in) acquires the element that has been received during an onPush(). It cannot be called again until the port is pushed again by the upstream. cancel(in) closes the input port.\nThe events corresponding to an input port can be received in an InHandler AbstractInHandler instance registered to the input port using setHandler(in, handler). This handler has three callbacks:\nonPush() is called when the input port has now a new element. Now it is possible to acquire this element using grab(in) and/or call pull(in) on the port to request the next element. It is not mandatory to grab the element, but if it is pulled while the element has not been grabbed it will drop the buffered element. onUpstreamFinish() is called once the upstream has completed and no longer can be pulled for new elements. No more onPush() will arrive after this event. If not overridden this will default to stopping the operator. onUpstreamFailure() is called if the upstream failed with an exception and no longer can be pulled for new elements. No more onPush() will arrive after this event. If not overridden this will default to failing the operator.\nAlso, there are three query methods available for input ports:\nisAvailable(in) returns true if the port can be grabbed. hasBeenPulled(in) returns true if the port has been already pulled. Calling pull(in) in this state is illegal. isClosed(in) returns true if the port is closed. At this point the port can not be pulled and will not be pushed anymore.\nThe relationship of the above operations, events and queries are summarized in the state machine below. Green shows the initial state while orange indicates the end state. If an operation is not listed for a state, then it is invalid to call it while the port is in that state. If an event is not listed for a state, then that event cannot happen in that state.","title":"Input port"},{"location":"/stream/stream-customize.html#complete-and-fail","text":"Finally, there are two methods available for convenience to complete the operator and all of its ports:\ncompleteStage()completeStage() is equivalent to closing all output ports and cancelling all input ports. failStage(exception)failStage(exception) is equivalent to failing all output ports and cancelling all input ports.","title":"Complete and fail"},{"location":"/stream/stream-customize.html#emit","text":"In some cases it is inconvenient and error prone to react on the regular state machine events with the signal based API described above. For those cases there is an API which allows for a more declarative sequencing of actions which will greatly simplify some use cases at the cost of some extra allocations. The difference between the two APIs could be described as that the first one is signal driven from the outside, while this API is more active and drives its surroundings.\nThe operations of this part of the GraphStageGraphStage API are:\nemit(out, elem) and emitMultiple(out, Iterable(elem1, elem2)) replaces the OutHandler with a handler that emits one or more elements when there is demand, and then reinstalls the current handlers read(in)(andThen) and readN(in, n)(andThen) replaces the InHandler with a handler that reads one or more elements as they are pushed and allows the handler to react once the requested number of elements has been read. abortEmitting() and abortReading() which will cancel an ongoing emit or read\nNote that since the above methods are implemented by temporarily replacing the handlers of the operator you should never call setHandler while they are running emit or read as that interferes with how they are implemented. The following methods are safe to call after invoking emit and read (and will lead to actually running the operation when those are done): complete(out), completeStage(), emit, emitMultiple, abortEmitting() and abortReading()\nAn example of how this API simplifies an operator can be found below in the second version of the Duplicator.","title":"Emit"},{"location":"/stream/stream-customize.html#custom-linear-operators-using-graphstage","text":"To define custom linear operators, you should extend GraphStageGraphStage using FlowShapeFlowShape which has one input and one output.\nSuch an operator can be illustrated as a box with two flows as it is seen in the illustration below. Demand flowing upstream leading to elements flowing downstream.\nTo illustrate these concepts we create a small GraphStage that implements the map transformation.\nMap calls push(out) from the onPush() handler and it also calls pull() from the onPull handler resulting in the conceptual wiring above, and fully expressed in code below:\nScala copysourceclass Map[A, B](f: A => B) extends GraphStage[FlowShape[A, B]] {\n\n  val in = Inlet[A](\"Map.in\")\n  val out = Outlet[B](\"Map.out\")\n\n  override val shape = FlowShape.of(in, out)\n\n  override def createLogic(attr: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = {\n            push(out, f(grab(in)))\n          }\n        })\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            pull(in)\n          }\n        })\n    }\n} Java copysourcepublic class Map<A, B> extends GraphStage<FlowShape<A, B>> {\n\n  private final Function<A, B> f;\n\n  public Map(Function<A, B> f) {\n    this.f = f;\n  }\n\n  public final Inlet<A> in = Inlet.create(\"Map.in\");\n  public final Outlet<B> out = Outlet.create(\"Map.out\");\n\n  private final FlowShape<A, B> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, B> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() throws Exception {\n                push(out, f.apply(grab(in)));\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                pull(in);\n              }\n            });\n      }\n    };\n  }\n}\nMap is a typical example of a one-to-one transformation of a stream where demand is passed along upstream elements passed on downstream.\nTo demonstrate a many-to-one operator we will implement filter. The conceptual wiring of Filter looks like this:\nAs we see above, if the given predicate matches the current element we are propagating it downwards, otherwise we return the “ball” to our upstream so that we get the new element. This is achieved by modifying the map example by adding a conditional in the onPush handler and decide between a pull(in) or push(out) call (and not having a mapping f function).\nScala copysourceclass Filter[A](p: A => Boolean) extends GraphStage[FlowShape[A, A]] {\n\n  val in = Inlet[A](\"Filter.in\")\n  val out = Outlet[A](\"Filter.out\")\n\n  val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = {\n            val elem = grab(in)\n            if (p(elem)) push(out, elem)\n            else pull(in)\n          }\n        })\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            pull(in)\n          }\n        })\n    }\n} Java copysourcepublic final class Filter<A> extends GraphStage<FlowShape<A, A>> {\n\n  private final Predicate<A> p;\n\n  public Filter(Predicate<A> p) {\n    this.p = p;\n  }\n\n  public final Inlet<A> in = Inlet.create(\"Filter.in\");\n  public final Outlet<A> out = Outlet.create(\"Filter.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                A elem = grab(in);\n                if (p.test(elem)) {\n                  push(out, elem);\n                } else {\n                  pull(in);\n                }\n              }\n            });\n\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                pull(in);\n              }\n            });\n      }\n    };\n  }\n}\nTo complete the picture we define a one-to-many transformation as the next step. We chose a straightforward example operator that emits every upstream element twice downstream. The conceptual wiring of this operator looks like this:\nThis is an operator that has state: an option with the last element it has seen indicating if it has duplicated this last element already or not. We must also make sure to emit the extra element if the upstream completes.\nScala copysourceclass Duplicator[A] extends GraphStage[FlowShape[A, A]] {\n\n  val in = Inlet[A](\"Duplicator.in\")\n  val out = Outlet[A](\"Duplicator.out\")\n\n  val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n      // Again: note that all mutable state\n      // MUST be inside the GraphStageLogic\n      var lastElem: Option[A] = None\n\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = {\n            val elem = grab(in)\n            lastElem = Some(elem)\n            push(out, elem)\n          }\n\n          override def onUpstreamFinish(): Unit = {\n            if (lastElem.isDefined) emit(out, lastElem.get)\n            complete(out)\n          }\n\n        })\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            if (lastElem.isDefined) {\n              push(out, lastElem.get)\n              lastElem = None\n            } else {\n              pull(in)\n            }\n          }\n        })\n    }\n} Java copysourcepublic class Duplicator<A> extends GraphStage<FlowShape<A, A>> {\n\n  public final Inlet<A> in = Inlet.create(\"Duplicator.in\");\n  public final Outlet<A> out = Outlet.create(\"Duplicator.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      // Again: note that all mutable state\n      // MUST be inside the GraphStageLogic\n      Option<A> lastElem = Option.none();\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                A elem = grab(in);\n                lastElem = Option.some(elem);\n                push(out, elem);\n              }\n\n              @Override\n              public void onUpstreamFinish() {\n                if (lastElem.isDefined()) {\n                  emit(out, lastElem.get());\n                }\n                complete(out);\n              }\n            });\n\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                if (lastElem.isDefined()) {\n                  push(out, lastElem.get());\n                  lastElem = Option.none();\n                } else {\n                  pull(in);\n                }\n              }\n            });\n      }\n    };\n  }\n}\nIn this case a pull from downstream might be consumed by the operator itself rather than passed along upstream as the operator might contain an element it wants to push. Note that we also need to handle the case where the upstream closes while the operator still has elements it wants to push downstream. This is done by overriding onUpstreamFinish in the InHandlerAbstractInHandler and provide custom logic that should happen when the upstream has been finished.\nThis example can be simplified by replacing the usage of a mutable state with calls to emitMultiple which will replace the handlers, emit each of multiple elements and then reinstate the original handlers:\nScala copysourceclass Duplicator[A] extends GraphStage[FlowShape[A, A]] {\n\n  val in = Inlet[A](\"Duplicator.in\")\n  val out = Outlet[A](\"Duplicator.out\")\n\n  val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = {\n            val elem = grab(in)\n            // this will temporarily suspend this handler until the two elems\n            // are emitted and then reinstates it\n            emitMultiple(out, Iterable(elem, elem))\n          }\n        })\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            pull(in)\n          }\n        })\n    }\n} Java copysourcepublic class Duplicator2<A> extends GraphStage<FlowShape<A, A>> {\n\n  public final Inlet<A> in = Inlet.create(\"Duplicator.in\");\n  public final Outlet<A> out = Outlet.create(\"Duplicator.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                A elem = grab(in);\n                // this will temporarily suspend this handler until the two elems\n                // are emitted and then reinstates it\n                emitMultiple(out, Arrays.asList(elem, elem).iterator());\n              }\n            });\n\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                pull(in);\n              }\n            });\n      }\n    };\n  }\n}\nFinally, to demonstrate all of the operators above, we put them together into a processing chain, which conceptually would correspond to the following structure:\nIn code this is only a few lines, using the viavia use our custom operators in a stream:\nScala copysourceval resultFuture =\n  Source(1 to 5).via(new Filter(_ % 2 == 0)).via(new Duplicator()).via(new Map(_ / 2)).runWith(sink)\n Java copysourceCompletionStage<String> resultFuture =\n    Source.from(Arrays.asList(1, 2, 3, 4, 5))\n        .via(new Filter<Integer>((n) -> n % 2 == 0))\n        .via(new Duplicator<Integer>())\n        .via(new Map<Integer, Integer>((n) -> n / 2))\n        .runWith(sink, system);\nIf we attempt to draw the sequence of events, it shows that there is one “event token” in circulation in a potential chain of operators, just like our conceptual “railroad tracks” representation predicts.","title":"Custom linear operators using GraphStage"},{"location":"/stream/stream-customize.html#completion","text":"Completion handling usually (but not exclusively) comes into the picture when operators need to emit a few more elements after their upstream source has been completed. We have seen an example of this in our first Duplicator implementation where the last element needs to be doubled even after the upstream neighbor operator has been completed. This can be done by overriding the onUpstreamFinish method in InHandlerAbstractInHandler.\nOperators by default automatically stop once all of their ports (input and output) have been closed externally or internally. It is possible to opt out from this behavior by invoking setKeepGoing(true) (which is not supported from the operator’s constructor and usually done in preStart). In this case the operator must be explicitly closed by calling completeStage()completeStage() or `failStage(exception)failStage(exception). This feature carries the risk of leaking streams and actors, therefore it should be used with care.","title":"Completion"},{"location":"/stream/stream-customize.html#logging-inside-graphstages","text":"Logging debug or other important information in your operators is often a very good idea, especially when developing more advanced operators which may need to be debugged at some point.\nThe helper trait stream.stage.StageLogging is provided to enable you to obtain a LoggingAdapterLoggingAdapter inside of a GraphStageGraphStage as long as the MaterializerMaterializer you’re using is able to provide you with a logger. In that sense, it serves a very similar purpose as ActorLoggingActorLogging does for Actors.\nYou can extend the GraphStageLogicWithLoggingGraphStageLogicWithLogging or TimerGraphStageLogicWithLoggingTimerGraphStageLogicWithLogging classes instead of the usual GraphStageLogicGraphStageLogic to enable you to obtain a LoggingAdapterLoggingAdapter inside your operator as long as the MaterializerMaterializer you’re using is able to provide you with a logger.\nNote Please note that you can always use a logging library directly inside an operator. Make sure to use an asynchronous appender however, to not accidentally block the operator when writing to files etc. See Using the SLF4J API directly for more details on setting up async appenders in SLF4J.\nThe operator then gets access to the log field which it can safely use from any GraphStage callbacks:\nScala copysourceimport org.apache.pekko.stream.stage.{ GraphStage, GraphStageLogic, OutHandler, StageLogging }\n\nfinal class RandomLettersSource extends GraphStage[SourceShape[String]] {\n  val out = Outlet[String](\"RandomLettersSource.out\")\n  override val shape: SourceShape[String] = SourceShape(out)\n\n  override def createLogic(inheritedAttributes: Attributes) =\n    new GraphStageLogic(shape) with StageLogging {\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            val c = nextChar() // ASCII lower case letters\n\n            // `log` is obtained from materializer automatically (via StageLogging)\n            log.debug(\"Randomly generated: [{}]\", c)\n\n            push(out, c.toString)\n          }\n        })\n    }\n\n  def nextChar(): Char =\n    ThreadLocalRandom.current().nextInt('a', 'z'.toInt + 1).toChar\n} Java copysourcepublic class RandomLettersSource extends GraphStage<SourceShape<String>> {\n  public final Outlet<String> out = Outlet.create(\"RandomLettersSource.in\");\n\n  private final SourceShape<String> shape = SourceShape.of(out);\n\n  @Override\n  public SourceShape<String> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogicWithLogging(shape()) {\n\n      {\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                final String s = nextChar(); // ASCII lower case letters\n\n                // `log` is obtained from materializer automatically (via StageLogging)\n                log().debug(\"Randomly generated: [{}]\", s);\n\n                push(out, s);\n              }\n\n              private String nextChar() {\n                final char i = (char) ThreadLocalRandom.current().nextInt('a', 'z' + 1);\n                return String.valueOf(i);\n              }\n            });\n      }\n    };\n  }\n}\nNote SPI Note: If you’re implementing a Materializer, you can add this ability to your materializer by implementing MaterializerLoggingProviderMaterializerLoggingProvider in your MaterializerMaterializer.","title":"Logging inside GraphStages"},{"location":"/stream/stream-customize.html#using-timers","text":"It is possible to use timers in GraphStages by using TimerGraphStageLogicTimerGraphStageLogic as the base class for the returned logic. Timers can be scheduled by calling one of scheduleOnce(timerKey,delay), scheduleAtFixedRate(timerKey,initialDelay,interval) or scheduleWithFixedDelay(timerKey,initialDelay,interval) and passing an object as a key for that timer (can be any object, for example a String). The onTimer(timerKey) method needs to be overridden, and it will be called once the timer of timerKey fires. It is possible to cancel a timer using cancelTimer(timerKey) and check the status of a timer with isTimerActive(timerKey). Timers will be automatically cleaned up when the operator completes.\nTimers can not be scheduled from the constructor of the logic, but it is possible to schedule them from the preStart() lifecycle hook.\nIn this sample the operator toggles between open and closed, where open means no elements are passed through. The operator starts out as closed but as soon as an element is pushed downstream the gate becomes open for a duration of time during which it will consume and drop upstream messages:\nScala copysource// each time an event is pushed through it will trigger a period of silence\nclass TimedGate[A](silencePeriod: FiniteDuration) extends GraphStage[FlowShape[A, A]] {\n\n  val in = Inlet[A](\"TimedGate.in\")\n  val out = Outlet[A](\"TimedGate.out\")\n\n  val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new TimerGraphStageLogic(shape) {\n\n      var open = false\n\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = {\n            val elem = grab(in)\n            if (open) pull(in)\n            else {\n              push(out, elem)\n              open = true\n              scheduleOnce(None, silencePeriod)\n            }\n          }\n        })\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = { pull(in) }\n        })\n\n      override protected def onTimer(timerKey: Any): Unit = {\n        open = false\n      }\n    }\n} Java copysource// each time an event is pushed through it will trigger a period of silence\npublic class TimedGate<A> extends GraphStage<FlowShape<A, A>> {\n\n  private final int silencePeriodInSeconds;\n\n  public TimedGate(int silencePeriodInSeconds) {\n    this.silencePeriodInSeconds = silencePeriodInSeconds;\n  }\n\n  public final Inlet<A> in = Inlet.create(\"TimedGate.in\");\n  public final Outlet<A> out = Outlet.create(\"TimedGate.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new TimerGraphStageLogic(shape) {\n\n      private boolean open = false;\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() throws Exception {\n                A elem = grab(in);\n                if (open) pull(in);\n                else {\n                  push(out, elem);\n                  open = true;\n                  scheduleOnce(\"key\", java.time.Duration.ofSeconds(silencePeriodInSeconds));\n                }\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                pull(in);\n              }\n            });\n      }\n\n      @Override\n      public void onTimer(Object key) {\n        if (key.equals(\"key\")) {\n          open = false;\n        }\n      }\n    };\n  }\n}","title":"Using timers"},{"location":"/stream/stream-customize.html#using-asynchronous-side-channels","text":"In order to receive asynchronous events that are not arriving as stream elements (for example a completion of a future or a callback from a 3rd party API) one must acquire a AsyncCallbackAsyncCallback by calling getAsyncCallback()getAsyncCallback() from the operator logic. The method getAsyncCallback takes as a parameter a callback that will be called once the asynchronous event fires. It is important to not call the callback directly, instead, the external API must call the invoke(event) method on the returned AsyncCallback. The execution engine will take care of calling the provided callback in a thread-safe way. The callback can safely access the state of the GraphStageLogicGraphStageLogic implementation.\nSharing the AsyncCallback from the constructor risks race conditions, therefore it is recommended to use the preStart() lifecycle hook instead.\nThis example shows an asynchronous side channel operator that starts dropping elements when a future completes:\nScala copysource// will close upstream in all materializations of the graph stage instance\n// when the future completes\nclass KillSwitch[A](switch: Future[Unit]) extends GraphStage[FlowShape[A, A]] {\n\n  val in = Inlet[A](\"KillSwitch.in\")\n  val out = Outlet[A](\"KillSwitch.out\")\n\n  val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n\n      override def preStart(): Unit = {\n        val callback = getAsyncCallback[Unit] { _ =>\n          completeStage()\n        }\n        switch.foreach(callback.invoke)\n      }\n\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = { push(out, grab(in)) }\n        })\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = { pull(in) }\n        })\n    }\n} Java copysource// will close upstream in all materializations of the stage instance\n// when the completion stage completes\npublic class KillSwitch<A> extends GraphStage<FlowShape<A, A>> {\n\n  private final CompletionStage<Done> switchF;\n\n  public KillSwitch(CompletionStage<Done> switchF) {\n    this.switchF = switchF;\n  }\n\n  public final Inlet<A> in = Inlet.create(\"KillSwitch.in\");\n  public final Outlet<A> out = Outlet.create(\"KillSwitch.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                push(out, grab(in));\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                pull(in);\n              }\n            });\n      }\n\n      @Override\n      public void preStart() {\n        AsyncCallback<Done> callback =\n            createAsyncCallback(\n                new Procedure<Done>() {\n                  @Override\n                  public void apply(Done param) throws Exception {\n                    completeStage();\n                  }\n                });\n\n        ExecutionContext ec = system.dispatcher();\n        switchF.thenAccept(callback::invoke);\n      }\n    };\n  }\n}","title":"Using asynchronous side-channels"},{"location":"/stream/stream-customize.html#integration-with-actors","text":"This section is a stub and will be extended in the next release This is a may change feature*\nIt is possible to acquire an ActorRef that can be addressed from the outside of the operator, similarly how AsyncCallbackAsyncCallback allows injecting asynchronous events into an operator logic. This reference can be obtained by calling getStageActor(receive) passing in a function that takes a Pair of the sender ActorRefActorRef and the received message. This reference can be used to watch other actors by calling its watch(ref) or unwatch(ref) methods. The reference can be also watched by external actors. The current limitations of this ActorRef are:\nthey are not location transparent, they cannot be accessed via remoting. they cannot be returned as materialized values. they cannot be accessed from the constructor of the GraphStageLogic, but they can be accessed from the preStart() method.","title":"Integration with actors"},{"location":"/stream/stream-customize.html#custom-materialized-values","text":"Custom operators can return materialized values instead of NotUsedNotUsed by inheriting from GraphStageWithMaterializedValueAbstractGraphStageWithMaterializedValue instead of the simpler GraphStage. The difference is that in this case the method createLogicAndMaterializedValue(inheritedAttributes)createLogicAndMaterializedValue(inheritedAttributes needs to be overridden, and in addition to the operator logic the materialized value must be provided\nWarning There is no built-in synchronization of accessing this value from both of the thread where the logic runs and the thread that got hold of the materialized value. It is the responsibility of the programmer to add the necessary (non-blocking) synchronization and visibility guarantees to this shared object.\nIn this sample the materialized value is a future containing the first element to go through the stream:\nScala copysourceclass FirstValue[A] extends GraphStageWithMaterializedValue[FlowShape[A, A], Future[A]] {\n\n  val in = Inlet[A](\"FirstValue.in\")\n  val out = Outlet[A](\"FirstValue.out\")\n\n  val shape = FlowShape.of(in, out)\n\n  override def createLogicAndMaterializedValue(inheritedAttributes: Attributes): (GraphStageLogic, Future[A]) = {\n    val promise = Promise[A]()\n    val logic = new GraphStageLogic(shape) {\n\n      setHandler(in,\n        new InHandler {\n          override def onPush(): Unit = {\n            val elem = grab(in)\n            promise.success(elem)\n            push(out, elem)\n\n            // replace handler with one that only forwards elements\n            setHandler(in,\n              new InHandler {\n                override def onPush(): Unit = {\n                  push(out, grab(in))\n                }\n              })\n          }\n        })\n\n      setHandler(out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            pull(in)\n          }\n        })\n\n    }\n\n    (logic, promise.future)\n  }\n} Java copysourcepublic class FirstValue<A>\n    extends AbstractGraphStageWithMaterializedValue<FlowShape<A, A>, CompletionStage<A>> {\n\n  public final Inlet<A> in = Inlet.create(\"FirstValue.in\");\n  public final Outlet<A> out = Outlet.create(\"FirstValue.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  @Override\n  public Pair<GraphStageLogic, CompletionStage<A>> createLogicAndMaterializedValuePair(\n      Attributes inheritedAttributes) {\n    CompletableFuture<A> promise = new CompletableFuture<>();\n\n    GraphStageLogic logic =\n        new GraphStageLogic(shape) {\n          {\n            setHandler(\n                in,\n                new AbstractInHandler() {\n                  @Override\n                  public void onPush() {\n                    A elem = grab(in);\n                    promise.complete(elem);\n                    push(out, elem);\n\n                    // replace handler with one that only forwards elements\n                    setHandler(\n                        in,\n                        new AbstractInHandler() {\n                          @Override\n                          public void onPush() {\n                            push(out, grab(in));\n                          }\n                        });\n                  }\n                });\n\n            setHandler(\n                out,\n                new AbstractOutHandler() {\n                  @Override\n                  public void onPull() throws Exception {\n                    pull(in);\n                  }\n                });\n          }\n        };\n\n    return new Pair<>(logic, promise);\n  }\n}","title":"Custom materialized values"},{"location":"/stream/stream-customize.html#using-attributes-to-affect-the-behavior-of-an-operator","text":"This section is a stub and will be extended in the next release\nOperators can access the AttributesAttributes object created by the materializer. This contains all the applied (inherited) attributes applying to the operator, ordered from least specific (outermost) towards the most specific (innermost) attribute. It is the responsibility of the operator to decide how to reconcile this inheritance chain to a final effective decision.\nSee Modularity, Composition and Hierarchy for an explanation on how attributes work.","title":"Using attributes to affect the behavior of an operator"},{"location":"/stream/stream-customize.html#rate-decoupled-operators","text":"Sometimes, it is desirable to decouple the rate of the upstream and downstream of an operator, synchronizing only when needed.\nThis is achieved in the model by representing a GraphStage as a boundary between two regions where the demand sent upstream is decoupled from the demand that arrives from downstream. One immediate consequence of this difference is that an onPush call does not always lead to calling push and an onPull call does not always lead to calling pull.\nOne of the important use-case for this is to build buffer-like entities, that allow independent progress of upstream and downstream operators when the buffer is not full or empty, and slowing down the appropriate side if the buffer becomes empty or full.\nThe next diagram illustrates the event sequence for a buffer with capacity of two elements in a setting where the downstream demand is slow to start and the buffer will fill up with upstream elements before any demand is seen from downstream.\nAnother scenario would be where the demand from downstream starts coming in before any element is pushed into the buffer operator.\nThe first difference we can notice is that our Buffer operator is automatically pulling its upstream on initialization. The buffer has demand for up to two elements without any downstream demand.\nThe following code example demonstrates a buffer class corresponding to the message sequence chart above.\nScala copysourceclass TwoBuffer[A] extends GraphStage[FlowShape[A, A]] {\n\n  val in = Inlet[A](\"TwoBuffer.in\")\n  val out = Outlet[A](\"TwoBuffer.out\")\n\n  val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic =\n    new GraphStageLogic(shape) {\n\n      val buffer = mutable.Queue[A]()\n      def bufferFull = buffer.size == 2\n      var downstreamWaiting = false\n\n      override def preStart(): Unit = {\n        // a detached stage needs to start upstream demand\n        // itself as it is not triggered by downstream demand\n        pull(in)\n      }\n\n      setHandler(\n        in,\n        new InHandler {\n          override def onPush(): Unit = {\n            val elem = grab(in)\n            buffer.enqueue(elem)\n            if (downstreamWaiting) {\n              downstreamWaiting = false\n              val bufferedElem = buffer.dequeue()\n              push(out, bufferedElem)\n            }\n            if (!bufferFull) {\n              pull(in)\n            }\n          }\n\n          override def onUpstreamFinish(): Unit = {\n            if (buffer.nonEmpty) {\n              // emit the rest if possible\n              emitMultiple(out, buffer.toIterator)\n            }\n            completeStage()\n          }\n        })\n\n      setHandler(\n        out,\n        new OutHandler {\n          override def onPull(): Unit = {\n            if (buffer.isEmpty) {\n              downstreamWaiting = true\n            } else {\n              val elem = buffer.dequeue()\n              push(out, elem)\n            }\n            if (!bufferFull && !hasBeenPulled(in)) {\n              pull(in)\n            }\n          }\n        })\n    }\n\n} Java copysourcepublic class TwoBuffer<A> extends GraphStage<FlowShape<A, A>> {\n\n  public final Inlet<A> in = Inlet.create(\"TwoBuffer.in\");\n  public final Outlet<A> out = Outlet.create(\"TwoBuffer.out\");\n\n  private final FlowShape<A, A> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<A, A> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n\n      private final int SIZE = 2;\n      private Queue<A> buffer = new ArrayDeque<>(SIZE);\n      private boolean downstreamWaiting = false;\n\n      private boolean isBufferFull() {\n        return buffer.size() == SIZE;\n      }\n\n      @Override\n      public void preStart() {\n        // a detached stage needs to start upstream demand\n        // itself as it is not triggered by downstream demand\n        pull(in);\n      }\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                A elem = grab(in);\n                buffer.add(elem);\n                if (downstreamWaiting) {\n                  downstreamWaiting = false;\n                  A bufferedElem = buffer.poll();\n                  push(out, bufferedElem);\n                }\n                if (!isBufferFull()) {\n                  pull(in);\n                }\n              }\n\n              @Override\n              public void onUpstreamFinish() {\n                if (!buffer.isEmpty()) {\n                  // emit the rest if possible\n                  emitMultiple(out, buffer.iterator());\n                }\n                completeStage();\n              }\n            });\n\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                if (buffer.isEmpty()) {\n                  downstreamWaiting = true;\n                } else {\n                  A elem = buffer.poll();\n                  push(out, elem);\n                }\n                if (!isBufferFull() && !hasBeenPulled(in)) {\n                  pull(in);\n                }\n              }\n            });\n      }\n    };\n  }\n}","title":"Rate decoupled operators"},{"location":"/stream/stream-customize.html#thread-safety-of-custom-operators","text":"All of the above custom operators (linear or graph) provide a few simple guarantees that implementors can rely on.\nThe callbacks exposed by all of these classes are never called concurrently. The state encapsulated by these classes can be safely modified from the provided callbacks, without any further synchronization.\nIn essence, the above guarantees are similar to what Actor s provide, if one thinks of the state of a custom operator as state of an actor, and the callbacks as the receive block of the actor.\nWarning It is not safe to access the state of any custom operator outside of the callbacks that it provides, just like it is unsafe to access the state of an actor from the outside. This means that Future callbacks should not close over internal state of custom operators because such access can be concurrent with the provided callbacks, leading to undefined behavior.","title":"Thread safety of custom operators"},{"location":"/stream/stream-customize.html#resources-and-the-operator-lifecycle","text":"If an operator manages a resource with a lifecycle, for example objects that need to be shutdown when they are not used anymore it is important to make sure this will happen in all circumstances when the operator shuts down.\nCleaning up resources should be done in GraphStageLogic.postStopGraphStageLogic.postStop and not in the InHandlerAbstractInHandler and OutHandler AbstractOutHandler callbacks. The reason for this is that when the operator itself completes or is failed there is no signal from the upstreams or the downstreams. Even for operators that do not complete or fail in this manner, this can happen when the MaterializerMaterializer is shutdown or the ActorSystemActorSystem is terminated while a stream is still running, what is called an “abrupt termination”.\nExtending Flow Operators with Custom Operators The most general way of extending any SourceSource, FlowFlow or SubFlowSubFlow (e.g. from groupBy) is demonstrated above: create an operator of flow-shape like the Duplicator example given above and use the .via(...).via(...) operator to integrate it into your stream topology. This works with all FlowOps sub-types, including the ports that you connect with the graph DSL. Advanced Scala users may wonder whether it is possible to write extension methods that enrich FlowOps to allow nicer syntax. The short answer is that Scala 2 does not support this in a fully generic fashion, the problem is that it is impossible to abstract over the kind of stream that is being extended because Source, Flow and SubFlow differ in the number and kind of their type parameters. While it would be possible to write an implicit class that enriches them generically, this class would require explicit instantiation with all type parameters due to SI-2712. For a partial workaround that unifies extensions to Source and Flow see this sketch by R. Kuhn. A lot simpler is the task of adding an extension method to Source as shown below: copysourceimplicit class SourceDuplicator[Out, Mat](s: Source[Out, Mat]) {\n  def duplicateElements: Source[Out, Mat] = s.via(new Duplicator)\n}\n\nval s = Source(1 to 3).duplicateElements\n\ns.runWith(Sink.seq).futureValue should ===(Seq(1, 1, 2, 2, 3, 3)) The analog works for Flow as well: copysourceimplicit class FlowDuplicator[In, Out, Mat](s: Flow[In, Out, Mat]) {\n  def duplicateElements: Flow[In, Out, Mat] = s.via(new Duplicator)\n}\n\nval f = Flow[Int].duplicateElements\n\nSource(1 to 3).via(f).runWith(Sink.seq).futureValue should ===(Seq(1, 1, 2, 2, 3, 3)) If you try to write this for SubFlow, though, you will run into the same issue as when trying to unify the two solutions above, only on a higher level (the type constructors needed for that unification would have rank two, meaning that some of their type arguments are type constructors themselves—when trying to extend the solution shown in the linked sketch the author encountered such a density of compiler StackOverflowErrors and IDE failures that he gave up). It is interesting to note that a simplified form of this problem has found its way into the dotty test suite. Dotty is the development version of Scala on its way to Scala 3.","title":"Resources and the operator lifecycle"},{"location":"/stream/futures-interop.html","text":"","title":"Futures interop"},{"location":"/stream/futures-interop.html#futures-interop","text":"","title":"Futures interop"},{"location":"/stream/futures-interop.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/futures-interop.html#overview","text":"Stream transformations and side effects involving external non-stream based services can be performed with mapAsync or mapAsyncUnordered.\nFor example, sending emails to the authors of selected tweets using an external email service:\nScala copysourcedef send(email: Email): Future[Unit] = {\n  // ...\n} Java copysourcepublic CompletionStage<Email> send(Email email) {\n  // ...\n}\nWe start with the tweet stream of authors:\nScala copysourceval authors: Source[Author, NotUsed] =\n  tweets.filter(_.hashtags.contains(pekkoTag)).map(_.author) Java copysourcefinal Source<Author, NotUsed> authors =\n    tweets.filter(t -> t.hashtags().contains(PEKKO)).map(t -> t.author);\nAssume that we can look up their email address using:\nScala copysourcedef lookupEmail(handle: String): Future[Option[String]] = Java copysourcepublic CompletionStage<Optional<String>> lookupEmail(String handle)\nTransforming the stream of authors to a stream of email addresses by using the lookupEmail service can be done with mapAsync:\nScala copysourceval emailAddresses: Source[String, NotUsed] =\n  authors.mapAsync(4)(author => addressSystem.lookupEmail(author.handle)).collect {\n    case Some(emailAddress) => emailAddress\n  } Java copysourcefinal Source<String, NotUsed> emailAddresses =\n    authors\n        .mapAsync(4, author -> addressSystem.lookupEmail(author.handle))\n        .filter(o -> o.isPresent())\n        .map(o -> o.get());\nFinally, sending the emails:\nScala copysourceval sendEmails: RunnableGraph[NotUsed] =\n  emailAddresses\n    .mapAsync(4)(address => {\n      emailServer.send(Email(to = address, title = \"pekko\", body = \"I like your tweet\"))\n    })\n    .to(Sink.ignore)\n\nsendEmails.run() Java copysourcefinal RunnableGraph<NotUsed> sendEmails =\n    emailAddresses\n        .mapAsync(\n            4,\n            address -> emailServer.send(new Email(address, \"PEKKO\", \"I like your tweet\")))\n        .to(Sink.ignore());\n\nsendEmails.run(system);\nmapAsync is applying the given function that is calling out to the external service to each of the elements as they pass through this processing step. The function returns a FutureCompletionStage and the value of that future will be emitted downstream. The number of Futures that shall run in parallel is given as the first argument to mapAsync. These Futures may complete in any order, but the elements that are emitted downstream are in the same order as received from upstream.\nThat means that back-pressure works as expected. For example if the emailServer.send is the bottleneck it will limit the rate at which incoming tweets are retrieved and email addresses looked up.\nThe final piece of this pipeline is to generate the demand that pulls the tweet authors information through the emailing pipeline: we attach a Sink.ignore which makes it all run. If our email process would return some interesting data for further transformation then we would not ignore it but send that result stream onwards for further processing or storage.\nNote that mapAsync preserves the order of the stream elements. In this example the order is not important and then we can use the more efficient mapAsyncUnordered:\nScala copysourceval authors: Source[Author, NotUsed] =\n  tweets.filter(_.hashtags.contains(pekkoTag)).map(_.author)\n\nval emailAddresses: Source[String, NotUsed] =\n  authors.mapAsyncUnordered(4)(author => addressSystem.lookupEmail(author.handle)).collect {\n    case Some(emailAddress) => emailAddress\n  }\n\nval sendEmails: RunnableGraph[NotUsed] =\n  emailAddresses\n    .mapAsyncUnordered(4)(address => {\n      emailServer.send(Email(to = address, title = \"Pekko\", body = \"I like your tweet\"))\n    })\n    .to(Sink.ignore)\n\nsendEmails.run() Java copysourcefinal Source<Author, NotUsed> authors =\n    tweets.filter(t -> t.hashtags().contains(PEKKO)).map(t -> t.author);\n\nfinal Source<String, NotUsed> emailAddresses =\n    authors\n        .mapAsyncUnordered(4, author -> addressSystem.lookupEmail(author.handle))\n        .filter(o -> o.isPresent())\n        .map(o -> o.get());\n\nfinal RunnableGraph<NotUsed> sendEmails =\n    emailAddresses\n        .mapAsyncUnordered(\n            4,\n            address -> emailServer.send(new Email(address, \"Pekko\", \"I like your tweet\")))\n        .to(Sink.ignore());\n\nsendEmails.run(system);\nIn the above example the services conveniently returned a FutureCompletionStage of the result. If that is not the case you need to wrap the call in a FutureCompletionStage. If the service call involves blocking you must also make sure that you run it on a dedicated execution context, to avoid starvation and disturbance of other tasks in the system.\nScala copysourceval blockingExecutionContext = system.dispatchers.lookup(\"blocking-dispatcher\")\n\nval sendTextMessages: RunnableGraph[NotUsed] =\n  phoneNumbers\n    .mapAsync(4)(phoneNo => {\n      Future {\n        smsServer.send(TextMessage(to = phoneNo, body = \"I like your tweet\"))\n      }(blockingExecutionContext)\n    })\n    .to(Sink.ignore)\n\nsendTextMessages.run() Java copysourcefinal Executor blockingEc = system.dispatchers().lookup(\"blocking-dispatcher\");\n\nfinal RunnableGraph<NotUsed> sendTextMessages =\n    phoneNumbers\n        .mapAsync(\n            4,\n            phoneNo ->\n                CompletableFuture.supplyAsync(\n                    () -> smsServer.send(new TextMessage(phoneNo, \"I like your tweet\")),\n                    blockingEc))\n        .to(Sink.ignore());\n\nsendTextMessages.run(system);\nThe configuration of the \"blocking-dispatcher\" may look something like:\ncopysourceblocking-dispatcher {\n  executor = \"thread-pool-executor\"\n  thread-pool-executor {\n    core-pool-size-min    = 10\n    core-pool-size-max    = 10\n  }\n}\nAn alternative for blocking calls is to perform them in a map operation, still using a dedicated dispatcher for that operation.\nScala copysourceval send = Flow[String]\n  .map { phoneNo =>\n    smsServer.send(TextMessage(to = phoneNo, body = \"I like your tweet\"))\n  }\n  .withAttributes(ActorAttributes.dispatcher(\"blocking-dispatcher\"))\nval sendTextMessages: RunnableGraph[NotUsed] =\n  phoneNumbers.via(send).to(Sink.ignore)\n\nsendTextMessages.run() Java copysourcefinal Flow<String, Boolean, NotUsed> send =\n    Flow.of(String.class)\n        .map(phoneNo -> smsServer.send(new TextMessage(phoneNo, \"I like your tweet\")))\n        .withAttributes(ActorAttributes.dispatcher(\"blocking-dispatcher\"));\nfinal RunnableGraph<?> sendTextMessages = phoneNumbers.via(send).to(Sink.ignore());\n\nsendTextMessages.run(system);\nHowever, that is not exactly the same as mapAsync, since the mapAsync may run several calls concurrently, but map performs them one at a time.\nFor a service that is exposed as an actor, or if an actor is used as a gateway in front of an external service, you can use ask:\nScala copysourceimport org.apache.pekko.pattern.ask\n\nval PekkoTweets: Source[Tweet, NotUsed] = tweets.filter(_.hashtags.contains(pekkoTag))\n\nimplicit val timeout: Timeout = 3.seconds\nval saveTweets: RunnableGraph[NotUsed] =\n  PekkoTweets.mapAsync(4)(tweet => database ? Save(tweet)).to(Sink.ignore) Java copysourcefinal Source<Tweet, NotUsed> pekkoTweets = tweets.filter(t -> t.hashtags().contains(PEKKO));\n\nfinal RunnableGraph<NotUsed> saveTweets =\n    pekkoTweets\n        .mapAsync(4, tweet -> ask(database, new Save(tweet), Duration.ofMillis(300L)))\n        .to(Sink.ignore());\nNote that if the ask is not completed within the given timeout the stream is completed with failure. If that is not desired outcome you can use recover on the ask FutureCompletionStage.","title":"Overview"},{"location":"/stream/futures-interop.html#illustrating-ordering-and-parallelism","text":"Let us look at another example to get a better understanding of the ordering and parallelism characteristics of mapAsync and mapAsyncUnordered.\nSeveral mapAsync and mapAsyncUnordered futures may run concurrently. The number of concurrent futures are limited by the downstream demand. For example, if 5 elements have been requested by downstream there will be at most 5 futures in progress.\nmapAsync emits the future results in the same order as the input elements were received. That means that completed results are only emitted downstream when earlier results have been completed and emitted. One slow call will thereby delay the results of all successive calls, even though they are completed before the slow call.\nmapAsyncUnordered emits the future results as soon as they are completed, i.e. it is possible that the elements are not emitted downstream in the same order as received from upstream. One slow call will thereby not delay the results of faster successive calls as long as there is downstream demand of several elements.\nHere is a fictive service that we can use to illustrate these aspects.\nScala copysourceclass SometimesSlowService(implicit ec: ExecutionContext) {\n\n  private val runningCount = new AtomicInteger\n\n  def convert(s: String): Future[String] = {\n    println(s\"running: $s (${runningCount.incrementAndGet()})\")\n    Future {\n      if (s.nonEmpty && s.head.isLower)\n        Thread.sleep(500)\n      else\n        Thread.sleep(20)\n      println(s\"completed: $s (${runningCount.decrementAndGet()})\")\n      s.toUpperCase\n    }\n  }\n} Java copysourcestatic class SometimesSlowService {\n  private final Executor ec;\n\n  public SometimesSlowService(Executor ec) {\n    this.ec = ec;\n  }\n\n  private final AtomicInteger runningCount = new AtomicInteger();\n\n  public CompletionStage<String> convert(String s) {\n    System.out.println(\"running: \" + s + \"(\" + runningCount.incrementAndGet() + \")\");\n    return CompletableFuture.supplyAsync(\n        () -> {\n          if (!s.isEmpty() && Character.isLowerCase(s.charAt(0)))\n            try {\n              Thread.sleep(500);\n            } catch (InterruptedException e) {\n            }\n          else\n            try {\n              Thread.sleep(20);\n            } catch (InterruptedException e) {\n            }\n          System.out.println(\"completed: \" + s + \"(\" + runningCount.decrementAndGet() + \")\");\n          return s.toUpperCase();\n        },\n        ec);\n  }\n}\nElements starting with a lower case character are simulated to take longer time to process.\nHere is how we can use it with mapAsync:\nScala copysourceimplicit val blockingExecutionContext = system.dispatchers.lookup(\"blocking-dispatcher\")\nval service = new SometimesSlowService\n\nSource(List(\"a\", \"B\", \"C\", \"D\", \"e\", \"F\", \"g\", \"H\", \"i\", \"J\"))\n  .map(elem => { println(s\"before: $elem\"); elem })\n  .mapAsync(4)(service.convert)\n  .to(Sink.foreach(elem => println(s\"after: $elem\")))\n  .withAttributes(Attributes.inputBuffer(initial = 4, max = 4))\n  .run() Java copysourcefinal Executor blockingEc = system.dispatchers().lookup(\"blocking-dispatcher\");\nfinal SometimesSlowService service = new SometimesSlowService(blockingEc);\n\nSource.from(Arrays.asList(\"a\", \"B\", \"C\", \"D\", \"e\", \"F\", \"g\", \"H\", \"i\", \"J\"))\n    .map(\n        elem -> {\n          System.out.println(\"before: \" + elem);\n          return elem;\n        })\n    .mapAsync(4, service::convert)\n    .to(Sink.foreach(elem -> System.out.println(\"after: \" + elem)))\n    .withAttributes(Attributes.inputBuffer(4, 4))\n    .run(system);\nThe output may look like this:\nbefore: a\nbefore: B\nbefore: C\nbefore: D\nrunning: a (1)\nrunning: B (2)\nbefore: e\nrunning: C (3)\nbefore: F\nrunning: D (4)\nbefore: g\nbefore: H\ncompleted: C (3)\ncompleted: B (2)\ncompleted: D (1)\ncompleted: a (0)\nafter: A\nafter: B\nrunning: e (1)\nafter: C\nafter: D\nrunning: F (2)\nbefore: i\nbefore: J\nrunning: g (3)\nrunning: H (4)\ncompleted: H (2)\ncompleted: F (3)\ncompleted: e (1)\ncompleted: g (0)\nafter: E\nafter: F\nrunning: i (1)\nafter: G\nafter: H\nrunning: J (2)\ncompleted: J (1)\ncompleted: i (0)\nafter: I\nafter: J\nNote that after lines are in the same order as the before lines even though elements are completed in a different order. For example H is completed before g, but still emitted afterwards.\nThe numbers in parentheses illustrate how many calls that are in progress at the same time. Here the downstream demand and thereby the number of concurrent calls are limited by the buffer size (4) set with an attribute.\nHere is how we can use the same service with mapAsyncUnordered:\nScala copysourceimplicit val blockingExecutionContext = system.dispatchers.lookup(\"blocking-dispatcher\")\nval service = new SometimesSlowService\n\nSource(List(\"a\", \"B\", \"C\", \"D\", \"e\", \"F\", \"g\", \"H\", \"i\", \"J\"))\n  .map(elem => { println(s\"before: $elem\"); elem })\n  .mapAsyncUnordered(4)(service.convert)\n  .to(Sink.foreach(elem => println(s\"after: $elem\")))\n  .withAttributes(Attributes.inputBuffer(initial = 4, max = 4))\n  .run() Java copysourcefinal Executor blockingEc = system.dispatchers().lookup(\"blocking-dispatcher\");\nfinal SometimesSlowService service = new SometimesSlowService(blockingEc);\n\nSource.from(Arrays.asList(\"a\", \"B\", \"C\", \"D\", \"e\", \"F\", \"g\", \"H\", \"i\", \"J\"))\n    .map(\n        elem -> {\n          System.out.println(\"before: \" + elem);\n          return elem;\n        })\n    .mapAsyncUnordered(4, service::convert)\n    .to(Sink.foreach(elem -> System.out.println(\"after: \" + elem)))\n    .withAttributes(Attributes.inputBuffer(4, 4))\n    .run(system);\nThe output may look like this:\nbefore: a\nbefore: B\nbefore: C\nbefore: D\nrunning: a (1)\nrunning: B (2)\nbefore: e\nrunning: C (3)\nbefore: F\nrunning: D (4)\nbefore: g\nbefore: H\ncompleted: B (3)\ncompleted: C (1)\ncompleted: D (2)\nafter: B\nafter: D\nrunning: e (2)\nafter: C\nrunning: F (3)\nbefore: i\nbefore: J\ncompleted: F (2)\nafter: F\nrunning: g (3)\nrunning: H (4)\ncompleted: H (3)\nafter: H\ncompleted: a (2)\nafter: A\nrunning: i (3)\nrunning: J (4)\ncompleted: J (3)\nafter: J\ncompleted: e (2)\nafter: E\ncompleted: g (1)\nafter: G\ncompleted: i (0)\nafter: I\nNote that after lines are not in the same order as the before lines. For example H overtakes the slow G.\nThe numbers in parentheses illustrate how many calls that are in progress at the same time. Here the downstream demand and thereby the number of concurrent calls are limited by the buffer size (4) set with an attribute.","title":"Illustrating ordering and parallelism"},{"location":"/stream/actor-interop.html","text":"","title":"Actors interop"},{"location":"/stream/actor-interop.html#actors-interop","text":"","title":"Actors interop"},{"location":"/stream/actor-interop.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/actor-interop.html#overview","text":"There are various use cases where it might be reasonable to use actors and streams together:\nwhen integrating existing API’s that might be streams- or actors-based. when there is any mutable state that should be shared across multiple streams. when there is any mutable state or logic that can be influenced ‘from outside’ while the stream is running.\nFor piping the elements of a stream as messages to an ordinary actor you can use ask in a mapAsync or use Sink.actorRefWithBackpressure.\nMessages can be sent to a stream with Source.queue or via the ActorRef that is materialized by Source.actorRef.\nAdditionally you can use ActorSource.actorRef, ActorSource.actorRefWithBackpressure, ActorSink.actorRef and ActorSink.actorRefWithBackpressure shown below.","title":"Overview"},{"location":"/stream/actor-interop.html#ask","text":"Note See also: Flow.ask operator reference docs, ActorFlow.ask operator reference docs for Pekko Typed\nA nice way to delegate some processing of elements in a stream to an actor is to use ask. The back-pressure of the stream is maintained by the FutureCompletionStage of the ask and the mailbox of the actor will not be filled with more messages than the given parallelism of the ask operator (similarly to how the mapAsync operator works).\nScala copysourceimplicit val askTimeout: Timeout = 5.seconds\nval words: Source[String, NotUsed] =\n  Source(List(\"hello\", \"hi\"))\n\nwords\n  .ask[String](parallelism = 5)(ref)\n  // continue processing of the replies from the actor\n  .map(_.toLowerCase)\n  .runWith(Sink.ignore) Java copysourceSource<String, NotUsed> words = Source.from(Arrays.asList(\"hello\", \"hi\"));\nTimeout askTimeout = Timeout.apply(5, TimeUnit.SECONDS);\n\nwords\n    .ask(5, ref, String.class, askTimeout)\n    // continue processing of the replies from the actor\n    .map(elem -> elem.toLowerCase())\n    .runWith(Sink.ignore(), system);\nNote that the messages received in the actor will be in the same order as the stream elements, i.e. the parallelism does not change the ordering of the messages. There is a performance advantage of using parallelism > 1 even though the actor will only process one message at a time because then there is already a message in the mailbox when the actor has completed previous message.\nThe actor must reply to the sender()getSender() for each message from the stream. That reply will complete the FutureCompletionStage of the ask and it will be the element that is emitted downstream.\nIn case the target actor is stopped, the operator will fail with an AskStageTargetActorTerminatedException\nScala copysourceclass Translator extends Actor {\n  def receive = {\n    case word: String =>\n      // ... process message\n      val reply = word.toUpperCase\n      sender() ! reply // reply to the ask\n  }\n} Java copysourcestatic class Translator extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            word -> {\n              // ... process message\n              String reply = word.toUpperCase();\n              // reply to the ask\n              getSender().tell(reply, getSelf());\n            })\n        .build();\n  }\n}\nThe stream can be completed with failure by sending org.apache.pekko.actor.Status.Failure as reply from the actor.\nIf the ask fails due to timeout the stream will be completed with TimeoutException failure. If that is not desired outcome you can use recover on the ask FutureCompletionStage, or use the other “restart” operators to restart it.\nIf you don’t care about the reply values and only use them as back-pressure signals you can use Sink.ignore after the ask operator and then actor is effectively a sink of the stream.\nNote that while you may implement the same concept using mapAsync, that style would not be aware of the actor terminating.\nIf you are intending to ask multiple actors by using Actor routers, then you should use mapAsyncUnordered and perform the ask manually in there, as the ordering of the replies is not important, since multiple actors are being asked concurrently to begin with, and no single actor is the one to be watched by the operator.","title":"ask"},{"location":"/stream/actor-interop.html#sink-actorrefwithbackpressure","text":"Note See also: Sink.actorRefWithBackpressure operator reference docs\nThe sink sends the elements of the stream to the given ActorRef that sends back back-pressure signal. First element is always onInitMessage, then stream is waiting for the given acknowledgement message from the given actor which means that it is ready to process elements. It also requires the given acknowledgement message after each stream element to make back-pressure work.\nIf the target actor terminates the stream will be cancelled. When the stream is completed successfully the given onCompleteMessage will be sent to the destination actor. When the stream is completed with failure a org.apache.pekko.actor.Status.Failure message will be sent to the destination actor.\nScala copysourceval words: Source[String, NotUsed] =\n  Source(List(\"hello\", \"hi\"))\n\n// sent from actor to stream to \"ack\" processing of given element\nval AckMessage = AckingReceiver.Ack\n\n// sent from stream to actor to indicate start, end or failure of stream:\nval InitMessage = AckingReceiver.StreamInitialized\nval OnCompleteMessage = AckingReceiver.StreamCompleted\nval onErrorMessage = (ex: Throwable) => AckingReceiver.StreamFailure(ex)\n\nval probe = TestProbe()\nval receiver = system.actorOf(Props(new AckingReceiver(probe.ref)))\nval sink = Sink.actorRefWithBackpressure(\n  receiver,\n  onInitMessage = InitMessage,\n  ackMessage = AckMessage,\n  onCompleteMessage = OnCompleteMessage,\n  onFailureMessage = onErrorMessage)\n\nwords.map(_.toLowerCase).runWith(sink)\n\nprobe.expectMsg(\"Stream initialized!\")\nprobe.expectMsg(\"hello\")\nprobe.expectMsg(\"hi\")\nprobe.expectMsg(\"Stream completed!\") Java copysourceSource<String, NotUsed> words = Source.from(Arrays.asList(\"hello\", \"hi\"));\n\nfinal TestKit probe = new TestKit(system);\n\nActorRef receiver = system.actorOf(Props.create(AckingReceiver.class, probe.getRef()));\n\nSink<String, NotUsed> sink =\n    Sink.<String>actorRefWithBackpressure(\n        receiver,\n        new StreamInitialized(),\n        Ack.INSTANCE,\n        new StreamCompleted(),\n        ex -> new StreamFailure(ex));\n\nwords.map(el -> el.toLowerCase()).runWith(sink, system);\n\nprobe.expectMsg(\"Stream initialized\");\nprobe.expectMsg(\"hello\");\nprobe.expectMsg(\"hi\");\nprobe.expectMsg(\"Stream completed\");\nThe receiving actor would then need to be implemented similar to the following:\nScala copysourceobject AckingReceiver {\n  case object Ack\n\n  case object StreamInitialized\n  case object StreamCompleted\n  final case class StreamFailure(ex: Throwable)\n}\n\nclass AckingReceiver(probe: ActorRef) extends Actor with ActorLogging {\n  import AckingReceiver._\n\n  def receive: Receive = {\n    case StreamInitialized =>\n      log.info(\"Stream initialized!\")\n      probe ! \"Stream initialized!\"\n      sender() ! Ack // ack to allow the stream to proceed sending more elements\n\n    case el: String =>\n      log.info(\"Received element: {}\", el)\n      probe ! el\n      sender() ! Ack // ack to allow the stream to proceed sending more elements\n\n    case StreamCompleted =>\n      log.info(\"Stream completed!\")\n      probe ! \"Stream completed!\"\n    case StreamFailure(ex) =>\n      log.error(ex, \"Stream failed!\")\n  }\n} Java copysourceenum Ack {\n  INSTANCE;\n}\n\nstatic class StreamInitialized {}\n\nstatic class StreamCompleted {}\n\nstatic class StreamFailure {\n  private final Throwable cause;\n\n  public StreamFailure(Throwable cause) {\n    this.cause = cause;\n  }\n\n  public Throwable getCause() {\n    return cause;\n  }\n}\n\nstatic class AckingReceiver extends AbstractLoggingActor {\n\n  private final ActorRef probe;\n\n  public AckingReceiver(ActorRef probe) {\n    this.probe = probe;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            StreamInitialized.class,\n            init -> {\n              log().info(\"Stream initialized\");\n              probe.tell(\"Stream initialized\", getSelf());\n              sender().tell(Ack.INSTANCE, self());\n            })\n        .match(\n            String.class,\n            element -> {\n              log().info(\"Received element: {}\", element);\n              probe.tell(element, getSelf());\n              sender().tell(Ack.INSTANCE, self());\n            })\n        .match(\n            StreamCompleted.class,\n            completed -> {\n              log().info(\"Stream completed\");\n              probe.tell(\"Stream completed\", getSelf());\n            })\n        .match(\n            StreamFailure.class,\n            failed -> {\n              log().error(failed.getCause(), \"Stream failed!\");\n              probe.tell(\"Stream failed!\", getSelf());\n            })\n        .build();\n  }\n}\nNote that replying to the sender of the elements (the “stream”) is required as lack of those ack signals would be interpreted as back-pressure (as intended), and no new elements will be sent into the actor until it acknowledges some elements. Handling the other signals while is not required, however is a good practice, to see the state of the stream’s lifecycle in the connected actor as well. Technically it is also possible to use multiple sinks targeting the same actor, however it is not common practice to do so, and one should rather investigate using a Merge operator for this purpose.\nNote Using Sink.actorRef or ordinary tell from a map or foreach operator means that there is no back-pressure signal from the destination actor, i.e. if the actor is not consuming the messages fast enough the mailbox of the actor will grow, unless you use a bounded mailbox with zero mailbox-push-timeout-time or use a rate limiting operator in front. It’s often better to use Sink.actorRefWithBackpressure or ask in mapAsync, though.","title":"Sink.actorRefWithBackpressure"},{"location":"/stream/actor-interop.html#source-queue","text":"Source.queue is an improvement over Sink.actorRef, since it can provide backpressure. The offer method returns a FutureCompletionStage, which completes with the result of the enqueue operation.\nSource.queue can be used for emitting elements to a stream from an actor (or from anything running outside the stream). The elements will be buffered until the stream can process them. You can offer elements to the queue and they will be emitted to the stream if there is demand from downstream, otherwise they will be buffered until request for demand is received.\nUse overflow strategy org.apache.pekko.stream.OverflowStrategy.backpressure to avoid dropping of elements if the buffer is full, instead the returned FutureCompletionStage does not complete until there is space in the buffer and offer should not be called again until it completes.\nUsing Source.queue you can push elements to the queue and they will be emitted to the stream if there is demand from downstream, otherwise they will be buffered until request for demand is received. Elements in the buffer will be discarded if downstream is terminated.\nYou could combine it with the throttle operator is used to slow down the stream to 5 element per 3 seconds and other patterns.\nSourceQueue.offer returns Future[QueueOfferResult]CompletionStage<QueueOfferResult> which completes with QueueOfferResult.Enqueued if element was added to buffer or sent downstream. It completes with QueueOfferResult.Dropped if element was dropped. Can also complete with QueueOfferResult.Failure - when stream failed or QueueOfferResult.QueueClosed when downstream is completed.\nScala copysourceval bufferSize = 10\nval elementsToProcess = 5\n\nval queue = Source\n  .queue[Int](bufferSize)\n  .throttle(elementsToProcess, 3.second)\n  .map(x => x * x)\n  .toMat(Sink.foreach(x => println(s\"completed $x\")))(Keep.left)\n  .run()\n\nval source = Source(1 to 10)\n\nimplicit val ec = system.dispatcher\nsource\n  .map(x => {\n    queue.offer(x).map {\n      case QueueOfferResult.Enqueued    => println(s\"enqueued $x\")\n      case QueueOfferResult.Dropped     => println(s\"dropped $x\")\n      case QueueOfferResult.Failure(ex) => println(s\"Offer failed ${ex.getMessage}\")\n      case QueueOfferResult.QueueClosed => println(\"Source Queue closed\")\n    }\n  })\n  .runWith(Sink.ignore) Java copysourceint bufferSize = 10;\nint elementsToProcess = 5;\n\nBoundedSourceQueue<Integer> sourceQueue =\n    Source.<Integer>queue(bufferSize)\n        .throttle(elementsToProcess, Duration.ofSeconds(3))\n        .map(x -> x * x)\n        .to(Sink.foreach(x -> System.out.println(\"got: \" + x)))\n        .run(system);\n\nSource<Integer, NotUsed> source = Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));\n\nsource.map(x -> sourceQueue.offer(x)).runWith(Sink.ignore(), system);\nWhen used from an actor you typically pipe the result of the FutureCompletionStage back to the actor to continue processing.","title":"Source.queue"},{"location":"/stream/actor-interop.html#source-actorref","text":"Messages sent to the actor that is materialized by Source.actorRef will be emitted to the stream if there is demand from downstream, otherwise they will be buffered until request for demand is received.\nDepending on the defined OverflowStrategy it might drop elements if there is no space available in the buffer. The strategy OverflowStrategy.backpressure is not supported for this Source type, i.e. elements will be dropped if the buffer is filled by sending at a rate that is faster than the stream can consume. You should consider using Source.queue if you want a backpressured actor interface.\nThe stream can be completed successfully by sending any message to the actor that is handled by the completion matching function that was provided when the actor reference was created. If the returned completion strategy is org.apache.pekko.stream.CompletionStrategy.immediately the completion will be signaled immediately. If the completion strategy is org.apache.pekko.stream.CompletionStrategy.draining, already buffered elements will be processed before signaling completion. Any elements that are in the actor’s mailbox and subsequent elements sent to the actor will not be processed.\nThe stream can be completed with failure by sending any message to the actor that is handled by the failure matching function that was specified when the actor reference was created.\nThe actor will be stopped when the stream is completed, failed or cancelled from downstream. You can watch it to get notified when that happens.\nScala copysourceval bufferSize = 10\n\nval cm: PartialFunction[Any, CompletionStrategy] = {\n  case Done =>\n    CompletionStrategy.immediately\n}\n\nval ref = Source\n  .actorRef[Int](\n    completionMatcher = cm,\n    failureMatcher = PartialFunction.empty[Any, Throwable],\n    bufferSize = bufferSize,\n    overflowStrategy = OverflowStrategy.fail) // note: backpressure is not supported\n  .map(x => x * x)\n  .toMat(Sink.foreach((x: Int) => println(s\"completed $x\")))(Keep.left)\n  .run()\n\nref ! 1\nref ! 2\nref ! 3\nref ! Done Java copysourceint bufferSize = 10;\n\nSource<Integer, ActorRef> source =\n    Source.actorRef(\n        elem -> {\n          // complete stream immediately if we send it Done\n          if (elem == Done.done()) return Optional.of(CompletionStrategy.immediately());\n          else return Optional.empty();\n        },\n        // never fail the stream because of a message\n        elem -> Optional.empty(),\n        bufferSize,\n        OverflowStrategy.dropHead()); // note: backpressure is not supported\nActorRef actorRef =\n    source\n        .map(x -> x * x)\n        .to(Sink.foreach(x -> System.out.println(\"got: \" + x)))\n        .run(system);\n\nactorRef.tell(1, ActorRef.noSender());\nactorRef.tell(2, ActorRef.noSender());\nactorRef.tell(3, ActorRef.noSender());\nactorRef.tell(\n    new org.apache.pekko.actor.Status.Success(CompletionStrategy.draining()),\n    ActorRef.noSender());","title":"Source.actorRef"},{"location":"/stream/actor-interop.html#actorsource-actorref","text":"Materialize an ActorRef<T>ActorRef[T]; sending messages to it will emit them on the stream only if they are of the same type as the stream.\nNote See also: ActorSource.actorRef operator reference docs","title":"ActorSource.actorRef"},{"location":"/stream/actor-interop.html#actorsource-actorrefwithbackpressure","text":"Materialize an ActorRef<T>ActorRef[T]; sending messages to it will emit them on the stream. The source acknowledges reception after emitting a message, to provide back pressure from the source.\nNote See also: ActorSource.actorRefWithBackpressure operator reference docs","title":"ActorSource.actorRefWithBackpressure"},{"location":"/stream/actor-interop.html#actorsink-actorref","text":"Sends the elements of the stream to the given ActorRef<T>ActorRef[T], without considering backpressure.\nNote See also: ActorSink.actorRef operator reference docs","title":"ActorSink.actorRef"},{"location":"/stream/actor-interop.html#actorsink-actorrefwithbackpressure","text":"Sends the elements of the stream to the given ActorRef<T>ActorRef[T] with backpressure, to be able to signal demand when the actor is ready to receive more elements.\nNote See also: ActorSink.actorRefWithBackpressure operator reference docs","title":"ActorSink.actorRefWithBackpressure"},{"location":"/stream/actor-interop.html#topic-source","text":"A source that will subscribe to a TopicTopic and stream messages published to the topic.\nNote See also: ActorSink.actorRefWithBackpressure operator reference docs","title":"Topic.source"},{"location":"/stream/actor-interop.html#topic-sink","text":"A sink that will publish emitted messages to a TopicTopic.\nNote See also: ActorSink.actorRefWithBackpressure operator reference docs","title":"Topic.sink"},{"location":"/stream/reactive-streams-interop.html","text":"","title":"Reactive Streams Interop"},{"location":"/stream/reactive-streams-interop.html#reactive-streams-interop","text":"","title":"Reactive Streams Interop"},{"location":"/stream/reactive-streams-interop.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/reactive-streams-interop.html#overview","text":"Pekko Streams implements the Reactive Streams standard for asynchronous stream processing with non-blocking back pressure.\nSince Java 9 the APIs of Reactive Streams has been included in the Java Standard library, under the java.util.concurrent.Flow namespace. For Java 8 there is instead a separate Reactive Streams artifact with the same APIs in the package org.reactivestreams.\nPekko streams provides interoperability for both these two API versions, the Reactive Streams interfaces directly through factories on the regular Source and Sink APIs. For the Java 9 and later built in interfaces there is a separate set of factories in org.apache.pekko.stream.scaladsl.JavaFlowSupportorg.apache.pekko.stream.javadsl.JavaFlowSupport.\nIn the following samples the standalone Reactive Stream API factories has been used but each such call can be replaced with the corresponding method from JavaFlowSupport and the JDK java.util.concurrent.Flow._java.util.concurrent.Flow.* interfaces.\nNote that it is not possible to use JavaFlowSupport on Java 8 since the needed interfaces simply is not available in the Java standard library.\nThe two most important interfaces in Reactive Streams are the Publisher and Subscriber.\nScala copysourceimport org.reactivestreams.Publisher\nimport org.reactivestreams.Subscriber\nimport org.reactivestreams.Processor Java copysourceimport org.reactivestreams.Publisher;\nimport org.reactivestreams.Subscriber;\nimport org.reactivestreams.Processor;\nLet us assume that a library provides a publisher of tweets:\nScala copysourcedef tweets: Publisher[Tweet] Java copysourcePublisher<Tweet> tweets();\nand another library knows how to store author handles in a database:\nScala copysourcedef storage: Subscriber[Author] Java copysourceSubscriber<Author> storage();\nUsing a Pekko Streams Flow we can transform the stream and connect those:\nScala copysourceval authors = Flow[Tweet].filter(_.hashtags.contains(pekkoTag)).map(_.author)\n\nSource.fromPublisher(tweets).via(authors).to(Sink.fromSubscriber(storage)).run() Java copysourcefinal Flow<Tweet, Author, NotUsed> authors =\n    Flow.of(Tweet.class).filter(t -> t.hashtags().contains(PEKKO)).map(t -> t.author);\n\nSource.fromPublisher(rs.tweets()).via(authors).to(Sink.fromSubscriber(rs.storage()));\nThe Publisher is used as an input Source to the flow and the Subscriber is used as an output Sink.\nA Flow can also be converted to a RunnableGraph[Processor[In, Out]] which materializes to a Processor when run() is called. run() itself can be called multiple times, resulting in a new Processor instance each time.\nScala copysourceval processor: Processor[Tweet, Author] = authors.toProcessor.run()\n\ntweets.subscribe(processor)\nprocessor.subscribe(storage) Java copysourcefinal Processor<Tweet, Author> processor = authors.toProcessor().run(system);\n\nrs.tweets().subscribe(processor);\nprocessor.subscribe(rs.storage());\nA publisher can be connected to a subscriber with the subscribe method.\nIt is also possible to expose a Source as a Publisher by using the Publisher-Sink:\nScala copysourceval authorPublisher: Publisher[Author] =\n  Source.fromPublisher(tweets).via(authors).runWith(Sink.asPublisher(fanout = false))\n\nauthorPublisher.subscribe(storage) Java copysourcefinal Publisher<Author> authorPublisher =\n    Source.fromPublisher(rs.tweets())\n        .via(authors)\n        .runWith(Sink.asPublisher(AsPublisher.WITHOUT_FANOUT), system);\n\nauthorPublisher.subscribe(rs.storage());\nA publisher that is created with Sink.asPublisher(fanout = false)Sink.asPublisher(AsPublisher.WITHOUT_FANOUT) supports only a single subscription. Additional subscription attempts will be rejected with an IllegalStateException.\nA publisher that supports multiple subscribers using fan-out/broadcasting is created as follows:\nScala copysourcedef alert: Subscriber[Author]\ndef storage: Subscriber[Author] Java copysourceSubscriber<Author> alert();\nSubscriber<Author> storage();\nScala copysourceval authorPublisher: Publisher[Author] =\n  Source.fromPublisher(tweets).via(authors).runWith(Sink.asPublisher(fanout = true))\n\nauthorPublisher.subscribe(storage)\nauthorPublisher.subscribe(alert) Java copysourcefinal Publisher<Author> authorPublisher =\n    Source.fromPublisher(rs.tweets())\n        .via(authors)\n        .runWith(Sink.asPublisher(AsPublisher.WITH_FANOUT), system);\n\nauthorPublisher.subscribe(rs.storage());\nauthorPublisher.subscribe(rs.alert());\nThe input buffer size of the operator controls how far apart the slowest subscriber can be from the fastest subscriber before slowing down the stream.\nTo make the picture complete, it is also possible to expose a Sink as a Subscriber by using the Subscriber-Source:\nScala copysourceval tweetSubscriber: Subscriber[Tweet] =\n  authors.to(Sink.fromSubscriber(storage)).runWith(Source.asSubscriber[Tweet])\n\ntweets.subscribe(tweetSubscriber) Java copysourcefinal Subscriber<Author> storage = rs.storage();\n\nfinal Subscriber<Tweet> tweetSubscriber =\n    authors.to(Sink.fromSubscriber(storage)).runWith(Source.asSubscriber(), system);\n\nrs.tweets().subscribe(tweetSubscriber);\nIt is also possible to use re-wrap Processor instances as a Flow by passing a factory function that will create the Processor instances:\nScala copysource// An example Processor factory\ndef createProcessor: Processor[Int, Int] = Flow[Int].toProcessor.run()\n\nval flow: Flow[Int, Int, NotUsed] = Flow.fromProcessor(() => createProcessor) Java copysource// An example Processor factory\nfinal Creator<Processor<Integer, Integer>> factory =\n    new Creator<Processor<Integer, Integer>>() {\n      public Processor<Integer, Integer> create() {\n        return Flow.of(Integer.class).toProcessor().run(system);\n      }\n    };\n\nfinal Flow<Integer, Integer, NotUsed> flow = Flow.fromProcessor(factory);\nPlease note that a factory is necessary to achieve reusability of the resulting Flow.","title":"Overview"},{"location":"/stream/reactive-streams-interop.html#other-implementations","text":"Implementing Reactive Streams makes it possible to plug Pekko Streams together with other stream libraries that adhere to the standard. An incomplete list of other implementations:\nReactor (1.1+) RxJava Ratpack Slick","title":"Other implementations"},{"location":"/stream/stream-error.html","text":"","title":"Error Handling in Streams"},{"location":"/stream/stream-error.html#error-handling-in-streams","text":"","title":"Error Handling in Streams"},{"location":"/stream/stream-error.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-error.html#introduction","text":"When an operator in a stream fails this will normally lead to the entire stream being torn down. Each of the operators downstream gets informed about the failure and each upstream operator sees a cancellation.\nIn many cases you may want to avoid complete stream failure, this can be done in a few different ways:\nrecoverrecover to emit a final element then complete the stream normally on upstream failure recoverWithRetriesrecoverWithRetries to create a new upstream and start consuming from that on failure Restarting sections of the stream after a backoff Using a supervision strategy for operators that support it\nIn addition to these built in tools for error handling, a common pattern is to wrap the stream inside an actor, and have the actor restart the entire stream on failure.","title":"Introduction"},{"location":"/stream/stream-error.html#logging-errors","text":"log()log() enables logging of a stream, which is typically useful for error logging. The below stream fails with ArithmeticException when the element 0 goes through the mapmap operator,\nScala copysourceSource(-5 to 5)\n  .map(1 / _) // throwing ArithmeticException: / by zero\n  .log(\"error logging\")\n  .runWith(Sink.ignore) Java copysourceSource.from(Arrays.asList(-1, 0, 1))\n    .map(x -> 1 / x) // throwing ArithmeticException: / by zero\n    .log(\"error logging\")\n    .runWith(Sink.ignore(), system);\nand error messages like below will be logged.\n[error logging] Upstream failed.\njava.lang.ArithmeticException: / by zero\nIf you want to control logging levels on each element, completion, and failure, you can find more details in Logging in streams.","title":"Logging errors"},{"location":"/stream/stream-error.html#recover","text":"recoverrecover allows you to emit a final element and then complete the stream on an upstream failure. Deciding which exceptions should be recovered is done through a PartialFunction. If an exception does not have a matching case match defined the stream is failed.\nRecovering can be useful if you want to gracefully complete a stream on failure while letting downstream know that there was a failure.\nThrowing an exception inside recover will be logged on ERROR level automatically.\nMore details in recover\nScala copysourceSource(0 to 6)\n  .map(n =>\n    // assuming `4` and `5` are unexpected values that could throw exception\n    if (List(4, 5).contains(n)) throw new RuntimeException(s\"Boom! Bad value found: $n\")\n    else n.toString)\n  .recover {\n    case e: RuntimeException => e.getMessage\n  }\n  .runForeach(println) Java copysourceSource.from(Arrays.asList(0, 1, 2, 3, 4, 5, 6))\n    .map(\n        n -> {\n          // assuming `4` and `5` are unexpected values that could throw exception\n          if (Arrays.asList(4, 5).contains(n))\n            throw new RuntimeException(String.format(\"Boom! Bad value found: %s\", n));\n          else return n.toString();\n        })\n    .recover(\n        new PFBuilder<Throwable, String>()\n            .match(RuntimeException.class, Throwable::getMessage)\n            .build())\n    .runForeach(System.out::println, system);\nThis will output:\nScala copysource0\n1\n2\n3                         // last element before failure\nBoom! Bad value found: 4  // first element on failure Java copysource0\n1\n2\n3                         // last element before failure\nBoom! Bad value found: 4  // first element on failure","title":"Recover"},{"location":"/stream/stream-error.html#recover-with-retries","text":"recoverWithRetriesrecoverWithRetries allows you to put a new upstream in place of the failed one, recovering stream failures up to a specified maximum number of times.\nDeciding which exceptions should be recovered is done through a PartialFunction. If an exception does not have a matching case match defined the stream is failed.\nScala copysourceval planB = Source(List(\"five\", \"six\", \"seven\", \"eight\"))\n\nSource(0 to 10)\n  .map(n =>\n    if (n < 5) n.toString\n    else throw new RuntimeException(\"Boom!\"))\n  .recoverWithRetries(attempts = 1,\n    {\n      case _: RuntimeException => planB\n    })\n  .runForeach(println) Java copysourceSource<String, NotUsed> planB = Source.from(Arrays.asList(\"five\", \"six\", \"seven\", \"eight\"));\n\nSource.from(Arrays.asList(0, 1, 2, 3, 4, 5, 6))\n    .map(\n        n -> {\n          if (n < 5) return n.toString();\n          else throw new RuntimeException(\"Boom!\");\n        })\n    .recoverWithRetries(\n        1, // max attempts\n        new PFBuilder<Throwable, Source<String, NotUsed>>()\n            .match(RuntimeException.class, ex -> planB)\n            .build())\n    .runForeach(System.out::println, system);\nThis will output:\nScala copysource0\n1\n2\n3\n4\nfive\nsix\nseven\neight Java copysource0\n1\n2\n3\n4\nfive\nsix\nseven\neight","title":"Recover with retries"},{"location":"/stream/stream-error.html#delayed-restarts-with-a-backoff-operator","text":"Pekko streams provides a RestartSourceRestartSource, RestartSinkRestartSink and RestartFlowRestartFlow for implementing the so-called exponential backoff supervision strategy, starting an operator again when it fails or completes, each time with a growing time delay between restarts.\nThis pattern is useful when the operator fails or completes because some external resource is not available and we need to give it some time to start-up again. One of the prime examples when this is useful is when a WebSocket connection fails due to the HTTP server it’s running on going down, perhaps because it is overloaded. By using an exponential backoff, we avoid going into a tight reconnect loop, which both gives the HTTP server some time to recover, and it avoids using needless resources on the client side.\nThe various restart shapes mentioned all expect an RestartSettingsRestartSettings which configures the restart behaviour. Configurable parameters are:\nminBackoff is the initial duration until the underlying stream is restarted maxBackoff caps the exponential backoff randomFactor allows addition of a random delay following backoff calculation maxRestarts caps the total number of restarts maxRestartsWithin sets a timeframe during which restarts are counted towards the same total for maxRestarts\nThe following snippet shows how to create a backoff supervisor using RestartSourceRestartSource which will supervise the given SourceSource. The Source in this case is a stream of Server Sent Events, produced by pekko-http. If the stream fails or completes at any point, the request will be made again, in increasing intervals of 3, 6, 12, 24 and finally 30 seconds (at which point it will remain capped due to the maxBackoff parameter):\nScala copysourceval settings = RestartSettings(\n  minBackoff = 3.seconds,\n  maxBackoff = 30.seconds,\n  randomFactor = 0.2 // adds 20% \"noise\" to vary the intervals slightly\n).withMaxRestarts(20, 5.minutes) // limits the amount of restarts to 20 within 5 minutes\n\nval restartSource = RestartSource.withBackoff(settings) { () =>\n  // Create a source from a future of a source\n  Source.futureSource {\n    // Make a single request with pekko-http\n    Http()\n      .singleRequest(HttpRequest(uri = \"http://example.com/eventstream\"))\n      // Unmarshall it as a source of server sent events\n      .flatMap(Unmarshal(_).to[Source[ServerSentEvent, NotUsed]])\n  }\n} Java copysourceRestartSettings settings =\n    RestartSettings.create(\n            Duration.ofSeconds(3), // min backoff\n            Duration.ofSeconds(30), // max backoff\n            0.2 // adds 20% \"noise\" to vary the intervals slightly\n            )\n        .withMaxRestarts(\n            20, Duration.ofMinutes(5)); // limits the amount of restarts to 20 within 5 minutes\n\nSource<ServerSentEvent, NotUsed> eventStream =\n    RestartSource.withBackoff(\n        settings,\n        () ->\n            // Create a source from a future of a source\n            Source.completionStageSource(\n                // Issue a GET request on the event stream\n                Http.get(system)\n                    .singleRequest(HttpRequest.create(\"http://example.com/eventstream\"))\n                    .thenCompose(\n                        response ->\n                            // Unmarshall it to a stream of ServerSentEvents\n                            EventStreamUnmarshalling.fromEventStream()\n                                .unmarshall(response, materializer))));\nUsing a randomFactor to add a little bit of additional variance to the backoff intervals is highly recommended, in order to avoid multiple streams re-start at the exact same point in time, for example because they were stopped due to a shared resource such as the same server going down and re-starting after the same configured interval. By adding additional randomness to the re-start intervals the streams will start in slightly different points in time, thus avoiding large spikes of traffic hitting the recovering server or other resource that they all need to contact.\nThe above RestartSource will never terminate unless the SinkSink it’s fed into cancels. It will often be handy to use it in combination with a KillSwitch, so that you can terminate it when needed:\nScala copysourceval killSwitch = restartSource\n  .viaMat(KillSwitches.single)(Keep.right)\n  .toMat(Sink.foreach(event => println(s\"Got event: $event\")))(Keep.left)\n  .run()\n\ndoSomethingElse()\n\nkillSwitch.shutdown() Java copysourceKillSwitch killSwitch =\n    eventStream\n        .viaMat(KillSwitches.single(), Keep.right())\n        .toMat(Sink.foreach(event -> System.out.println(\"Got event: \" + event)), Keep.left())\n        .run(materializer);\n\ndoSomethingElse();\n\nkillSwitch.shutdown();\nSinks and flows can also be supervised, using RestartSinkRestartSink and RestartFlowRestartFlow. The RestartSink is restarted when it cancels, while the RestartFlow is restarted when either the in port cancels, the out port completes, or the out port sends an error.\nNote Care should be taken when using GraphStages that conditionally propagate termination signals inside a RestartSourceRestartSource, RestartSinkRestartSink or RestartFlowRestartFlow. An example is a Broadcast operator with the default eagerCancel = false where some of the outlets are for side-effecting branches (that do not re-join e.g. via a Merge). A failure on a side branch will not terminate the supervised stream which will not be restarted. Conversely, a failure on the main branch can trigger a restart but leave behind old running instances of side branches. In this example eagerCancel should probably be set to true, or, when only a single side branch is used, alsoTo or divertTo should be considered as alternatives.","title":"Delayed restarts with a backoff operator"},{"location":"/stream/stream-error.html#supervision-strategies","text":"Note The operators that support supervision strategies are explicitly documented to do so, if there is nothing in the documentation of an operator saying that it adheres to the supervision strategy it means it fails rather than applies supervision.\nThe error handling strategies are inspired by actor supervision strategies, but the semantics have been adapted to the domain of stream processing. The most important difference is that supervision is not automatically applied to stream operators but instead something that each operator has to implement explicitly.\nFor many operators it may not even make sense to implement support for supervision strategies, this is especially true for operators connecting to external technologies where for example a failed connection will likely still fail if a new connection is tried immediately (see Restart with back off for such scenarios).\nFor operators that do implement supervision, the strategies for how to handle exceptions from processing stream elements can be selected when materializing the stream through use of an attribute.\nThere are three ways to handle exceptions from application code:\nStopSupervision.stop() - The stream is completed with failure. ResumeSupervision.resume() - The element is dropped and the stream continues. RestartSupervision.restart() - The element is dropped and the stream continues after restarting the operator. Restarting an operator means that any accumulated state is cleared. This is typically performed by creating a new instance of the operator.\nBy default the stopping strategy is used for all exceptions, i.e. the stream will be completed with failure when an exception is thrown.\nScala copysourceval source = Source(0 to 5).map(100 / _)\nval result = source.runWith(Sink.fold(0)(_ + _))\n// division by zero will fail the stream and the\n// result here will be a Future completed with Failure(ArithmeticException) Java copysourcefinal Source<Integer, NotUsed> source =\n    Source.from(Arrays.asList(0, 1, 2, 3, 4, 5)).map(elem -> 100 / elem);\nfinal Sink<Integer, CompletionStage<Integer>> fold =\n    Sink.<Integer, Integer>fold(0, (acc, elem) -> acc + elem);\nfinal CompletionStage<Integer> result = source.runWith(fold, system);\n// division by zero will fail the stream and the\n// result here will be a CompletionStage failed with ArithmeticException\nThe default supervision strategy for a stream can be defined on the complete RunnableGraphRunnableGraph.\nScala copysourceval decider: Supervision.Decider = {\n  case _: ArithmeticException => Supervision.Resume\n  case _                      => Supervision.Stop\n}\nval source = Source(0 to 5).map(100 / _)\nval runnableGraph =\n  source.toMat(Sink.fold(0)(_ + _))(Keep.right)\n\nval withCustomSupervision = runnableGraph.withAttributes(ActorAttributes.supervisionStrategy(decider))\n\nval result = withCustomSupervision.run()\n// the element causing division by zero will be dropped\n// result here will be a Future completed with Success(228) Java copysourcefinal Function<Throwable, Supervision.Directive> decider =\n    exc -> {\n      if (exc instanceof ArithmeticException) return Supervision.resume();\n      else return Supervision.stop();\n    };\nfinal Source<Integer, NotUsed> source =\n    Source.from(Arrays.asList(0, 1, 2, 3, 4, 5))\n        .map(elem -> 100 / elem)\n        .withAttributes(ActorAttributes.withSupervisionStrategy(decider));\nfinal Sink<Integer, CompletionStage<Integer>> fold = Sink.fold(0, (acc, elem) -> acc + elem);\n\nfinal RunnableGraph<CompletionStage<Integer>> runnableGraph = source.toMat(fold, Keep.right());\n\nfinal RunnableGraph<CompletionStage<Integer>> withCustomSupervision =\n    runnableGraph.withAttributes(ActorAttributes.withSupervisionStrategy(decider));\n\nfinal CompletionStage<Integer> result = withCustomSupervision.run(system);\n// the element causing division by zero will be dropped\n// result here will be a CompletionStage completed with 228\nHere you can see that all ArithmeticException will resume the processing, i.e. the elements that cause the division by zero are effectively dropped.\nNote Be aware that dropping elements may result in deadlocks in graphs with cycles, as explained in Graph cycles, liveness and deadlocks.\nThe supervision strategy can also be defined for all operators of a flow.\nScala copysourceval decider: Supervision.Decider = {\n  case _: ArithmeticException => Supervision.Resume\n  case _                      => Supervision.Stop\n}\nval flow = Flow[Int]\n  .filter(100 / _ < 50)\n  .map(elem => 100 / (5 - elem))\n  .withAttributes(ActorAttributes.supervisionStrategy(decider))\nval source = Source(0 to 5).via(flow)\n\nval result = source.runWith(Sink.fold(0)(_ + _))\n// the elements causing division by zero will be dropped\n// result here will be a Future completed with Success(150) Java copysourcefinal Function<Throwable, Supervision.Directive> decider =\n    exc -> {\n      if (exc instanceof ArithmeticException) return Supervision.resume();\n      else return Supervision.stop();\n    };\nfinal Flow<Integer, Integer, NotUsed> flow =\n    Flow.of(Integer.class)\n        .filter(elem -> 100 / elem < 50)\n        .map(elem -> 100 / (5 - elem))\n        .withAttributes(ActorAttributes.withSupervisionStrategy(decider));\nfinal Source<Integer, NotUsed> source = Source.from(Arrays.asList(0, 1, 2, 3, 4, 5)).via(flow);\nfinal Sink<Integer, CompletionStage<Integer>> fold =\n    Sink.<Integer, Integer>fold(0, (acc, elem) -> acc + elem);\nfinal CompletionStage<Integer> result = source.runWith(fold, system);\n// the elements causing division by zero will be dropped\n// result here will be a Future completed with 150\nRestartSupervision.restart() works in a similar way as ResumeSupervision.resume() with the addition that accumulated state, if any, of the failing processing operator will be reset.\nScala copysourceval decider: Supervision.Decider = {\n  case _: IllegalArgumentException => Supervision.Restart\n  case _                           => Supervision.Stop\n}\nval flow = Flow[Int]\n  .scan(0) { (acc, elem) =>\n    if (elem < 0) throw new IllegalArgumentException(\"negative not allowed\")\n    else acc + elem\n  }\n  .withAttributes(ActorAttributes.supervisionStrategy(decider))\nval source = Source(List(1, 3, -1, 5, 7)).via(flow)\nval result = source.limit(1000).runWith(Sink.seq)\n// the negative element cause the scan stage to be restarted,\n// i.e. start from 0 again\n// result here will be a Future completed with Success(Vector(0, 1, 4, 0, 5, 12)) Java copysourcefinal Function<Throwable, Supervision.Directive> decider =\n    exc -> {\n      if (exc instanceof IllegalArgumentException) return Supervision.restart();\n      else return Supervision.stop();\n    };\nfinal Flow<Integer, Integer, NotUsed> flow =\n    Flow.of(Integer.class)\n        .scan(\n            0,\n            (acc, elem) -> {\n              if (elem < 0) throw new IllegalArgumentException(\"negative not allowed\");\n              else return acc + elem;\n            })\n        .withAttributes(ActorAttributes.withSupervisionStrategy(decider));\nfinal Source<Integer, NotUsed> source = Source.from(Arrays.asList(1, 3, -1, 5, 7)).via(flow);\nfinal CompletionStage<List<Integer>> result =\n    source.grouped(1000).runWith(Sink.<List<Integer>>head(), system);\n// the negative element cause the scan stage to be restarted,\n// i.e. start from 0 again\n// result here will be a Future completed with List(0, 1, 4, 0, 5, 12)","title":"Supervision Strategies"},{"location":"/stream/stream-error.html#errors-from-mapasync","text":"Stream supervision can also be applied to the futures of mapAsyncmapAsync and mapAsyncUnorderedmapAsyncUnordered even if such failures happen in the future rather than inside the operator itself.\nLet’s say that we use an external service to lookup email addresses and we would like to discard those that cannot be found.\nWe start with the tweet stream of authors:\nScala copysourceval authors: Source[Author, NotUsed] =\n  tweets.filter(_.hashtags.contains(pekkoTag)).map(_.author) Java copysourcefinal Source<Author, NotUsed> authors =\n    tweets.filter(t -> t.hashtags().contains(PEKKO)).map(t -> t.author);\nAssume that we can lookup their email address using:\nScala copysourcedef lookupEmail(handle: String): Future[String] = Java copysourcepublic CompletionStage<String> lookupEmail(String handle)\nThe Future CompletionStage is completed with Failure normally if the email is not found.\nTransforming the stream of authors to a stream of email addresses by using the lookupEmail service can be done with mapAsyncmapAsync and we use Supervision.resumingDecider Supervision.getResumingDecider() to drop unknown email addresses:\nScala copysourceimport ActorAttributes.supervisionStrategy\nimport Supervision.resumingDecider\n\nval emailAddresses: Source[String, NotUsed] =\n  authors.via(\n    Flow[Author]\n      .mapAsync(4)(author => addressSystem.lookupEmail(author.handle))\n      .withAttributes(supervisionStrategy(resumingDecider))) Java copysourcefinal Attributes resumeAttrib =\n    ActorAttributes.withSupervisionStrategy(Supervision.getResumingDecider());\nfinal Flow<Author, String, NotUsed> lookupEmail =\n    Flow.of(Author.class)\n        .mapAsync(4, author -> addressSystem.lookupEmail(author.handle))\n        .withAttributes(resumeAttrib);\nfinal Source<String, NotUsed> emailAddresses = authors.via(lookupEmail);\nIf we would not use ResumeSupervision.resume() the default stopping strategy would complete the stream with failure on the first Future CompletionStage that was completed with Failureexceptionally.","title":"Errors from mapAsync"},{"location":"/stream/stream-io.html","text":"","title":"Working with streaming IO"},{"location":"/stream/stream-io.html#working-with-streaming-io","text":"","title":"Working with streaming IO"},{"location":"/stream/stream-io.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-io.html#introduction","text":"Pekko Streams provides a way of handling File IO and TCP connections with Streams. While the general approach is very similar to the Actor based TCP handling using Pekko IO, by using Pekko Streams you are freed of having to manually react to back-pressure signals, as the library does it transparently for you.","title":"Introduction"},{"location":"/stream/stream-io.html#streaming-tcp","text":"","title":"Streaming TCP"},{"location":"/stream/stream-io.html#accepting-connections-echo-server","text":"In order to implement a simple EchoServer we bind to a given address, which returns a SourceSource[IncomingConnectionIncomingConnection, Future[ServerBindingServerBinding]SourceSource<IncomingConnectionIncomingConnection, CompletionStage<ServerBindingServerBinding>>, which will emit an IncomingConnection element for each new connection that the Server should handle:\nScala copysourceval binding: Future[ServerBinding] =\n  Tcp(system).bind(\"127.0.0.1\", 8888).to(Sink.ignore).run()\n\nbinding.map { b =>\n  b.unbind().onComplete {\n    case _ => // ...\n  }\n} Java copysource// IncomingConnection and ServerBinding imported from Tcp\nfinal Source<IncomingConnection, CompletionStage<ServerBinding>> connections =\n    Tcp.get(system).bind(\"127.0.0.1\", 8888);\nNext, we handle each incoming connection using a FlowFlow which will be used as the operator to handle and emit ByteStringByteString s from and to the TCP Socket. Since one ByteString does not have to necessarily correspond to exactly one line of text (the client might be sending the line in chunks) we use the Framing.delimiterFraming.delimiter helper Flow to chunk the inputs up into actual lines of text. The last boolean argument indicates that we require an explicit line ending even for the last message before the connection is closed. In this example we add exclamation marks to each incoming text message and push it through the flow:\nScala copysourceimport org.apache.pekko.stream.scaladsl.Framing\n\nval connections: Source[IncomingConnection, Future[ServerBinding]] =\n  Tcp(system).bind(host, port)\nconnections.runForeach { connection =>\n  println(s\"New connection from: ${connection.remoteAddress}\")\n\n  val echo = Flow[ByteString]\n    .via(Framing.delimiter(ByteString(\"\\n\"), maximumFrameLength = 256, allowTruncation = true))\n    .map(_.utf8String)\n    .map(_ + \"!!!\\n\")\n    .map(ByteString(_))\n\n  connection.handleWith(echo)\n} Java copysourceconnections.runForeach(\n    connection -> {\n      System.out.println(\"New connection from: \" + connection.remoteAddress());\n\n      final Flow<ByteString, ByteString, NotUsed> echo =\n          Flow.of(ByteString.class)\n              .via(\n                  Framing.delimiter(\n                      ByteString.fromString(\"\\n\"), 256, FramingTruncation.DISALLOW))\n              .map(ByteString::utf8String)\n              .map(s -> s + \"!!!\\n\")\n              .map(ByteString::fromString);\n\n      connection.handleWith(echo, system);\n    },\n    system);\nNotice that while most building blocks in Pekko Streams are reusable and freely shareable, this is not the case for the incoming connection Flow, since it directly corresponds to an existing, already accepted connection its handling can only ever be materialized once.\nClosing connections is possible by cancelling the incoming connection FlowFlow from your server logic (e.g. by connecting its downstream to a Sink.cancelledSink.cancelled and its upstream to a Source.emptySource.empty. It is also possible to shut down the server’s socket by cancelling the IncomingConnectionIncomingConnection source connections.\nWe can then test the TCP server by sending data to the TCP Socket using netcat:\n$ echo -n \"Hello World\" | netcat 127.0.0.1 8888\nHello World!!!","title":"Accepting connections: Echo Server"},{"location":"/stream/stream-io.html#connecting-repl-client","text":"In this example we implement a rather naive Read Evaluate Print Loop client over TCP. Let’s say we know a server has exposed a simple command line interface over TCP, and would like to interact with it using Pekko Streams over TCP. To open an outgoing connection socket we use the outgoingConnection method:\nScala copysourceval connection = Tcp(system).outgoingConnection(\"127.0.0.1\", 8888)\n\nval replParser =\n  Flow[String].takeWhile(_ != \"q\").concat(Source.single(\"BYE\")).map(elem => ByteString(s\"$elem\\n\"))\n\nval repl = Flow[ByteString]\n  .via(Framing.delimiter(ByteString(\"\\n\"), maximumFrameLength = 256, allowTruncation = true))\n  .map(_.utf8String)\n  .map(text => println(\"Server: \" + text))\n  .map(_ => readLine(\"> \"))\n  .via(replParser)\n\nval connected = connection.join(repl).run() Java copysourcefinal Flow<ByteString, ByteString, CompletionStage<OutgoingConnection>> connection =\n    Tcp.get(system).outgoingConnection(\"127.0.0.1\", 8888);\nfinal Flow<String, ByteString, NotUsed> replParser =\n    Flow.<String>create()\n        .takeWhile(elem -> !elem.equals(\"q\"))\n        .concat(Source.single(\"BYE\")) // will run after the original flow completes\n        .map(elem -> ByteString.fromString(elem + \"\\n\"));\n\nfinal Flow<ByteString, ByteString, NotUsed> repl =\n    Flow.of(ByteString.class)\n        .via(Framing.delimiter(ByteString.fromString(\"\\n\"), 256, FramingTruncation.DISALLOW))\n        .map(ByteString::utf8String)\n        .map(\n            text -> {\n              System.out.println(\"Server: \" + text);\n              return \"next\";\n            })\n        .map(elem -> readLine(\"> \"))\n        .via(replParser);\n\nCompletionStage<OutgoingConnection> connectionCS = connection.join(repl).run(system);\nThe repl flow we use to handle the server interaction first prints the servers response, then awaits on input from the command line (this blocking call is used here for the sake of simplicity) and converts it to a ByteStringByteString which is then sent over the wire to the server. Then we connect the TCP pipeline to this operator–at this point it will be materialized and start processing data once the server responds with an initial message.\nA resilient REPL client would be more sophisticated than this, for example it should split out the input reading into a separate mapAsync step and have a way to let the server write more data than one ByteString chunk at any given time, these improvements however are left as exercise for the reader.","title":"Connecting: REPL Client"},{"location":"/stream/stream-io.html#avoiding-deadlocks-and-liveness-issues-in-back-pressured-cycles","text":"When writing such end-to-end back-pressured systems you may sometimes end up in a situation of a loop, in which either side is waiting for the other one to start the conversation. One does not need to look far to find examples of such back-pressure loops. In the two examples shown previously, we always assumed that the side we are connecting to would start the conversation, which effectively means both sides are back-pressured and can not get the conversation started. There are multiple ways of dealing with this which are explained in depth in Graph cycles, liveness and deadlocks, however in client-server scenarios it is often the simplest to make either side send an initial message.\nNote In case of back-pressured cycles (which can occur even between different systems) sometimes you have to decide which of the sides has start the conversation in order to kick it off. This can be often done by injecting an initial message from one of the sides–a conversation starter.\nTo break this back-pressure cycle we need to inject some initial message, a “conversation starter”. First, we need to decide which side of the connection should remain passive and which active. Thankfully in most situations finding the right spot to start the conversation is rather simple, as it often is inherent to the protocol we are trying to implement using Streams. In chat-like applications, which our examples resemble, it makes sense to make the Server initiate the conversation by emitting a “hello” message:\nScala copysourceconnections\n  .to(Sink.foreach { connection =>\n    // server logic, parses incoming commands\n    val commandParser = Flow[String].takeWhile(_ != \"BYE\").map(_ + \"!\")\n\n    import connection._\n    val welcomeMsg = s\"Welcome to: $localAddress, you are: $remoteAddress!\"\n    val welcome = Source.single(welcomeMsg)\n\n    val serverLogic = Flow[ByteString]\n      .via(Framing.delimiter(ByteString(\"\\n\"), maximumFrameLength = 256, allowTruncation = true))\n      .map(_.utf8String)\n      .via(commandParser)\n      // merge in the initial banner after parser\n      .merge(welcome)\n      .map(_ + \"\\n\")\n      .map(ByteString(_))\n\n    connection.handleWith(serverLogic)\n  })\n  .run() Java copysourceconnections\n    .to(\n        Sink.foreach(\n            (IncomingConnection connection) -> {\n              // server logic, parses incoming commands\n              final Flow<String, String, NotUsed> commandParser =\n                  Flow.<String>create()\n                      .takeWhile(elem -> !elem.equals(\"BYE\"))\n                      .map(elem -> elem + \"!\");\n\n              final String welcomeMsg =\n                  \"Welcome to: \"\n                      + connection.localAddress()\n                      + \" you are: \"\n                      + connection.remoteAddress()\n                      + \"!\";\n\n              final Source<String, NotUsed> welcome = Source.single(welcomeMsg);\n              final Flow<ByteString, ByteString, NotUsed> serverLogic =\n                  Flow.of(ByteString.class)\n                      .via(\n                          Framing.delimiter(\n                              ByteString.fromString(\"\\n\"), 256, FramingTruncation.DISALLOW))\n                      .map(ByteString::utf8String)\n                      .via(commandParser)\n                      .merge(welcome)\n                      .map(s -> s + \"\\n\")\n                      .map(ByteString::fromString);\n\n              connection.handleWith(serverLogic, system);\n            }))\n    .run(system);\nTo emit the initial message we merge a SourceSource with a single element, after the command processing but before the framing and transformation to ByteStringByteString s this way we do not have to repeat such logic.\nIn this example both client and server may need to close the stream based on a parsed command - BYE in the case of the server, and q in the case of the client. This is implemented by taking from the stream until q and and concatenating a Source with a single BYE element which will then be sent after the original source completedusing a custom operator extending GraphStage which completes the stream once it encounters such command.","title":"Avoiding deadlocks and liveness issues in back-pressured cycles"},{"location":"/stream/stream-io.html#using-framing-in-your-protocol","text":"Streaming transport protocols like TCP only pass streams of bytes, and does not know what is a logical chunk of bytes from the application’s point of view. Often when implementing network protocols you will want to introduce your own framing. This can be done in two ways: An end-of-frame marker, e.g. end line \\n, can do framing via Framing.delimiterFraming.delimiter. Or a length-field can be used to build a framing protocol. There is a bidi implementing this protocol provided by Framing.simpleFramingProtocolFraming.simpleFramingProtocol.\nJsonFramingJsonFraming separates valid JSON objects from incoming ByteStringByteString objects:\nScala copysourceval input =\n  \"\"\"\n    |[\n    | { \"name\" : \"john\" },\n    | { \"name\" : \"Ég get etið gler án þess að meiða mig\" },\n    | { \"name\" : \"jack\" },\n    |]\n    |\"\"\".stripMargin // also should complete once notices end of array\n\nval result =\n  Source.single(ByteString(input)).via(JsonFraming.objectScanner(Int.MaxValue)).runFold(Seq.empty[String]) {\n    case (acc, entry) => acc ++ Seq(entry.utf8String)\n  } Java copysourceString input =\n    \"[{ \\\"name\\\" : \\\"john\\\" }, { \\\"name\\\" : \\\"Ég get etið gler án þess að meiða mig\\\" }, { \\\"name\\\" : \\\"jack\\\" }]\";\nCompletionStage<ArrayList<String>> result =\n    Source.single(ByteString.fromString(input))\n        .via(JsonFraming.objectScanner(Integer.MAX_VALUE))\n        .runFold(\n            new ArrayList<String>(),\n            (acc, entry) -> {\n              acc.add(entry.utf8String());\n              return acc;\n            },\n            system);","title":"Using framing in your protocol"},{"location":"/stream/stream-io.html#tls","text":"Similar factories as shown above for raw TCP but where the data is encrypted using TLS are available from Tcp through outgoingConnectionWithTls, bindWithTls and bindAndHandleWithTls, see the `Tcp Scaladoc``Tcp Javadoc` for details.\nUsing TLS requires a keystore and a truststore and then a somewhat involved dance of configuring the SSLEngine and the details for how the session should be negotiated:\nScala copysourceimport java.security.KeyStore\nimport javax.net.ssl.KeyManagerFactory\nimport javax.net.ssl.SSLContext\nimport javax.net.ssl.SSLEngine\nimport javax.net.ssl.TrustManagerFactory\n\nimport org.apache.pekko.stream.TLSRole\n\n// initialize SSLContext once\nlazy val sslContext: SSLContext = {\n  // Don't hardcode your password in actual code\n  val password = \"abcdef\".toCharArray\n\n  // trust store and keys in one keystore\n  val keyStore = KeyStore.getInstance(\"PKCS12\")\n  keyStore.load(getClass.getResourceAsStream(\"/tcp-spec-keystore.p12\"), password)\n\n  val trustManagerFactory = TrustManagerFactory.getInstance(\"SunX509\")\n  trustManagerFactory.init(keyStore)\n\n  val keyManagerFactory = KeyManagerFactory.getInstance(\"SunX509\")\n  keyManagerFactory.init(keyStore, password)\n\n  // init ssl context\n  val context = SSLContext.getInstance(\"TLSv1.2\")\n  context.init(keyManagerFactory.getKeyManagers, trustManagerFactory.getTrustManagers, new SecureRandom)\n  context\n}\n\n// create new SSLEngine from the SSLContext, which was initialized once\ndef createSSLEngine(role: TLSRole): SSLEngine = {\n  val engine = sslContext.createSSLEngine()\n\n  engine.setUseClientMode(role == pekko.stream.Client)\n  engine.setEnabledCipherSuites(Array(\"TLS_RSA_WITH_AES_128_CBC_SHA\"))\n  engine.setEnabledProtocols(Array(\"TLSv1.2\"))\n\n  engine\n} Java copysource// imports\nimport java.security.KeyStore;\nimport java.security.SecureRandom;\nimport javax.net.ssl.KeyManagerFactory;\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.SSLEngine;\nimport javax.net.ssl.TrustManagerFactory;\nimport org.apache.pekko.stream.TLSRole;\n\n  // initialize SSLContext once\n  private final SSLContext sslContext;\n\n  {\n    try {\n      // Don't hardcode your password in actual code\n      char[] password = \"abcdef\".toCharArray();\n\n      // trust store and keys in one keystore\n      KeyStore keyStore = KeyStore.getInstance(\"PKCS12\");\n      keyStore.load(getClass().getResourceAsStream(\"/tcp-spec-keystore.p12\"), password);\n\n      TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(\"SunX509\");\n      trustManagerFactory.init(keyStore);\n\n      KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(\"SunX509\");\n      keyManagerFactory.init(keyStore, password);\n\n      // init ssl context\n      SSLContext context = SSLContext.getInstance(\"TLSv1.2\");\n      context.init(\n          keyManagerFactory.getKeyManagers(),\n          trustManagerFactory.getTrustManagers(),\n          new SecureRandom());\n\n      sslContext = context;\n\n    } catch (KeyStoreException\n        | IOException\n        | NoSuchAlgorithmException\n        | CertificateException\n        | UnrecoverableKeyException\n        | KeyManagementException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n  // create new SSLEngine from the SSLContext, which was initialized once\n  public SSLEngine createSSLEngine(TLSRole role) {\n    SSLEngine engine = sslContext.createSSLEngine();\n\n    engine.setUseClientMode(role.equals(org.apache.pekko.stream.TLSRole.client()));\n    engine.setEnabledCipherSuites(new String[] {\"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\"});\n    engine.setEnabledProtocols(new String[] {\"TLSv1.2\"});\n\n    return engine;\n  }\nThe SSLEngine instance can then be used with the binding or outgoing connection factory methods.","title":"TLS"},{"location":"/stream/stream-io.html#streaming-file-io","text":"Pekko Streams provide simple Sources and Sinks that can work with ByteStringByteString instances to perform IO operations on files.\nStreaming data from a file is as easy as creating a FileIO.fromPath given a target path, and an optional chunkSize which determines the buffer size determined as one “element” in such stream:\nScala copysourceimport org.apache.pekko.stream.scaladsl._\nval file = Paths.get(\"example.csv\")\n\nval foreach: Future[IOResult] = FileIO.fromPath(file).to(Sink.ignore).run() Java copysourcefinal Path file = Paths.get(\"example.csv\");\n  Sink<ByteString, CompletionStage<Done>> printlnSink =\n      Sink.<ByteString>foreach(chunk -> System.out.println(chunk.utf8String()));\n\n  CompletionStage<IOResult> ioResult = FileIO.fromPath(file).to(printlnSink).run(system);\nPlease note that these operators are backed by Actors and by default are configured to run on a pre-configured threadpool-backed dispatcher dedicated for File IO. This is very important as it isolates the blocking file IO operations from the rest of the ActorSystem allowing each dispatcher to be utilised in the most efficient way. If you want to configure a custom dispatcher for file IO operations globally, you can do so by changing the pekko.stream.materializer.blocking-io-dispatcher, or for a specific operator by specifying a custom Dispatcher in code, like this:\nScala copysourceFileIO.fromPath(file).withAttributes(ActorAttributes.dispatcher(\"custom-blocking-io-dispatcher\")) Java copysourceFileIO.toPath(file)\n    .withAttributes(ActorAttributes.dispatcher(\"custom-blocking-io-dispatcher\"));","title":"Streaming File IO"},{"location":"/stream/stream-refs.html","text":"","title":"StreamRefs - Reactive Streams over the network"},{"location":"/stream/stream-refs.html#streamrefs-reactive-streams-over-the-network","text":"","title":"StreamRefs - Reactive Streams over the network"},{"location":"/stream/stream-refs.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-refs.html#introduction","text":"Warning This module is currently marked as may change in the sense of being the subject of final development. This means that the API or semantics can change without warning or deprecation period, and it is not recommended to use this module in production just yet.\nStream references, or “stream refs” for short, allow running Pekko Streams across multiple nodes within a Pekko Cluster.\nUnlike heavier “streaming data processing” frameworks, Pekko Streams are neither “deployed” nor automatically distributed. Pekko stream refs are, as the name implies, references to existing parts of a stream, and can be used to create a distributed processing framework or to introduce such capabilities in specific parts of your application.\nStream refs are trivial to use in existing clustered Pekko applications and require no additional configuration or setup. They automatically maintain flow-control / back-pressure over the network and employ Pekko’s failure detection mechanisms to fail-fast (“let it crash!”) in the case of failures of remote nodes. They can be seen as an implementation of the Work Pulling Pattern, which one would otherwise implement manually.\nNote A useful way to think about stream refs is: “like an ActorRef, but for Pekko Streams’s Source and Sink”. Stream refs refer to an already existing, possibly remote, Sink or Source. This is not to be mistaken with deploying streams remotely, which this feature is not intended for.\nIMPORTANT Use stream refs with Pekko Cluster. The failure detector can cause quarantining if plain Pekko remoting is used.","title":"Introduction"},{"location":"/stream/stream-refs.html#stream-references","text":"The prime use case for stream refs is to replace raw actor or HTTP messaging in systems that expect long-running streams of data between two entities. Often they can be used to effectively achieve point-to-point streaming without the need to set up additional message brokers or similar secondary clusters.\nStream refs are well-suited for any system in which you need to send messages between nodes in a flow-controlled fashion. Typical examples include sending work requests to worker nodes as fast as possible, but not faster than the worker nodes can process them, or sending data elements that the downstream may be slow at processing. It is recommended to mix and introduce stream refs in actor-messaging-based systems, where the actor messaging is used to orchestrate and prepare such message flows, and later the stream refs are used to do the flow-controlled message transfer.\nStream refs are not persistent. However, it is simple to build a resumable stream by introducing such a protocol in the actor messaging layer. Stream refs are absolutely expected to be sent over Pekko remoting to other nodes within a cluster using Pekko Cluster, and therefore complement, instead of compete, with plain Actor messaging. Actors would usually be used to establish the stream via some initial message saying, “I want to offer you many log elements (the stream ref),” or conversely, “if you need to send me much data, here is the stream ref you can use to do so”.\nSince the two sides (“local” and “remote”) of each reference may be confusing to refer to as “remote” and “local” – since either side can be seen as “local” or “remote” depending how we look at it – we propose using the terminology “origin” and “target”, which is defined by where the stream ref was created. For SourceRefs, the “origin” is the side which has the data that it is going to stream out. For SinkRefs, the “origin” side is the actor system that is ready to receive the data and has allocated the ref. Those two may be seen as duals of each other. However, to explain patterns about sharing references, we found this wording to be rather useful.","title":"Stream References"},{"location":"/stream/stream-refs.html#source-refs-offering-streaming-data-to-a-remote-system","text":"A `SourceRef``SourceRef` can be offered to a remote actor system in order for it to consume some source of data that we have prepared locally.\nIn order to share a Source with a remote endpoint you need to materialize it by running it into the Sink.sourceRef. That Sink materializes the SourceRef that you can then send to other nodes.\nScala copysourceimport org.apache.pekko\nimport pekko.stream.SourceRef\nimport pekko.pattern.pipe\n\ncase class RequestLogs(streamId: Int)\ncase class LogsOffer(streamId: Int, sourceRef: SourceRef[String])\n\nclass DataSource extends Actor {\n\n  def receive = {\n    case RequestLogs(streamId) =>\n      // obtain the source you want to offer:\n      val source: Source[String, NotUsed] = streamLogs(streamId)\n\n      // materialize the SourceRef:\n      val ref: SourceRef[String] = source.runWith(StreamRefs.sourceRef())\n\n      // wrap the SourceRef in some domain message, such that the sender knows what source it is\n      val reply = LogsOffer(streamId, ref)\n\n      // reply to sender\n      sender() ! reply\n  }\n\n  def streamLogs(streamId: Long): Source[String, NotUsed] = ???\n} Java copysourcestatic class RequestLogs {\n  public final long streamId;\n\n  public RequestLogs(long streamId) {\n    this.streamId = streamId;\n  }\n}\n\nstatic class LogsOffer {\n  final SourceRef<String> sourceRef;\n\n  public LogsOffer(SourceRef<String> sourceRef) {\n    this.sourceRef = sourceRef;\n  }\n}\n\nstatic class DataSource extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(RequestLogs.class, this::handleRequestLogs).build();\n  }\n\n  private void handleRequestLogs(RequestLogs requestLogs) {\n    Source<String, NotUsed> logs = streamLogs(requestLogs.streamId);\n    SourceRef<String> logsRef = logs.runWith(StreamRefs.sourceRef(), mat);\n\n    getSender().tell(new LogsOffer(logsRef), getSelf());\n  }\n\n  private Source<String, NotUsed> streamLogs(long streamId) {\n    return Source.repeat(\"[INFO] some interesting logs here (for id: \" + streamId + \")\");\n  }\n}\nThe origin actor which creates and owns the Source could also perform some validation or additional setup when preparing the Source. Once it has handed out the SourceRef, the remote side can run it like this:\nScala copysourceval sourceActor = system.actorOf(Props[DataSource](), \"dataSource\")\n\nsourceActor ! RequestLogs(1337)\nval offer = expectMsgType[LogsOffer]\n\n// implicitly converted to a Source:\noffer.sourceRef.runWith(Sink.foreach(println))\n// alternatively explicitly obtain Source from SourceRef:\n// offer.sourceRef.source.runWith(Sink.foreach(println))\n Java copysourceActorRef sourceActor = system.actorOf(Props.create(DataSource.class), \"dataSource\");\n\nsourceActor.tell(new RequestLogs(1337), getTestActor());\nLogsOffer offer = expectMsgClass(LogsOffer.class);\n\noffer.sourceRef.getSource().runWith(Sink.foreach(log -> System.out.println(log)), mat);\nThe process of preparing and running a SourceRef-powered distributed stream is shown by the animation below:\nWarning A SourceRef is by design “single-shot”; i.e., it may only be materialized once. This is in order to not complicate the mental model of what materialization means. Multicast can be mimicked by starting a BroadcastHub operator once, then attaching multiple new streams to it, each emitting a new stream ref. This way, materialization of the BroadcastHubs Source creates a unique single-shot stream ref, however they can all be powered using a single Source – located before the BroadcastHub operator.","title":"Source Refs - offering streaming data to a remote system"},{"location":"/stream/stream-refs.html#sink-refs-offering-to-receive-streaming-data-from-a-remote-system","text":"The dual of `SourceRef``SourceRef`s.\nThey can be used to offer the other side the capability to send to the origin side data in a streaming, flow-controlled fashion. The origin here allocates a Sink, which could be as simple as a Sink.foreach or as advanced as a complex Sink which streams the incoming data into various other systems (e.g., any of the Pekko connectors-provided Sinks).\nNote To form a good mental model of SinkRefs, you can think of them as being similar to “passive mode” in FTP.\nScala copysourceimport org.apache.pekko.stream.SinkRef\n\ncase class PrepareUpload(id: String)\ncase class MeasurementsSinkReady(id: String, sinkRef: SinkRef[String])\n\nclass DataReceiver extends Actor {\n\n  def receive = {\n    case PrepareUpload(nodeId) =>\n      // obtain the source you want to offer:\n      val sink: Sink[String, NotUsed] = logsSinkFor(nodeId)\n\n      // materialize the SinkRef (the remote is like a source of data for us):\n      val ref: SinkRef[String] = StreamRefs.sinkRef[String]().to(sink).run()\n\n      // wrap the SinkRef in some domain message, such that the sender knows what source it is\n      val reply = MeasurementsSinkReady(nodeId, ref)\n\n      // reply to sender\n      sender() ! reply\n  }\n\n  def logsSinkFor(nodeId: String): Sink[String, NotUsed] = ???\n}\n Java copysourcestatic class PrepareUpload {\n  final String id;\n\n  public PrepareUpload(String id) {\n    this.id = id;\n  }\n}\n\nstatic class MeasurementsSinkReady {\n  final String id;\n  final SinkRef<String> sinkRef;\n\n  public MeasurementsSinkReady(String id, SinkRef<String> ref) {\n    this.id = id;\n    this.sinkRef = ref;\n  }\n}\n\nstatic class DataReceiver extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            PrepareUpload.class,\n            prepare -> {\n              Sink<String, NotUsed> sink = logsSinkFor(prepare.id);\n              SinkRef<String> sinkRef = StreamRefs.<String>sinkRef().to(sink).run(mat);\n\n              getSender().tell(new MeasurementsSinkReady(prepare.id, sinkRef), getSelf());\n            })\n        .build();\n  }\n\n  private Sink<String, NotUsed> logsSinkFor(String id) {\n    return Sink.<String>ignore().mapMaterializedValue(done -> NotUsed.getInstance());\n  }\n}\nUsing the offered SinkRef to send data to the origin of the Sink is also simple, as we can treat the SinkRef as any other Sink and directly runWith or run with it.\nScala copysourceval receiver = system.actorOf(Props[DataReceiver](), \"receiver\")\n\nreceiver ! PrepareUpload(\"system-42-tmp\")\nval ready = expectMsgType[MeasurementsSinkReady]\n\n// stream local metrics to Sink's origin:\nlocalMetrics().runWith(ready.sinkRef) Java copysourceActorRef receiver = system.actorOf(Props.create(DataReceiver.class), \"dataReceiver\");\n\nreceiver.tell(new PrepareUpload(\"system-42-tmp\"), getTestActor());\nMeasurementsSinkReady ready = expectMsgClass(MeasurementsSinkReady.class);\n\nSource.repeat(\"hello\").runWith(ready.sinkRef.getSink(), mat);\nThe process of preparing and running a SinkRef-powered distributed stream is shown by the animation below:\nWarning A SinkRef is by design “single-shot”; i.e., it may only be materialized once. This is in order to not complicate the mental model of what materialization means. If you have a use case for building a fan-in operation that accepts writes from multiple remote nodes, you can build your Sink and prepend it with a MergeHub operator, each time materializing a new SinkRef targeting that MergeHub. This has the added benefit of giving you full control of how to merge these streams (i.e., by using “merge preferred” or any other variation of the fan-in operators).","title":"Sink Refs - offering to receive streaming data from a remote system"},{"location":"/stream/stream-refs.html#delivery-guarantees","text":"Stream refs utilise normal actor messaging for their transport, and therefore provide the same level of basic delivery guarantees. Stream refs do extend the semantics somewhat, through demand re-delivery and sequence fault detection. In other words:\nmessages are sent over actor remoting which relies on TCP (classic remoting or Artery TCP) or Aeron UDP for basic redelivery mechanisms messages are guaranteed to to be in-order messages can be lost, however: a dropped demand signal will be re-delivered automatically (similar to system messages) a dropped element signal will cause the stream to fail","title":"Delivery guarantees"},{"location":"/stream/stream-refs.html#bulk-stream-references","text":"Warning Bulk stream references are not implemented yet. See ticket Bulk Transfer Stream Refs #24276 to track progress or signal demand for this feature.\nBulk stream refs can be used to create simple side-channels to transfer humongous amounts of data such as huge log files, messages or even media, with as much ease as if it was a trivial local stream.","title":"Bulk Stream References"},{"location":"/stream/stream-refs.html#serialization-of-sourceref-and-sinkref","text":"StreamRefs require serialization, since the whole point is to send them between nodes of a cluster. A built in serializer is provided when SourceRef and SinkRef are sent directly as messages however the recommended use is to wrap them into your own actor message classes.\nWhen Pekko Jackson is used, serialization of wrapped SourceRef and SinkRef will work out of the box.\nIf you are using some other form of serialization you will need to use the StreamRefResolverStreamRefResolver extension from your serializer to get the SourceRef and SinkRef. The extension provides the methods toSerializationFormat(sink or source) to transform from refs to string and resolve{Sink,Source}Ref(String) to resolve refs from strings.","title":"Serialization of SourceRef and SinkRef"},{"location":"/stream/stream-refs.html#configuration","text":"","title":"Configuration"},{"location":"/stream/stream-refs.html#stream-reference-subscription-timeouts","text":"All stream references have a subscription timeout, which is intended to prevent resource leaks in case a remote node requests the allocation of many streams but never actually runs them. In order to prevent this, each stream reference has a default timeout (of 30 seconds), after which the origin will abort the stream offer if the target has not materialized the stream ref. After the timeout has triggered, materialization of the target side will fail, pointing out that the origin is missing.\nSince these timeouts are often very different based on the kind of stream offered, and there can be many different kinds of them in the same application, it is possible to not only configure this setting globally (pekko.stream.materializer.stream-ref.subscription-timeout), but also via attributes:\nScala copysource// configure the timeout for source\nimport scala.concurrent.duration._\nimport org.apache.pekko.stream.StreamRefAttributes\n\n// configuring Sink.sourceRef (notice that we apply the attributes to the Sink!):\nSource\n  .repeat(\"hello\")\n  .runWith(StreamRefs.sourceRef().addAttributes(StreamRefAttributes.subscriptionTimeout(5.seconds)))\n\n// configuring SinkRef.source:\nStreamRefs\n  .sinkRef()\n  .addAttributes(StreamRefAttributes.subscriptionTimeout(5.seconds))\n  .runWith(Sink.ignore) // not very interesting Sink, just an example Java copysourceFiniteDuration timeout = FiniteDuration.create(5, TimeUnit.SECONDS);\nAttributes timeoutAttributes = StreamRefAttributes.subscriptionTimeout(timeout);\n\n// configuring Sink.sourceRef (notice that we apply the attributes to the Sink!):\nSource.repeat(\"hello\")\n    .runWith(StreamRefs.<String>sourceRef().addAttributes(timeoutAttributes), mat);\n\n// configuring SinkRef.source:\nStreamRefs.<String>sinkRef()\n    .addAttributes(timeoutAttributes)\n    .runWith(Sink.<String>ignore(), mat); // not very interesting sink, just an example","title":"Stream reference subscription timeouts"},{"location":"/stream/stream-refs.html#general-configuration","text":"Other settings can be set globally in your application.conf, by overriding any of the following values in the pekko.stream.materializer.stream-ref.* keyspace:\ncopysource# configure defaults for SourceRef and SinkRef\nstream-ref {\n  # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref\n  #\n  # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,\n  # because the delay of requesting over network boundaries is much higher.\n  buffer-capacity = 32\n\n  # Demand is signalled by sending a cumulative demand message (\"requesting messages until the n-th sequence number)\n  # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should\n  # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).\n  #\n  # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.\n  #\n  # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive\n  # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.\n  demand-redelivery-interval = 1 second\n\n  # Subscription timeout, during which the \"remote side\" MUST subscribe (materialize) the handed out stream ref.\n  # This timeout does not have to be very low in normal situations, since the remote side may also need to\n  # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking\n  # in-active streams which are never subscribed to.\n  subscription-timeout = 30 seconds\n\n  # In order to guard the receiving end of a stream ref from never terminating (since awaiting a Completion or Failed\n  # message) after / before a Terminated is seen, a special timeout is applied once Terminated is received by it.\n  # This allows us to terminate stream refs that have been targeted to other nodes which are Downed, and as such the\n  # other side of the stream ref would never send the \"final\" terminal message.\n  #\n  # The timeout specifically means the time between the Terminated signal being received and when the local SourceRef\n  # determines to fail itself, assuming there was message loss or a complete partition of the completion signal.\n  final-termination-signal-deadline = 2 seconds\n}","title":"General configuration"},{"location":"/stream/stream-parallelism.html","text":"","title":"Pipelining and Parallelism"},{"location":"/stream/stream-parallelism.html#pipelining-and-parallelism","text":"","title":"Pipelining and Parallelism"},{"location":"/stream/stream-parallelism.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-parallelism.html#introduction","text":"Pekko Streams operators (be it simple operators on Flows and Sources or graph junctions) are “fused” together and executed sequentially by default. This avoids the overhead of events crossing asynchronous boundaries but limits the flow to execute at most one operator at any given time.\nIn many cases it is useful to be able to concurrently execute the operators of a flow, this is done by explicitly marking them as asynchronous using the asyncasync() method. Each operator marked as asynchronous will run in a dedicated actor internally, while all operators not marked asynchronous will run in one single actor.\nWe will illustrate through the example of pancake cooking how streams can be used for various processing patterns, exploiting the available parallelism on modern computers. The setting is the following: both Patrik and Roland like to make pancakes, but they need to produce sufficient amount in a cooking session to make all of the children happy. To increase their pancake production throughput they use two frying pans. How they organize their pancake processing is markedly different.","title":"Introduction"},{"location":"/stream/stream-parallelism.html#pipelining","text":"Roland uses the two frying pans in an asymmetric fashion. The first pan is only used to fry one side of the pancake then the half-finished pancake is flipped into the second pan for the finishing fry on the other side. Once the first frying pan becomes available it gets a new scoop of batter. As an effect, most of the time there are two pancakes being cooked at the same time, one being cooked on its first side and the second being cooked to completion. This is how this setup would look like implemented as a stream:\nScala copysource// Takes a scoop of batter and creates a pancake with one side cooked\nval fryingPan1: Flow[ScoopOfBatter, HalfCookedPancake, NotUsed] =\n  Flow[ScoopOfBatter].map { batter =>\n    HalfCookedPancake()\n  }\n\n// Finishes a half-cooked pancake\nval fryingPan2: Flow[HalfCookedPancake, Pancake, NotUsed] =\n  Flow[HalfCookedPancake].map { halfCooked =>\n    Pancake()\n  }\n\n  // With the two frying pans we can fully cook pancakes\n  val pancakeChef: Flow[ScoopOfBatter, Pancake, NotUsed] =\n    Flow[ScoopOfBatter].via(fryingPan1.async).via(fryingPan2.async) Java copysourceFlow<ScoopOfBatter, HalfCookedPancake, NotUsed> fryingPan1 =\n    Flow.of(ScoopOfBatter.class).map(batter -> new HalfCookedPancake());\n\nFlow<HalfCookedPancake, Pancake, NotUsed> fryingPan2 =\n    Flow.of(HalfCookedPancake.class).map(halfCooked -> new Pancake());\n\n  // With the two frying pans we can fully cook pancakes\n  Flow<ScoopOfBatter, Pancake, NotUsed> pancakeChef = fryingPan1.async().via(fryingPan2.async());\nThe two map operators in sequence (encapsulated in the “frying pan” flows) will be executed in a pipelined way, the same way that Roland was using his frying pans:\nA ScoopOfBatter enters fryingPan1 fryingPan1 emits a HalfCookedPancake once fryingPan2 becomes available fryingPan2 takes the HalfCookedPancake at this point fryingPan1 already takes the next scoop, without waiting for fryingPan2 to finish\nThe benefit of pipelining is that it can be applied to any sequence of processing steps that are otherwise not parallelisable (for example because the result of a processing step depends on all the information from the previous step). One drawback is that if the processing times of the operators are very different then some of the operators will not be able to operate at full throughput because they will wait on a previous or subsequent operator most of the time. In the pancake example frying the second half of the pancake is usually faster than frying the first half, fryingPan2 will not be able to operate at full capacity [1].\nNote Asynchronous stream operators have internal buffers to make communication between them more efficient. For more details about the behavior of these and how to add additional buffers refer to Buffers and working with rate.","title":"Pipelining"},{"location":"/stream/stream-parallelism.html#parallel-processing","text":"Patrik uses the two frying pans symmetrically. He uses both pans to fully fry a pancake on both sides, then puts the results on a shared plate. Whenever a pan becomes empty, he takes the next scoop from the shared bowl of batter. In essence he parallelizes the same process over multiple pans. This is how this setup will look like if implemented using streams:\nScala copysourceval fryingPan: Flow[ScoopOfBatter, Pancake, NotUsed] =\n  Flow[ScoopOfBatter].map { batter =>\n    Pancake()\n  }\n\nval pancakeChef: Flow[ScoopOfBatter, Pancake, NotUsed] = Flow.fromGraph(GraphDSL.create() { implicit builder =>\n  val dispatchBatter = builder.add(Balance[ScoopOfBatter](2))\n  val mergePancakes = builder.add(Merge[Pancake](2))\n\n  // Using two frying pans in parallel, both fully cooking a pancake from the batter.\n  // We always put the next scoop of batter to the first frying pan that becomes available.\n  dispatchBatter.out(0) ~> fryingPan.async ~> mergePancakes.in(0)\n  // Notice that we used the \"fryingPan\" flow without importing it via builder.add().\n  // Flows used this way are auto-imported, which in this case means that the two\n  // uses of \"fryingPan\" mean actually different stages in the graph.\n  dispatchBatter.out(1) ~> fryingPan.async ~> mergePancakes.in(1)\n\n  FlowShape(dispatchBatter.in, mergePancakes.out)\n})\n Java copysourceFlow<ScoopOfBatter, Pancake, NotUsed> fryingPan =\n    Flow.of(ScoopOfBatter.class).map(batter -> new Pancake());\n\nFlow<ScoopOfBatter, Pancake, NotUsed> pancakeChef =\n    Flow.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final UniformFanInShape<Pancake, Pancake> mergePancakes = b.add(Merge.create(2));\n              final UniformFanOutShape<ScoopOfBatter, ScoopOfBatter> dispatchBatter =\n                  b.add(Balance.create(2));\n\n              // Using two frying pans in parallel, both fully cooking a pancake from the\n              // batter.\n              // We always put the next scoop of batter to the first frying pan that becomes\n              // available.\n              b.from(dispatchBatter.out(0))\n                  .via(b.add(fryingPan.async()))\n                  .toInlet(mergePancakes.in(0));\n              // Notice that we used the \"fryingPan\" flow without importing it via\n              // builder.add().\n              // Flows used this way are auto-imported, which in this case means that the two\n              // uses of \"fryingPan\" mean actually different stages in the graph.\n              b.from(dispatchBatter.out(1))\n                  .via(b.add(fryingPan.async()))\n                  .toInlet(mergePancakes.in(1));\n\n              return FlowShape.of(dispatchBatter.in(), mergePancakes.out());\n            }));\nThe benefit of parallelizing is that it is easy to scale. In the pancake example it is easy to add a third frying pan with Patrik’s method, but Roland cannot add a third frying pan, since that would require a third processing step, which is not practically possible in the case of frying pancakes.\nOne drawback of the example code above is it does not preserve the ordering of pancakes. This might be a problem if children like to track their “own” pancakes. In those cases the BalanceBalance and MergeMerge operators should be replaced by round-robin balancing and merging operators which put in and take out pancakes in a strict order.\nA more detailed example of creating a worker pool can be found in the cookbook: Balancing jobs to a fixed pool of workers","title":"Parallel processing"},{"location":"/stream/stream-parallelism.html#combining-pipelining-and-parallel-processing","text":"The two concurrency patterns that we demonstrated as means to increase throughput are not exclusive. In fact, it is rather simple to combine the two approaches and streams provide a nice unifying language to express and compose them.\nFirst, let’s look at how we can parallelize pipelined operators. In the case of pancakes this means that we will employ two chefs, each working using Roland’s pipelining method, but we use the two chefs in parallel, just like Patrik used the two frying pans. This is how it looks like if expressed as streams:\nScala copysourceval pancakeChef: Flow[ScoopOfBatter, Pancake, NotUsed] =\n  Flow.fromGraph(GraphDSL.create() { implicit builder =>\n    val dispatchBatter = builder.add(Balance[ScoopOfBatter](2))\n    val mergePancakes = builder.add(Merge[Pancake](2))\n\n    // Using two pipelines, having two frying pans each, in total using\n    // four frying pans\n    dispatchBatter.out(0) ~> fryingPan1.async ~> fryingPan2.async ~> mergePancakes.in(0)\n    dispatchBatter.out(1) ~> fryingPan1.async ~> fryingPan2.async ~> mergePancakes.in(1)\n\n    FlowShape(dispatchBatter.in, mergePancakes.out)\n  }) Java copysourceFlow<ScoopOfBatter, Pancake, NotUsed> pancakeChef =\n    Flow.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final UniformFanInShape<Pancake, Pancake> mergePancakes = b.add(Merge.create(2));\n              final UniformFanOutShape<ScoopOfBatter, ScoopOfBatter> dispatchBatter =\n                  b.add(Balance.create(2));\n\n              // Using two pipelines, having two frying pans each, in total using\n              // four frying pans\n              b.from(dispatchBatter.out(0))\n                  .via(b.add(fryingPan1.async()))\n                  .via(b.add(fryingPan2.async()))\n                  .toInlet(mergePancakes.in(0));\n\n              b.from(dispatchBatter.out(1))\n                  .via(b.add(fryingPan1.async()))\n                  .via(b.add(fryingPan2.async()))\n                  .toInlet(mergePancakes.in(1));\n\n              return FlowShape.of(dispatchBatter.in(), mergePancakes.out());\n            }));\nThe above pattern works well if there are many independent jobs that do not depend on the results of each other, but the jobs themselves need multiple processing steps where each step builds on the result of the previous one. In our case individual pancakes do not depend on each other, they can be cooked in parallel, on the other hand it is not possible to fry both sides of the same pancake at the same time, so the two sides have to be fried in sequence.\nIt is also possible to organize parallelized operators into pipelines. This would mean employing four chefs:\nthe first two chefs prepare half-cooked pancakes from batter, in parallel, then putting those on a large enough flat surface. the second two chefs take these and fry their other side in their own pans, then they put the pancakes on a shared plate.\nThis is again straightforward to implement with the streams API:\nScala copysourceval pancakeChefs1: Flow[ScoopOfBatter, HalfCookedPancake, NotUsed] =\n  Flow.fromGraph(GraphDSL.create() { implicit builder =>\n    val dispatchBatter = builder.add(Balance[ScoopOfBatter](2))\n    val mergeHalfPancakes = builder.add(Merge[HalfCookedPancake](2))\n\n    // Two chefs work with one frying pan for each, half-frying the pancakes then putting\n    // them into a common pool\n    dispatchBatter.out(0) ~> fryingPan1.async ~> mergeHalfPancakes.in(0)\n    dispatchBatter.out(1) ~> fryingPan1.async ~> mergeHalfPancakes.in(1)\n\n    FlowShape(dispatchBatter.in, mergeHalfPancakes.out)\n  })\n\nval pancakeChefs2: Flow[HalfCookedPancake, Pancake, NotUsed] =\n  Flow.fromGraph(GraphDSL.create() { implicit builder =>\n    val dispatchHalfPancakes = builder.add(Balance[HalfCookedPancake](2))\n    val mergePancakes = builder.add(Merge[Pancake](2))\n\n    // Two chefs work with one frying pan for each, finishing the pancakes then putting\n    // them into a common pool\n    dispatchHalfPancakes.out(0) ~> fryingPan2.async ~> mergePancakes.in(0)\n    dispatchHalfPancakes.out(1) ~> fryingPan2.async ~> mergePancakes.in(1)\n\n    FlowShape(dispatchHalfPancakes.in, mergePancakes.out)\n  })\n\nval kitchen: Flow[ScoopOfBatter, Pancake, NotUsed] = pancakeChefs1.via(pancakeChefs2) Java copysourceFlow<ScoopOfBatter, HalfCookedPancake, NotUsed> pancakeChefs1 =\n    Flow.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final UniformFanInShape<HalfCookedPancake, HalfCookedPancake> mergeHalfCooked =\n                  b.add(Merge.create(2));\n              final UniformFanOutShape<ScoopOfBatter, ScoopOfBatter> dispatchBatter =\n                  b.add(Balance.create(2));\n\n              // Two chefs work with one frying pan for each, half-frying the pancakes then\n              // putting\n              // them into a common pool\n              b.from(dispatchBatter.out(0))\n                  .via(b.add(fryingPan1.async()))\n                  .toInlet(mergeHalfCooked.in(0));\n              b.from(dispatchBatter.out(1))\n                  .via(b.add(fryingPan1.async()))\n                  .toInlet(mergeHalfCooked.in(1));\n\n              return FlowShape.of(dispatchBatter.in(), mergeHalfCooked.out());\n            }));\n\nFlow<HalfCookedPancake, Pancake, NotUsed> pancakeChefs2 =\n    Flow.fromGraph(\n        GraphDSL.create(\n            b -> {\n              final UniformFanInShape<Pancake, Pancake> mergePancakes = b.add(Merge.create(2));\n              final UniformFanOutShape<HalfCookedPancake, HalfCookedPancake>\n                  dispatchHalfCooked = b.add(Balance.create(2));\n\n              // Two chefs work with one frying pan for each, finishing the pancakes then\n              // putting\n              // them into a common pool\n              b.from(dispatchHalfCooked.out(0))\n                  .via(b.add(fryingPan2.async()))\n                  .toInlet(mergePancakes.in(0));\n              b.from(dispatchHalfCooked.out(1))\n                  .via(b.add(fryingPan2.async()))\n                  .toInlet(mergePancakes.in(1));\n\n              return FlowShape.of(dispatchHalfCooked.in(), mergePancakes.out());\n            }));\n\nFlow<ScoopOfBatter, Pancake, NotUsed> kitchen = pancakeChefs1.via(pancakeChefs2);\nThis usage pattern is less common but might be usable if a certain step in the pipeline might take wildly different times to finish different jobs. The reason is that there are more balance-merge steps in this pattern compared to the parallel pipelines. This pattern rebalances after each step, while the previous pattern only balances at the entry point of the pipeline. This only matters however if the processing time distribution has a large deviation.\n[1] Roland’s reason for this seemingly suboptimal procedure is that he prefers the temperature of the second pan to be slightly lower than the first in order to achieve a more homogeneous result.","title":"Combining pipelining and parallel processing"},{"location":"/stream/stream-testkit.html","text":"","title":"Testing streams"},{"location":"/stream/stream-testkit.html#testing-streams","text":"","title":"Testing streams"},{"location":"/stream/stream-testkit.html#dependency","text":"To use Pekko Stream TestKit, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-testkit\" % PekkoVersion % Test Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  testImplementation \"org.apache.pekko:pekko-stream-testkit_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-testkit.html#introduction","text":"Verifying behavior of Pekko Stream sources, flows and sinks can be done using various code patterns and libraries. Here we will discuss testing these elements using:\nsimple sources, sinks and flows; sources and sinks in combination with TestProbeTestProbe from the pekko-testkit module; sources and sinks specifically crafted for writing tests from the pekko-stream-testkit module.\nIt is important to keep your data processing pipeline as separate sources, flows and sinks. This makes them testable by wiring them up to other sources or sinks, or some test harnesses that pekko-testkit or pekko-stream-testkit provide.","title":"Introduction"},{"location":"/stream/stream-testkit.html#built-in-sources-sinks-and-operators","text":"Testing a custom sink can be as simple as attaching a source that emits elements from a predefined collection, running a constructed test flow and asserting on the results that sink produced. Here is an example of a test for a sink:\nScala copysourceval sinkUnderTest =\n  Flow[Int].map(_ * 2).toMat(Sink.fold(0)(_ + _))(Keep.right)\n\nval future = Source(1 to 4).runWith(sinkUnderTest)\nval result = Await.result(future, 3.seconds)\nassert(result == 20) Java copysourcefinal Sink<Integer, CompletionStage<Integer>> sinkUnderTest =\n    Flow.of(Integer.class)\n        .map(i -> i * 2)\n        .toMat(Sink.fold(0, (agg, next) -> agg + next), Keep.right());\n\nfinal CompletionStage<Integer> future =\n    Source.from(Arrays.asList(1, 2, 3, 4)).runWith(sinkUnderTest, system);\nfinal Integer result = future.toCompletableFuture().get(3, TimeUnit.SECONDS);\nassertEquals(20, result.intValue());\nThe same strategy can be applied for sources as well. In the next example we have a source that produces an infinite stream of elements. Such source can be tested by asserting that first arbitrary number of elements hold some condition. Here the taketake operator and Sink.seqSink.seq are very useful.\nScala copysourceimport system.dispatcher\nimport org.apache.pekko.pattern.pipe\n\nval sourceUnderTest = Source.repeat(1).map(_ * 2)\n\nval future = sourceUnderTest.take(10).runWith(Sink.seq)\nval result = Await.result(future, 3.seconds)\nassert(result == Seq.fill(10)(2)) Java copysourcefinal Source<Integer, NotUsed> sourceUnderTest = Source.repeat(1).map(i -> i * 2);\n\nfinal CompletionStage<List<Integer>> future =\n    sourceUnderTest.take(10).runWith(Sink.seq(), system);\nfinal List<Integer> result = future.toCompletableFuture().get(3, TimeUnit.SECONDS);\nassertEquals(Collections.nCopies(10, 2), result);\nWhen testing a flow we need to attach a source and a sink. As both stream ends are under our control, we can choose sources that tests various edge cases of the flow and sinks that ease assertions.\nScala copysourceval flowUnderTest = Flow[Int].takeWhile(_ < 5)\n\nval future = Source(1 to 10).via(flowUnderTest).runWith(Sink.fold(Seq.empty[Int])(_ :+ _))\nval result = Await.result(future, 3.seconds)\nassert(result == (1 to 4)) Java copysourcefinal Flow<Integer, Integer, NotUsed> flowUnderTest =\n    Flow.of(Integer.class).takeWhile(i -> i < 5);\n\nfinal CompletionStage<Integer> future =\n    Source.from(Arrays.asList(1, 2, 3, 4, 5, 6))\n        .via(flowUnderTest)\n        .runWith(Sink.fold(0, (agg, next) -> agg + next), system);\nfinal Integer result = future.toCompletableFuture().get(3, TimeUnit.SECONDS);\nassertEquals(10, result.intValue());","title":"Built-in sources, sinks and operators"},{"location":"/stream/stream-testkit.html#testkit","text":"Pekko Stream offers integration with Actors out of the box. This support can be used for writing stream tests that use familiar TestProbeTestProbe from the pekko-testkit API.\nOne of the more straightforward tests would be to materialize stream to a FutureCompletionStage and then use pipePatterns.pipe pattern to pipe the result of that future to the probe.\nScala copysourceimport system.dispatcher\nimport org.apache.pekko.pattern.pipe\n\nval sourceUnderTest = Source(1 to 4).grouped(2)\n\nval probe = TestProbe()\nsourceUnderTest.runWith(Sink.seq).pipeTo(probe.ref)\nprobe.expectMsg(3.seconds, Seq(Seq(1, 2), Seq(3, 4))) Java copysourcefinal Source<List<Integer>, NotUsed> sourceUnderTest =\n    Source.from(Arrays.asList(1, 2, 3, 4)).grouped(2);\n\nfinal TestKit probe = new TestKit(system);\nfinal CompletionStage<List<List<Integer>>> future =\n    sourceUnderTest.grouped(2).runWith(Sink.head(), system);\norg.apache.pekko.pattern.Patterns.pipe(future, system.dispatcher()).to(probe.getRef());\nprobe.expectMsg(Duration.ofSeconds(3), Arrays.asList(Arrays.asList(1, 2), Arrays.asList(3, 4)));\nInstead of materializing to a future, we can use a Sink.actorRefSink.actorRef that sends all incoming elements to the given ActorRefActorRef. Now we can use assertion methods on TestProbeTestProbe and expect elements one by one as they arrive. We can also assert stream completion by expecting for onCompleteMessage which was given to Sink.actorRef.\nScala copysourcecase object Tick\nval sourceUnderTest = Source.tick(0.seconds, 200.millis, Tick)\n\nval probe = TestProbe()\nval cancellable = sourceUnderTest\n  .to(Sink.actorRef(probe.ref, onCompleteMessage = \"completed\", onFailureMessage = _ => \"failed\"))\n  .run()\n\nprobe.expectMsg(1.second, Tick)\nprobe.expectNoMessage(100.millis)\nprobe.expectMsg(3.seconds, Tick)\ncancellable.cancel()\nprobe.expectMsg(3.seconds, \"completed\") Java copysourcefinal Source<Tick, Cancellable> sourceUnderTest =\n    Source.tick(Duration.ZERO, Duration.ofMillis(200), Tick.TOCK);\n\nfinal TestKit probe = new TestKit(system);\nfinal Cancellable cancellable =\n    sourceUnderTest.to(Sink.actorRef(probe.getRef(), Tick.COMPLETED)).run(system);\nprobe.expectMsg(Duration.ofSeconds(3), Tick.TOCK);\nprobe.expectNoMessage(Duration.ofMillis(100));\nprobe.expectMsg(Duration.ofSeconds(3), Tick.TOCK);\ncancellable.cancel();\nprobe.expectMsg(Duration.ofSeconds(3), Tick.COMPLETED);\nSimilarly to Sink.actorRef that provides control over received elements, we can use Source.actorRefSource.actorRef and have full control over elements to be sent.\nScala copysourceval sinkUnderTest = Flow[Int].map(_.toString).toMat(Sink.fold(\"\")(_ + _))(Keep.right)\n\nval (ref, future) = Source\n  .actorRef(\n    completionMatcher = {\n      case Done =>\n        CompletionStrategy.draining\n    },\n    // Never fail the stream because of a message:\n    failureMatcher = PartialFunction.empty,\n    bufferSize = 8,\n    overflowStrategy = OverflowStrategy.fail)\n  .toMat(sinkUnderTest)(Keep.both)\n  .run()\n\nref ! 1\nref ! 2\nref ! 3\nref ! Done\n\nval result = Await.result(future, 3.seconds)\nassert(result == \"123\") Java copysourcefinal Sink<Integer, CompletionStage<String>> sinkUnderTest =\n    Flow.of(Integer.class)\n        .map(i -> i.toString())\n        .toMat(Sink.fold(\"\", (agg, next) -> agg + next), Keep.right());\n\nfinal Pair<ActorRef, CompletionStage<String>> refAndCompletionStage =\n    Source.<Integer>actorRef(\n            elem -> {\n              // complete stream immediately if we send it Done\n              if (elem == Done.done()) return Optional.of(CompletionStrategy.immediately());\n              else return Optional.empty();\n            },\n            // never fail the stream because of a message\n            elem -> Optional.empty(),\n            8,\n            OverflowStrategy.fail())\n        .toMat(sinkUnderTest, Keep.both())\n        .run(system);\nfinal ActorRef ref = refAndCompletionStage.first();\nfinal CompletionStage<String> future = refAndCompletionStage.second();\n\nref.tell(1, ActorRef.noSender());\nref.tell(2, ActorRef.noSender());\nref.tell(3, ActorRef.noSender());\nref.tell(Done.getInstance(), ActorRef.noSender());\n\nfinal String result = future.toCompletableFuture().get(1, TimeUnit.SECONDS);\nassertEquals(\"123\", result);","title":"TestKit"},{"location":"/stream/stream-testkit.html#streams-testkit","text":"You may have noticed various code patterns that emerge when testing stream pipelines. Pekko Stream has a separate pekko-stream-testkit module that provides tools specifically for writing stream tests. This module comes with two main components that are TestSourceTestSource and TestSinkTestSink which provide sources and sinks that materialize to probes that allow fluent API.","title":"Streams TestKit"},{"location":"/stream/stream-testkit.html#using-the-testkit","text":"A sink returned by TestSink.probeTestSink.probe allows manual control over demand and assertions over elements coming downstream.\nScala copysourceval sourceUnderTest = Source(1 to 4).filter(_ % 2 == 0).map(_ * 2)\n\nsourceUnderTest.runWith(TestSink[Int]()).request(2).expectNext(4, 8).expectComplete() Java copysourcefinal Source<Integer, NotUsed> sourceUnderTest =\n    Source.from(Arrays.asList(1, 2, 3, 4)).filter(elem -> elem % 2 == 0).map(elem -> elem * 2);\n\nsourceUnderTest\n    .runWith(TestSink.probe(system), system)\n    .request(2)\n    .expectNext(4, 8)\n    .expectComplete();\nA source returned by TestSource.probeTestSource.probe can be used for asserting demand or controlling when stream is completed or ended with an error.\nScala copysourceval sinkUnderTest = Sink.cancelled\n\nTestSource.probe[Int].toMat(sinkUnderTest)(Keep.left).run().expectCancellation() Java copysourcefinal Sink<Integer, NotUsed> sinkUnderTest = Sink.cancelled();\n\nTestSource.<Integer>probe(system)\n    .toMat(sinkUnderTest, Keep.left())\n    .run(system)\n    .expectCancellation();\nYou can also inject exceptions and test sink behavior on error conditions.\nScala copysourceval sinkUnderTest = Sink.head[Int]\n\nval (probe, future) = TestSource.probe[Int].toMat(sinkUnderTest)(Keep.both).run()\nprobe.sendError(new Exception(\"boom\"))\n\nassert(future.failed.futureValue.getMessage == \"boom\") Java copysourcefinal Sink<Integer, CompletionStage<Integer>> sinkUnderTest = Sink.head();\n\nfinal Pair<TestPublisher.Probe<Integer>, CompletionStage<Integer>> probeAndCompletionStage =\n    TestSource.<Integer>probe(system).toMat(sinkUnderTest, Keep.both()).run(system);\nfinal TestPublisher.Probe<Integer> probe = probeAndCompletionStage.first();\nfinal CompletionStage<Integer> future = probeAndCompletionStage.second();\nprobe.sendError(new Exception(\"boom\"));\n\nExecutionException exception =\n    Assert.assertThrows(\n        ExecutionException.class, () -> future.toCompletableFuture().get(3, TimeUnit.SECONDS));\nassertEquals(\"boom\", exception.getCause().getMessage());\nTest source and sink can be used together in combination when testing flows.\nScala copysourceval flowUnderTest = Flow[Int].mapAsyncUnordered(2) { sleep =>\n  pattern.after(10.millis * sleep, using = system.scheduler)(Future.successful(sleep))\n}\n\nval (pub, sub) = TestSource.probe[Int].via(flowUnderTest).toMat(TestSink[Int]())(Keep.both).run()\n\nsub.request(n = 3)\npub.sendNext(3)\npub.sendNext(2)\npub.sendNext(1)\nsub.expectNextUnordered(1, 2, 3)\n\npub.sendError(new Exception(\"Power surge in the linear subroutine C-47!\"))\nval ex = sub.expectError()\nassert(ex.getMessage.contains(\"C-47\")) Java copysourcefinal Flow<Integer, Integer, NotUsed> flowUnderTest =\n    Flow.of(Integer.class)\n        .mapAsyncUnordered(\n            2,\n            sleep ->\n                org.apache.pekko.pattern.Patterns.after(\n                    Duration.ofMillis(10),\n                    system.scheduler(),\n                    system.dispatcher(),\n                    () -> CompletableFuture.completedFuture(sleep)));\n\nfinal Pair<TestPublisher.Probe<Integer>, TestSubscriber.Probe<Integer>> pubAndSub =\n    TestSource.<Integer>probe(system)\n        .via(flowUnderTest)\n        .toMat(TestSink.<Integer>probe(system), Keep.both())\n        .run(system);\nfinal TestPublisher.Probe<Integer> pub = pubAndSub.first();\nfinal TestSubscriber.Probe<Integer> sub = pubAndSub.second();\n\nsub.request(3);\npub.sendNext(3);\npub.sendNext(2);\npub.sendNext(1);\nsub.expectNextUnordered(1, 2, 3);\n\npub.sendError(new Exception(\"Power surge in the linear subroutine C-47!\"));\nfinal Throwable ex = sub.expectError();\nassertTrue(ex.getMessage().contains(\"C-47\"));","title":"Using the TestKit"},{"location":"/stream/stream-testkit.html#fuzzing-mode","text":"For testing, it is possible to enable a special stream execution mode that exercises concurrent execution paths more aggressively (at the cost of reduced performance) and therefore helps exposing race conditions in tests. To enable this setting add the following line to your configuration:\npekko.stream.materializer.debug.fuzzing-mode = on\nWarning Never use this setting in production or benchmarks. This is a testing tool to provide more coverage of your code during tests, but it reduces the throughput of streams. A warning message will be logged if you have this setting enabled.","title":"Fuzzing Mode"},{"location":"/stream/stream-substream.html","text":"","title":"Substreams"},{"location":"/stream/stream-substream.html#substreams","text":"","title":"Substreams"},{"location":"/stream/stream-substream.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-substream.html#introduction","text":"Substreams are represented as SubSource or SubFlowSubFlow instances, on which you can multiplex a single SourceSource or FlowFlow into a stream of streams.\nSubFlows cannot contribute to the super-flow’s materialized value since they are materialized later, during the runtime of the stream processing.\noperators that create substreams are listed on Nesting and flattening operators","title":"Introduction"},{"location":"/stream/stream-substream.html#nesting-operators","text":"","title":"Nesting operators"},{"location":"/stream/stream-substream.html#groupby","text":"A typical operation that generates substreams is groupBygroupBy.\nScala copysourceval source = Source(1 to 10).groupBy(3, _ % 3) Java copysourceSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).groupBy(3, elem -> elem % 3);\nThis operation splits the incoming stream into separate output streams, one for each element key. The key is computed for each element using the given function, which is f in the above diagram. When a new key is encountered for the first time a new substream is opened and subsequently fed with all elements belonging to that key. If allowClosedSubstreamRecreation is set to true a substream belonging to a specific key will be recreated if it was closed before, otherwise elements belonging to that key will be dropped.\nIf you add a SinkSink or FlowFlow right after the groupBy operator, all transformations are applied to all encountered substreams in the same fashion. So, if you add the following Sink, that is added to each of the substreams as in the below diagram.\nScala copysourceSource(1 to 10).groupBy(3, _ % 3).to(Sink.ignore).run() Java copysourceSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n    .groupBy(3, elem -> elem % 3)\n    .to(Sink.ignore())\n    .run(system);\nAlso substreams, more precisely, SubFlowSubFlow and SubSource have methods that allow you to merge or concat substreams into the main stream again.\nThe mergeSubstreamsmergeSubstreams method merges an unbounded number of substreams back to the main stream.\nScala copysourceSource(1 to 10).groupBy(3, _ % 3).mergeSubstreams.runWith(Sink.ignore) Java copysourceSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n    .groupBy(3, elem -> elem % 3)\n    .mergeSubstreams()\n    .runWith(Sink.ignore(), system);\nYou can limit the number of active substreams running and being merged at a time, with either the mergeSubstreamsWithParallelismmergeSubstreamsWithParallelism or concatSubstreamsconcatSubstreams method.\nScala copysourceSource(1 to 10).groupBy(3, _ % 3).mergeSubstreamsWithParallelism(2).runWith(Sink.ignore)\n\n// concatSubstreams is equivalent to mergeSubstreamsWithParallelism(1)\nSource(1 to 10).groupBy(3, _ % 3).concatSubstreams.runWith(Sink.ignore) Java copysourceSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n    .groupBy(3, elem -> elem % 3)\n    .mergeSubstreamsWithParallelism(2)\n    .runWith(Sink.ignore(), system);\n// concatSubstreams is equivalent to mergeSubstreamsWithParallelism(1)\nSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n    .groupBy(3, elem -> elem % 3)\n    .concatSubstreams()\n    .runWith(Sink.ignore(), system);\nHowever, since the number of running (i.e. not yet completed) substreams is capped, be careful so that these methods do not cause deadlocks with back pressure like in the below diagram.\nElement one and two leads to two created substreams, but since the number of substreams are capped to 2 when element 3 comes in it cannot lead to creation of a new substream until one of the previous two are completed and this leads to the stream being deadlocked.","title":"groupBy"},{"location":"/stream/stream-substream.html#splitwhen-and-splitafter","text":"splitWhensplitWhen and splitAftersplitAfter are two other operations which generate substreams.\nThe difference from groupBygroupBy is that, if the predicate for splitWhen and splitAfter returns true, a new substream is generated, and the succeeding elements after split will flow into the new substream.\nsplitWhen flows the element on which the predicate returned true to a new substream, whereas splitAfter flows the next element to the new substream after the element on which predicate returned true.\nScala copysourceSource(1 to 10).splitWhen(SubstreamCancelStrategy.drain)(_ == 3)\n\nSource(1 to 10).splitAfter(SubstreamCancelStrategy.drain)(_ == 3) Java copysourceSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).splitWhen(elem -> elem == 3);\n\nSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).splitAfter(elem -> elem == 3);\nThese are useful when you scanned over something and you don’t need to care about anything behind it. A typical example is counting the number of characters for each line like below.\nScala copysourceval text =\n  \"This is the first line.\\n\" +\n  \"The second line.\\n\" +\n  \"There is also the 3rd line\\n\"\n\nval charCount = Source(text.toList)\n  .splitAfter { _ == '\\n' }\n  .filter(_ != '\\n')\n  .map(_ => 1)\n  .reduce(_ + _)\n  .to(Sink.foreach(println))\n  .run() Java copysourceString text =\n    \"This is the first line.\\n\" + \"The second line.\\n\" + \"There is also the 3rd line\\n\";\n\nSource.from(Arrays.asList(text.split(\"\")))\n    .map(x -> x.charAt(0))\n    .splitAfter(x -> x == '\\n')\n    .filter(x -> x != '\\n')\n    .map(x -> 1)\n    .reduce((x, y) -> x + y)\n    .to(Sink.foreach(x -> System.out.println(x)))\n    .run(system);\nThis prints out the following output.\n23\n16\n26","title":"splitWhen and splitAfter"},{"location":"/stream/stream-substream.html#flattening-operators","text":"","title":"Flattening operators"},{"location":"/stream/stream-substream.html#flatmapconcat","text":"flatMapConcatflatMapConcat and flatMapMergeflatMapMerge are substream operations different from groupBygroupBy and splitWhen/After.\nflatMapConcat takes a function, which is f in the following diagram. The function f of flatMapConcat transforms each input element into a SourceSource that is then flattened into the output stream by concatenation.\nScala copysourceSource(1 to 2).flatMapConcat(i => Source(List.fill(3)(i))).runWith(Sink.ignore) Java copysourceSource.from(Arrays.asList(1, 2))\n    .flatMapConcat(i -> Source.from(Arrays.asList(i, i, i)))\n    .runWith(Sink.ignore(), system);\nLike the concat operation on FlowFlow, it fully consumes one SourceSource after the other. So, there is only one substream actively running at a given time.\nThen once the active substream is fully consumed, the next substream can start running. Elements from all the substreams are concatenated to the sink.","title":"flatMapConcat"},{"location":"/stream/stream-substream.html#flatmapmerge","text":"flatMapMerge is similar to flatMapConcat, but it doesn’t wait for one Source to be fully consumed. Instead, up to breadth number of streams emit elements at any given time.\nScala copysourceSource(1 to 2).flatMapMerge(2, i => Source(List.fill(3)(i))).runWith(Sink.ignore) Java copysourceSource.from(Arrays.asList(1, 2))\n    .flatMapMerge(2, i -> Source.from(Arrays.asList(i, i, i)))\n    .runWith(Sink.ignore(), system);","title":"flatMapMerge"},{"location":"/stream/stream-cookbook.html","text":"","title":"Streams Cookbook"},{"location":"/stream/stream-cookbook.html#streams-cookbook","text":"","title":"Streams Cookbook"},{"location":"/stream/stream-cookbook.html#dependency","text":"To use Pekko Streams, add the module to your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/stream-cookbook.html#introduction","text":"This is a collection of patterns to demonstrate various usage of the Pekko Streams API by solving small targeted problems in the format of “recipes”. The purpose of this page is to give inspiration and ideas how to approach various small tasks involving streams. The recipes in this page can be used directly as-is, but they are most powerful as starting points: customization of the code snippets is warmly encouraged. The recipes can be extended or can provide a basis for the implementation of other patterns involving Pekko Connectors.\nThis part also serves as supplementary material for the main body of documentation. It is a good idea to have this page open while reading the manual and look for examples demonstrating various streaming concepts as they appear in the main body of documentation.\nIf you need a quick reference of the available operators used in the recipes see operator index.","title":"Introduction"},{"location":"/stream/stream-cookbook.html#working-with-flows","text":"In this collection we show simple recipes that involve linear flows. The recipes in this section are rather general, more targeted recipes are available as separate sections (Buffers and working with rate, Working with streaming IO).","title":"Working with Flows"},{"location":"/stream/stream-cookbook.html#logging-in-streams","text":"Situation: During development it is sometimes helpful to see what happens in a particular section of a stream.\nThe simplest solution is to use a map operation and use println to print the elements received to the console. While this recipe is rather simplistic, it is often suitable for a quick debug session.\nScala copysourceval loggedSource = mySource.map { elem =>\n  println(elem); elem\n} Java copysourcemySource.map(\n    elem -> {\n      System.out.println(elem);\n      return elem;\n    });\nAnother approach to logging is to use log() operation. This approach gives you more fine-grained control of logging levels for elements flowing through the stream, finish and failure of the stream.\nScala copysource// customise log levels\nmySource\n  .log(\"before-map\")\n  .withAttributes(Attributes\n    .logLevels(onElement = Logging.WarningLevel, onFinish = Logging.InfoLevel, onFailure = Logging.DebugLevel))\n  .map(analyse)\n// or provide custom logging adapter\nimplicit val adapter: LoggingAdapter = Logging(system, \"customLogger\")\nmySource.log(\"custom\") Java copysource// customise log levels\nmySource\n    .log(\"before-map\")\n    .withAttributes(\n        Attributes.createLogLevels(\n            Logging.WarningLevel(), // onElement\n            Logging.InfoLevel(), // onFinish\n            Logging.DebugLevel() // onFailure\n            ))\n    .map(i -> analyse(i));\n\n// or provide custom logging adapter\nfinal LoggingAdapter adapter = Logging.getLogger(system, \"customLogger\");\nmySource.log(\"custom\", adapter);","title":"Logging in streams"},{"location":"/stream/stream-cookbook.html#creating-a-source-that-continuously-evaluates-a-function","text":"Situation: A source is required that continuously provides elements obtained by evaluating a given function, so long as there is demand.\nThe simplest implementation is to use a Source.repeat that produces some arbitrary element - e.g. NotUsed - and then map those elements to the function evaluation. E.g. if we have some builderFunction(), we can use:\nScala copysourceval source = Source.repeat(NotUsed).map(_ => builderFunction()) Java copysourcefinal Source<String, NotUsed> source =\n    Source.repeat(NotUsed.getInstance()).map(elem -> builderFunction());\nNote: if the element-builder function touches mutable state, then a guaranteed single-threaded source should be used instead; e.g. Source.unfold or Source.unfoldResource.","title":"Creating a source that continuously evaluates a function"},{"location":"/stream/stream-cookbook.html#flattening-a-stream-of-sequences","text":"Situation: A stream is given as a stream of sequence of elements, but a stream of elements needed instead, streaming all the nested elements inside the sequences separately.\nThe mapConcat operation can be used to implement a one-to-many transformation of elements using a mapper function in the form of In => immutable.Seq[Out] In -> List<Out>. In this case we want to map a Seq List of elements to the elements in the collection itself, so we can call mapConcat(identity) mapConcat(l -> l).\nScala copysourceval myData: Source[List[Message], NotUsed] = someDataSource\nval flattened: Source[Message, NotUsed] = myData.mapConcat(identity) Java copysourceSource<List<Message>, NotUsed> myData = someDataSource;\nSource<Message, NotUsed> flattened = myData.mapConcat(i -> i);","title":"Flattening a stream of sequences"},{"location":"/stream/stream-cookbook.html#draining-a-stream-to-a-strict-collection","text":"Situation: A possibly unbounded sequence of elements is given as a stream, which needs to be collected into a Scala collection while ensuring boundedness\nA common situation when working with streams is one where we need to collect incoming elements into a Scala collection. This operation is supported via Sink.seq which materializes into a Future[Seq[T]] CompletionStage<List<T>>.\nThe function limit or take should always be used in conjunction in order to guarantee stream boundedness, thus preventing the program from running out of memory.\nFor example, this is best avoided:\nScala copysource// Dangerous: might produce a collection with 2 billion elements!\nval f: Future[Seq[String]] = mySource.runWith(Sink.seq) Java copysource// Dangerous: might produce a collection with 2 billion elements!\nfinal CompletionStage<List<String>> strings = mySource.runWith(Sink.seq(), system);\nRather, use limit or take to ensure that the resulting Seq List will contain only up to max MAX_ALLOWED_SIZE elements:\nScala copysourceval MAX_ALLOWED_SIZE = 100\n\n// OK. Future will fail with a `StreamLimitReachedException`\n// if the number of incoming elements is larger than max\nval limited: Future[Seq[String]] =\n  mySource.limit(MAX_ALLOWED_SIZE).runWith(Sink.seq)\n\n// OK. Collect up until max-th elements only, then cancel upstream\nval ignoreOverflow: Future[Seq[String]] =\n  mySource.take(MAX_ALLOWED_SIZE).runWith(Sink.seq) Java copysourcefinal int MAX_ALLOWED_SIZE = 100;\n\n// OK. Future will fail with a `StreamLimitReachedException`\n// if the number of incoming elements is larger than max\nfinal CompletionStage<List<String>> strings =\n    mySource.limit(MAX_ALLOWED_SIZE).runWith(Sink.seq(), system);\n\n// OK. Collect up until max-th elements only, then cancel upstream\nfinal CompletionStage<List<String>> strings =\n    mySource.take(MAX_ALLOWED_SIZE).runWith(Sink.seq(), system);","title":"Draining a stream to a strict collection"},{"location":"/stream/stream-cookbook.html#calculating-the-digest-of-a-bytestring-stream","text":"Situation: A stream of bytes is given as a stream of ByteString s and we want to calculate the cryptographic digest of the stream.\nThis recipe uses a GraphStage to define a custom Pekko Stream operator, to host a mutable MessageDigest class (part of the Java Cryptography API) and update it with the bytes arriving from the stream. When the stream starts, the onPull handler of the operator is called, which bubbles up the pull event to its upstream. As a response to this pull, a ByteString chunk will arrive (onPush) which we use to update the digest, then it will pull for the next chunk.\nEventually the stream of ByteString s depletes and we get a notification about this event via onUpstreamFinish. At this point we want to emit the digest value, but we cannot do it with push in this handler directly since there may be no downstream demand. Instead we call emit which will temporarily replace the handlers, emit the provided value when demand comes in and then reset the operator state. It will then complete the operator.\nScala copysourceimport java.security.MessageDigest\n\nimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.{ Attributes, FlowShape, Inlet, Outlet }\nimport pekko.stream.scaladsl.{ Sink, Source }\nimport pekko.util.ByteString\n\nimport pekko.stream.stage._\n\nval data: Source[ByteString, NotUsed] = Source.single(ByteString(\"abc\"))\n\nclass DigestCalculator(algorithm: String) extends GraphStage[FlowShape[ByteString, ByteString]] {\n  val in = Inlet[ByteString](\"DigestCalculator.in\")\n  val out = Outlet[ByteString](\"DigestCalculator.out\")\n  override val shape = FlowShape(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {\n    private val digest = MessageDigest.getInstance(algorithm)\n\n    setHandler(out,\n      new OutHandler {\n        override def onPull(): Unit = pull(in)\n      })\n\n    setHandler(in,\n      new InHandler {\n        override def onPush(): Unit = {\n          val chunk = grab(in)\n          digest.update(chunk.toArray)\n          pull(in)\n        }\n\n        override def onUpstreamFinish(): Unit = {\n          emit(out, ByteString(digest.digest()))\n          completeStage()\n        }\n      })\n  }\n}\n\nval digest: Source[ByteString, NotUsed] = data.via(new DigestCalculator(\"SHA-256\")) Java copysourceclass DigestCalculator extends GraphStage<FlowShape<ByteString, ByteString>> {\n  private final String algorithm;\n  public Inlet<ByteString> in = Inlet.create(\"DigestCalculator.in\");\n  public Outlet<ByteString> out = Outlet.create(\"DigestCalculator.out\");\n  private FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);\n\n  public DigestCalculator(String algorithm) {\n    this.algorithm = algorithm;\n  }\n\n  @Override\n  public FlowShape<ByteString, ByteString> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      final MessageDigest digest;\n\n      {\n        try {\n          digest = MessageDigest.getInstance(algorithm);\n        } catch (NoSuchAlgorithmException ex) {\n          throw new RuntimeException(ex);\n        }\n\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() {\n                pull(in);\n              }\n            });\n\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() {\n                ByteString chunk = grab(in);\n                digest.update(chunk.toArray());\n                pull(in);\n              }\n\n              @Override\n              public void onUpstreamFinish() {\n                // If the stream is finished, we need to emit the digest\n                // before completing\n                emit(out, ByteString.fromArray(digest.digest()));\n                completeStage();\n              }\n            });\n      }\n    };\n  }\n} copysourcefinal Source<ByteString, NotUsed> digest = data.via(new DigestCalculator(\"SHA-256\"));","title":"Calculating the digest of a ByteString stream"},{"location":"/stream/stream-cookbook.html#parsing-lines-from-a-stream-of-bytestrings","text":"Situation: A stream of bytes is given as a stream of ByteString s containing lines terminated by line ending characters (or, alternatively, containing binary frames delimited by a special delimiter byte sequence) which needs to be parsed.\nThe Framing helper object class contains a convenience method to parse messages from a stream of ByteString s:\nScala copysourceimport org.apache.pekko.stream.scaladsl.Framing\nval linesStream = rawData\n  .via(Framing.delimiter(ByteString(\"\\r\\n\"), maximumFrameLength = 100, allowTruncation = true))\n  .map(_.utf8String) Java copysourcefinal Source<String, NotUsed> lines =\n    rawData\n        .via(Framing.delimiter(ByteString.fromString(\"\\r\\n\"), 100, FramingTruncation.ALLOW))\n        .map(b -> b.utf8String());","title":"Parsing lines from a stream of ByteStrings"},{"location":"/stream/stream-cookbook.html#dealing-with-compressed-data-streams","text":"Situation: A gzipped stream of bytes is given as a stream of ByteString s, for example from a FileIO source.\nThe Compression helper object class contains convenience methods for decompressing data streams compressed with Gzip or Deflate.\nScala copysourceimport org.apache.pekko.stream.scaladsl.Compression\nval uncompressed = compressed.via(Compression.gunzip()).map(_.utf8String) Java copysourcefinal Source<ByteString, NotUsed> decompressedStream =\n    compressedStream.via(Compression.gunzip(100));","title":"Dealing with compressed data streams"},{"location":"/stream/stream-cookbook.html#implementing-a-splitter","text":"Situation: Given a stream of messages, where each message is a composition of different elements, we want to split the message into a series of individual sub-messages, each of which may be processed in a different way.\nThe Splitter is an integration pattern as described in Enterprise Integration Patterns. Let’s say that we have a stream containing strings. Each string contains a few numbers separated by “-”. We want to create out of this a stream that only contains the numbers.\nScala copysource// Sample Source\nval source: Source[String, NotUsed] = Source(List(\"1-2-3\", \"2-3\", \"3-4\"))\n\nval ret = source\n  .map(s => s.split(\"-\").toList)\n  .mapConcat(identity)\n  // Sub-streams logic\n  .map(s => s.toInt)\n  .runWith(Sink.seq)\n\n// Verify results\n\nret.futureValue should be(Vector(1, 2, 3, 2, 3, 3, 4)) Java copysource// Sample Source\nSource<String, NotUsed> source = Source.from(Arrays.asList(\"1-2-3\", \"2-3\", \"3-4\"));\n\nCompletionStage<List<Integer>> ret =\n    source\n        .map(s -> Arrays.asList(s.split(\"-\")))\n        .mapConcat(f -> f)\n        // Sub-streams logic\n        .map(s -> Integer.valueOf(s))\n        .runWith(Sink.seq(), system);\n\n// Verify results\nList<Integer> list = ret.toCompletableFuture().get();\nassert list.equals(Arrays.asList(1, 2, 3, 2, 3, 3, 4));","title":"Implementing a Splitter"},{"location":"/stream/stream-cookbook.html#implementing-a-splitter-and-aggregator","text":"Situation: Given a message, we want to split the message and aggregate its sub-messages into a new message\nSometimes, it’s very useful to split a message and aggregate its sub-messages into a new message. This involves a combination of Splitter and Aggregator\nLet’s say that now we want to create a new stream containing the sums of the numbers in each original string.\nScala copysource// Sample Source\nval source: Source[String, NotUsed] = Source(List(\"1-2-3\", \"2-3\", \"3-4\"))\n\nval result = source\n  .map(s => s.split(\"-\").toList)\n  // split all messages into sub-streams\n  .splitWhen(a => true)\n  // now split each collection\n  .mapConcat(identity)\n  // Sub-streams logic\n  .map(s => s.toInt)\n  // aggregate each sub-stream\n  .reduce((a, b) => a + b)\n  // and merge back the result into the original stream\n  .mergeSubstreams\n  .runWith(Sink.seq);\n\n// Verify results\nresult.futureValue should be(Vector(6, 5, 7)) Java copysource// Sample Source\nSource<String, NotUsed> source = Source.from(Arrays.asList(\"1-2-3\", \"2-3\", \"3-4\"));\n\nCompletionStage<List<Integer>> ret =\n    source\n        .map(s -> Arrays.asList(s.split(\"-\")))\n        // split all messages into sub-streams\n        .splitWhen(a -> true)\n        // now split each collection\n        .mapConcat(f -> f)\n        // Sub-streams logic\n        .map(s -> Integer.valueOf(s))\n        // aggregate each sub-stream\n        .reduce((a, b) -> a + b)\n        // and merge back the result into the original stream\n        .mergeSubstreams()\n        .runWith(Sink.seq(), system);\n\n// Verify results\nList<Integer> list = ret.toCompletableFuture().get();\nassert list.equals(Arrays.asList(6, 5, 7));\nWhile in real life this solution is overkill for such a simple problem (you can just do everything in a map), more complex scenarios, involving in particular I/O, will benefit from the fact that you can parallelize sub-streams and get back-pressure for “free”.","title":"Implementing a Splitter and Aggregator"},{"location":"/stream/stream-cookbook.html#implementing-reduce-by-key","text":"Situation: Given a stream of elements, we want to calculate some aggregated value on different subgroups of the elements.\nThe “hello world” of reduce-by-key style operations is wordcount which we demonstrate below. Given a stream of words we first create a new stream that groups the words according to the identity i -> i function, i.e. now we have a stream of streams, where every substream will serve identical words.\nTo count the words, we need to process the stream of streams (the actual groups containing identical words). groupBy returns a SubFlow SubSource, which means that we transform the resulting substreams directly. In this case we use the reduce operator to aggregate the word itself and the number of its occurrences within a tuple (String, Integer) Pair<String, Integer>. Each substream will then emit one final value—precisely such a pair—when the overall input completes. As a last step we merge back these values from the substreams into one single output stream.\nOne noteworthy detail pertains to the MaximumDistinctWords MAXIMUM_DISTINCT_WORDS parameter: this defines the breadth of the groupBy and merge operations. Pekko Streams is focused on bounded resource consumption and the number of concurrently open inputs to the merge operator describes the amount of resources needed by the merge itself. Therefore only a finite number of substreams can be active at any given time. If the groupBy operator encounters more keys than this number then the stream cannot continue without violating its resource bound, in this case groupBy will terminate with a failure.\nScala copysourceval counts: Source[(String, Int), NotUsed] = words\n  // split the words into separate streams first\n  .groupBy(MaximumDistinctWords, identity)\n  // transform each element to pair with number of words in it\n  .map(_ -> 1)\n  // add counting logic to the streams\n  .reduce((l, r) => (l._1, l._2 + r._2))\n  // get a stream of word counts\n  .mergeSubstreams Java copysourcefinal int MAXIMUM_DISTINCT_WORDS = 1000;\n\nfinal Source<Pair<String, Integer>, NotUsed> counts =\n    words\n        // split the words into separate streams first\n        .groupBy(MAXIMUM_DISTINCT_WORDS, i -> i)\n        // transform each element to pair with number of words in it\n        .map(i -> new Pair<>(i, 1))\n        // add counting logic to the streams\n        .reduce((left, right) -> new Pair<>(left.first(), left.second() + right.second()))\n        // get a stream of word counts\n        .mergeSubstreams();\nBy extracting the parts specific to wordcount into\na groupKey function that defines the groups a map map each element to value that is used by the reduce on the substream a reduce function that does the actual reduction\nwe get a generalized version below:\nScala copysourcedef reduceByKey[In, K, Out](maximumGroupSize: Int, groupKey: (In) => K, map: (In) => Out)(\n    reduce: (Out, Out) => Out): Flow[In, (K, Out), NotUsed] = {\n\n  Flow[In]\n    .groupBy[K](maximumGroupSize, groupKey)\n    .map(e => groupKey(e) -> map(e))\n    .reduce((l, r) => l._1 -> reduce(l._2, r._2))\n    .mergeSubstreams\n}\n\nval wordCounts = words.via(\n  reduceByKey(MaximumDistinctWords, groupKey = (word: String) => word, map = (word: String) => 1)(\n    (left: Int, right: Int) => left + right)) Java copysourcepublic static <In, K, Out> Flow<In, Pair<K, Out>, NotUsed> reduceByKey(\n    int maximumGroupSize,\n    Function<In, K> groupKey,\n    Function<In, Out> map,\n    Function2<Out, Out, Out> reduce) {\n\n  return Flow.<In>create()\n      .groupBy(maximumGroupSize, groupKey)\n      .map(i -> new Pair<>(groupKey.apply(i), map.apply(i)))\n      .reduce(\n          (left, right) -> new Pair<>(left.first(), reduce.apply(left.second(), right.second())))\n      .mergeSubstreams();\n} copysourcefinal int MAXIMUM_DISTINCT_WORDS = 1000;\n\nSource<Pair<String, Integer>, NotUsed> counts =\n    words.via(\n        reduceByKey(\n            MAXIMUM_DISTINCT_WORDS,\n            word -> word,\n            word -> 1,\n            (left, right) -> left + right));\nNote Please note that the reduce-by-key version we discussed above is sequential in reading the overall input stream, in other words it is NOT a parallelization pattern like MapReduce and similar frameworks.","title":"Implementing reduce-by-key"},{"location":"/stream/stream-cookbook.html#sorting-elements-to-multiple-groups-with-groupby","text":"Situation: The groupBy operation strictly partitions incoming elements, each element belongs to exactly one group. Sometimes, we want to map elements into multiple groups simultaneously.\nTo achieve the desired result, we attack the problem in two steps:\nfirst, using a function topicMapper that gives a list of topics (groups) a message belongs to, we transform our stream of Message to a stream of (Message, Topic) Pair<Message, Topic> where for each topic the message belongs to a separate pair will be emitted. This is achieved by using mapConcat Then we take this new stream of message topic pairs (containing a separate pair for each topic a given message belongs to) and feed it into groupBy, using the topic as the group key.\nScala copysourceval topicMapper: (Message) => immutable.Seq[Topic] = extractTopics\n\nval messageAndTopic: Source[(Message, Topic), NotUsed] = elems.mapConcat { (msg: Message) =>\n  val topicsForMessage = topicMapper(msg)\n  // Create a (Msg, Topic) pair for each of the topics\n  // the message belongs to\n  topicsForMessage.map(msg -> _)\n}\n\nval multiGroups = messageAndTopic.groupBy(2, _._2).map {\n  case (msg, topic) =>\n    // do what needs to be done\n} Java copysourcefinal Function<Message, List<Topic>> topicMapper = m -> extractTopics(m);\n\nfinal Source<Pair<Message, Topic>, NotUsed> messageAndTopic =\n    elems.mapConcat(\n        (Message msg) -> {\n          List<Topic> topicsForMessage = topicMapper.apply(msg);\n          // Create a (Msg, Topic) pair for each of the topics\n\n          // the message belongs to\n          return topicsForMessage.stream()\n              .map(topic -> new Pair<Message, Topic>(msg, topic))\n              .collect(toList());\n        });\n\nSubSource<Pair<Message, Topic>, NotUsed> multiGroups =\n    messageAndTopic\n        .groupBy(2, pair -> pair.second())\n        .map(\n            pair -> {\n              Message message = pair.first();\n              Topic topic = pair.second();\n\n              // do what needs to be done\n            });","title":"Sorting elements to multiple groups with groupBy"},{"location":"/stream/stream-cookbook.html#adhoc-source","text":"Situation: The idea is that you have a source which you don’t want to start until you have a demand. Also, you want to shut it down when there is no more demand, and start it up again there is new demand again.\nYou can achieve this behavior by combining lazySource, backpressureTimeout and recoverWithRetries as follows:\nScala copysourcedef adhocSource[T](source: Source[T, _], timeout: FiniteDuration, maxRetries: Int): Source[T, _] =\n  Source.lazySource(() =>\n    source\n      .backpressureTimeout(timeout)\n      .recoverWithRetries(maxRetries,\n        {\n          case t: TimeoutException =>\n            Source.lazySource(() => source.backpressureTimeout(timeout)).mapMaterializedValue(_ => NotUsed)\n        })) Java copysourcepublic <T> Source<T, ?> adhocSource(Source<T, ?> source, Duration timeout, int maxRetries) {\n  return Source.lazySource(\n      () ->\n          source\n              .backpressureTimeout(timeout)\n              .recoverWithRetries(\n                  maxRetries,\n                  new PFBuilder<Throwable, Source<T, NotUsed>>()\n                      .match(\n                          TimeoutException.class,\n                          ex ->\n                              Source.lazySource(() -> source.backpressureTimeout(timeout))\n                                  .mapMaterializedValue(v -> NotUsed.getInstance()))\n                      .build()));\n}","title":"Adhoc source"},{"location":"/stream/stream-cookbook.html#working-with-operators","text":"In this collection we show recipes that use stream operators to achieve various goals.","title":"Working with Operators"},{"location":"/stream/stream-cookbook.html#triggering-the-flow-of-elements-programmatically","text":"Situation: Given a stream of elements we want to control the emission of those elements according to a trigger signal. In other words, even if the stream would be able to flow (not being backpressured) we want to hold back elements until a trigger signal arrives.\nThis recipe solves the problem by zipping the stream of Message elements with the stream of Trigger signals. Since Zip produces pairs, we map the output stream selecting the first element of the pair.\nScala copysourceval graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n  import GraphDSL.Implicits._\n  val zip = builder.add(Zip[Message, Trigger]())\n  elements      ~> zip.in0\n  triggerSource ~> zip.in1\n  zip.out       ~> Flow[(Message, Trigger)].map { case (msg, trigger) => msg } ~> sink\n  ClosedShape\n}) Java copysourcefinal RunnableGraph<Pair<TestPublisher.Probe<Trigger>, TestSubscriber.Probe<Message>>> g =\n    RunnableGraph\n        .<Pair<TestPublisher.Probe<Trigger>, TestSubscriber.Probe<Message>>>fromGraph(\n            GraphDSL.create(\n                triggerSource,\n                messageSink,\n                (p, s) -> new Pair<>(p, s),\n                (builder, source, sink) -> {\n                  SourceShape<Message> elements =\n                      builder.add(\n                          Source.from(Arrays.asList(\"1\", \"2\", \"3\", \"4\"))\n                              .map(t -> new Message(t)));\n                  FlowShape<Pair<Message, Trigger>, Message> takeMessage =\n                      builder.add(\n                          Flow.<Pair<Message, Trigger>>create().map(p -> p.first()));\n                  final FanInShape2<Message, Trigger, Pair<Message, Trigger>> zip =\n                      builder.add(Zip.create());\n                  builder.from(elements).toInlet(zip.in0());\n                  builder.from(source).toInlet(zip.in1());\n                  builder.from(zip.out()).via(takeMessage).to(sink);\n                  return ClosedShape.getInstance();\n                }));\nAlternatively, instead of using a Zip, and then using map to get the first element of the pairs, we can avoid creating the pairs in the first place by using ZipWith which takes a two argument function to produce the output element. If this function would return a pair of the two argument it would be exactly the behavior of Zip so ZipWith is a generalization of zipping.\nScala copysourceval graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder =>\n  import GraphDSL.Implicits._\n  val zip = builder.add(ZipWith((msg: Message, trigger: Trigger) => msg))\n\n  elements      ~> zip.in0\n  triggerSource ~> zip.in1\n  zip.out       ~> sink\n  ClosedShape\n}) Java copysourcefinal RunnableGraph<Pair<TestPublisher.Probe<Trigger>, TestSubscriber.Probe<Message>>> g =\n    RunnableGraph\n        .<Pair<TestPublisher.Probe<Trigger>, TestSubscriber.Probe<Message>>>fromGraph(\n            GraphDSL.create(\n                triggerSource,\n                messageSink,\n                (p, s) -> new Pair<>(p, s),\n                (builder, source, sink) -> {\n                  final SourceShape<Message> elements =\n                      builder.add(\n                          Source.from(Arrays.asList(\"1\", \"2\", \"3\", \"4\"))\n                              .map(t -> new Message(t)));\n                  final FanInShape2<Message, Trigger, Message> zipWith =\n                      builder.add(ZipWith.create((msg, trigger) -> msg));\n                  builder.from(elements).toInlet(zipWith.in0());\n                  builder.from(source).toInlet(zipWith.in1());\n                  builder.from(zipWith.out()).to(sink);\n                  return ClosedShape.getInstance();\n                }));","title":"Triggering the flow of elements programmatically"},{"location":"/stream/stream-cookbook.html#balancing-jobs-to-a-fixed-pool-of-workers","text":"Situation: Given a stream of jobs and a worker process expressed as a Flow create a pool of workers that automatically balances incoming jobs to available workers, then merges the results.\nWe will express our solution as a function that takes a worker flow and the number of workers to be allocated and gives a flow that internally contains a pool of these workers. To achieve the desired result we will create a Flow from an operator.\nThe operator consists of a Balance node which is a special fan-out operation that tries to route elements to available downstream consumers. In a for loop we wire all of our desired workers as outputs of this balancer element, then we wire the outputs of these workers to a Merge element that will collect the results from the workers.\nTo make the worker operators run in parallel we mark them as asynchronous with async.\nScala copysourcedef balancer[In, Out](worker: Flow[In, Out, Any], workerCount: Int): Flow[In, Out, NotUsed] = {\n  import GraphDSL.Implicits._\n\n  Flow.fromGraph(GraphDSL.create() { implicit b =>\n    val balancer = b.add(Balance[In](workerCount, waitForAllDownstreams = true))\n    val merge = b.add(Merge[Out](workerCount))\n\n    for (_ <- 1 to workerCount) {\n      // for each worker, add an edge from the balancer to the worker, then wire\n      // it to the merge element\n      balancer ~> worker.async ~> merge\n    }\n\n    FlowShape(balancer.in, merge.out)\n  })\n}\n\nval processedJobs: Source[Result, NotUsed] = myJobs.via(balancer(worker, 3)) Java copysourcepublic static <In, Out> Flow<In, Out, NotUsed> balancer(\n    Flow<In, Out, NotUsed> worker, int workerCount) {\n  return Flow.fromGraph(\n      GraphDSL.create(\n          b -> {\n            boolean waitForAllDownstreams = true;\n            final UniformFanOutShape<In, In> balance =\n                b.add(Balance.<In>create(workerCount, waitForAllDownstreams));\n            final UniformFanInShape<Out, Out> merge = b.add(Merge.<Out>create(workerCount));\n\n            for (int i = 0; i < workerCount; i++) {\n              b.from(balance.out(i)).via(b.add(worker.async())).toInlet(merge.in(i));\n            }\n\n            return FlowShape.of(balance.in(), merge.out());\n          }));\n} copysourceFlow<Message, Message, NotUsed> balancer = balancer(worker, 3);\nSource<Message, NotUsed> processedJobs = data.via(balancer);","title":"Balancing jobs to a fixed pool of workers"},{"location":"/stream/stream-cookbook.html#working-with-rate","text":"This collection of recipes demonstrate various patterns where rate differences between upstream and downstream needs to be handled by other strategies than simple backpressure.","title":"Working with rate"},{"location":"/stream/stream-cookbook.html#dropping-elements","text":"Situation: Given a fast producer and a slow consumer, we want to drop elements if necessary to not slow down the producer too much.\nThis can be solved by using a versatile rate-transforming operation, conflate. Conflate can be thought as a special reduce operation that collapses multiple upstream elements into one aggregate element if needed to keep the speed of the upstream unaffected by the downstream.\nWhen the upstream is faster, the reducing process of the conflate starts. Our reducer function takes the freshest element. This in a simple dropping operation.\nScala copysourceval droppyStream: Flow[Message, Message, NotUsed] =\n  Flow[Message].conflate((lastMessage, newMessage) => newMessage) Java copysourcefinal Flow<Message, Message, NotUsed> droppyStream =\n    Flow.of(Message.class).conflate((lastMessage, newMessage) -> newMessage);\nThere is a more general version of conflate named conflateWithSeed that allows to express more complex aggregations, more similar to a fold.","title":"Dropping elements"},{"location":"/stream/stream-cookbook.html#dropping-broadcast","text":"Situation: The default Broadcast operator is properly backpressured, but that means that a slow downstream consumer can hold back the other downstream consumers resulting in lowered throughput. In other words the rate of Broadcast is the rate of its slowest downstream consumer. In certain cases it is desirable to allow faster consumers to progress independently of their slower siblings by dropping elements if necessary.\nOne solution to this problem is to append a buffer element in front of all of the downstream consumers defining a dropping strategy instead of the default Backpressure. This allows small temporary rate differences between the different consumers (the buffer smooths out small rate variances), but also allows faster consumers to progress by dropping from the buffer of the slow consumers if necessary.\nScala copysourceval graph = RunnableGraph.fromGraph(GraphDSL.createGraph(mySink1, mySink2, mySink3)((_, _, _)) {\n  implicit b => (sink1, sink2, sink3) =>\n    import GraphDSL.Implicits._\n\n    val bcast = b.add(Broadcast[Int](3))\n    myElements ~> bcast\n\n    bcast.buffer(10, OverflowStrategy.dropHead) ~> sink1\n    bcast.buffer(10, OverflowStrategy.dropHead) ~> sink2\n    bcast.buffer(10, OverflowStrategy.dropHead) ~> sink3\n    ClosedShape\n}) Java copysource// Makes a sink drop elements if too slow\npublic <T> Sink<T, CompletionStage<Done>> droppySink(\n    Sink<T, CompletionStage<Done>> sink, int size) {\n  return Flow.<T>create().buffer(size, OverflowStrategy.dropHead()).toMat(sink, Keep.right());\n} copysourceRunnableGraph.fromGraph(\n    GraphDSL.create(\n        builder -> {\n          final int outputCount = 3;\n          final UniformFanOutShape<Integer, Integer> bcast =\n              builder.add(Broadcast.create(outputCount));\n          builder.from(builder.add(myData)).toFanOut(bcast);\n          builder.from(bcast).to(builder.add(droppySink(mySink1, 10)));\n          builder.from(bcast).to(builder.add(droppySink(mySink2, 10)));\n          builder.from(bcast).to(builder.add(droppySink(mySink3, 10)));\n          return ClosedShape.getInstance();\n        }));","title":"Dropping broadcast"},{"location":"/stream/stream-cookbook.html#collecting-missed-ticks","text":"Situation: Given a regular (stream) source of ticks, instead of trying to backpressure the producer of the ticks we want to keep a counter of the missed ticks instead and pass it down when possible.\nWe will use conflateWithSeed to solve the problem. The seed version of conflate takes two functions:\nA seed function that produces the zero element for the folding process that happens when the upstream is faster than the downstream. In our case the seed function is a constant function that returns 0 since there were no missed ticks at that point. A fold function that is invoked when multiple upstream messages needs to be collapsed to an aggregate value due to the insufficient processing rate of the downstream. Our folding function increments the currently stored count of the missed ticks so far.\nAs a result, we have a flow of Int where the number represents the missed ticks. A number 0 means that we were able to consume the tick fast enough (i.e. zero means: 1 non-missed tick + 0 missed ticks)\nScala copysourceval missedTicks: Flow[Tick, Int, NotUsed] =\n  Flow[Tick].conflateWithSeed(seed = _ => 0)((missedTicks, tick) => missedTicks + 1) Java copysourcefinal Flow<Tick, Integer, NotUsed> missedTicks =\n    Flow.of(Tick.class).conflateWithSeed(tick -> 0, (missed, tick) -> missed + 1);","title":"Collecting missed ticks"},{"location":"/stream/stream-cookbook.html#create-a-stream-processor-that-repeats-the-last-element-seen","text":"Situation: Given a producer and consumer, where the rate of neither is known in advance, we want to ensure that none of them is slowing down the other by dropping earlier unconsumed elements from the upstream if necessary, and repeating the last value for the downstream if necessary.\nWe have two options to implement this feature. In both cases we will use GraphStage, to build our custom operator. In the first version we will use a provided initial value initial that will be used to feed the downstream if no upstream element is ready yet. In the onPush() handler we overwrite the currentValue variable and immediately relieve the upstream by calling pull(). The downstream onPull handler is very similar, we immediately relieve the downstream by emitting currentValue.\nScala copysourceimport org.apache.pekko\nimport pekko.stream._\nimport pekko.stream.stage._\nfinal class HoldWithInitial[T](initial: T) extends GraphStage[FlowShape[T, T]] {\n  val in = Inlet[T](\"HoldWithInitial.in\")\n  val out = Outlet[T](\"HoldWithInitial.out\")\n\n  override val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {\n    private var currentValue: T = initial\n\n    setHandlers(in, out,\n      new InHandler with OutHandler {\n        override def onPush(): Unit = {\n          currentValue = grab(in)\n          pull(in)\n        }\n\n        override def onPull(): Unit = {\n          push(out, currentValue)\n        }\n      })\n\n    override def preStart(): Unit = {\n      pull(in)\n    }\n  }\n\n} Java copysourceclass HoldWithInitial<T> extends GraphStage<FlowShape<T, T>> {\n\n  public Inlet<T> in = Inlet.<T>create(\"HoldWithInitial.in\");\n  public Outlet<T> out = Outlet.<T>create(\"HoldWithInitial.out\");\n  private FlowShape<T, T> shape = FlowShape.of(in, out);\n\n  private final T initial;\n\n  public HoldWithInitial(T initial) {\n    this.initial = initial;\n  }\n\n  @Override\n  public FlowShape<T, T> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      private T currentValue = initial;\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() throws Exception {\n                currentValue = grab(in);\n                pull(in);\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                push(out, currentValue);\n              }\n            });\n      }\n\n      @Override\n      public void preStart() {\n        pull(in);\n      }\n    };\n  }\n}\nWhile it is relatively simple, the drawback of the first version is that it needs an arbitrary initial element which is not always possible to provide. Hence, we create a second version where the downstream might need to wait in one single case: if the very first element is not yet available.\nWe introduce a boolean variable waitingFirstValue to denote whether the first element has been provided or not (alternatively an Option Optional can be used for currentValue or if the element type is a subclass of AnyRef Object a null can be used with the same purpose). In the downstream onPull() handler the difference from the previous version is that we check if we have received the first value and only emit if we have. This leads to that when the first element comes in we must check if there possibly already was demand from downstream so that we in that case can push the element directly.\nScala copysourceimport org.apache.pekko\nimport pekko.stream._\nimport pekko.stream.stage._\nfinal class HoldWithWait[T] extends GraphStage[FlowShape[T, T]] {\n  val in = Inlet[T](\"HoldWithWait.in\")\n  val out = Outlet[T](\"HoldWithWait.out\")\n\n  override val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {\n    private var currentValue: T = _\n    private var waitingFirstValue = true\n\n    setHandlers(\n      in,\n      out,\n      new InHandler with OutHandler {\n        override def onPush(): Unit = {\n          currentValue = grab(in)\n          if (waitingFirstValue) {\n            waitingFirstValue = false\n            if (isAvailable(out)) push(out, currentValue)\n          }\n          pull(in)\n        }\n\n        override def onPull(): Unit = {\n          if (!waitingFirstValue) push(out, currentValue)\n        }\n      })\n\n    override def preStart(): Unit = {\n      pull(in)\n    }\n  }\n} Java copysourceclass HoldWithWait<T> extends GraphStage<FlowShape<T, T>> {\n  public Inlet<T> in = Inlet.<T>create(\"HoldWithInitial.in\");\n  public Outlet<T> out = Outlet.<T>create(\"HoldWithInitial.out\");\n  private FlowShape<T, T> shape = FlowShape.of(in, out);\n\n  @Override\n  public FlowShape<T, T> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      private T currentValue = null;\n      private boolean waitingFirstValue = true;\n\n      {\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() throws Exception {\n                currentValue = grab(in);\n                if (waitingFirstValue) {\n                  waitingFirstValue = false;\n                  if (isAvailable(out)) push(out, currentValue);\n                }\n                pull(in);\n              }\n            });\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                if (!waitingFirstValue) push(out, currentValue);\n              }\n            });\n      }\n\n      @Override\n      public void preStart() {\n        pull(in);\n      }\n    };\n  }\n}","title":"Create a stream processor that repeats the last element seen"},{"location":"/stream/stream-cookbook.html#globally-limiting-the-rate-of-a-set-of-streams","text":"Situation: Given a set of independent streams that we cannot merge, we want to globally limit the aggregate throughput of the set of streams.\nOne possible solution uses a shared actor as the global limiter combined with mapAsync to create a reusable Flow that can be plugged into a stream to limit its rate.\nAs the first step we define an actor that will do the accounting for the global rate limit. The actor maintains a timer, a counter for pending permit tokens and a queue for possibly waiting participants. The actor has an open and closed state. The actor is in the open state while it has still pending permits. Whenever a request for permit arrives as a WantToPass message to the actor the number of available permits is decremented and we notify the sender that it can pass by answering with a MayPass message. If the amount of permits reaches zero, the actor transitions to the closed state. In this state requests are not immediately answered, instead the reference of the sender is added to a queue. Once the timer for replenishing the pending permits fires by sending a ReplenishTokens message, we increment the pending permits counter and send a reply to each of the waiting senders. If there are more waiting senders than permits available we will stay in the closed state.\nScala copysourceobject Limiter {\n  case object WantToPass\n  case object MayPass\n\n  case object ReplenishTokens\n\n  def props(maxAvailableTokens: Int, tokenRefreshPeriod: FiniteDuration, tokenRefreshAmount: Int): Props =\n    Props(new Limiter(maxAvailableTokens, tokenRefreshPeriod, tokenRefreshAmount))\n}\n\nclass Limiter(val maxAvailableTokens: Int, val tokenRefreshPeriod: FiniteDuration, val tokenRefreshAmount: Int)\n    extends Actor {\n  import Limiter._\n  import context.dispatcher\n  import org.apache.pekko.actor.Status\n\n  private var waitQueue = immutable.Queue.empty[ActorRef]\n  private var permitTokens = maxAvailableTokens\n  private val replenishTimer = system.scheduler.scheduleWithFixedDelay(\n    initialDelay = tokenRefreshPeriod,\n    delay = tokenRefreshPeriod,\n    receiver = self,\n    ReplenishTokens)\n\n  override def receive: Receive = open\n\n  val open: Receive = {\n    case ReplenishTokens =>\n      permitTokens = math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens)\n    case WantToPass =>\n      permitTokens -= 1\n      sender() ! MayPass\n      if (permitTokens == 0) context.become(closed)\n  }\n\n  val closed: Receive = {\n    case ReplenishTokens =>\n      permitTokens = math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens)\n      releaseWaiting()\n    case WantToPass =>\n      waitQueue = waitQueue.enqueue(sender())\n  }\n\n  private def releaseWaiting(): Unit = {\n    val (toBeReleased, remainingQueue) = waitQueue.splitAt(permitTokens)\n    waitQueue = remainingQueue\n    permitTokens -= toBeReleased.size\n    toBeReleased.foreach(_ ! MayPass)\n    if (permitTokens > 0) context.become(open)\n  }\n\n  override def postStop(): Unit = {\n    replenishTimer.cancel()\n    waitQueue.foreach(_ ! Status.Failure(new IllegalStateException(\"limiter stopped\")))\n  }\n} Java copysourcestatic class Limiter extends AbstractActor {\n\n  public static class WantToPass {}\n\n  public static final WantToPass WANT_TO_PASS = new WantToPass();\n\n  public static class MayPass {}\n\n  public static final MayPass MAY_PASS = new MayPass();\n\n  public static class ReplenishTokens {}\n\n  public static final ReplenishTokens REPLENISH_TOKENS = new ReplenishTokens();\n\n  private final int maxAvailableTokens;\n  private final Duration tokenRefreshPeriod;\n  private final int tokenRefreshAmount;\n\n  private final List<ActorRef> waitQueue = new ArrayList<>();\n  private final Cancellable replenishTimer;\n\n  private int permitTokens;\n\n  public static Props props(\n      int maxAvailableTokens, Duration tokenRefreshPeriod, int tokenRefreshAmount) {\n    return Props.create(\n        Limiter.class, maxAvailableTokens, tokenRefreshPeriod, tokenRefreshAmount);\n  }\n\n  private Limiter(int maxAvailableTokens, Duration tokenRefreshPeriod, int tokenRefreshAmount) {\n    this.maxAvailableTokens = maxAvailableTokens;\n    this.tokenRefreshPeriod = tokenRefreshPeriod;\n    this.tokenRefreshAmount = tokenRefreshAmount;\n    this.permitTokens = maxAvailableTokens;\n\n    this.replenishTimer =\n        system\n            .scheduler()\n            .scheduleWithFixedDelay(\n                this.tokenRefreshPeriod,\n                this.tokenRefreshPeriod,\n                getSelf(),\n                REPLENISH_TOKENS,\n                getContext().getSystem().dispatcher(),\n                getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return open();\n  }\n\n  private Receive open() {\n    return receiveBuilder()\n        .match(\n            ReplenishTokens.class,\n            rt -> {\n              permitTokens = Math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens);\n            })\n        .match(\n            WantToPass.class,\n            wtp -> {\n              permitTokens -= 1;\n              getSender().tell(MAY_PASS, getSelf());\n              if (permitTokens == 0) {\n                getContext().become(closed());\n              }\n            })\n        .build();\n  }\n\n  private Receive closed() {\n    return receiveBuilder()\n        .match(\n            ReplenishTokens.class,\n            rt -> {\n              permitTokens = Math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens);\n              releaseWaiting();\n            })\n        .match(\n            WantToPass.class,\n            wtp -> {\n              waitQueue.add(getSender());\n            })\n        .build();\n  }\n\n  private void releaseWaiting() {\n    final List<ActorRef> toBeReleased = new ArrayList<>(permitTokens);\n    for (Iterator<ActorRef> it = waitQueue.iterator(); permitTokens > 0 && it.hasNext(); ) {\n      toBeReleased.add(it.next());\n      it.remove();\n      permitTokens--;\n    }\n\n    toBeReleased.stream().forEach(ref -> ref.tell(MAY_PASS, getSelf()));\n    if (permitTokens > 0) {\n      getContext().become(open());\n    }\n  }\n\n  @Override\n  public void postStop() {\n    replenishTimer.cancel();\n    waitQueue.stream()\n        .forEach(\n            ref -> {\n              ref.tell(\n                  new Status.Failure(new IllegalStateException(\"limiter stopped\")), getSelf());\n            });\n  }\n}\nTo create a Flow that uses this global limiter actor we use the mapAsync function with the combination of the ask pattern. We also define a timeout, so if a reply is not received during the configured maximum wait period the returned future from ask will fail, which will fail the corresponding stream as well.\nScala copysourcedef limitGlobal[T](limiter: ActorRef, maxAllowedWait: FiniteDuration): Flow[T, T, NotUsed] = {\n  import org.apache.pekko\n  import pekko.pattern.ask\n  import pekko.util.Timeout\n  Flow[T].mapAsync(4)((element: T) => {\n    import system.dispatcher\n    implicit val triggerTimeout = Timeout(maxAllowedWait)\n    val limiterTriggerFuture = limiter ? Limiter.WantToPass\n    limiterTriggerFuture.map(_ => element)\n  })\n\n} Java copysourcepublic <T> Flow<T, T, NotUsed> limitGlobal(ActorRef limiter, Duration maxAllowedWait) {\n  final int parallelism = 4;\n  final Flow<T, T, NotUsed> f = Flow.create();\n\n  return f.mapAsync(\n      parallelism,\n      element -> {\n        final CompletionStage<Object> limiterTriggerFuture =\n            Patterns.ask(limiter, Limiter.WANT_TO_PASS, maxAllowedWait);\n        return limiterTriggerFuture.thenApplyAsync(response -> element, system.dispatcher());\n      });\n}\nNote The global actor used for limiting introduces a global bottleneck. You might want to assign a dedicated dispatcher for this actor.","title":"Globally limiting the rate of a set of streams"},{"location":"/stream/stream-cookbook.html#working-with-io","text":"","title":"Working with IO"},{"location":"/stream/stream-cookbook.html#chunking-up-a-stream-of-bytestrings-into-limited-size-bytestrings","text":"Situation: Given a stream of ByteString s we want to produce a stream of ByteString s containing the same bytes in the same sequence, but capping the size of ByteString s. In other words we want to slice up ByteString s into smaller chunks if they exceed a size threshold.\nThis can be achieved with a single GraphStage to define a custom Pekko Stream operator. The main logic of our operator is in emitChunk() which implements the following logic:\nif the buffer is empty, and upstream is not closed we pull for more bytes, if it is closed we complete if the buffer is nonEmpty, we split it according to the chunkSize. This will give a next chunk that we will emit, and an empty or nonempty remaining buffer.\nBoth onPush() and onPull() calls emitChunk() the only difference is that the push handler also stores the incoming chunk by appending to the end of the buffer.\nScala copysourceimport org.apache.pekko.stream.stage._\n\nclass Chunker(val chunkSize: Int) extends GraphStage[FlowShape[ByteString, ByteString]] {\n  val in = Inlet[ByteString](\"Chunker.in\")\n  val out = Outlet[ByteString](\"Chunker.out\")\n  override val shape = FlowShape.of(in, out)\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {\n    private var buffer = ByteString.empty\n\n    setHandler(out,\n      new OutHandler {\n        override def onPull(): Unit = {\n          emitChunk()\n        }\n      })\n    setHandler(\n      in,\n      new InHandler {\n        override def onPush(): Unit = {\n          val elem = grab(in)\n          buffer ++= elem\n          emitChunk()\n        }\n\n        override def onUpstreamFinish(): Unit = {\n          if (buffer.isEmpty) completeStage()\n          else {\n            // There are elements left in buffer, so\n            // we keep accepting downstream pulls and push from buffer until emptied.\n            //\n            // It might be though, that the upstream finished while it was pulled, in which\n            // case we will not get an onPull from the downstream, because we already had one.\n            // In that case we need to emit from the buffer.\n            if (isAvailable(out)) emitChunk()\n          }\n        }\n      })\n\n    private def emitChunk(): Unit = {\n      if (buffer.isEmpty) {\n        if (isClosed(in)) completeStage()\n        else pull(in)\n      } else {\n        val (chunk, nextBuffer) = buffer.splitAt(chunkSize)\n        buffer = nextBuffer\n        push(out, chunk)\n      }\n    }\n  }\n}\n\nval chunksStream = rawBytes.via(new Chunker(ChunkLimit)) Java copysourceclass Chunker extends GraphStage<FlowShape<ByteString, ByteString>> {\n\n  private final int chunkSize;\n\n  public Inlet<ByteString> in = Inlet.<ByteString>create(\"Chunker.in\");\n  public Outlet<ByteString> out = Outlet.<ByteString>create(\"Chunker.out\");\n  private FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);\n\n  public Chunker(int chunkSize) {\n    this.chunkSize = chunkSize;\n  }\n\n  @Override\n  public FlowShape<ByteString, ByteString> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      private ByteString buffer = emptyByteString();\n\n      {\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                emitChunk();\n              }\n            });\n\n        setHandler(\n            in,\n            new AbstractInHandler() {\n\n              @Override\n              public void onPush() throws Exception {\n                ByteString elem = grab(in);\n                buffer = buffer.concat(elem);\n                emitChunk();\n              }\n\n              @Override\n              public void onUpstreamFinish() throws Exception {\n                if (buffer.isEmpty()) completeStage();\n                else {\n                  // There are elements left in buffer, so\n                  // we keep accepting downstream pulls and push from buffer until emptied.\n                  //\n                  // It might be though, that the upstream finished while it was pulled, in\n                  // which\n                  // case we will not get an onPull from the downstream, because we already\n                  // had one.\n                  // In that case we need to emit from the buffer.\n                  if (isAvailable(out)) emitChunk();\n                }\n              }\n            });\n      }\n\n      private void emitChunk() {\n        if (buffer.isEmpty()) {\n          if (isClosed(in)) completeStage();\n          else pull(in);\n        } else {\n          Tuple2<ByteString, ByteString> split = buffer.splitAt(chunkSize);\n          ByteString chunk = split._1();\n          buffer = split._2();\n          push(out, chunk);\n        }\n      }\n    };\n  }\n} copysourceSource<ByteString, NotUsed> chunksStream = rawBytes.via(new Chunker(CHUNK_LIMIT));","title":"Chunking up a stream of ByteStrings into limited size ByteStrings"},{"location":"/stream/stream-cookbook.html#limit-the-number-of-bytes-passing-through-a-stream-of-bytestrings","text":"Situation: Given a stream of ByteString s we want to fail the stream if more than a given maximum of bytes has been consumed.\nThis recipe uses a GraphStage to implement the desired feature. In the only handler we override, onPush() we update a counter and see if it gets larger than maximumBytes. If a violation happens we signal failure, otherwise we forward the chunk we have received.\nScala copysourceimport org.apache.pekko.stream.stage._\nclass ByteLimiter(val maximumBytes: Long) extends GraphStage[FlowShape[ByteString, ByteString]] {\n  val in = Inlet[ByteString](\"ByteLimiter.in\")\n  val out = Outlet[ByteString](\"ByteLimiter.out\")\n  override val shape = FlowShape.of(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {\n    private var count = 0\n\n    setHandlers(in, out,\n      new InHandler with OutHandler {\n\n        override def onPull(): Unit = {\n          pull(in)\n        }\n\n        override def onPush(): Unit = {\n          val chunk = grab(in)\n          count += chunk.size\n          if (count > maximumBytes) failStage(new IllegalStateException(\"Too much bytes\"))\n          else push(out, chunk)\n        }\n      })\n  }\n}\n\nval limiter = Flow[ByteString].via(new ByteLimiter(SizeLimit)) Java copysourceclass ByteLimiter extends GraphStage<FlowShape<ByteString, ByteString>> {\n\n  final long maximumBytes;\n\n  public Inlet<ByteString> in = Inlet.<ByteString>create(\"ByteLimiter.in\");\n  public Outlet<ByteString> out = Outlet.<ByteString>create(\"ByteLimiter.out\");\n  private FlowShape<ByteString, ByteString> shape = FlowShape.of(in, out);\n\n  public ByteLimiter(long maximumBytes) {\n    this.maximumBytes = maximumBytes;\n  }\n\n  @Override\n  public FlowShape<ByteString, ByteString> shape() {\n    return shape;\n  }\n\n  @Override\n  public GraphStageLogic createLogic(Attributes inheritedAttributes) {\n    return new GraphStageLogic(shape) {\n      private int count = 0;\n\n      {\n        setHandler(\n            out,\n            new AbstractOutHandler() {\n              @Override\n              public void onPull() throws Exception {\n                pull(in);\n              }\n            });\n        setHandler(\n            in,\n            new AbstractInHandler() {\n              @Override\n              public void onPush() throws Exception {\n                ByteString chunk = grab(in);\n                count += chunk.size();\n                if (count > maximumBytes) {\n                  failStage(new IllegalStateException(\"Too much bytes\"));\n                } else {\n                  push(out, chunk);\n                }\n              }\n            });\n      }\n    };\n  }\n} copysourceFlow<ByteString, ByteString, NotUsed> limiter =\n    Flow.of(ByteString.class).via(new ByteLimiter(SIZE_LIMIT));","title":"Limit the number of bytes passing through a stream of ByteStrings"},{"location":"/stream/stream-cookbook.html#compact-bytestrings-in-a-stream-of-bytestrings","text":"Situation: After a long stream of transformations, due to their immutable, structural sharing nature ByteString s may refer to multiple original ByteString instances unnecessarily retaining memory. As the final step of a transformation chain we want to have clean copies that are no longer referencing the original ByteString s.\nThe recipe is a simple use of map, calling the compact() method of the ByteString elements. This does copying of the underlying arrays, so this should be the last element of a long chain if used.\nScala copysourceval compacted: Source[ByteString, NotUsed] = data.map(_.compact) Java copysourceSource<ByteString, NotUsed> compacted = rawBytes.map(ByteString::compact);","title":"Compact ByteStrings in a stream of ByteStrings"},{"location":"/stream/stream-cookbook.html#injecting-keep-alive-messages-into-a-stream-of-bytestrings","text":"Situation: Given a communication channel expressed as a stream of ByteString s we want to inject keep-alive messages but only if this does not interfere with normal traffic.\nThere is a built-in operation that allows to do this directly:\nScala copysourceimport scala.concurrent.duration._\nval injectKeepAlive: Flow[ByteString, ByteString, NotUsed] =\n  Flow[ByteString].keepAlive(1.second, () => keepaliveMessage) Java copysourceFlow<ByteString, ByteString, NotUsed> keepAliveInject =\n    Flow.of(ByteString.class).keepAlive(Duration.ofSeconds(1), () -> keepAliveMessage);","title":"Injecting keep-alive messages into a stream of ByteStrings"},{"location":"/general/stream/stream-configuration.html","text":"","title":"Configuration"},{"location":"/general/stream/stream-configuration.html#configuration","text":"copysource#####################################\n# Pekko Stream Reference Config File #\n#####################################\n\n# eager creation of the system wide materializer\npekko.library-extensions += \"org.apache.pekko.stream.SystemMaterializer$\"\npekko {\n  stream {\n\n    # Default materializer settings\n    materializer {\n\n      # Initial size of buffers used in stream elements\n      initial-input-buffer-size = 4\n      # Maximum size of buffers used in stream elements\n      max-input-buffer-size = 16\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # or full dispatcher configuration to be used by ActorMaterializer when creating Actors.\n      dispatcher = \"pekko.actor.default-dispatcher\"\n\n      # Fully qualified config path which holds the dispatcher configuration\n      # or full dispatcher configuration to be used by stream operators that\n      # perform blocking operations\n      blocking-io-dispatcher = \"pekko.actor.default-blocking-io-dispatcher\"\n\n      # Cleanup leaked publishers and subscribers when they are not used within a given\n      # deadline\n      subscription-timeout {\n        # when the subscription timeout is reached one of the following strategies on\n        # the \"stale\" publisher:\n        # cancel - cancel it (via `onError` or subscribing to the publisher and\n        #          `cancel()`ing the subscription right away\n        # warn   - log a warning statement about the stale element (then drop the\n        #          reference to it)\n        # noop   - do nothing (not recommended)\n        mode = cancel\n\n        # time after which a subscriber / publisher is considered stale and eligible\n        # for cancelation (see `pekko.stream.subscription-timeout.mode`)\n        timeout = 5s\n      }\n\n      # Enable additional troubleshooting logging at DEBUG log level\n      debug-logging = off\n\n      # Maximum number of elements emitted in batch if downstream signals large demand\n      output-burst-limit = 1000\n\n      # Enable automatic fusing of all graphs that are run. For short-lived streams\n      # this may cause an initial runtime overhead, but most of the time fusing is\n      # desirable since it reduces the number of Actors that are created.\n      # Deprecated, since Pekko 2.5.0, setting does not have any effect.\n      auto-fusing = on\n\n      # Those stream elements which have explicit buffers (like mapAsync, mapAsyncUnordered,\n      # buffer, flatMapMerge, Source.actorRef, Source.queue, etc.) will preallocate a fixed\n      # buffer upon stream materialization if the requested buffer size is less than this\n      # configuration parameter. The default is very high because failing early is better\n      # than failing under load.\n      #\n      # Buffers sized larger than this will dynamically grow/shrink and consume more memory\n      # per element than the fixed size buffers.\n      max-fixed-buffer-size = 1000000000\n\n      # Maximum number of sync messages that actor can process for stream to substream communication.\n      # Parameter allows to interrupt synchronous processing to get upstream/downstream messages.\n      # Allows to accelerate message processing that happening within same actor but keep system responsive.\n      sync-processing-limit = 1000\n\n      debug {\n        # Enables the fuzzing mode which increases the chance of race conditions\n        # by aggressively reordering events and making certain operations more\n        # concurrent than usual.\n        # This setting is for testing purposes, NEVER enable this in a production\n        # environment!\n        # To get the best results, try combining this setting with a throughput\n        # of 1 on the corresponding dispatchers.\n        fuzzing-mode = off\n      }\n\n      io.tcp {\n        # The outgoing bytes are accumulated in a buffer while waiting for acknowledgment\n        # of pending write. This improves throughput for small messages (frames) without\n        # sacrificing latency. While waiting for the ack the stage will eagerly pull\n        # from upstream until the buffer exceeds this size. That means that the buffer may hold\n        # slightly more bytes than this limit (at most one element more). It can be set to 0\n        # to disable the usage of the buffer.\n        write-buffer-size = 16 KiB\n\n        # In addition to the buffering described for property write-buffer-size, try to collect\n        # more consecutive writes from the upstream stream producers.\n        #\n        # The rationale is to increase write efficiency by avoiding separate small \n        # writes to the network which is expensive to do. Merging those writes together\n        # (up to `write-buffer-size`) improves throughput for small writes.\n        #\n        # The idea is that a running stream may produce multiple small writes consecutively\n        # in one go without waiting for any external input. To probe the stream for\n        # data, this features delays sending a write immediately by probing the stream\n        # for more writes. This works by rescheduling the TCP connection stage via the\n        # actor mailbox of the underlying actor. Thus, before the stage is reactivated\n        # the upstream gets another opportunity to emit writes.\n        #\n        # When the stage is reactivated and if new writes are detected another round-trip\n        # is scheduled. The loop repeats until either the number of round trips given in this\n        # setting is reached, the buffer reaches `write-buffer-size`, or no new writes\n        # were detected during the last round-trip.\n        #\n        # This mechanism ensures that a write is guaranteed to be sent when the remaining stream\n        # becomes idle waiting for external signals.\n        #\n        # In most cases, the extra latency this mechanism introduces should be negligible,\n        # but depending on the stream setup it may introduce a noticeable delay,\n        # if the upstream continuously produces small amounts of writes in a\n        # blocking (CPU-bound) way.\n        #\n        # In that case, the feature can either be disabled, or the producing CPU-bound\n        # work can be taken off-stream to avoid excessive delays (e.g. using `mapAsync` instead of `map`).\n        #\n        # A value of 0 disables this feature.\n        coalesce-writes = 10\n      }\n\n      # Time to wait for async materializer creation before throwing an exception\n      creation-timeout = 20 seconds\n\n      //#stream-ref\n      # configure defaults for SourceRef and SinkRef\n      stream-ref {\n        # Buffer of a SinkRef that is used to batch Request elements from the other side of the stream ref\n        #\n        # The buffer will be attempted to be filled eagerly even while the local stage did not request elements,\n        # because the delay of requesting over network boundaries is much higher.\n        buffer-capacity = 32\n\n        # Demand is signalled by sending a cumulative demand message (\"requesting messages until the n-th sequence number)\n        # Using a cumulative demand model allows us to re-deliver the demand message in case of message loss (which should\n        # be very rare in any case, yet possible -- mostly under connection break-down and re-establishment).\n        #\n        # The semantics of handling and updating the demand however are in-line with what Reactive Streams dictates.\n        #\n        # In normal operation, demand is signalled in response to arriving elements, however if no new elements arrive\n        # within `demand-redelivery-interval` a re-delivery of the demand will be triggered, assuming that it may have gotten lost.\n        demand-redelivery-interval = 1 second\n\n        # Subscription timeout, during which the \"remote side\" MUST subscribe (materialize) the handed out stream ref.\n        # This timeout does not have to be very low in normal situations, since the remote side may also need to\n        # prepare things before it is ready to materialize the reference. However the timeout is needed to avoid leaking\n        # in-active streams which are never subscribed to.\n        subscription-timeout = 30 seconds\n\n        # In order to guard the receiving end of a stream ref from never terminating (since awaiting a Completion or Failed\n        # message) after / before a Terminated is seen, a special timeout is applied once Terminated is received by it.\n        # This allows us to terminate stream refs that have been targeted to other nodes which are Downed, and as such the\n        # other side of the stream ref would never send the \"final\" terminal message.\n        #\n        # The timeout specifically means the time between the Terminated signal being received and when the local SourceRef\n        # determines to fail itself, assuming there was message loss or a complete partition of the completion signal.\n        final-termination-signal-deadline = 2 seconds\n      }\n      //#stream-ref\n    }\n\n    # Deprecated, left here to not break Pekko HTTP which refers to it\n    blocking-io-dispatcher = \"pekko.actor.default-blocking-io-dispatcher\"\n\n    # Deprecated, will not be used unless user code refer to it, use 'pekko.stream.materializer.blocking-io-dispatcher'\n    # instead, or if from code, prefer the 'ActorAttributes.IODispatcher' attribute\n    default-blocking-io-dispatcher = \"pekko.actor.default-blocking-io-dispatcher\"\n  }\n\n  # configure overrides to ssl-configuration here (to be used by pekko-streams, and pekko-http – i.e. when serving https connections)\n  ssl-config {\n    protocol = \"TLSv1.2\"\n  }\n\n  actor {\n\n    serializers {\n      pekko-stream-ref = \"org.apache.pekko.stream.serialization.StreamRefSerializer\"\n    }\n\n    serialization-bindings {\n      \"org.apache.pekko.stream.SinkRef\"                           = pekko-stream-ref\n      \"org.apache.pekko.stream.SourceRef\"                         = pekko-stream-ref\n      \"org.apache.pekko.stream.impl.streamref.StreamRefsProtocol\" = pekko-stream-ref\n    }\n\n    serialization-identifiers {\n      \"org.apache.pekko.stream.serialization.StreamRefSerializer\" = 30\n    }\n  }\n}\n\n# ssl configuration\n# folded in from former ssl-config-pekko module\nssl-config {\n  logger = \"com.typesafe.sslconfig.pekko.util.PekkoLoggerBridge\"\n}","title":"Configuration"},{"location":"/stream/operators/index.html","text":"","title":"Operators"},{"location":"/stream/operators/index.html#operators","text":"","title":"Operators"},{"location":"/stream/operators/index.html#source-operators","text":"These built-in sources are available from org.apache.pekko.stream.scaladsl.Source org.apache.pekko.stream.javadsl.Source:\nOperator Description Source asSourceWithContext Extracts context data from the elements of a Source so that it can be turned into a SourceWithContext which can propagate that context per element along a stream. Source asSubscriber Integration with Reactive Streams, materializes into a Subscriber. Source combine Combine several sources, using a given strategy such as merge or concat, into one source. Source completionStage Send the single value of the CompletionStage when it completes and there is demand. Source completionStageSource Streams the elements of an asynchronous source once its given completion operator completes. Source cycle Stream iterator in cycled manner. Source empty Complete right away without ever emitting any elements. Source failed Fail directly with a user specified exception. Source applyfrom Stream the values of an immutable.SeqIterable. Source fromCompletionStage Deprecated by Source.completionStage. Source fromFuture Deprecated by Source.future. Source fromFutureSource Deprecated by Source.futureSource. Source fromIterator Stream the values from an Iterator, requesting the next value when there is demand. Source fromJavaStream Stream the values from a Java 8 Stream, requesting the next value when there is demand. Source fromPublisher Integration with Reactive Streams, subscribes to a Publisher. Source fromSourceCompletionStage Deprecated by Source.completionStageSource. Source future Send the single value of the Future when it completes and there is demand. Source futureSource Streams the elements of the given future source once it successfully completes. Source lazily Deprecated by Source.lazySource. Source lazilyAsync Deprecated by Source.lazyFutureSource. Source lazyCompletionStage Defers creation of a future of a single element source until there is demand. Source lazyCompletionStageSource Defers creation of a future source until there is demand. Source lazyFuture Defers creation of a future of a single element source until there is demand. Source lazyFutureSource Defers creation and materialization of a Source until there is demand. Source lazySingle Defers creation of a single element source until there is demand. Source lazySource Defers creation and materialization of a Source until there is demand. Source maybe Create a source that emits once the materialized Promise CompletableFuture is completed with a value. Source never Never emit any elements, never complete and never fail. Source queue Materialize a BoundedSourceQueue or SourceQueue onto which elements can be pushed for emitting from the source. Source range Emit each integer in a range, with an option to take bigger steps than 1. Source repeat Stream a single object repeatedly. Source single Stream a single object once. Source tick A periodical repetition of an arbitrary object. Source unfold Stream the result of a function as long as it returns a Some non empty Optional. Source unfoldAsync Just like unfold but the fold function returns a Future CompletionStage. Source unfoldResource Wrap any resource that can be opened, queried for next element (in a blocking way) and closed using three distinct functions into a source. Source unfoldResourceAsync Wrap any resource that can be opened, queried for next element and closed in an asynchronous way. Source zipN Combine the elements of multiple sources into a source of sequences of value. Source zipWithN Combine the elements of multiple streams into a stream of sequences using a combiner function.","title":"Source operators"},{"location":"/stream/operators/index.html#sink-operators","text":"These built-in sinks are available from org.apache.pekko.stream.scaladsl.Sink org.apache.pekko.stream.javadsl.Sink:\nOperator Description Sink asPublisher Integration with Reactive Streams, materializes into a org.reactivestreams.Publisher. Sink cancelled Immediately cancel the stream Sink collect Collect all input elements using a Java Collector. Sink collection Collect all values emitted from the stream into a collection.Operator only available in the Scala API. The closest operator in the Java API is Sink.seq. Sink combine Combine several sinks into one using a user specified strategy Sink completionStageSink Streams the elements to the given future sink once it successfully completes. Sink fold Fold over emitted element with a function, where each invocation will get the new element and the result from the previous fold invocation. Sink foreach Invoke a given procedure for each element received. Sink foreachAsync Invoke a given procedure asynchronously for each element received. Sink foreachParallel Like foreach but allows up to parallellism procedure calls to happen in parallel. Sink fromMaterializer Defer the creation of a Sink until materialization and access Materializer and Attributes Sink fromSubscriber Integration with Reactive Streams, wraps a org.reactivestreams.Subscriber as a sink. Sink futureSink Streams the elements to the given future sink once it successfully completes. Sink head Materializes into a Future CompletionStage which completes with the first value arriving, after this the stream is canceled. Sink headOption Materializes into a Future[Option[T]] CompletionStage<Optional<T>> which completes with the first value arriving wrapped in Some Optional, or a None an empty Optional if the stream completes without any elements emitted. Sink ignore Consume all elements but discards them. Sink last Materializes into a Future CompletionStage which will complete with the last value emitted when the stream completes. Sink lastOption Materialize a Future[Option[T]] CompletionStage<Optional<T>> which completes with the last value emitted wrapped in an Some Optional when the stream completes. Sink lazyCompletionStageSink Defers creation and materialization of a Sink until there is a first element. Sink lazyFutureSink Defers creation and materialization of a Sink until there is a first element. Sink lazyInitAsync Deprecated by Sink.lazyFutureSink. Sink lazySink Defers creation and materialization of a Sink until there is a first element. Sink never Always backpressure never cancel and never consume any elements from the stream. Sink onComplete Invoke a callback when the stream has completed or failed. Sink preMaterialize Materializes this Sink, immediately returning (1) its materialized value, and (2) a new Sink that can be consume elements ‘into’ the pre-materialized one. Sink queue Materialize a SinkQueue that can be pulled to trigger demand through the sink. Sink reduce Apply a reduction function on the incoming elements and pass the result to the next invocation. Sink seq Collect values emitted from the stream into a collection. Sink setup Defer the creation of a Sink until materialization and access ActorMaterializer and Attributes Sink takeLast Collect the last n values emitted from the stream into a collection.","title":"Sink operators"},{"location":"/stream/operators/index.html#additional-sink-and-source-converters","text":"Sources and sinks for integrating with java.io.InputStream and java.io.OutputStream can be found on StreamConverters. As they are blocking APIs the implementations of these operators are run on a separate dispatcher configured through the pekko.stream.blocking-io-dispatcher.\nWarning Be aware that asInputStream and asOutputStream materialize InputStream and OutputStream respectively as blocking API implementation. They will block the thread until data will be available from upstream. Because of blocking nature these objects cannot be used in mapMaterializeValue section as it causes deadlock of the stream materialization process. For example, following snippet will fall with timeout exception: ...\n.toMat(StreamConverters.asInputStream().mapMaterializedValue { inputStream =>\n        inputStream.read()  // this could block forever\n        ...\n}).run()\nOperator Description StreamConverters asInputStream Create a sink which materializes into an InputStream that can be read to trigger demand through the sink. StreamConverters asJavaStream Create a sink which materializes into Java 8 Stream that can be run to trigger demand through the sink. StreamConverters asOutputStream Create a source that materializes into an OutputStream. StreamConverters fromInputStream Create a source that wraps an InputStream. StreamConverters fromJavaStream Create a source that wraps a Java 8 java.util.stream.Stream. StreamConverters fromOutputStream Create a sink that wraps an OutputStream. StreamConverters javaCollector Create a sink which materializes into a Future CompletionStage which will be completed with a result of the Java 8 Collector transformation and reduction operations. StreamConverters javaCollectorParallelUnordered Create a sink which materializes into a Future CompletionStage which will be completed with a result of the Java 8 Collector transformation and reduction operations.","title":"Additional Sink and Source converters"},{"location":"/stream/operators/index.html#file-io-sinks-and-sources","text":"Sources and sinks for reading and writing files can be found on FileIO.\nOperator Description FileIO fromFile Emits the contents of a file. FileIO fromPath Emits the contents of a file from the given path. FileIO toFile Create a sink which will write incoming ByteString s to a given file. FileIO toPath Create a sink which will write incoming ByteString s to a given file path.","title":"File IO Sinks and Sources"},{"location":"/stream/operators/index.html#simple-operators","text":"These operators can transform the rate of incoming elements since there are operators that emit multiple elements for a single input (e.g. mapConcat) or consume multiple elements before emitting one output (e.g. filter). However, these rate transformations are data-driven, i.e. it is the incoming elements that define how the rate is affected. This is in contrast with detached operators which can change their processing behavior depending on being backpressured by downstream or not.\nOperator Description Flow asFlowWithContext Extracts context data from the elements of a Flow so that it can be turned into a FlowWithContext which can propagate that context per element along a stream. Source/Flow collect Apply a partial function to each incoming element, if the partial function is defined for a value the returned value is passed downstream. Source/Flow collectType Transform this stream by testing the type of each of the elements on which the element is an instance of the provided type as they pass through this processing step. Flow completionStageFlow Streams the elements through the given future flow once it successfully completes. Source/Flow detach Detach upstream demand from downstream demand without detaching the stream rates. Source/Flow drop Drop n elements and then pass any subsequent element downstream. Source/Flow dropWhile Drop elements as long as a predicate function return true for the element Source/Flow filter Filter the incoming elements using a predicate. Source/Flow filterNot Filter the incoming elements using a predicate. Flow flattenOptional Collect the value of Optional from all the elements passing through this flow , empty Optional is filtered out. Source/Flow fold Start with current value zero and then apply the current and next value to the given function. When upstream completes, the current value is emitted downstream. Source/Flow foldAsync Just like fold but receives a function that results in a Future CompletionStage to the next value. Source/Flow fromMaterializer Defer the creation of a Source/Flow until materialization and access Materializer and Attributes Flow futureFlow Streams the elements through the given future flow once it successfully completes. Source/Flow grouped Accumulate incoming events until the specified number of elements have been accumulated and then pass the collection of elements downstream. Source/Flow groupedWeighted Accumulate incoming events until the combined weight of elements is greater than or equal to the minimum weight and then pass the collection of elements downstream. Source/Flow intersperse Intersperse stream with provided element similar to List.mkString. Flow lazyCompletionStageFlow Defers creation and materialization of a Flow until there is a first element. Flow lazyFlow Defers creation and materialization of a Flow until there is a first element. Flow lazyFutureFlow Defers creation and materialization of a Flow until there is a first element. Flow lazyInitAsync Deprecated by Flow.lazyFutureFlow in combination with prefixAndTail. Source/Flow limit Limit number of element from upstream to given max number. Source/Flow limitWeighted Limit the total weight of incoming elements Source/Flow log Log elements flowing through the stream as well as completion and erroring. Source/Flow logWithMarker Log elements flowing through the stream as well as completion and erroring. Source/Flow map Transform each element in the stream by calling a mapping function with it and passing the returned value downstream. Source/Flow mapConcat Transform each element into zero or more elements that are individually passed downstream. Source/Flow preMaterialize Materializes this Graph, immediately returning (1) its materialized value, and (2) a new pre-materialized Graph. Source/Flow reduce Start with first element and then apply the current and next value to the given function, when upstream complete the current value is emitted downstream. Source/Flow scan Emit its current value, which starts at zero, and then apply the current and next value to the given function, emitting the next current value. Source/Flow scanAsync Just like scan but receives a function that results in a Future CompletionStage to the next value. Source/Flow setup Defer the creation of a Source/Flow until materialization and access Materializer and Attributes Source/Flow sliding Provide a sliding window over the incoming stream and pass the windows as groups of elements downstream. Source/Flow statefulMap Transform each stream element with the help of a state. Source/Flow statefulMapConcat Transform each element into zero or more elements that are individually passed downstream. Source/Flow take Pass n incoming elements downstream and then complete Source/Flow takeWhile Pass elements downstream as long as a predicate function returns true and then complete. Source/Flow throttle Limit the throughput to a specific number of elements per time unit, or a specific total cost per time unit, where a function has to be provided to calculate the individual cost of each element.","title":"Simple operators"},{"location":"/stream/operators/index.html#flow-operators-composed-of-sinks-and-sources","text":"Operator Description Flow fromSinkAndSource Creates a Flow from a Sink and a Source where the Flow’s input will be sent to the Sink and the Flow ’s output will come from the Source. Flow fromSinkAndSourceCoupled Allows coupling termination (cancellation, completion, erroring) of Sinks and Sources while creating a Flow between them.","title":"Flow operators composed of Sinks and Sources"},{"location":"/stream/operators/index.html#asynchronous-operators","text":"These operators encapsulate an asynchronous computation, properly handling backpressure while taking care of the asynchronous operation at the same time (usually handling the completion of a Future CompletionStage).\nOperator Description Source/Flow mapAsync Pass incoming elements to a function that return a Future CompletionStage result. Source/Flow mapAsyncUnordered Like mapAsync but Future CompletionStage results are passed downstream as they arrive regardless of the order of the elements that triggered them.","title":"Asynchronous operators"},{"location":"/stream/operators/index.html#timer-driven-operators","text":"These operators process elements using timers, delaying, dropping or grouping elements for certain time durations.\nOperator Description Source/Flow delay Delay every element passed through with a specific duration. Source/Flow delayWith Delay every element passed through with a duration that can be controlled dynamically. Source/Flow dropWithin Drop elements until a timeout has fired Source/Flow groupedWeightedWithin Chunk up this stream into groups of elements received within a time window, or limited by the weight of the elements, whatever happens first. Source/Flow groupedWithin Chunk up this stream into groups of elements received within a time window, or limited by the number of the elements, whatever happens first. Source/Flow initialDelay Delays the initial element by the specified duration. Source/Flow takeWithin Pass elements downstream within a timeout and then complete.","title":"Timer driven operators"},{"location":"/stream/operators/index.html#backpressure-aware-operators","text":"These operators are aware of the backpressure provided by their downstreams and able to adapt their behavior to that signal.\nOperator Description Source/Flow aggregateWithBoundary Aggregate and emit until custom boundary condition met. Source/Flow batch Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum number of batched elements is not yet reached. Source/Flow batchWeighted Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum weight batched elements is not yet reached. Source/Flow buffer Allow for a temporarily faster upstream events by buffering size elements. Source/Flow conflate Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure. Source/Flow conflateWithSeed Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure. Source/Flow expand Like extrapolate, but does not have the initial argument, and the Iterator is also used in lieu of the original element, allowing for it to be rewritten and/or filtered. Source/Flow extrapolate Allow for a faster downstream by expanding the last emitted element to an Iterator.","title":"Backpressure aware operators"},{"location":"/stream/operators/index.html#nesting-and-flattening-operators","text":"These operators either take a stream and turn it into a stream of streams (nesting) or they take a stream that contains nested streams and turn them into a stream of elements instead (flattening).\nSee the Substreams page for more detail and code samples.\nOperator Description Source/Flow flatMapConcat Transform each input element into a Source whose elements are then flattened into the output stream through concatenation. Source/Flow flatMapMerge Transform each input element into a Source whose elements are then flattened into the output stream through merging. Source/Flow flatMapPrefix Use the first n elements from the stream to determine how to process the rest. Source/Flow groupBy Demultiplex the incoming stream into separate output streams. Source/Flow prefixAndTail Take up to n elements from the stream (less than n only if the upstream completes before emitting n elements) and returns a pair containing a strict sequence of the taken element and a stream representing the remaining elements. Source/Flow splitAfter End the current substream whenever a predicate returns true, starting a new substream for the next element. Source/Flow splitWhen Split off elements into a new substream whenever a predicate function return true.","title":"Nesting and flattening operators"},{"location":"/stream/operators/index.html#time-aware-operators","text":"Those operators operate taking time into consideration.\nOperator Description Source/Flow backpressureTimeout If the time between the emission of an element and the following downstream demand exceeds the provided timeout, the stream is failed with a TimeoutException. Source/Flow completionTimeout If the completion of the stream does not happen until the provided timeout, the stream is failed with a TimeoutException. Source/Flow idleTimeout If the time between two processed elements exceeds the provided timeout, the stream is failed with a TimeoutException. Source/Flow initialTimeout If the first element has not passed through this operators before the provided timeout, the stream is failed with a TimeoutException. Source/Flow keepAlive Injects additional (configured) elements if upstream does not emit for a configured amount of time.","title":"Time aware operators"},{"location":"/stream/operators/index.html#fan-in-operators","text":"These operators take multiple streams as their input and provide a single output combining the elements from all of the inputs in different ways.\nOperator Description MergeSequence Merge a linear sequence partitioned across multiple sources. Source/Flow concat After completion of the original upstream the elements of the given source will be emitted. Source/Flow concatAllLazy After completion of the original upstream the elements of the given sources will be emitted sequentially. Source/Flow concatLazy After completion of the original upstream the elements of the given source will be emitted. Source/Flow interleave Emits a specifiable number of elements from the original source, then from the provided source and repeats. Source/Flow interleaveAll Emits a specifiable number of elements from the original source, then from the provided sources and repeats. Source/Flow merge Merge multiple sources. Source/Flow mergeAll Merge multiple sources. Source/Flow mergeLatest Merge multiple sources. Source/Flow mergePreferred Merge multiple sources. Source/Flow mergePrioritized Merge multiple sources. Source mergePrioritizedN Merge multiple sources with priorities. Source/Flow mergeSorted Merge multiple sources. Source/Flow orElse If the primary source completes without emitting any elements, the elements from the secondary source are emitted. Source/Flow prepend Prepends the given source to the flow, consuming it until completion before the original source is consumed. Source/Flow prependLazy Prepends the given source to the flow, consuming it until completion before the original source is consumed. Source/Flow zip Combines elements from each of multiple sources into tuples Pair and passes the tuples pairs downstream. Source/Flow zipAll Combines elements from two sources into tuples Pair handling early completion of either source. Source/Flow zipLatest Combines elements from each of multiple sources into tuples Pair and passes the tuples pairs downstream, picking always the latest element of each. Source/Flow zipLatestWith Combines elements from multiple sources through a combine function and passes the returned value downstream, picking always the latest element of each. Source/Flow zipWith Combines elements from multiple sources through a combine function and passes the returned value downstream. Source/Flow zipWithIndex Zips elements of current flow with its indices.","title":"Fan-in operators"},{"location":"/stream/operators/index.html#fan-out-operators","text":"These have one input and multiple outputs. They might route the elements between different outputs, or emit elements on multiple outputs at the same time.\nThere is a number of fan-out operators for which currently no ‘fluent’ is API available. To use those you will have to use the Graph DSL.\nOperator Description Balance Fan-out the stream to several streams. Broadcast Emit each incoming element each of n outputs. Partition Fan-out the stream to several streams. Unzip Takes a stream of two element tuples and unzips the two elements ino two different downstreams. UnzipWith Splits each element of input into multiple downstreams using a function Source/Flow alsoTo Attaches the given Sink to this Flow, meaning that elements that pass through this Flow will also be sent to the Sink. Source/Flow alsoToAll Attaches the given SourceSources to this FlowFlow, meaning that elements that pass through this FlowFlow will also be sent to all those SinkSinks. Source/Flow divertTo Each upstream element will either be diverted to the given sink, or the downstream consumer according to the predicate function applied to the element. Source/Flow wireTap Attaches the given Sink to this Flow as a wire tap, meaning that elements that pass through will also be sent to the wire-tap Sink, without the latter affecting the mainline flow.","title":"Fan-out operators"},{"location":"/stream/operators/index.html#watching-status-operators","text":"Operator Description Source/Flow monitor Materializes to a FlowMonitor that monitors messages flowing through or completion of the operators. Source/Flow watchTermination Materializes to a Future CompletionStage that will be completed with Done or failed depending whether the upstream of the operators has been completed or failed.","title":"Watching status operators"},{"location":"/stream/operators/index.html#actor-interop-operators","text":"Operators meant for inter-operating between Pekko Streams and Actors:\nOperator Description Source actorRef Materialize an ActorRef of the classic actors API; sending messages to it will emit them on the stream. Sink actorRef Send the elements from the stream to an ActorRef of the classic actors API. ActorSource actorRef Materialize an ActorRef<T>ActorRef[T] of the new actors API; sending messages to it will emit them on the stream only if they are of the same type as the stream. ActorSink actorRef Sends the elements of the stream to the given ActorRef<T>ActorRef[T] of the new actors API, without considering backpressure. Source actorRefWithBackpressure Materialize an ActorRef of the classic actors API; sending messages to it will emit them on the stream. The source acknowledges reception after emitting a message, to provide back pressure from the source. Sink actorRefWithBackpressure Send the elements from the stream to an ActorRef (of the classic actors API) which must then acknowledge reception after completing a message, to provide back pressure onto the sink. ActorSource actorRefWithBackpressure Materialize an ActorRef<T>ActorRef[T] of the new actors API; sending messages to it will emit them on the stream. The source acknowledges reception after emitting a message, to provide back pressure from the source. ActorSink actorRefWithBackpressure Sends the elements of the stream to the given ActorRef<T>ActorRef[T] of the new actors API with backpressure, to be able to signal demand when the actor is ready to receive more elements. Source/Flow ask Use the “Ask Pattern” to send a request-reply message to the target ref actor (of the classic actors API). ActorFlow ask Use the “Ask Pattern” to send each stream element as an ask to the target actor (of the new actors API), and expect a reply that will be emitted downstream. ActorFlow askWithContext Use the “Ask Pattern” to send each stream element (without the context) as an ask to the target actor (of the new actors API), and expect a reply that will be emitted downstream. ActorFlow askWithStatus Use the “Ask Pattern” to send each stream element as an ask to the target actor (of the new actors API), and expect a reply of Type StatusReply[T]StatusReply<T> where the T will be unwrapped and emitted downstream. ActorFlow askWithStatusAndContext Use the “Ask Pattern” to send each stream element (without the context) as an ask to the target actor (of the new actors API), and expect a reply of Type StatusReply[T]StatusReply<T> where the T will be unwrapped and emitted downstream. PubSub sink A sink that will publish emitted messages to a TopicTopic. PubSub source A source that will subscribe to a TopicTopic and stream messages published to the topic. Source/Flow watch Watch a specific ActorRef and signal a failure downstream once the actor terminates.","title":"Actor interop operators"},{"location":"/stream/operators/index.html#compression-operators","text":"Flow operators to (de)compress.\nOperator Description Compression deflate Creates a flow that deflate-compresses a stream of ByteStrings. Compression gunzip Creates a flow that gzip-decompresses a stream of ByteStrings. Compression gzip Creates a flow that gzip-compresses a stream of ByteStrings. Compression inflate Creates a flow that deflate-decompresses a stream of ByteStrings.","title":"Compression operators"},{"location":"/stream/operators/index.html#error-handling","text":"For more background see the Error Handling in Streams section.\nOperator Description Source/Flow mapError While similar to recover this operators can be used to transform an error signal to a different one without logging it as an error in the process. RestartSource onFailuresWithBackoff Wrap the given SourceSource with a SourceSource that will restart it when it fails using an exponential backoff. Notice that this SourceSource will not restart on completion of the wrapped flow. RestartFlow onFailuresWithBackoff Wrap the given FlowFlow with a FlowFlow that will restart it when it fails using an exponential backoff. Notice that this FlowFlow will not restart on completion of the wrapped flow. Source/Flow recover Allow sending of one last element downstream when a failure has happened upstream. Source/Flow recoverWith Allow switching to alternative Source when a failure has happened upstream. Source/Flow recoverWithRetries RecoverWithRetries allows to switch to alternative Source on flow failure. RestartSource withBackoff Wrap the given SourceSource with a SourceSource that will restart it when it fails or completes using an exponential backoff. RestartFlow withBackoff Wrap the given FlowFlow with a FlowFlow that will restart it when it fails or complete using an exponential backoff. RestartSink withBackoff Wrap the given SinkSink with a SinkSink that will restart it when it fails or complete using an exponential backoff. RetryFlow withBackoff Wrap the given FlowFlow and retry individual elements in that stream with an exponential backoff. A decider function tests every emitted element and can return a new element to be sent to the wrapped flow for another try. RetryFlow withBackoffAndContext Wrap the given FlowWithContextFlowWithContext and retry individual elements in that stream with an exponential backoff. A decider function tests every emitted element and can return a new element to be sent to the wrapped flow for another try.","title":"Error handling"},{"location":"/stream/operators/Source/actorRef.html","text":"","title":"Source.actorRef"},{"location":"/stream/operators/Source/actorRef.html#source-actorref","text":"Materialize an ActorRef of the classic actors API; sending messages to it will emit them on the stream.\nActor interop operators","title":"Source.actorRef"},{"location":"/stream/operators/Source/actorRef.html#signature","text":"Source.actorRefSource.actorRef","title":"Signature"},{"location":"/stream/operators/Source/actorRef.html#description","text":"Materialize an ActorRef, sending messages to it will emit them on the stream. The actor contains a buffer but since communication is one way, there is no back pressure. Handling overflow is done by either dropping elements or failing the stream; the strategy is chosen by the user.\nThe stream can be completed successfully by sending the actor reference a org.apache.pekko.actor.Status.Success. If the content is org.apache.pekko.stream.CompletionStrategy.immediately the completion will be signaled immediately. Otherwise, if the content is org.apache.pekko.stream.CompletionStrategy.draining (or anything else) already buffered elements will be sent out before signaling completion. Sending org.apache.pekko.actor.PoisonPill will signal completion immediately but this behavior is deprecated and scheduled to be removed. Using org.apache.pekko.actor.ActorSystem.stop to stop the actor and complete the stream is not supported.\nSee also:\nSource.actorRefWithBackpressure This operator, but with backpressure control ActorSource.actorRef The corresponding operator for the new actors API ActorSource.actorRefWithBackpressure The operator for the new actors API with backpressure control Source.queue Materialize a SourceQueue onto which elements can be pushed for emitting from the source","title":"Description"},{"location":"/stream/operators/Source/actorRef.html#examples","text":"Scala copysourceimport org.apache.pekko\nimport pekko.Done\nimport pekko.actor.ActorRef\nimport pekko.stream.OverflowStrategy\nimport pekko.stream.CompletionStrategy\nimport pekko.stream.scaladsl._\n\nval source: Source[Any, ActorRef] = Source.actorRef(\n  completionMatcher = {\n    case Done =>\n      // complete stream immediately if we send it Done\n      CompletionStrategy.immediately\n  },\n  // never fail the stream because of a message\n  failureMatcher = PartialFunction.empty,\n  bufferSize = 100,\n  overflowStrategy = OverflowStrategy.dropHead)\nval actorRef: ActorRef = source.to(Sink.foreach(println)).run()\n\nactorRef ! \"hello\"\nactorRef ! \"hello\"\n\n// The stream completes successfully with the following message\nactorRef ! Done Java copysourceimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.stream.OverflowStrategy;\nimport org.apache.pekko.stream.CompletionStrategy;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.testkit.TestProbe;\n\nint bufferSize = 100;\nSource<Object, ActorRef> source =\n    Source.actorRef(\n        elem -> {\n          // complete stream immediately if we send it Done\n          if (elem == Done.done()) return Optional.of(CompletionStrategy.immediately());\n          else return Optional.empty();\n        },\n        // never fail the stream because of a message\n        elem -> Optional.empty(),\n        bufferSize,\n        OverflowStrategy.dropHead());\n\nActorRef actorRef = source.to(Sink.foreach(System.out::println)).run(system);\nactorRef.tell(\"hello\", ActorRef.noSender());\nactorRef.tell(\"hello\", ActorRef.noSender());\n\n// The stream completes successfully with the following message\nactorRef.tell(Done.done(), ActorRef.noSender());","title":"Examples"},{"location":"/stream/operators/Source/actorRef.html#reactive-streams-semantics","text":"emits when there is demand and there are messages in the buffer or a message is sent to the ActorRef completes when the actor is stopped by sending it a particular message as described above","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/actorRef.html","text":"","title":"Sink.actorRef"},{"location":"/stream/operators/Sink/actorRef.html#sink-actorref","text":"Send the elements from the stream to an ActorRef of the classic actors API.\nActor interop operators","title":"Sink.actorRef"},{"location":"/stream/operators/Sink/actorRef.html#signature","text":"Sink.actorRefSink.actorRef","title":"Signature"},{"location":"/stream/operators/Sink/actorRef.html#description","text":"Send the elements from the stream to an ActorRef. No backpressure so care must be taken to not overflow the inbox.\nSee also:\nSink.actorRefWithBackpressue Send elements to an actor with backpressure support ActorSink.actorRef The corresponding operator for the new actors API ActorSink.actorRefWithBackpressure Send elements to an actor of the new actors API supporting backpressure","title":"Description"},{"location":"/stream/operators/Sink/actorRef.html#reactive-streams-semantics","text":"cancels when the actor terminates backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/ActorSource/actorRef.html","text":"","title":"ActorSource.actorRef"},{"location":"/stream/operators/ActorSource/actorRef.html#actorsource-actorref","text":"Materialize an ActorRef<T>ActorRef[T] of the new actors API; sending messages to it will emit them on the stream only if they are of the same type as the stream.\nActor interop operators","title":"ActorSource.actorRef"},{"location":"/stream/operators/ActorSource/actorRef.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/ActorSource/actorRef.html#signature","text":"ActorSource.actorRefActorSource.actorRef","title":"Signature"},{"location":"/stream/operators/ActorSource/actorRef.html#description","text":"Materialize an ActorRef<T>ActorRef[T] which only accepts messages that are of the same type as the stream.\nSee also:\nActorSource.actorRefWithBackpressure This operator, but with backpressure control Source.actorRef The corresponding operator for the classic actors API Source.actorRefWithBackpressure The operator for the classic actors API with backpressure control Source.queue Materialize a SourceQueue onto which elements can be pushed for emitting from the source","title":"Description"},{"location":"/stream/operators/ActorSource/actorRef.html#examples","text":"Scala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.stream.OverflowStrategy\nimport pekko.stream.scaladsl.{ Sink, Source }\nimport pekko.stream.typed.scaladsl.ActorSource\n\ntrait Protocol\ncase class Message(msg: String) extends Protocol\ncase object Complete extends Protocol\ncase class Fail(ex: Exception) extends Protocol\n\nval source: Source[Protocol, ActorRef[Protocol]] = ActorSource.actorRef[Protocol](completionMatcher = {\n    case Complete =>\n  },\n  failureMatcher = {\n    case Fail(ex) => ex\n  }, bufferSize = 8, overflowStrategy = OverflowStrategy.fail)\n\nval ref = source\n  .collect {\n    case Message(msg) => msg\n  }\n  .to(Sink.foreach(println))\n  .run()\n\nref ! Message(\"msg1\")\n// ref ! \"msg2\" Does not compile Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.japi.JavaPartialFunction;\nimport org.apache.pekko.stream.OverflowStrategy;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.typed.javadsl.ActorSource;\n\nimport java.util.Optional;\n\ninterface Protocol {}\n\nclass Message implements Protocol {\n  private final String msg;\n\n  public Message(String msg) {\n    this.msg = msg;\n  }\n}\n\nclass Complete implements Protocol {}\n\nclass Fail implements Protocol {\n  private final Exception ex;\n\n  public Fail(Exception ex) {\n    this.ex = ex;\n  }\n}\n\n  final Source<Protocol, ActorRef<Protocol>> source =\n      ActorSource.actorRef(\n          (m) -> m instanceof Complete,\n          (m) -> (m instanceof Fail) ? Optional.of(((Fail) m).ex) : Optional.empty(),\n          8,\n          OverflowStrategy.fail());\n\n  final ActorRef<Protocol> ref =\n      source\n          .collect(\n              new JavaPartialFunction<Protocol, String>() {\n                public String apply(Protocol p, boolean isCheck) {\n                  if (p instanceof Message) {\n                    return ((Message) p).msg;\n                  } else {\n                    throw noMatch();\n                  }\n                }\n              })\n          .to(Sink.foreach(System.out::println))\n          .run(system);\n\n  ref.tell(new Message(\"msg1\"));\n  // ref.tell(\"msg2\"); Does not compile","title":"Examples"},{"location":"/stream/operators/ActorSink/actorRef.html","text":"","title":"ActorSink.actorRef"},{"location":"/stream/operators/ActorSink/actorRef.html#actorsink-actorref","text":"Sends the elements of the stream to the given ActorRef<T>ActorRef[T] of the new actors API, without considering backpressure.\nActor interop operators","title":"ActorSink.actorRef"},{"location":"/stream/operators/ActorSink/actorRef.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/ActorSink/actorRef.html#signature","text":"ActorSink.actorRefActorSink.actorRef","title":"Signature"},{"location":"/stream/operators/ActorSink/actorRef.html#description","text":"Sends the elements of the stream to the given ActorRef. If the target actor terminates the stream will be canceled. When the stream completes successfully the given onCompleteMessage will be sent to the destination actor. When the stream completes with failure the throwable that was signaled to the stream is adapted to the Actor’s protocol using onFailureMessage and then sent to the destination actor.\nIt will request at most maxInputBufferSize number of elements from upstream, but there is no back-pressure signal from the destination actor, i.e. if the actor is not consuming the messages fast enough the mailbox of the actor will grow. For potentially slow consumer actors it is recommended to use a bounded mailbox with zero mailbox-push-timeout-time or use a rate limiting operator in front of this Sink.\nSee also:\nActorSink.actorRefWithBackpressure Send elements to an actor of the new actors API supporting backpressure Sink.actorRef The corresponding operator for the classic actors API Sink.actorRefWithBackpressue Send elements to an actor of the classic actors API supporting backpressure","title":"Description"},{"location":"/stream/operators/ActorSink/actorRef.html#reactive-streams-semantics","text":"cancels when the actor terminates backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/actorRefWithBackpressure.html","text":"","title":"Source.actorRefWithBackpressure"},{"location":"/stream/operators/Source/actorRefWithBackpressure.html#source-actorrefwithbackpressure","text":"Materialize an ActorRef of the classic actors API; sending messages to it will emit them on the stream. The source acknowledges reception after emitting a message, to provide back pressure from the source.\nActor interop operators","title":"Source.actorRefWithBackpressure"},{"location":"/stream/operators/Source/actorRefWithBackpressure.html#signature","text":"Source.actorRefWithBackpressureSource.actorRefWithBackpressure","title":"Signature"},{"location":"/stream/operators/Source/actorRefWithBackpressure.html#description","text":"Materialize an ActorRef, sending messages to it will emit them on the stream. The actor responds with the provided ack message once the element could be emitted allowing for backpressure from the source. Sending another message before the previous one has been acknowledged will fail the stream.\nSee also:\nSource.actorRef This operator without backpressure control ActorSource.actorRef The operator for the new actors API without backpressure control ActorSource.actorRefWithBackpressure The corresponding operator for the new actors API Source.queue Materialize a SourceQueue onto which elements can be pushed for emitting from the source","title":"Description"},{"location":"/stream/operators/Source/actorRefWithBackpressure.html#examples","text":"Scala copysource import org.apache.pekko\nimport pekko.actor.Status.Success\nimport pekko.actor.ActorRef\nimport pekko.stream.CompletionStrategy\nimport pekko.stream.scaladsl._\n\nval probe = TestProbe()\n\nval source: Source[String, ActorRef] = Source.actorRefWithBackpressure[String](\n  ackMessage = \"ack\",\n  // complete when we send pekko.actor.status.Success\n  completionMatcher = {\n    case _: Success => CompletionStrategy.immediately\n  },\n  // do not fail on any message\n  failureMatcher = PartialFunction.empty)\nval actorRef: ActorRef = source.to(Sink.foreach(println)).run()\n\nprobe.send(actorRef, \"hello\")\nprobe.expectMsg(\"ack\")\nprobe.send(actorRef, \"hello\")\nprobe.expectMsg(\"ack\")\n\n// The stream completes successfully with the following message\nactorRef ! Success(()) Java copysourceimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.stream.OverflowStrategy;\nimport org.apache.pekko.stream.CompletionStrategy;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.testkit.TestProbe;\nSource<String, ActorRef> source =\n    Source.<String>actorRefWithBackpressure(\n        \"ack\",\n        // complete when we send \"complete\"\n        o -> {\n          if (o == \"complete\") return Optional.of(CompletionStrategy.draining());\n          else return Optional.empty();\n        },\n        // do not fail on any message\n        o -> Optional.empty());\n\nActorRef actorRef = source.to(Sink.foreach(System.out::println)).run(system);\nprobe.send(actorRef, \"hello\");\nprobe.expectMsg(\"ack\");\nprobe.send(actorRef, \"hello\");\nprobe.expectMsg(\"ack\");\n\n// The stream completes successfully with the following message\nactorRef.tell(\"complete\", ActorRef.noSender());","title":"Examples"},{"location":"/stream/operators/Source/actorRefWithBackpressure.html#reactive-streams-semantics","text":"emits when there is demand and there are messages in the buffer or a message is sent to the ActorRef completes when the passed completion matcher returns a CompletionStrategy or fails if the passed failure matcher returns an exception","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/actorRefWithBackpressure.html","text":"","title":"Sink.actorRefWithBackpressure"},{"location":"/stream/operators/Sink/actorRefWithBackpressure.html#sink-actorrefwithbackpressure","text":"Send the elements from the stream to an ActorRef (of the classic actors API) which must then acknowledge reception after completing a message, to provide back pressure onto the sink.\nActor interop operators","title":"Sink.actorRefWithBackpressure"},{"location":"/stream/operators/Sink/actorRefWithBackpressure.html#signature","text":"Sink.actorRefWithBackpressureSink.actorRefWithBackpressure","title":"Signature"},{"location":"/stream/operators/Sink/actorRefWithBackpressure.html#description","text":"Send the elements from the stream to an ActorRef which must then acknowledge reception after completing a message, to provide back pressure onto the sink. There is also a variant without a concrete acknowledge message accepting any message as such.\nSee also:\nSink.actorRef Send elements to an actor, without considering backpressure ActorSink.actorRef The corresponding operator for the new actors API ActorSink.actorRefWithBackpressure Send elements to an actor of the new actors API supporting backpressure","title":"Description"},{"location":"/stream/operators/Sink/actorRefWithBackpressure.html#example","text":"Actor to be interacted with:\nScala copysourceobject AckingReceiver {\n  case object Ack\n\n  case object StreamInitialized\n  case object StreamCompleted\n  final case class StreamFailure(ex: Throwable)\n}\n\nclass AckingReceiver(probe: ActorRef) extends Actor with ActorLogging {\n  import AckingReceiver._\n\n  def receive: Receive = {\n    case StreamInitialized =>\n      log.info(\"Stream initialized!\")\n      probe ! \"Stream initialized!\"\n      sender() ! Ack // ack to allow the stream to proceed sending more elements\n\n    case el: String =>\n      log.info(\"Received element: {}\", el)\n      probe ! el\n      sender() ! Ack // ack to allow the stream to proceed sending more elements\n\n    case StreamCompleted =>\n      log.info(\"Stream completed!\")\n      probe ! \"Stream completed!\"\n    case StreamFailure(ex) =>\n      log.error(ex, \"Stream failed!\")\n  }\n} Java copysourceenum Ack {\n  INSTANCE;\n}\n\nstatic class StreamInitialized {}\n\nstatic class StreamCompleted {}\n\nstatic class StreamFailure {\n  private final Throwable cause;\n\n  public StreamFailure(Throwable cause) {\n    this.cause = cause;\n  }\n\n  public Throwable getCause() {\n    return cause;\n  }\n}\n\nstatic class AckingReceiver extends AbstractLoggingActor {\n\n  private final ActorRef probe;\n\n  public AckingReceiver(ActorRef probe) {\n    this.probe = probe;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            StreamInitialized.class,\n            init -> {\n              log().info(\"Stream initialized\");\n              probe.tell(\"Stream initialized\", getSelf());\n              sender().tell(Ack.INSTANCE, self());\n            })\n        .match(\n            String.class,\n            element -> {\n              log().info(\"Received element: {}\", element);\n              probe.tell(element, getSelf());\n              sender().tell(Ack.INSTANCE, self());\n            })\n        .match(\n            StreamCompleted.class,\n            completed -> {\n              log().info(\"Stream completed\");\n              probe.tell(\"Stream completed\", getSelf());\n            })\n        .match(\n            StreamFailure.class,\n            failed -> {\n              log().error(failed.getCause(), \"Stream failed!\");\n              probe.tell(\"Stream failed!\", getSelf());\n            })\n        .build();\n  }\n}\nUsing the actorRefWithBackpressure operator with the above actor:\nScala copysourceval words: Source[String, NotUsed] =\n  Source(List(\"hello\", \"hi\"))\n\n// sent from actor to stream to \"ack\" processing of given element\nval AckMessage = AckingReceiver.Ack\n\n// sent from stream to actor to indicate start, end or failure of stream:\nval InitMessage = AckingReceiver.StreamInitialized\nval OnCompleteMessage = AckingReceiver.StreamCompleted\nval onErrorMessage = (ex: Throwable) => AckingReceiver.StreamFailure(ex)\n\nval probe = TestProbe()\nval receiver = system.actorOf(Props(new AckingReceiver(probe.ref)))\nval sink = Sink.actorRefWithBackpressure(\n  receiver,\n  onInitMessage = InitMessage,\n  ackMessage = AckMessage,\n  onCompleteMessage = OnCompleteMessage,\n  onFailureMessage = onErrorMessage)\n\nwords.map(_.toLowerCase).runWith(sink)\n\nprobe.expectMsg(\"Stream initialized!\")\nprobe.expectMsg(\"hello\")\nprobe.expectMsg(\"hi\")\nprobe.expectMsg(\"Stream completed!\") Java copysourceSource<String, NotUsed> words = Source.from(Arrays.asList(\"hello\", \"hi\"));\n\nfinal TestKit probe = new TestKit(system);\n\nActorRef receiver = system.actorOf(Props.create(AckingReceiver.class, probe.getRef()));\n\nSink<String, NotUsed> sink =\n    Sink.<String>actorRefWithBackpressure(\n        receiver,\n        new StreamInitialized(),\n        Ack.INSTANCE,\n        new StreamCompleted(),\n        ex -> new StreamFailure(ex));\n\nwords.map(el -> el.toLowerCase()).runWith(sink, system);\n\nprobe.expectMsg(\"Stream initialized\");\nprobe.expectMsg(\"hello\");\nprobe.expectMsg(\"hi\");\nprobe.expectMsg(\"Stream completed\");","title":"Example"},{"location":"/stream/operators/Sink/actorRefWithBackpressure.html#reactive-streams-semantics","text":"cancels when the actor terminates backpressures when the actor acknowledgement has not arrived","title":"Reactive Streams semantics"},{"location":"/stream/operators/ActorSource/actorRefWithBackpressure.html","text":"","title":"ActorSource.actorRefWithBackpressure"},{"location":"/stream/operators/ActorSource/actorRefWithBackpressure.html#actorsource-actorrefwithbackpressure","text":"Materialize an ActorRef<T>ActorRef[T] of the new actors API; sending messages to it will emit them on the stream. The source acknowledges reception after emitting a message, to provide back pressure from the source.\nActor interop operators","title":"ActorSource.actorRefWithBackpressure"},{"location":"/stream/operators/ActorSource/actorRefWithBackpressure.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/ActorSource/actorRefWithBackpressure.html#signature","text":"ActorSource.actorRefWithBackpressureActorSource.actorRefWithBackpressure","title":"Signature"},{"location":"/stream/operators/ActorSource/actorRefWithBackpressure.html#description","text":"Materialize an ActorRef<T>ActorRef[T], sending messages to it will emit them on the stream. The actor responds with the provided ack message once the element could be emitted allowing for backpressure from the source. Sending another message before the previous one has been acknowledged will fail the stream.\nSee also:\nActorSource.actorRef This operator, but without backpressure control Source.actorRef This operator, but without backpressure control for the classic actors API Source.actorRefWithBackpressure This operator for the classic actors API Source.queue Materialize a SourceQueue onto which elements can be pushed for emitting from the source","title":"Description"},{"location":"/stream/operators/ActorSource/actorRefWithBackpressure.html#example","text":"With actorRefWithBackpressure two actors get into play:\nAn actor that is materialized when the stream runs. It feeds the stream. An actor provided by the user. It gets the ack signal when an element is emitted into the stream.\nFor the ack signal we create an Emitted objectempty Emitted class.\nFor “feeding” the stream we use the Event traitinterface.\nIn this example we create the stream in an actor which itself reacts on the demand of the stream and sends more messages.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.stream.CompletionStrategy\nimport pekko.stream.scaladsl.Sink\nimport pekko.stream.typed.scaladsl.ActorSource\n\nobject StreamFeeder {\n\n  /** Signals that the latest element is emitted into the stream */\n  case object Emitted\n\n  sealed trait Event\n  case class Element(content: String) extends Event\n  case object ReachedEnd extends Event\n  case class FailureOccured(ex: Exception) extends Event\n\n  def apply(): Behavior[Emitted.type] =\n    Behaviors.setup { context =>\n      val streamActor = runStream(context.self)(context.system)\n      streamActor ! Element(\"first\")\n      sender(streamActor, 0)\n    }\n\n  private def runStream(ackReceiver: ActorRef[Emitted.type])(implicit system: ActorSystem[_]): ActorRef[Event] = {\n    val source =\n      ActorSource.actorRefWithBackpressure[Event, Emitted.type](\n        // get demand signalled to this actor receiving Ack\n        ackTo = ackReceiver,\n        ackMessage = Emitted,\n        // complete when we send ReachedEnd\n        completionMatcher = {\n          case ReachedEnd => CompletionStrategy.draining\n        },\n        failureMatcher = {\n          case FailureOccured(ex) => ex\n        })\n\n    val streamActor: ActorRef[Event] = source\n      .collect {\n        case Element(msg) => msg\n      }\n      .to(Sink.foreach(println))\n      .run()\n\n    streamActor\n  }\n\n  private def sender(streamSource: ActorRef[Event], counter: Int): Behavior[Emitted.type] =\n    Behaviors.receiveMessage {\n      case Emitted if counter < 5 =>\n        streamSource ! Element(counter.toString)\n        sender(streamSource, counter + 1)\n      case _ =>\n        streamSource ! ReachedEnd\n        Behaviors.stopped\n    }\n}\n\nActorSystem(StreamFeeder(), \"stream-feeder\")\n\n// Will print:\n// first\n// 0\n// 1\n// 2\n// 3\n// 4 Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.actor.typed.Behavior;\nimport org.apache.pekko.actor.typed.javadsl.AbstractBehavior;\nimport org.apache.pekko.actor.typed.javadsl.ActorContext;\nimport org.apache.pekko.actor.typed.javadsl.Behaviors;\nimport org.apache.pekko.actor.typed.javadsl.Receive;\nimport org.apache.pekko.stream.CompletionStrategy;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.typed.javadsl.ActorSource;\n\nimport java.util.Optional;\n\nclass StreamFeeder extends AbstractBehavior<StreamFeeder.Emitted> {\n  /** Signals that the latest element is emitted into the stream */\n  public enum Emitted {\n    INSTANCE;\n  }\n\n  public interface Event {}\n\n  public static class Element implements Event {\n    public final String content;\n\n    public Element(String content) {\n      this.content = content;\n    }\n\n    @Override\n    public String toString() {\n      return \"Element(\" + content + \")\";\n    }\n  }\n\n  public enum ReachedEnd implements Event {\n    INSTANCE;\n  }\n\n  public static class FailureOccured implements Event {\n    public final Exception ex;\n\n    public FailureOccured(Exception ex) {\n      this.ex = ex;\n    }\n  }\n\n  public static Behavior<Emitted> create() {\n    return Behaviors.setup(StreamFeeder::new);\n  }\n\n  private int counter = 0;\n  private final ActorRef<Event> streamSource;\n\n  private StreamFeeder(ActorContext<Emitted> context) {\n    super(context);\n    streamSource = runStream(context.getSelf(), context.getSystem());\n    streamSource.tell(new Element(\"first\"));\n  }\n\n  @Override\n  public Receive<Emitted> createReceive() {\n    return newReceiveBuilder().onMessage(Emitted.class, this::onEmitted).build();\n  }\n\n  private static ActorRef<Event> runStream(ActorRef<Emitted> ackReceiver, ActorSystem<?> system) {\n    Source<Event, ActorRef<Event>> source =\n        ActorSource.actorRefWithBackpressure(\n            ackReceiver,\n            Emitted.INSTANCE,\n            // complete when we send ReachedEnd\n            (msg) -> {\n              if (msg == ReachedEnd.INSTANCE) return Optional.of(CompletionStrategy.draining());\n              else return Optional.empty();\n            },\n            (msg) -> {\n              if (msg instanceof FailureOccured) return Optional.of(((FailureOccured) msg).ex);\n              else return Optional.empty();\n            });\n\n    return source.to(Sink.foreach(System.out::println)).run(system);\n  }\n\n  private Behavior<Emitted> onEmitted(Emitted message) {\n    if (counter < 5) {\n      streamSource.tell(new Element(String.valueOf(counter)));\n      counter++;\n      return this;\n    } else {\n      streamSource.tell(ReachedEnd.INSTANCE);\n      return Behaviors.stopped();\n    }\n  }\n}\n    ActorSystem<StreamFeeder.Emitted> system =\n        ActorSystem.create(StreamFeeder.create(), \"stream-feeder\");\n\n    // will print:\n    // Element(first)\n    // Element(0)\n    // Element(1)\n    // Element(2)\n    // Element(3)\n    // Element(4)","title":"Example"},{"location":"/stream/operators/ActorSource/actorRefWithBackpressure.html#reactive-streams-semantics","text":"emits when a message is sent to the materialized ActorRef[T]ActorRef<T> it is emitted as soon as there is demand from downstream completes when the passed completion matcher returns a CompletionStrategy","title":"Reactive Streams semantics"},{"location":"/stream/operators/ActorSink/actorRefWithBackpressure.html","text":"","title":"ActorSink.actorRefWithBackpressure"},{"location":"/stream/operators/ActorSink/actorRefWithBackpressure.html#actorsink-actorrefwithbackpressure","text":"Sends the elements of the stream to the given ActorRef<T>ActorRef[T] of the new actors API with backpressure, to be able to signal demand when the actor is ready to receive more elements.\nActor interop operators","title":"ActorSink.actorRefWithBackpressure"},{"location":"/stream/operators/ActorSink/actorRefWithBackpressure.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/ActorSink/actorRefWithBackpressure.html#signature","text":"ActorSink.actorRefWithBackpressureActorSink.actorRefWithBackpressure","title":"Signature"},{"location":"/stream/operators/ActorSink/actorRefWithBackpressure.html#description","text":"Sends the elements of the stream to the given ActorRef<T>ActorRef[T] with backpressure, to be able to signal demand when the actor is ready to receive more elements. There is also a variant without a concrete acknowledge message accepting any message as such.\nSee also:\nActorSink.actorRef Send elements to an actor of the new actors API, without considering backpressure Sink.actorRef Send elements to an actor of the classic actors API, without considering backpressure Sink.actorRefWithBackpressue The corresponding operator for the classic actors API","title":"Description"},{"location":"/stream/operators/ActorSink/actorRefWithBackpressure.html#examples","text":"Scala copysourceimport org.apache.pekko\nimport pekko.actor.typed.ActorRef\nimport pekko.stream.scaladsl.{ Sink, Source }\nimport pekko.stream.typed.scaladsl.ActorSink\n\ntrait Ack\nobject Ack extends Ack\n\ntrait Protocol\ncase class Init(ackTo: ActorRef[Ack]) extends Protocol\ncase class Message(ackTo: ActorRef[Ack], msg: String) extends Protocol\ncase object Complete extends Protocol\ncase class Fail(ex: Throwable) extends Protocol\n\nval actor: ActorRef[Protocol] = targetActor()\n\nval sink: Sink[String, NotUsed] = ActorSink.actorRefWithBackpressure(\n  ref = actor,\n  messageAdapter = (responseActorRef: ActorRef[Ack], element) => Message(responseActorRef, element),\n  onInitMessage = (responseActorRef: ActorRef[Ack]) => Init(responseActorRef),\n  ackMessage = Ack,\n  onCompleteMessage = Complete,\n  onFailureMessage = exception => Fail(exception))\n\nSource.single(\"msg1\").runWith(sink) Java copysourceimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.typed.javadsl.ActorSink;\n\nenum Ack {\n  INSTANCE;\n}\n\ninterface Protocol {}\n\nclass Init implements Protocol {\n  private final ActorRef<Ack> ack;\n\n  public Init(ActorRef<Ack> ack) {\n    this.ack = ack;\n  }\n}\n\nclass Message implements Protocol {\n  private final ActorRef<Ack> ackTo;\n  private final String msg;\n\n  public Message(ActorRef<Ack> ackTo, String msg) {\n    this.ackTo = ackTo;\n    this.msg = msg;\n  }\n}\n\nclass Complete implements Protocol {}\n\nclass Fail implements Protocol {\n  private final Throwable ex;\n\n  public Fail(Throwable ex) {\n    this.ex = ex;\n  }\n}\n\n  final ActorRef<Protocol> actorRef = // spawned actor\n\n  final Complete completeMessage = new Complete();\n\n  final Sink<String, NotUsed> sink =\n      ActorSink.actorRefWithBackpressure(\n          actorRef,\n          (responseActorRef, element) -> new Message(responseActorRef, element),\n          (responseActorRef) -> new Init(responseActorRef),\n          Ack.INSTANCE,\n          completeMessage,\n          (exception) -> new Fail(exception));\n\n  Source.single(\"msg1\").runWith(sink, system);","title":"Examples"},{"location":"/stream/operators/ActorSink/actorRefWithBackpressure.html#reactive-streams-semantics","text":"cancels when the actor terminates backpressures when the actor acknowledgement has not arrived","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/aggregateWithBoundary.html","text":"","title":"aggregateWithBoundary"},{"location":"/stream/operators/Source-or-Flow/aggregateWithBoundary.html#aggregatewithboundary","text":"Aggregate and emit until custom boundary condition met.\nBackpressure aware operators\nTimer driven operators","title":"aggregateWithBoundary"},{"location":"/stream/operators/Source-or-Flow/aggregateWithBoundary.html#signature","text":"Source.aggregateWithBoundarySource.aggregateWithBoundary Flow.aggregateWithBoundaryFlow.aggregateWithBoundary","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/aggregateWithBoundary.html#description","text":"This operator can be customized into a broad class of aggregate/group/fold operators, based on custom state or timer conditions.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/aggregateWithBoundary.html#reactive-streams-semantics","text":"emits when the aggregation function decides the aggregate is complete or the timer function returns true backpressures when downstream backpressures and the aggregate is complete completes when upstream completes and the last aggregate has been emitted downstream cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/alsoTo.html","text":"","title":"alsoTo"},{"location":"/stream/operators/Source-or-Flow/alsoTo.html#alsoto","text":"Attaches the given Sink to this Flow, meaning that elements that pass through this Flow will also be sent to the Sink.\nFan-out operators","title":"alsoTo"},{"location":"/stream/operators/Source-or-Flow/alsoTo.html#signature","text":"Source.alsoToSource.alsoTo Flow.alsoToFlow.alsoTo","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/alsoTo.html#description","text":"Attaches the given Sink to this Flow, meaning that elements that pass through this Flow will also be sent to the Sink.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/alsoTo.html#reactive-streams-semantics","text":"emits when an element is available and demand exists both from the Sink and the downstream backpressures when downstream or Sink backpressures completes when upstream completes cancels when downstream or Sink cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/alsoToAll.html","text":"","title":"alsoToAll"},{"location":"/stream/operators/Source-or-Flow/alsoToAll.html#alsotoall","text":"Attaches the given SourceSources to this FlowFlow, meaning that elements that pass through this FlowFlow will also be sent to all those SinkSinks.\nFan-out operators","title":"alsoToAll"},{"location":"/stream/operators/Source-or-Flow/alsoToAll.html#signature","text":"Source.alsoToAllSource.alsoToAll Flow.alsoToAllFlow.alsoToAll","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/alsoToAll.html#description","text":"Attaches the given SourceSource s to this FlowFlow, meaning that elements that pass through this FlowFlow will also be sent to all those SinkSinks.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/alsoToAll.html#reactive-streams-semantics","text":"emits when an element is available and demand exists both from the SinkSinks and the downstream backpressures when downstream or any of the SinkSinks backpressures completes when upstream completes cancels when downstream or or any of the SinkSinks cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/asFlowWithContext.html","text":"","title":"Flow.asFlowWithContext"},{"location":"/stream/operators/Flow/asFlowWithContext.html#flow-asflowwithcontext","text":"Extracts context data from the elements of a Flow so that it can be turned into a FlowWithContext which can propagate that context per element along a stream.\nSimple operators","title":"Flow.asFlowWithContext"},{"location":"/stream/operators/Flow/asFlowWithContext.html#signature","text":"Flow.asFlowWithContextFlow.asFlowWithContext","title":"Signature"},{"location":"/stream/operators/Flow/asFlowWithContext.html#description","text":"See Context Propagation for a general overview of context propagation.\nExtracts context data from the elements of a FlowFlow so that it can be turned into a FlowWithContextFlowWithContext which can propagate that context per element along a stream. The first function passed into asFlowWithContext must turn each incoming pair of element and context value into an element of this FlowFlow. The second function passed into asFlowWithContext must turn each outgoing element of this FlowFlow into an outgoing context value.\nSee also:\nContext Propagation Source.asSourceWithContext Turns a Source into a SourceWithContext which can propagate a context per element along a stream.","title":"Description"},{"location":"/stream/operators/Flow/asFlowWithContext.html#example","text":"Elements from this flow have a correlation number, but the flow structure should focus on the text message in the elements. The first converter in asFlowWithContext applies to the end of the “with context” flow to turn it into a regular flow again. The second converter function chooses the second value in the tuplepair as the context. Another map operator makes the first value the stream elements in the FlowWithContext.\nScala copysourceimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.scaladsl.Flow\nimport pekko.stream.scaladsl.FlowWithContext\n// a regular flow with pairs as elements\nval flow: Flow[(String, Int), (String, Int), NotUsed] = // ???\n\n// Declare the \"flow with context\"\n// ingoing: String and Integer\n// outgoing: String and Integer\nval flowWithContext: FlowWithContext[String, Int, String, Int, NotUsed] =\n  // convert the flow of pairs into a \"flow with context\"\n  flow\n    .asFlowWithContext[String, Int, Int](\n      // at the end of this flow: put the elements and the context back into a tuple\n      collapseContext = Tuple2.apply)(\n      // pick the second element of the incoming pair as context\n      extractContext = _._2)\n    .map(_._1) // keep the first pair element as stream element\n\nval mapped = flowWithContext\n  // regular operators apply to the element without seeing the context\n  .map(_.reverse)\n\n// running the flow with some sample data and asserting the outcome\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.Sink\nimport scala.collection.immutable\n\nval values: immutable.Seq[(String, Int)] = immutable.Seq(\"eins\" -> 1, \"zwei\" -> 2, \"drei\" -> 3)\nval source = Source(values).asSourceWithContext(_._2).map(_._1)\n\nval result = source.via(mapped).runWith(Sink.seq)\nresult.futureValue should contain theSameElementsInOrderAs immutable.Seq(\"snie\" -> 1, \"iewz\" -> 2, \"ierd\" -> 3) Java copysourceimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.japi.Pair;\nimport org.apache.pekko.stream.javadsl.*;\n\n// a regular flow with pairs as elements\nFlow<Pair<String, Integer>, Pair<String, Integer>, NotUsed> flow = // ...\n\n// Declare the \"flow with context\"\n// ingoing: String and Integer\n// outgoing: String and Integer\nFlowWithContext<String, Integer, String, Integer, NotUsed> flowWithContext =\n    // convert the flow of pairs into a \"flow with context\"\n    flow.<String, Integer, Integer>asFlowWithContext(\n            // at the end of this flow: put the elements and the context back into a pair\n            Pair::create,\n            // pick the second element of the incoming pair as context\n            Pair::second)\n        // keep the first pair element as stream element\n        .map(Pair::first);\n\nFlowWithContext<String, Integer, String, Integer, NotUsed> mapped =\n    flowWithContext\n        // regular operators apply to the element without seeing the context\n        .map(s -> s.replace('e', 'y'));\n\n// running the flow with some sample data and asserting the outcome\nCollection<Pair<String, Integer>> values =\n    Arrays.asList(Pair.create(\"eins\", 1), Pair.create(\"zwei\", 2), Pair.create(\"drei\", 3));\n\nSourceWithContext<String, Integer, NotUsed> source =\n    Source.from(values).asSourceWithContext(Pair::second).map(Pair::first);\n\nCompletionStage<List<Pair<String, Integer>>> result =\n    source.via(mapped).runWith(Sink.seq(), system);\nList<Pair<String, Integer>> list = result.toCompletableFuture().get(1, TimeUnit.SECONDS);\nassertThat(\n    list, hasItems(Pair.create(\"yins\", 1), Pair.create(\"zwyi\", 2), Pair.create(\"dryi\", 3)));","title":"Example"},{"location":"/stream/operators/StreamConverters/asInputStream.html","text":"","title":"StreamConverters.asInputStream"},{"location":"/stream/operators/StreamConverters/asInputStream.html#streamconverters-asinputstream","text":"Create a sink which materializes into an InputStream that can be read to trigger demand through the sink.\nAdditional Sink and Source converters","title":"StreamConverters.asInputStream"},{"location":"/stream/operators/StreamConverters/asInputStream.html#signature","text":"StreamConverters.asInputStreamStreamConverters.asInputStream","title":"Signature"},{"location":"/stream/operators/StreamConverters/asInputStream.html#description","text":"Create a sink which materializes into an InputStream that can be read to trigger demand through the sink. Bytes emitted through the stream will be available for reading through the InputStream\nThe InputStream will be ended when the stream flowing into this Sink completes, and the closing the InputStream will cancel the inflow of this Sink.","title":"Description"},{"location":"/stream/operators/StreamConverters/asInputStream.html#reactive-streams-semantics","text":"cancels when the InputStream is closed backpressures when no read is pending on the InputStream","title":"Reactive Streams semantics"},{"location":"/stream/operators/StreamConverters/asInputStream.html#example","text":"Here is an example of a SinkSink that reads the contents from the source, converts it into uppercase and materializes into a java.io.InputStream\nScala copysourceval toUpperCase: Flow[ByteString, ByteString, NotUsed] = Flow[ByteString].map(_.map(_.toChar.toUpper.toByte))\nval source: Source[ByteString, NotUsed] = Source.single(ByteString(\"some random input\"))\nval sink: Sink[ByteString, InputStream] = StreamConverters.asInputStream()\n\nval inputStream: InputStream = source.via(toUpperCase).runWith(sink) Java copysourceCharset charset = Charset.defaultCharset();\nFlow<ByteString, ByteString, NotUsed> toUpperCase =\n    Flow.<ByteString>create()\n        .map(\n            bs -> {\n              String str = bs.decodeString(charset).toUpperCase();\n              return ByteString.fromString(str, charset);\n            });\n\nfinal Sink<ByteString, InputStream> sink = StreamConverters.asInputStream();\nfinal InputStream stream =\n    Source.single(ByteString.fromString(\"Some random input\"))\n        .via(toUpperCase)\n        .runWith(sink, system);","title":"Example"},{"location":"/stream/operators/StreamConverters/asJavaStream.html","text":"","title":"StreamConverters.asJavaStream"},{"location":"/stream/operators/StreamConverters/asJavaStream.html#streamconverters-asjavastream","text":"Create a sink which materializes into Java 8 Stream that can be run to trigger demand through the sink.\nAdditional Sink and Source converters","title":"StreamConverters.asJavaStream"},{"location":"/stream/operators/StreamConverters/asJavaStream.html#signature","text":"StreamConvertersStreamConverters","title":"Signature"},{"location":"/stream/operators/StreamConverters/asJavaStream.html#description","text":"Create a sink which materializes into Java 8 Stream that can be run to trigger demand through the sink. Elements emitted through the stream will be available for reading through the Java 8 Stream.\nThe Java 8 Stream will be ended when the stream flowing into this Sink completes, and closing the Java Stream will cancel the inflow of this Sink. If the Java Stream throws an exception, the Pekko stream is cancelled.\nBe aware that Java Stream blocks current thread while waiting on next element from downstream.","title":"Description"},{"location":"/stream/operators/StreamConverters/asJavaStream.html#example","text":"Here is an example of a SinkSink that materializes into a java.util.stream.Stream.\nScala copysourceimport java.util.stream\nimport java.util.stream.IntStream\n\nimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.scaladsl.Keep\nimport pekko.stream.scaladsl.Sink\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.StreamConverters\nval source: Source[Int, NotUsed] = Source(0 to 9).filter(_ % 2 == 0)\n\nval sink: Sink[Int, stream.Stream[Int]] = StreamConverters.asJavaStream[Int]()\n\nval jStream: java.util.stream.Stream[Int] = source.runWith(sink) Java copysourceimport org.apache.pekko.japi.function.Creator;\nimport org.apache.pekko.stream.Materializer;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.StreamConverters;\n\nimport java.util.concurrent.CompletionStage;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.stream.BaseStream;\nimport java.util.stream.IntStream;\nimport java.util.stream.Stream;\n\nSource<Integer, NotUsed> source = Source.range(0, 9).filter(i -> i % 2 == 0);\n\nSink<Integer, java.util.stream.Stream<Integer>> sink = StreamConverters.<Integer>asJavaStream();\n\nStream<Integer> jStream = source.runWith(sink, system);","title":"Example"},{"location":"/stream/operators/StreamConverters/asJavaStream.html#reactive-streams-semantics","text":"cancels when the Java Stream is closed backpressures when no read is pending on the Java Stream","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/ask.html","text":"","title":"ask"},{"location":"/stream/operators/Source-or-Flow/ask.html#ask","text":"Use the “Ask Pattern” to send a request-reply message to the target ref actor (of the classic actors API).\nActor interop operators","title":"ask"},{"location":"/stream/operators/Source-or-Flow/ask.html#signature","text":"Source.askSource.ask Flow.askFlow.ask","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/ask.html#description","text":"Use the Ask Pattern to send a request-reply message to the target ref actor. If any of the asks times out it will fail the stream with a AskTimeoutExceptionAskTimeoutException.\nThe mapTo classS generic parameter is used to cast the responses from the actor to the expected outgoing flow type.\nSimilar to the plain ask pattern, the target actor is allowed to reply with StatusStatus. An Status.FailureStatus.Failure will cause the operator to fail with the cause carried in the Failure message.\nAdheres to the ActorAttributes.SupervisionStrategyActorAttributes.SupervisionStrategy attribute.\nSee also:\nActorFlow.ask for the org.apache.pekko.actor.typed.ActorRef[_] variant","title":"Description"},{"location":"/stream/operators/Source-or-Flow/ask.html#reactive-streams-semantics","text":"emits when the ask Future CompletionStage returned by the provided function finishes for the next element in sequence backpressures when the number of ask Future s CompletionStage s reaches the configured parallelism and the downstream backpressures completes when upstream completes and all ask Future s CompletionStage s has been completed and all elements has been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/ActorFlow/ask.html","text":"","title":"ActorFlow.ask"},{"location":"/stream/operators/ActorFlow/ask.html#actorflow-ask","text":"Use the “Ask Pattern” to send each stream element as an ask to the target actor (of the new actors API), and expect a reply that will be emitted downstream.\nActor interop operators","title":"ActorFlow.ask"},{"location":"/stream/operators/ActorFlow/ask.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/ActorFlow/ask.html#signature","text":"ActorFlow.askActorFlow.ask","title":"Signature"},{"location":"/stream/operators/ActorFlow/ask.html#description","text":"Use the Ask pattern to send a request-reply message to the target ref actor. If any of the asks times out it will fail the stream with an AskTimeoutExceptionAskTimeoutException.\nThe ask operator requires\nthe actor ref, a makeMessage function to create the message sent to the actor from the incoming element, and the actor ref accepting the actor’s reply message a timeout.\nSee also:\nFlow.ask for the classic actors variant","title":"Description"},{"location":"/stream/operators/ActorFlow/ask.html#examples","text":"The ActorFlow.ask sends a message to the actor. The actor expects Asking messages which contain the actor ref for replies of type Reply. When the actor for replies receives a reply, the ActorFlow.ask stream stage emits the reply and the map extracts the message String.\nScala copysourceimport org.apache.pekko\nimport pekko.stream.scaladsl.{ Flow, Sink, Source }\nimport pekko.stream.typed.scaladsl.ActorFlow\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.util.Timeout\n\nfinal case class Asking(s: String, replyTo: ActorRef[Reply])\nfinal case class Reply(msg: String)\n\nfinal case class AskingWithStatus(s: String, replyTo: ActorRef[StatusReply[String]])\n\n    val ref = spawn(Behaviors.receiveMessage[Asking] { asking =>\n      asking.replyTo ! Reply(asking.s + \"!!!\")\n      Behaviors.same\n    })\n\nimplicit val timeout: Timeout = 1.second\n\nval askFlow: Flow[String, Reply, NotUsed] =\n  ActorFlow.ask(ref)(Asking.apply)\n\n// explicit creation of the sent message\nval askFlowExplicit: Flow[String, Reply, NotUsed] =\n  ActorFlow.ask(ref)(makeMessage = (el, replyTo: ActorRef[Reply]) => Asking(el, replyTo))\n\nval in: Future[immutable.Seq[String]] =\n  Source(1 to 50).map(_.toString).via(askFlow).map(_.msg).runWith(Sink.seq) Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.pattern.StatusReply;\nimport org.apache.pekko.stream.javadsl.Flow;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.typed.javadsl.ActorFlow;\n\nclass Asking {\n  final String payload;\n  final ActorRef<Reply> replyTo;\n\n  public Asking(String payload, ActorRef<Reply> replyTo) {\n    this.payload = payload;\n    this.replyTo = replyTo;\n  }\n}\n\nstatic class AskingWithStatus {\n  final String payload;\n  final ActorRef<StatusReply<String>> replyTo;\n\n  public AskingWithStatus(String payload, ActorRef<StatusReply<String>> replyTo) {\n    this.payload = payload;\n    this.replyTo = replyTo;\n  }\n}\n\nclass Reply {\n  public final String msg;\n\n  public Reply(String msg) {\n    this.msg = msg;\n  }\n}\n\nfinal ActorRef<Asking> actorRef = // ???\nfinal ActorRef<AskingWithStatus> actorWithStatusRef = // ???\nDuration timeout = Duration.ofSeconds(1);\n\n// method reference notation\nFlow<String, Reply, NotUsed> askFlow = ActorFlow.ask(actorRef, timeout, Asking::new);\n\n// explicit creation of the sent message\nFlow<String, Reply, NotUsed> askFlowExplicit =\n    ActorFlow.ask(actorRef, timeout, (msg, replyTo) -> new Asking(msg, replyTo));\n\nFlow<String, String, NotUsed> askFlowExplicitWithStatus =\n    ActorFlow.askWithStatus(\n        actorWithStatusRef, timeout, (msg, replyTo) -> new AskingWithStatus(msg, replyTo));\n\nSource.repeat(\"hello\").via(askFlow).map(reply -> reply.msg).runWith(Sink.seq(), system);","title":"Examples"},{"location":"/stream/operators/ActorFlow/ask.html#reactive-streams-semantics","text":"emits when the futures (in submission order) created by the ask pattern internally are completed backpressures when the number of futures reaches the configured parallelism and the downstream backpressures completes when upstream completes and all futures have been completed and all elements have been emitted fails when the passed-in actor terminates, or when any of the asks exceed a timeout cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/ActorFlow/askWithContext.html","text":"","title":"ActorFlow.askWithContext"},{"location":"/stream/operators/ActorFlow/askWithContext.html#actorflow-askwithcontext","text":"Use the “Ask Pattern” to send each stream element (without the context) as an ask to the target actor (of the new actors API), and expect a reply that will be emitted downstream.\nActor interop operators","title":"ActorFlow.askWithContext"},{"location":"/stream/operators/ActorFlow/askWithContext.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/ActorFlow/askWithContext.html#signature","text":"ActorFlow.askWithContextActorFlow.askWithContext","title":"Signature"},{"location":"/stream/operators/ActorFlow/askWithContext.html#description","text":"Use the Ask pattern to send a request-reply message to the target ref actor. The stream context is not sent, instead it is locally recombined to the actor’s reply.\nIf any of the asks times out it will fail the stream with an AskTimeoutExceptionAskTimeoutException.\nThe ask operator requires\nthe actor ref, a makeMessage function to create the message sent to the actor from the incoming element, and the actor ref accepting the actor’s reply message a timeout.","title":"Description"},{"location":"/stream/operators/ActorFlow/askWithContext.html#reactive-streams-semantics","text":"emits when the futures (in submission order) created by the ask pattern internally are completed backpressures when the number of futures reaches the configured parallelism and the downstream backpressures completes when upstream completes and all futures have been completed and all elements have been emitted fails when the passed-in actor terminates, or when any of the asks exceed a timeout cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/ActorFlow/askWithStatus.html","text":"","title":"ActorFlow.askWithStatus"},{"location":"/stream/operators/ActorFlow/askWithStatus.html#actorflow-askwithstatus","text":"Use the “Ask Pattern” to send each stream element as an ask to the target actor (of the new actors API), and expect a reply of Type StatusReply[T]StatusReply<T> where the T will be unwrapped and emitted downstream.\nActor interop operators","title":"ActorFlow.askWithStatus"},{"location":"/stream/operators/ActorFlow/askWithStatus.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <pekko.version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</pekko.version>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n    <version>${pekko.version}</version>\n  </dependency>\n</dependencies> Gradle def versions = [\n  PekkoVersion: \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\",\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}:${versions.PekkoVersion}\"\n}","title":"Dependency"},{"location":"/stream/operators/ActorFlow/askWithStatus.html#signature","text":"ActorFlow.askWithStatusActorFlow.askWithStatus { scala=“#askWithStatusI,Q,A(ref:org.apache.pekko.actor.typed.ActorRef[Q])(makeMessage:(I,org.apache.pekko.actor.typed.ActorRef[org.apache.pekko.pattern.StatusReply[A]])=>Q)(implicittimeout:org.apache.pekko.util.Timeout):org.apache.pekko.stream.scaladsl.Flow[I,A,org.apache.pekko.NotUsed]” java =“#askWithStatusI,Q,A:org.apache.pekko.stream.javadsl.Flow[I,A,org.apache.pekko.NotUsed]” } ActorFlow.askWithStatusActorFlow.askWithStatus { scala=“#askWithStatusI,Q,A(makeMessage:(I,org.apache.pekko.actor.typed.ActorRef[org.apache.pekko.pattern.StatusReply[A]])=>Q)(implicittimeout:org.apache.pekko.util.Timeout):org.apache.pekko.stream.scaladsl.Flow[I,A,org.apache.pekko.NotUsed]” java =“#askWithStatusI,Q,A:org.apache.pekko.stream.javadsl.Flow[I,A,org.apache.pekko.NotUsed]” }","title":"Signature"},{"location":"/stream/operators/ActorFlow/askWithStatus.html#description","text":"Use the Ask pattern to send a request-reply message to the target ref actor when you expect the reply to be org.apache.pekko.pattern.StatusReply. If any of the asks times out it will fail the stream with an AskTimeoutExceptionAskTimeoutException.\nThe askWithStatus operator requires\nthe actor ref, a makeMessage function to create the message sent to the actor from the incoming element, and the actor ref accepting the actor’s reply message a timeout.","title":"Description"},{"location":"/stream/operators/ActorFlow/askWithStatus.html#examples","text":"The ActorFlow.askWithStatus sends a message to the actor. The actor expects AskingWithStatus messages which contain the actor ref for replies of type StatusReply[String]StatusReply<String>. When the actor for replies receives a reply, the ActorFlow.askWihStatus stream stage emits the reply and the map extracts the message String.\nScala copysourceimport org.apache.pekko\nimport pekko.stream.scaladsl.{ Flow, Sink, Source }\nimport pekko.stream.typed.scaladsl.ActorFlow\nimport pekko.actor.typed.ActorRef\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.util.Timeout\n\nfinal case class Asking(s: String, replyTo: ActorRef[Reply])\nfinal case class Reply(msg: String)\n\nfinal case class AskingWithStatus(s: String, replyTo: ActorRef[StatusReply[String]])\n\n    val ref = spawn(Behaviors.receiveMessage[Asking] { asking =>\n      asking.replyTo ! Reply(asking.s + \"!!!\")\n      Behaviors.same\n    })\n\nimplicit val timeout: Timeout = 1.second\n\nval askFlow: Flow[String, Reply, NotUsed] =\n  ActorFlow.ask(ref)(Asking.apply)\n\n// explicit creation of the sent message\nval askFlowExplicit: Flow[String, Reply, NotUsed] =\n  ActorFlow.ask(ref)(makeMessage = (el, replyTo: ActorRef[Reply]) => Asking(el, replyTo))\n\nval in: Future[immutable.Seq[String]] =\n  Source(1 to 50).map(_.toString).via(askFlow).map(_.msg).runWith(Sink.seq) Java copysourceimport org.apache.pekko.actor.typed.ActorRef;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.pattern.StatusReply;\nimport org.apache.pekko.stream.javadsl.Flow;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.typed.javadsl.ActorFlow;\n\nclass Asking {\n  final String payload;\n  final ActorRef<Reply> replyTo;\n\n  public Asking(String payload, ActorRef<Reply> replyTo) {\n    this.payload = payload;\n    this.replyTo = replyTo;\n  }\n}\n\nstatic class AskingWithStatus {\n  final String payload;\n  final ActorRef<StatusReply<String>> replyTo;\n\n  public AskingWithStatus(String payload, ActorRef<StatusReply<String>> replyTo) {\n    this.payload = payload;\n    this.replyTo = replyTo;\n  }\n}\n\nclass Reply {\n  public final String msg;\n\n  public Reply(String msg) {\n    this.msg = msg;\n  }\n}\n\nfinal ActorRef<Asking> actorRef = // ???\nfinal ActorRef<AskingWithStatus> actorWithStatusRef = // ???\nDuration timeout = Duration.ofSeconds(1);\n\n// method reference notation\nFlow<String, Reply, NotUsed> askFlow = ActorFlow.ask(actorRef, timeout, Asking::new);\n\n// explicit creation of the sent message\nFlow<String, Reply, NotUsed> askFlowExplicit =\n    ActorFlow.ask(actorRef, timeout, (msg, replyTo) -> new Asking(msg, replyTo));\n\nFlow<String, String, NotUsed> askFlowExplicitWithStatus =\n    ActorFlow.askWithStatus(\n        actorWithStatusRef, timeout, (msg, replyTo) -> new AskingWithStatus(msg, replyTo));\n\nSource.repeat(\"hello\").via(askFlow).map(reply -> reply.msg).runWith(Sink.seq(), system);","title":"Examples"},{"location":"/stream/operators/ActorFlow/askWithStatus.html#reactive-streams-semantics","text":"emits when the futures (in submission order) created by the ask pattern internally are completed backpressures when the number of futures reaches the configured parallelism and the downstream backpressures completes when upstream completes and all futures have been completed and all elements have been emitted fails when the passed-in actor terminates, or when any of the askWithStatuss exceed a timeout cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/ActorFlow/askWithStatusAndContext.html","text":"","title":"ActorFlow.askWithContext"},{"location":"/stream/operators/ActorFlow/askWithStatusAndContext.html#actorflow-askwithcontext","text":"Use the “Ask Pattern” to send each stream element (without the context) as an ask to the target actor (of the new actors API), and expect a reply of Type StatusReply[T]StatusReply<T> where the T will be unwrapped and emitted downstream.\nActor interop operators","title":"ActorFlow.askWithContext"},{"location":"/stream/operators/ActorFlow/askWithStatusAndContext.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/ActorFlow/askWithStatusAndContext.html#signature","text":"ActorFlow.askWithStatusAndContextActorFlow.askWithStatusAndContext { scala=“#askWithStatusAndContextI,Q,A,Ctx(ref:org.apache.pekko.actor.typed.ActorRef[Q])(makeMessage:(I,org.apache.pekko.actor.typed.ActorRef[org.apache.pekko.pattern.StatusReply[A]])=>Q)(implicittimeout:org.apache.pekko.util.Timeout):org.apache.pekko.stream.scaladsl.Flow[(I,Ctx),(A,Ctx),org.apache.pekko.NotUsed]” java =“#askWithStatusAndContextI,Q,A,Ctx” }","title":"Signature"},{"location":"/stream/operators/ActorFlow/askWithStatusAndContext.html#description","text":"Use the Ask pattern to send a request-reply message to the target ref actor when you expect the reply to be org.apache.pekko.pattern.StatusReply. The stream context is not sent, instead it is locally recombined to the actor’s reply.\nIf any of the asks times out it will fail the stream with an AskTimeoutExceptionAskTimeoutException.\nThe ask operator requires\nthe actor ref, a makeMessage function to create the message sent to the actor from the incoming element, and the actor ref accepting the actor’s reply message a timeout.","title":"Description"},{"location":"/stream/operators/ActorFlow/askWithStatusAndContext.html#reactive-streams-semantics","text":"emits when the futures (in submission order) created by the ask pattern internally are completed backpressures when the number of futures reaches the configured parallelism and the downstream backpressures completes when upstream completes and all futures have been completed and all elements have been emitted fails when the passed-in actor terminates, or when any of the asks exceed a timeout cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/StreamConverters/asOutputStream.html","text":"","title":"StreamConverters.asOutputStream"},{"location":"/stream/operators/StreamConverters/asOutputStream.html#streamconverters-asoutputstream","text":"Create a source that materializes into an OutputStream.\nAdditional Sink and Source converters","title":"StreamConverters.asOutputStream"},{"location":"/stream/operators/StreamConverters/asOutputStream.html#signature","text":"StreamConverters.asOutputStreamStreamConverters.asOutputStream","title":"Signature"},{"location":"/stream/operators/StreamConverters/asOutputStream.html#description","text":"Create a source that materializes into an OutputStream. When bytes are written to the OutputStream they are emitted from the source.\nThe OutputStream will no longer be writable when the Source has been canceled from its downstream, and closing the OutputStream will complete the Source.","title":"Description"},{"location":"/stream/operators/StreamConverters/asOutputStream.html#reactive-streams-semantics","text":"emits when bytes are written to the OutputStream completes when the OutputStream is closed","title":"Reactive Streams semantics"},{"location":"/stream/operators/StreamConverters/asOutputStream.html#example","text":"Here is an example of a SourceSource that materializes into a java.io.OutputStream, and is connected to a Sink which concatenates the incoming ByteStringByteStrings\nScala copysourceval source: Source[ByteString, OutputStream] = StreamConverters.asOutputStream()\nval sink: Sink[ByteString, Future[ByteString]] = Sink.fold[ByteString, ByteString](ByteString.empty)(_ ++ _)\n\nval (outputStream, result): (OutputStream, Future[ByteString]) =\n  source.toMat(sink)(Keep.both).run()\n Java copysourcefinal Source<ByteString, OutputStream> source = StreamConverters.asOutputStream();\nfinal Sink<ByteString, CompletionStage<ByteString>> sink =\n    Sink.fold(emptyByteString(), (ByteString arg1, ByteString arg2) -> arg1.concat(arg2));\n\nfinal Pair<OutputStream, CompletionStage<ByteString>> output =\n    source.toMat(sink, Keep.both()).run(system);","title":"Example"},{"location":"/stream/operators/Sink/asPublisher.html","text":"","title":"Sink.asPublisher"},{"location":"/stream/operators/Sink/asPublisher.html#sink-aspublisher","text":"Integration with Reactive Streams, materializes into a org.reactivestreams.Publisher.\nSink operators","title":"Sink.asPublisher"},{"location":"/stream/operators/Sink/asPublisher.html#signature","text":"Sink.asPublisherSink.asPublisher","title":"Signature"},{"location":"/stream/operators/Sink/asPublisher.html#description","text":"This method gives you the capability to publish the data from the Sink through a Reactive Streams Publisher. Generally, in Pekko Streams a Sink is considered a subscriber, which consumes the data from source. To integrate with other Reactive Stream implementations Sink.asPublisher provides a Publisher materialized value when run. Now, the data from this publisher can be consumed by subscribing to it. We can control if we allow more than one downstream subscriber from the single running Pekko stream through the fanout parameter. In Java 9, the Reactive Stream API was included in the JDK, and Publisher is available through Flow.Publisher. Since those APIs are identical but exist at different package namespaces and does not depend on the Reactive Streams package a separate publisher sink for those is available through org.apache.pekko.stream.scaladsl.JavaFlowSupport.Sink#asPublisherorg.apache.pekko.stream.javadsl.JavaFlowSupport.Sink#asPublisher.","title":"Description"},{"location":"/stream/operators/Sink/asPublisher.html#example","text":"In the example we are using a source and then creating a Publisher. After that, we see that when fanout is true multiple subscribers can subscribe to it, but when it is false only the first subscriber will be able to subscribe and others will be rejected.\nScala copysourceval source = Source(1 to 5)\n\nval publisher = source.runWith(Sink.asPublisher(false))\nSource.fromPublisher(publisher).runWith(Sink.foreach(println)) // 1 2 3 4 5\nSource\n  .fromPublisher(publisher)\n  .runWith(Sink.foreach(println)) // No output, because the source was not able to subscribe to the publisher. Java copysourceSource<Integer, NotUsed> source = Source.range(1, 5);\n\nPublisher<Integer> publisherFalse =\n    source.runWith(Sink.asPublisher(AsPublisher.WITHOUT_FANOUT), system);\nCompletionStage<Integer> resultFromFirstSubscriberFalse =\n    Source.fromPublisher(publisherFalse)\n        .runWith(Sink.fold(0, (acc, element) -> acc + element), system);\nCompletionStage<Integer> resultFromSecondSubscriberFalse =\n    Source.fromPublisher(publisherFalse)\n        .runWith(Sink.fold(1, (acc, element) -> acc * element), system);\n\nresultFromFirstSubscriberFalse.thenAccept(System.out::println); // 15\nresultFromSecondSubscriberFalse.thenAccept(\n    System.out\n        ::println); // No output, because the source was not able to subscribe to the publisher.","title":"Example"},{"location":"/stream/operators/Sink/asPublisher.html#reactive-streams-semantics","text":"emits the materialized publisher completes after the source is consumed and materialized publisher is created","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/asSourceWithContext.html","text":"","title":"Source.asSourceWithContext"},{"location":"/stream/operators/Source/asSourceWithContext.html#source-assourcewithcontext","text":"Extracts context data from the elements of a Source so that it can be turned into a SourceWithContext which can propagate that context per element along a stream.\nSource operators","title":"Source.asSourceWithContext"},{"location":"/stream/operators/Source/asSourceWithContext.html#signature","text":"Source.asSourceWithContextSource.asSourceWithContext","title":"Signature"},{"location":"/stream/operators/Source/asSourceWithContext.html#description","text":"See Context Propagation for a general overview of context propagation.\nExtracts context data from the elements of a SourceSource so that it can be turned into a SourceWithContextSourceWithContext which can propagate that context per element along a stream. The function passed into asSourceWithContext must turn elements into contexts, one context for every element.\nSee also:\nContext Propagation Flow.asFlowWithContext Turns a Flow into a FlowWithContext which can propagate a context per element along a stream.","title":"Description"},{"location":"/stream/operators/Source/asSourceWithContext.html#example","text":"Elements from this source have a correlation number, but the flow structure should focus on the text message in the elements. asSourceWithContext chooses the second value in the tuplepair as the context. Another map operator makes the first value the stream elements in the SourceWithContext.\nScala copysourceimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.SourceWithContext\nimport scala.collection.immutable\n\n// values with their contexts as tuples\nval values: immutable.Seq[(String, Int)] = immutable.Seq(\"eins\" -> 1, \"zwei\" -> 2, \"drei\" -> 3)\n\n// a regular source with the tuples as elements\nval source: Source[(String, Int), NotUsed] = Source(values)\n\n// split the tuple into stream elements and their context\nval sourceWithContext: SourceWithContext[String, Int, NotUsed] =\n  source\n    .asSourceWithContext(_._2) // pick the second tuple element as context\n    .map(_._1) // keep the first tuple element as stream element\n\nval mapped: SourceWithContext[String, Int, NotUsed] = sourceWithContext\n  // regular operators apply to the element without seeing the context\n  .map(s => s.reverse)\n\n// running the source and asserting the outcome\nimport org.apache.pekko.stream.scaladsl.Sink\nval result = mapped.runWith(Sink.seq)\nresult.futureValue should contain theSameElementsInOrderAs immutable.Seq(\"snie\" -> 1, \"iewz\" -> 2, \"ierd\" -> 3) Java copysourceimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.japi.Pair;\nimport org.apache.pekko.stream.javadsl.*;\n\n// values with their contexts as pairs\nCollection<Pair<String, Integer>> values =\n    Arrays.asList(Pair.create(\"eins\", 1), Pair.create(\"zwei\", 2), Pair.create(\"drei\", 3));\n\n// a regular source with pairs as elements\nSource<Pair<String, Integer>, NotUsed> source = Source.from(values);\n\n// split the pair into stream elements and their context\nSourceWithContext<String, Integer, NotUsed> sourceWithContext =\n    source\n        .asSourceWithContext(Pair::second) // pick the second pair element as context\n        .map(Pair::first); // keep the first pair element as stream element\n\nSourceWithContext<String, Integer, NotUsed> mapped =\n    sourceWithContext\n        // regular operators apply to the element without seeing the context\n        .map(s -> s.replace('e', 'y'));\n\n// running the source and asserting the outcome\nCompletionStage<List<Pair<String, Integer>>> result = mapped.runWith(Sink.seq(), system);\nList<Pair<String, Integer>> list = result.toCompletableFuture().get(1, TimeUnit.SECONDS);\nassertThat(\n    list, hasItems(Pair.create(\"yins\", 1), Pair.create(\"zwyi\", 2), Pair.create(\"dryi\", 3)));","title":"Example"},{"location":"/stream/operators/Source/asSubscriber.html","text":"","title":"Source.asSubscriber"},{"location":"/stream/operators/Source/asSubscriber.html#source-assubscriber","text":"Integration with Reactive Streams, materializes into a Subscriber.\nSource operators","title":"Source.asSubscriber"},{"location":"/stream/operators/Source/asSubscriber.html#signature","text":"Scala copysourcedef asSubscriber[T]: Source[T, java.util.concurrent.Flow.Subscriber[T]] = Java copysourcestatic <T> pekko.stream.javadsl.Source<T, Subscriber<T>> asSubscriber()","title":"Signature"},{"location":"/stream/operators/Source/asSubscriber.html#description","text":"If you want to create a SourceSource that gets its elements from another library that supports Reactive Streams, you can use JavaFlowSupport.Source.asSubscriber. Each time this SourceSource is materialized, it produces a materialized value of type java.util.concurrent.Flow.Subscriber. This Subscriber can be attached to a Reactive Streams Publisher to populate it.\nIf the API you want to consume elements from provides a Publisher instead of accepting a Subscriber, see fromPublisher.\nNote For JDK 8 users: since java.util.concurrent.Flow was introduced in JDK version 9, if you are still on version 8 you may use the org.reactivestreams library with Source.asSubscriberSource.asSubscriber.","title":"Description"},{"location":"/stream/operators/Source/asSubscriber.html#example","text":"Suppose we use a database client that supports Reactive Streams, we could create a SourceSource that queries the database for its rows. That SourceSource can then be used for further processing, for example creating a SourceSource that contains the names of the rows.\nNote that since the database is queried for each materialization, the rowSource can be safely re-used. Because both the database driver and Pekko Streams support Reactive Streams, backpressure is applied throughout the stream, preventing us from running out of memory when the database rows are consumed slower than they are produced by the database.\nScala copysourceimport java.util.concurrent.Flow.Subscriber\nimport java.util.concurrent.Flow.Publisher\n\nimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.JavaFlowSupport\n\nval rowSource: Source[Row, NotUsed] =\n  JavaFlowSupport.Source.asSubscriber\n    .mapMaterializedValue((subscriber: Subscriber[Row]) => {\n      // For each materialization, fetch the rows from the database:\n      val rows: Publisher[Row] = databaseClient.fetchRows()\n      rows.subscribe(subscriber)\n      NotUsed\n    });\n\nval names: Source[String, NotUsed] =\n  // rowSource can be re-used, since it will start a new\n  // query for each materialization, fully supporting backpressure\n  // for each materialized stream:\n  rowSource.map(row => row.name) Java copysourceimport java.util.concurrent.Flow.Subscriber;\nimport java.util.concurrent.Flow.Publisher;\n\nimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.JavaFlowSupport;\n\nclass Example {\n    Source<Row, NotUsed> rowSource =\n            JavaFlowSupport.Source.<Row>asSubscriber()\n                    .mapMaterializedValue(\n                            subscriber -> {\n                                // For each materialization, fetch the rows from the database:\n                                Publisher<Row> rows = databaseClient.fetchRows();\n                                rows.subscribe(subscriber);\n\n                                return NotUsed.getInstance();\n                            });\n\n    public Source<String, NotUsed> names() {\n        // rowSource can be re-used, since it will start a new\n        // query for each materialization, fully supporting backpressure\n        // for each materialized stream:\n        return rowSource.map(row -> row.getField(\"name\"));\n    }\n}","title":"Example"},{"location":"/stream/operators/Source-or-Flow/backpressureTimeout.html","text":"","title":"backpressureTimeout"},{"location":"/stream/operators/Source-or-Flow/backpressureTimeout.html#backpressuretimeout","text":"If the time between the emission of an element and the following downstream demand exceeds the provided timeout, the stream is failed with a TimeoutException.\nTime aware operators","title":"backpressureTimeout"},{"location":"/stream/operators/Source-or-Flow/backpressureTimeout.html#signature","text":"Source.backpressureTimeoutSource.backpressureTimeout Flow.backpressureTimeoutFlow.backpressureTimeout","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/backpressureTimeout.html#description","text":"If the time between the emission of an element and the following downstream demand exceeds the provided timeout, the stream is failed with a TimeoutException. The timeout is checked periodically, so the resolution of the check is one period (equals to timeout value).","title":"Description"},{"location":"/stream/operators/Source-or-Flow/backpressureTimeout.html#reactive-streams-semantics","text":"emits when upstream emits an element backpressures when downstream backpressures completes when upstream completes or fails if timeout elapses between element emission and downstream demand. cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Balance.html","text":"","title":"Balance"},{"location":"/stream/operators/Balance.html#balance","text":"Fan-out the stream to several streams.\nFan-out operators","title":"Balance"},{"location":"/stream/operators/Balance.html#description","text":"Fan-out the stream to several streams. Each upstream element is emitted to the first available downstream consumer.","title":"Description"},{"location":"/stream/operators/Balance.html#reactive-streams-semantics","text":"emits when any of the outputs stops backpressuring; emits the element to the first available output backpressures when all of the outputs backpressure completes when upstream completes cancels depends on the eagerCancel flag. If it is true, when any downstream cancels, if false, when all downstreams cancel.","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/batch.html","text":"","title":"batch"},{"location":"/stream/operators/Source-or-Flow/batch.html#batch","text":"Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum number of batched elements is not yet reached.\nBackpressure aware operators","title":"batch"},{"location":"/stream/operators/Source-or-Flow/batch.html#signature","text":"Source.batchSource.batch Flow.batchFlow.batch","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/batch.html#description","text":"Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum number of batched elements is not yet reached. When the maximum number is reached and downstream still backpressures batch will also backpressure.\nWhen backpressure starts or there is no backpressure element is passed into a seed function to transform it to the summary type.\nWill eagerly pull elements, this behavior may result in a single pending (i.e. buffered) element which cannot be aggregated to the batched value.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/batch.html#reactive-streams-semantics","text":"emits when downstream stops backpressuring and there is a batched element available backpressures when batched elements reached the max limit of allowed batched elements & downstream backpressures completes when upstream completes and a “possibly pending” element was drained","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/batchWeighted.html","text":"","title":"batchWeighted"},{"location":"/stream/operators/Source-or-Flow/batchWeighted.html#batchweighted","text":"Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum weight batched elements is not yet reached.\nBackpressure aware operators","title":"batchWeighted"},{"location":"/stream/operators/Source-or-Flow/batchWeighted.html#signature","text":"Source.batchWeightedSource.batchWeighted Flow.batchWeightedFlow.batchWeighted","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/batchWeighted.html#description","text":"Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure and a maximum weight batched elements is not yet reached. The weight of each element is determined by applying costFn. When the maximum total weight is reached and downstream still backpressures batch will also backpressure.\nWill eagerly pull elements, this behavior may result in a single pending (i.e. buffered) element which cannot be aggregated to the batched value.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/batchWeighted.html#reactive-streams-semantics","text":"emits downstream stops backpressuring and there is a batched element available backpressures batched elements reached the max weight limit of allowed batched elements & downstream backpressures completes upstream completes and a “possibly pending” element was drained","title":"Reactive Streams semantics"},{"location":"/stream/operators/Broadcast.html","text":"","title":"Broadcast"},{"location":"/stream/operators/Broadcast.html#broadcast","text":"Emit each incoming element each of n outputs.\nFan-out operators","title":"Broadcast"},{"location":"/stream/operators/Broadcast.html#signature","text":"BroadcastBroadcast","title":"Signature"},{"location":"/stream/operators/Broadcast.html#description","text":"Emit each incoming element each of n outputs.","title":"Description"},{"location":"/stream/operators/Broadcast.html#example","text":"Here is an example that is using Broadcast to aggregate different values from a Source of integers.\nScala copysourceimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.ClosedShape\nimport pekko.stream.scaladsl.GraphDSL\nimport pekko.stream.scaladsl.RunnableGraph\nimport pekko.stream.scaladsl.Sink\nimport pekko.stream.scaladsl.Source\n\nval source: Source[Int, NotUsed] =\n  Source.fromIterator(() => Iterator.continually(ThreadLocalRandom.current().nextInt(100))).take(100)\n\nval countSink: Sink[Int, Future[Int]] = Sink.fold(0)((acc, elem) => acc + 1)\nval minSink: Sink[Int, Future[Int]] = Sink.fold(0)((acc, elem) => math.min(acc, elem))\nval maxSink: Sink[Int, Future[Int]] = Sink.fold(0)((acc, elem) => math.max(acc, elem))\n\nval (count: Future[Int], min: Future[Int], max: Future[Int]) =\n  RunnableGraph\n    .fromGraph(GraphDSL.createGraph(countSink, minSink, maxSink)(Tuple3.apply) {\n      implicit builder => (countS, minS, maxS) =>\n        import GraphDSL.Implicits._\n        val broadcast = builder.add(Broadcast[Int](3))\n        source           ~> broadcast\n        broadcast.out(0) ~> countS\n        broadcast.out(1) ~> minS\n        broadcast.out(2) ~> maxS\n        ClosedShape\n    })\n    .run() Java copysourceimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.japi.tuple.Tuple3;\nimport org.apache.pekko.stream.ClosedShape;\nimport org.apache.pekko.stream.UniformFanOutShape;\nimport org.apache.pekko.stream.javadsl.Broadcast;\nimport org.apache.pekko.stream.javadsl.Flow;\nimport org.apache.pekko.stream.javadsl.GraphDSL;\nimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.RunnableGraph;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.Source;\nimport java.util.concurrent.CompletionStage;\n\nSource<Integer, NotUsed> source = Source.range(1, 10);\n\nSink<Integer, CompletionStage<Integer>> countSink =\n    Flow.of(Integer.class).toMat(Sink.fold(0, (acc, elem) -> acc + 1), Keep.right());\nSink<Integer, CompletionStage<Integer>> minSink =\n    Flow.of(Integer.class).toMat(Sink.fold(0, Math::min), Keep.right());\nSink<Integer, CompletionStage<Integer>> maxSink =\n    Flow.of(Integer.class).toMat(Sink.fold(0, Math::max), Keep.right());\n\nfinal Tuple3<CompletionStage<Integer>, CompletionStage<Integer>, CompletionStage<Integer>>\n    result =\n        RunnableGraph.fromGraph(\n                GraphDSL.create3(\n                    countSink,\n                    minSink,\n                    maxSink,\n                    Tuple3::create,\n                    (builder, countS, minS, maxS) -> {\n                      final UniformFanOutShape<Integer, Integer> broadcast =\n                          builder.add(Broadcast.create(3));\n                      builder.from(builder.add(source)).viaFanOut(broadcast);\n                      builder.from(broadcast.out(0)).to(countS);\n                      builder.from(broadcast.out(1)).to(minS);\n                      builder.from(broadcast.out(2)).to(maxS);\n                      return ClosedShape.getInstance();\n                    }))\n            .run(system);\nNote that asynchronous boundary for the output streams must be added explicitly if it’s desired to run them in parallel.\nScala copysourceRunnableGraph.fromGraph(GraphDSL.createGraph(countSink.async, minSink.async, maxSink.async)(Tuple3.apply) {\n  implicit builder => (countS, minS, maxS) =>\n    import GraphDSL.Implicits._\n    val broadcast = builder.add(Broadcast[Int](3))\n    source           ~> broadcast\n    broadcast.out(0) ~> countS\n    broadcast.out(1) ~> minS\n    broadcast.out(2) ~> maxS\n    ClosedShape\n}) Java copysourceRunnableGraph.fromGraph(\n    GraphDSL.create3(\n        countSink.async(),\n        minSink.async(),\n        maxSink.async(),\n        Tuple3::create,\n        (builder, countS, minS, maxS) -> {\n          final UniformFanOutShape<Integer, Integer> broadcast =\n              builder.add(Broadcast.create(3));\n          builder.from(builder.add(source)).viaFanOut(broadcast);\n\n          builder.from(broadcast.out(0)).to(countS);\n          builder.from(broadcast.out(1)).to(minS);\n          builder.from(broadcast.out(2)).to(maxS);\n          return ClosedShape.getInstance();\n        }));","title":"Example"},{"location":"/stream/operators/Broadcast.html#reactive-streams-semantics","text":"emits when all of the outputs stops backpressuring and there is an input element available backpressures when any of the outputs backpressures completes when upstream completes cancels depends on the eagerCancel flag. If it is true, when any downstream cancels, if false, when all downstreams cancel.","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/buffer.html","text":"","title":"buffer"},{"location":"/stream/operators/Source-or-Flow/buffer.html#buffer","text":"Allow for a temporarily faster upstream events by buffering size elements.\nBackpressure aware operators","title":"buffer"},{"location":"/stream/operators/Source-or-Flow/buffer.html#signature","text":"Source.bufferSource.buffer Flow.bufferFlow.buffer","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/buffer.html#description","text":"Allow for a temporarily faster upstream events by buffering size elements. When the buffer is full, a new element is handled according to the specified OverflowStrategy:\nbackpressure backpressure is applied upstream dropHead drops the oldest element in the buffer to make space for the new element dropTail drops the youngest element in the buffer to make space for the new element dropBuffer drops the entire buffer and buffers the new element dropNew drops the new element fail fails the flow with a BufferOverflowException","title":"Description"},{"location":"/stream/operators/Source-or-Flow/buffer.html#reactive-streams-semantics","text":"emits when downstream stops backpressuring and there is a pending element in the buffer backpressures when OverflowStrategy is backpressure and buffer is full completes when upstream completes and buffered elements has been drained, or when OverflowStrategy is fail, the buffer is full and a new element arrives","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/cancelled.html","text":"","title":"Sink.cancelled"},{"location":"/stream/operators/Sink/cancelled.html#sink-cancelled","text":"Immediately cancel the stream\nSink operators","title":"Sink.cancelled"},{"location":"/stream/operators/Sink/cancelled.html#signature","text":"Sink.cancelledSink.cancelled","title":"Signature"},{"location":"/stream/operators/Sink/cancelled.html#description","text":"Immediately cancel the stream","title":"Description"},{"location":"/stream/operators/Sink/cancelled.html#example","text":"In this example, we have a source that generates numbers from 1 to 5 but as we have used cancelled we get NotUsed as materialized value and stream cancels.\nScala copysourceval source = Source(1 to 5)\nsource.runWith(Sink.cancelled) Java copysourceSource<Integer, NotUsed> source = Source.range(1, 5);\nNotUsed sum = source.runWith(Sink.cancelled(), system);\nreturn sum;","title":"Example"},{"location":"/stream/operators/Sink/cancelled.html#reactive-streams-semantics","text":"cancels immediately","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/collect.html","text":"","title":"collect"},{"location":"/stream/operators/Source-or-Flow/collect.html#collect","text":"Apply a partial function to each incoming element, if the partial function is defined for a value the returned value is passed downstream.\nSimple operators","title":"collect"},{"location":"/stream/operators/Source-or-Flow/collect.html#signature","text":"Source.collectSource.collect Flow.collectFlow.collect","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/collect.html#description","text":"Apply a partial function to each incoming element, if the partial function is defined for a value the returned value is passed downstream. This can often replace filter followed by map to achieve the same in one single operator.\ncollect is supposed to be used with PFBuilderPFBuilder to construct the partial function. There is also a collectType that often can be easier to use than the PFBuilder and then combine with ordinary filter and map operators.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/collect.html#example","text":"Given stream element classes Message, Ping, and Pong, where Ping extends Message and Pong is an unrelated class.\nScala copysourcetrait Message\nfinal case class Ping(id: Int) extends Message\nfinal case class Pong(id: Int) Java copysourcestatic interface Message {}\n\nstatic class Ping implements Message {\n  final int id;\n\n  Ping(int id) {\n    this.id = id;\n  }\n}\n\nstatic class Pong {\n  final int id;\n\n  Pong(int id) {\n    this.id = id;\n  }\n}\nFrom a stream of Message elements we would like to collect all elements of type Ping that have an id != 0, and then covert to Pong with same id.\nScala copysourceval flow: Flow[Message, Pong, NotUsed] =\n  Flow[Message].collect {\n    case Ping(id) if id != 0 => Pong(id)\n  } Java copysourceFlow<Message, Pong, NotUsed> flow =\n    Flow.of(Message.class)\n        .collect(\n            new PFBuilder<Message, Pong>()\n                .match(Ping.class, p -> p.id != 0, p -> new Pong(p.id))\n                .build());\nAn alternative is to use collectType. The same conversion be written as follows, and it is as efficient. Java copysourceFlow<Message, Pong, NotUsed> flow =\n    Flow.of(Message.class)\n        .collectType(Ping.class)\n        .filter(p -> p.id != 0)\n        .map(p -> new Pong(p.id));","title":"Example"},{"location":"/stream/operators/Source-or-Flow/collect.html#reactive-streams-semantics","text":"emits when the provided partial function is defined for the element backpressures the partial function is defined for the element and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/collect.html","text":"","title":"Sink.collect"},{"location":"/stream/operators/Sink/collect.html#sink-collect","text":"Collect all input elements using a Java Collector.\nSink operators","title":"Sink.collect"},{"location":"/stream/operators/Sink/collect.html#signature","text":"Sink.collectSink.collect","title":"Signature"},{"location":"/stream/operators/Sink/collect.html#description","text":"A Sink which materializes into a CompletionStage which will be completed with a result of the Java Collector transformation and reduction operations.","title":"Description"},{"location":"/stream/operators/Sink/collect.html#example","text":"Given a stream of numbers we can collect the numbers into a collection with the seq operator\nJava copysourcefinal List<Integer> list = Arrays.asList(1, 2, 3);\nCompletionStage<List<Integer>> result =\n    Source.from(list).runWith(Sink.collect(Collectors.toList()), system);","title":"Example"},{"location":"/stream/operators/Sink/collect.html#reactive-streams-semantics","text":"cancels when the Collector throws an exception backpressures when the Collector’s previous accumulation is still in progress","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/collection.html","text":"","title":"Sink.collection"},{"location":"/stream/operators/Sink/collection.html#sink-collection","text":"Collect all values emitted from the stream into a collection.Operator only available in the Scala API. The closest operator in the Java API is Sink.seq.\nSink operators\nSignature Sink.collectionSink.collection Description Collect values emitted from the stream into an arbitrary collection That. The resulting collection is available through a Future[That] or when the stream completes. Note that the collection boundaries are those defined in the CanBuildFrom associated with the chosen collection. See The Architecture of Scala 2.13’s Collections for more info. The seq operator is a shorthand for Sink.collection[T, Vector[T]]. Example This example reads the numbers from a source and stores them in the List collection. Scala copysourceval source = Source(1 to 5)\nval result: Future[List[Int]] = source.runWith(Sink.collection[Int, List[Int]])\nresult.foreach(println)\n// List(1, 2, 3, 4, 5) Reactive Streams semantics cancels If too many values are collected","title":"Sink.collection"},{"location":"/stream/operators/Source-or-Flow/collectType.html","text":"","title":"collectType"},{"location":"/stream/operators/Source-or-Flow/collectType.html#collecttype","text":"Transform this stream by testing the type of each of the elements on which the element is an instance of the provided type as they pass through this processing step.\nSimple operators","title":"collectType"},{"location":"/stream/operators/Source-or-Flow/collectType.html#signature","text":"Source.collectTypeSource.collectType Flow.collectTypeFlow.collectType","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/collectType.html#description","text":"Filter elements that is of a given type.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/collectType.html#example","text":"Given stream element classes Message, Ping, and Pong, where Ping extends Message and Pong is an unrelated class.\nScala copysourcetrait Message\nfinal case class Ping(id: Int) extends Message\nfinal case class Pong(id: Int) Java copysourcestatic interface Message {}\n\nstatic class Ping implements Message {\n  final int id;\n\n  Ping(int id) {\n    this.id = id;\n  }\n}\n\nstatic class Pong {\n  final int id;\n\n  Pong(int id) {\n    this.id = id;\n  }\n}\nFrom a stream of Message elements we would like to collect all elements of type Ping that have an id != 0, and then covert to Pong with same id.\nScala copysourceval flow: Flow[Message, Pong, NotUsed] =\n  Flow[Message].collectType[Ping].filter(_.id != 0).map(p => Pong(p.id)) Java copysourceFlow<Message, Pong, NotUsed> flow =\n    Flow.of(Message.class)\n        .collectType(Ping.class)\n        .filter(p -> p.id != 0)\n        .map(p -> new Pong(p.id));","title":"Example"},{"location":"/stream/operators/Source-or-Flow/collectType.html#reactive-streams-semantics","text":"emits when the element is of the given type backpressures the element is of the given type and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/combine.html","text":"","title":"Source.combine"},{"location":"/stream/operators/Source/combine.html#source-combine","text":"Combine several sources, using a given strategy such as merge or concat, into one source.\nSource operators","title":"Source.combine"},{"location":"/stream/operators/Source/combine.html#signature","text":"Source.combineSource.combine","title":"Signature"},{"location":"/stream/operators/Source/combine.html#description","text":"Provides a way to create a “fan-in” of multiple sources without having to use the more advanced GraphDSL.\nThe way the elements from the sources are combined is pluggable through the strategy parameter which accepts a function Int => Graph[FanInShape]Integer -> Graph<FanInShape> where the integer parameter specifies the number of sources that the graph must accept. This makes it possible to use combine with the built-in Concat and Merge by expanding their apply methods to functionsusing a method reference to their create methods, but also to use an arbitrary strategy.\nCombine is most useful when you have more sources than 2 or want to use a custom operator, as there are more concise operators for 2-source concat and merge\nSome of the built-in operators that can be used as strategy are:\nMergeMerge ConcatConcat MergePrioritizedMergePrioritized MergeLatestMergeLatest ZipNZipN ZipWithNZipWithN","title":"Description"},{"location":"/stream/operators/Source/combine.html#examples","text":"In this example we Merge three different sources of integers. The three sources will immediately start contributing elements to the combined source. The individual elements from each source will be in order but the order compared to elements from other sources is not deterministic:\nScala copysourceimport org.apache.pekko.stream.scaladsl.{ Concat, Merge, Source }\n// ...\n\nval source1 = Source(1 to 3)\nval source2 = Source(8 to 10)\nval source3 = Source(12 to 14)\nval combined = Source.combine(source1, source2, source3)(Merge(_))\ncombined.runForeach(println)\n// could print (order between sources is not deterministic)\n// 1\n// 12\n// 8\n// 9\n// 13\n// 14\n// 2\n// 10\n// 3 Java copysourceimport org.apache.pekko.stream.javadsl.Concat;\nimport org.apache.pekko.stream.javadsl.Merge;\nimport org.apache.pekko.stream.javadsl.Source;\n// ...\n\nSource<Integer, NotUsed> source1 = Source.range(1, 3);\nSource<Integer, NotUsed> source2 = Source.range(8, 10);\nSource<Integer, NotUsed> source3 = Source.range(12, 14);\n\nfinal Source<Integer, NotUsed> combined =\n    Source.combine(source1, source2, Collections.singletonList(source3), Merge::create);\n\ncombined.runForeach(System.out::println, system);\n// could print (order between sources is not deterministic)\n// 1\n// 12\n// 8\n// 9\n// 13\n// 14\n// 2\n// 10\n// 3\nIf we instead use Concat the first source will get to emit elements until it completes, then the second source until that completes and so on until all the sources has completed.\nScala copysourceval source1 = Source(1 to 3)\nval source2 = Source(8 to 10)\nval source3 = Source(12 to 14)\nval sources = Source.combine(source1, source2, source3)(Concat(_))\nsources.runForeach(println)\n// prints (order is deterministic)\n// 1\n// 2\n// 3\n// 8\n// 9\n// 10\n// 12\n// 13\n// 14 Java copysourceSource<Integer, NotUsed> source1 = Source.range(1, 3);\nSource<Integer, NotUsed> source2 = Source.range(8, 10);\nSource<Integer, NotUsed> source3 = Source.range(12, 14);\n\nfinal Source<Integer, NotUsed> sources =\n    Source.combine(source1, source2, Collections.singletonList(source3), Concat::create);\n\nsources.runForeach(System.out::println, system);\n// prints (order is deterministic)\n// 1\n// 2\n// 3\n// 8\n// 9\n// 10\n// 12\n// 13\n// 14","title":"Examples"},{"location":"/stream/operators/Source/combine.html#reactive-streams-semantics","text":"emits when there is demand, but depending on the strategy completes depends on the strategy","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/combine.html","text":"","title":"Sink.combine"},{"location":"/stream/operators/Sink/combine.html#sink-combine","text":"Combine several sinks into one using a user specified strategy\nSink operators","title":"Sink.combine"},{"location":"/stream/operators/Sink/combine.html#signature","text":"Sink.combineSink.combine","title":"Signature"},{"location":"/stream/operators/Sink/combine.html#description","text":"Combine several sinks into one using a user specified strategy","title":"Description"},{"location":"/stream/operators/Sink/combine.html#example","text":"This example shows how to combine multiple sinks with a Fan-out Junction.\nScala copysourceval sendRemotely = Sink.actorRef(actorRef, \"Done\", _ => \"Failed\")\nval localProcessing = Sink.foreach[Int](_ => /* do something useful */ ())\n\nval sink = Sink.combine(sendRemotely, localProcessing)(Broadcast[Int](_))\n\nSource(List(0, 1, 2)).runWith(sink) Java copysourceSink<Integer, NotUsed> sendRemotely = Sink.actorRef(actorRef, \"Done\");\nSink<Integer, CompletionStage<Done>> localProcessing =\n    Sink.<Integer>foreach(\n        a -> {\n          /*do something useful*/\n        });\nSink<Integer, NotUsed> sinks =\n    Sink.combine(sendRemotely, localProcessing, new ArrayList<>(), a -> Broadcast.create(a));\n\nSource.<Integer>from(Arrays.asList(new Integer[] {0, 1, 2})).runWith(sinks, system);","title":"Example"},{"location":"/stream/operators/Sink/combine.html#reactive-streams-semantics","text":"cancels depends on the strategy backpressures depends on the strategy","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/completionStage.html","text":"","title":"Source.completionStage"},{"location":"/stream/operators/Source/completionStage.html#source-completionstage","text":"Send the single value of the CompletionStage when it completes and there is demand.\nSource operators","title":"Source.completionStage"},{"location":"/stream/operators/Source/completionStage.html#signature","text":"Source.completionStageSource.completionStage","title":"Signature"},{"location":"/stream/operators/Source/completionStage.html#description","text":"Send the single value of the CompletionStage when it completes and there is demand. If the CompletionStage completes with null stage is completed without emitting a value. If the CompletionStage fails the stream is failed with that exception.\nFor the corresponding operator for the Scala standard library Future see future.","title":"Description"},{"location":"/stream/operators/Source/completionStage.html#example","text":"Java copysourceimport java.util.concurrent.CompletionStage;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.Done;\nimport org.apache.pekko.actor.typed.ActorSystem;\nimport org.apache.pekko.stream.javadsl.*;\n\nCompletionStage<Integer> stage = CompletableFuture.completedFuture(10);\n\nSource<Integer, NotUsed> source = Source.completionStage(stage);\n\nSink<Integer, CompletionStage<Done>> sink = Sink.foreach(i -> System.out.println(i.toString()));\n\nsource.runWith(sink, system); // 10","title":"Example"},{"location":"/stream/operators/Source/completionStage.html#reactive-streams-semantics","text":"emits the future completes completes after the future has completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/completionStageFlow.html","text":"","title":"Flow.completionStageFlow"},{"location":"/stream/operators/Flow/completionStageFlow.html#flow-completionstageflow","text":"Streams the elements through the given future flow once it successfully completes.\nSimple operators","title":"Flow.completionStageFlow"},{"location":"/stream/operators/Flow/completionStageFlow.html#signature","text":"Flow.completionStageFlowFlow.completionStageFlow","title":"Signature"},{"location":"/stream/operators/Flow/completionStageFlow.html#description","text":"Streams the elements through the given flow once the CompletionStage successfully completes. If the future fails the stream fails.","title":"Description"},{"location":"/stream/operators/Flow/completionStageFlow.html#examples","text":"A deferred creation of the stream based on the initial element by combining completionStageFlow with prefixAndTail like so:\nScala copysourceCompletionStage<Flow<Integer, String, NotUsed>> processingFlow(int id) {\n  return CompletableFuture.completedFuture(\n      Flow.of(Integer.class).map(n -> \"id: \" + id + \" value: \" + n));\n}\n\n  Source<String, NotUsed> source =\n      Source.range(1, 10)\n          .prefixAndTail(1)\n          .flatMapConcat(\n              (pair) -> {\n                List<Integer> head = pair.first();\n                Source<Integer, NotUsed> tail = pair.second();\n\n                int id = head.get(0);\n\n                return tail.via(Flow.completionStageFlow(processingFlow(id)));\n              });","title":"Examples"},{"location":"/stream/operators/Flow/completionStageFlow.html#reactive-streams-semantics","text":"emits when the internal flow is successfully created and it emits backpressures when the internal flow is successfully created and it backpressures completes when upstream completes and all elements have been emitted from the internal flow completes when upstream completes and all futures have been completed and all elements have been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/completionStageSink.html","text":"","title":"Sink.completionStageSink"},{"location":"/stream/operators/Sink/completionStageSink.html#sink-completionstagesink","text":"Streams the elements to the given future sink once it successfully completes.\nSink operators","title":"Sink.completionStageSink"},{"location":"/stream/operators/Sink/completionStageSink.html#description","text":"Streams the elements through the given future flow once it successfully completes. If the future fails the stream is failed.","title":"Description"},{"location":"/stream/operators/Sink/completionStageSink.html#reactive-streams-semantics","text":"cancels if the future fails or if the created sink cancels backpressures when initialized and when created sink backpressures","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/completionStageSource.html","text":"","title":"Source.completionStageSource"},{"location":"/stream/operators/Source/completionStageSource.html#source-completionstagesource","text":"Streams the elements of an asynchronous source once its given completion operator completes.\nSource operators\nSignature Source.completionStageSourceSource.completionStageSource","title":"Source.completionStageSource"},{"location":"/stream/operators/Source/completionStageSource.html#description","text":"Streams the elements of an asynchronous source once its given completion operator completes. If the completion fails the stream is failed with that exception.\nFor the corresponding operator for the Scala standard library Future see futureSource.","title":"Description"},{"location":"/stream/operators/Source/completionStageSource.html#example","text":"Suppose we are accessing a remote service that streams user data over HTTP/2 or a WebSocket. We can model that as a Source<User,NotUsed>Source[User,NotUsed] but that source will only be available once the connection has been established.\nJava copysourceimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.stream.javadsl.Source;\n\nimport java.util.concurrent.CompletionStage;\n\npublic class CompletionStageSource {\n\n  public static void sourceCompletionStageSource() {\n    UserRepository userRepository = null; // an abstraction over the remote service\n    Source<User, CompletionStage<NotUsed>> userCompletionStageSource =\n        Source.completionStageSource(userRepository.loadUsers());\n    // ...\n  }\n\n  interface UserRepository {\n    CompletionStage<Source<User, NotUsed>> loadUsers();\n  }\n\n  static class User {}\n}","title":"Example"},{"location":"/stream/operators/Source/completionStageSource.html#reactive-streams-semantics","text":"emits the next value from the asynchronous source, once its completion operator has completed completes after the asynchronous source completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/completionTimeout.html","text":"","title":"completionTimeout"},{"location":"/stream/operators/Source-or-Flow/completionTimeout.html#completiontimeout","text":"If the completion of the stream does not happen until the provided timeout, the stream is failed with a TimeoutException.\nTime aware operators","title":"completionTimeout"},{"location":"/stream/operators/Source-or-Flow/completionTimeout.html#signature","text":"Source.completionTimeoutSource.completionTimeout Flow.completionTimeoutFlow.completionTimeout","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/completionTimeout.html#description","text":"If the completion of the stream does not happen until the provided timeout, the stream is failed with a TimeoutException.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/completionTimeout.html#example","text":"This example reads the numbers from a source and do some calculation in the flow with a completion timeout of 10 milliseconds. It will fail the stream, leading to failing the materialized Future CompletionStage if the stream has not completed mapping the numbers from the source when the timeout hits.\nScala copysourceval source = Source(1 to 10000).map(number => number * number)\nsource.completionTimeout(10.milliseconds).run() Java copysourceSource<Integer, NotUsed> source = Source.range(1, 100000).map(number -> number * number);\nCompletionStage<Done> result = source.completionTimeout(Duration.ofMillis(10)).run(system);\nreturn result;","title":"Example"},{"location":"/stream/operators/Source-or-Flow/completionTimeout.html#reactive-streams-semantics","text":"emits when upstream emits an element backpressures when downstream backpressures completes when upstream completes or fails if timeout elapses before upstream completes cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/concat.html","text":"","title":"concat"},{"location":"/stream/operators/Source-or-Flow/concat.html#concat","text":"After completion of the original upstream the elements of the given source will be emitted.\nFan-in operators","title":"concat"},{"location":"/stream/operators/Source-or-Flow/concat.html#signature","text":"Source.concatSource.concat Flow.concatFlow.concat","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/concat.html#description","text":"After completion of the original upstream the elements of the given source will be emitted.\nBoth streams will be materialized together.\nNote The concat operator is for backwards compatibility reasons “detached” and will eagerly demand an element from both upstreams when the stream is materialized and will then have a one element buffer for each of the upstreams, this is most often not what you want, instead use concatLazy","title":"Description"},{"location":"/stream/operators/Source-or-Flow/concat.html#example","text":"Scala copysource val sourceA = Source(List(1, 2, 3, 4))\nval sourceB = Source(List(10, 20, 30, 40))\n\nsourceA.concat(sourceB).runWith(Sink.foreach(println))\n// prints 1, 2, 3, 4, 10, 20, 30, 40 Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(10, 20, 30, 40));\nsourceA.concat(sourceB).runForeach(System.out::println, system);\n// prints 1, 2, 3, 4, 10, 20, 30, 40","title":"Example"},{"location":"/stream/operators/Source-or-Flow/concat.html#reactive-streams-semantics","text":"emits when the current stream has an element available; if the current input completes, it tries the next one backpressures when downstream backpressures completes when all upstreams complete","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/concatAllLazy.html","text":"","title":"concatAllLazy"},{"location":"/stream/operators/Source-or-Flow/concatAllLazy.html#concatalllazy","text":"After completion of the original upstream the elements of the given sources will be emitted sequentially.\nFan-in operators","title":"concatAllLazy"},{"location":"/stream/operators/Source-or-Flow/concatAllLazy.html#signature","text":"Source.concatAllLazySource.concatAllLazy Flow.concatAllLazyFlow.concatAllLazy","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/concatAllLazy.html#description","text":"After completion of the original upstream the elements of the given sources will be emitted sequentially.\nBoth streams will be materialized together, however, the given streams will be pulled for the first time only after the original upstream was completed.\nTo defer the materialization of the given sources (or to completely avoid its materialization if the original upstream fails or cancels), wrap it into Source.lazySource.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/concatAllLazy.html#example","text":"Scala copysourceval sourceA = Source(List(1, 2, 3))\nval sourceB = Source(List(4, 5, 6))\nval sourceC = Source(List(7, 8, 9))\nsourceA\n  .concatAllLazy(sourceB, sourceC)\n  .fold(new StringJoiner(\",\"))((joiner, input) => joiner.add(String.valueOf(input)))\n  .runWith(Sink.foreach(println))\n// prints 1,2,3,4,5,6,7,8,9 Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(4, 5, 6));\nSource<Integer, NotUsed> sourceC = Source.from(Arrays.asList(7, 8, 9));\nsourceA\n    .concatAllLazy(sourceB, sourceC)\n    .fold(new StringJoiner(\",\"), (joiner, input) -> joiner.add(String.valueOf(input)))\n    .runForeach(System.out::println, system);\n// prints 1,2,3,4,5,6,7,8,9","title":"Example"},{"location":"/stream/operators/Source-or-Flow/concatAllLazy.html#reactive-streams-semantics","text":"emits when the current stream has an element available; if the current input completes, it tries the next one backpressures when downstream backpressures completes when all upstreams complete","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/concatLazy.html","text":"","title":"concatLazy"},{"location":"/stream/operators/Source-or-Flow/concatLazy.html#concatlazy","text":"After completion of the original upstream the elements of the given source will be emitted.\nFan-in operators","title":"concatLazy"},{"location":"/stream/operators/Source-or-Flow/concatLazy.html#signature","text":"Source.concatSource.concat Flow.concatFlow.concat","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/concatLazy.html#description","text":"After completion of the original upstream the elements of the given source will be emitted.\nBoth streams will be materialized together, however, the given stream will be pulled for the first time only after the original upstream was completed. (In contrast, concat, introduces single-element buffers after both, original and given sources so that the given source is also pulled once immediately.)\nTo defer the materialization of the given source (or to completely avoid its materialization if the original upstream fails or cancels), wrap it into Source.lazySource.\nIf materialized values needs to be collected concatLazyMat is available.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/concatLazy.html#example","text":"Scala copysourceval sourceA = Source(List(1, 2, 3, 4))\nval sourceB = Source(List(10, 20, 30, 40))\n\nsourceA.concatLazy(sourceB).runWith(Sink.foreach(println)) Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(10, 20, 30, 40));\nsourceA.concatLazy(sourceB).runForeach(System.out::println, system);\n// prints 1, 2, 3, 4, 10, 20, 30, 40","title":"Example"},{"location":"/stream/operators/Source-or-Flow/concatLazy.html#reactive-streams-semantics","text":"emits when the current stream has an element available; if the current input completes, it tries the next one backpressures when downstream backpressures completes when all upstreams complete","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/conflate.html","text":"","title":"conflate"},{"location":"/stream/operators/Source-or-Flow/conflate.html#conflate","text":"Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure.\nBackpressure aware operators","title":"conflate"},{"location":"/stream/operators/Source-or-Flow/conflate.html#signature","text":"Source.conflateSource.conflate Flow.conflateFlow.conflate","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/conflate.html#description","text":"Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure. The summary value must be of the same type as the incoming elements, for example the sum or average of incoming numbers, if aggregation should lead to a different type conflateWithSeed can be used:","title":"Description"},{"location":"/stream/operators/Source-or-Flow/conflate.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl.Source\n\nimport scala.concurrent.duration._\n\nSource\n  .cycle(() => List(1, 10, 100, 1000).iterator)\n  .throttle(10, per = 1.second) // faster upstream\n  .conflate((acc, el) => acc + el) // acc: Int, el: Int\n  .throttle(1, per = 1.second) // slow downstream Java copysourceSource.cycle(() -> Arrays.asList(1, 10, 100).iterator())\n    .throttle(10, Duration.ofSeconds(1)) // fast upstream\n    .conflate((Integer acc, Integer el) -> acc + el)\n    .throttle(1, Duration.ofSeconds(1)); // slow downstream\nIf downstream is slower the elements are conflated by summing them. This means that upstream can continue producing elements while downstream is applying backpressure. For example: downstream is backpressuring while 1, 10 and 100 arrives from upstream, then backpressure stops and the conflated 111 is emitted downstream.\nSee Rate transformation for more information and examples.","title":"Example"},{"location":"/stream/operators/Source-or-Flow/conflate.html#reactive-streams-semantics","text":"emits when downstream stops backpressuring and there is a conflated element available backpressures when the aggregate function cannot keep up with incoming elements completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/conflateWithSeed.html","text":"","title":"conflateWithSeed"},{"location":"/stream/operators/Source-or-Flow/conflateWithSeed.html#conflatewithseed","text":"Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure.\nBackpressure aware operators","title":"conflateWithSeed"},{"location":"/stream/operators/Source-or-Flow/conflateWithSeed.html#signature","text":"Source.conflateWithSeedSource.conflateWithSeed Flow.conflateWithSeedFlow.conflateWithSeed","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/conflateWithSeed.html#description","text":"Allow for a slower downstream by passing incoming elements and a summary into an aggregate function as long as there is backpressure. When backpressure starts or there is no backpressure element is passed into a seed function to transform it to the summary type.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/conflateWithSeed.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl.Source\n\nimport scala.concurrent.duration._\n\ncase class Summed(i: Int) {\n  def sum(other: Summed) = Summed(this.i + other.i)\n}\n\nSource\n  .cycle(() => List(1, 10, 100, 1000).iterator)\n  .throttle(10, per = 1.second) // faster upstream\n  .conflateWithSeed(el => Summed(el))((acc, el) => acc.sum(Summed(el))) // (Summed, Int) => Summed\n  .throttle(1, per = 1.second) // slow downstream Java copysourceclass Summed {\n\n  private final Integer el;\n\n  public Summed(Integer el) {\n    this.el = el;\n  }\n\n  public Summed sum(Summed other) {\n    return new Summed(this.el + other.el);\n  }\n}\n\nSource.cycle(() -> Arrays.asList(1, 10, 100).iterator())\n    .throttle(10, Duration.ofSeconds(1)) // fast upstream\n    .conflateWithSeed(Summed::new, (Summed acc, Integer el) -> acc.sum(new Summed(el)))\n    .throttle(1, Duration.ofSeconds(1)); // slow downstream\nIf downstream is slower, the “seed” function is called which is able to change the type of the to be conflated elements if needed (it can also be an identity function, in which case this conflateWithSeed is equivalent to a plain conflate). Next, the conflating function is applied while there is back-pressure from the downstream, such that the upstream can produce elements at an rate independent of the downstream.\nYou may want to use this operation for example to apply an average operation on the upstream elements, while the downstream backpressures. This allows us to keep processing upstream elements, and give an average number to the downstream once it is ready to process the next one.\nSee Rate transformation for more information and examples.","title":"Example"},{"location":"/stream/operators/Source-or-Flow/conflateWithSeed.html#reactive-streams-semantics","text":"emits when downstream stops backpressuring and there is a conflated element available backpressures when the aggregate or seed functions cannot keep up with incoming elements completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/cycle.html","text":"","title":"Source.cycle"},{"location":"/stream/operators/Source/cycle.html#source-cycle","text":"Stream iterator in cycled manner.\nSource operators","title":"Source.cycle"},{"location":"/stream/operators/Source/cycle.html#signature","text":"Source.cycleSource.cycle","title":"Signature"},{"location":"/stream/operators/Source/cycle.html#description","text":"Stream iterator in cycled manner. Internally a new iterator is being created to cycle the one provided via argument meaning when the original iterator runs out of elements to process it will start all over again from the beginning of the iterator provided by the evaluation of provided parameter. If the method argument provides an empty iterator the stream will be terminated with an exception.","title":"Description"},{"location":"/stream/operators/Source/cycle.html#examples","text":"Scala copysourceSource\n  .cycle(() => List(1, 2, 3).iterator)\n  .grouped(9)\n  .runWith(Sink.head)\n  // This will produce the Seq(1, 2, 3, 1, 2, 3, 1, 2, 3) Java copysourcefinal Source<Integer, NotUsed> source = Source.cycle(() -> Arrays.asList(1, 2, 3).iterator());\nCompletionStage<List<Integer>> result = source.grouped(9).runWith(Sink.head(), system);\nList<Integer> emittedValues = result.toCompletableFuture().get();\nassertThat(emittedValues, is(Arrays.asList(1, 2, 3, 1, 2, 3, 1, 2, 3)));\nWhen iterator is empty the stream will be terminated with IllegalArgumentException\nScala copysourceval empty = Iterator.empty\nSource\n  .cycle(() => empty)\n  .runWith(Sink.head)\n  // This will return a failed future with an `IllegalArgumentException` Java copysourceIterator<Integer> emptyIterator = Collections.<Integer>emptyList().iterator();\nSource.cycle(() -> emptyIterator)\n    .runWith(Sink.head(), system)\n    // stream will be terminated with IllegalArgumentException","title":"Examples"},{"location":"/stream/operators/Source/cycle.html#reactive-streams-semantics","text":"emits the next value returned from cycled iterator completes never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Compression/deflate.html","text":"","title":"Compression.deflate"},{"location":"/stream/operators/Compression/deflate.html#compression-deflate","text":"Creates a flow that deflate-compresses a stream of ByteStrings.\nCompression operators","title":"Compression.deflate"},{"location":"/stream/operators/Compression/deflate.html#signature","text":"Compression.deflateCompression.deflate","title":"Signature"},{"location":"/stream/operators/Compression/deflate.html#description","text":"Creates a flow that deflate-compresses a stream of ByteStrings. Note that the compressor will SYNC_FLUSH after every ByteStringByteString so that it is guaranteed that every ByteStringByteString coming out of the flow can be fully decompressed without waiting for additional data. This may come at a compression performance cost for very small chunks.\nUse the overload method with parameters to control the compression level and compatibility with GZip.","title":"Description"},{"location":"/stream/operators/Compression/deflate.html#reactive-streams-semantics","text":"emits when the compression algorithm produces output for the received ByteString backpressures when downstream backpressures completes when upstream completes (may emit finishing bytes in an extra ByteString )","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/delay.html","text":"","title":"delay"},{"location":"/stream/operators/Source-or-Flow/delay.html#delay","text":"Delay every element passed through with a specific duration.\nTimer driven operators","title":"delay"},{"location":"/stream/operators/Source-or-Flow/delay.html#signature","text":"Source.delaySource.delay Flow.delayFlow.delay","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/delay.html#description","text":"Delay every element passed through with a specific duration.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/delay.html#reactive-streams-semantics","text":"emits there is a pending element in the buffer and configured time for this element elapsed backpressures differs, depends on OverflowStrategy set completes when upstream completes and buffered elements has been drained","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/delayWith.html","text":"","title":"delayWith"},{"location":"/stream/operators/Source-or-Flow/delayWith.html#delaywith","text":"Delay every element passed through with a duration that can be controlled dynamically.\nTimer driven operators","title":"delayWith"},{"location":"/stream/operators/Source-or-Flow/delayWith.html#signature","text":"Source.delayWithSource.delayWith Flow.delayWithFlow.delayWith","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/delayWith.html#description","text":"Delay every element passed through with a duration that can be controlled dynamically, individually for each elements (via the DelayStrategy).\nemits there is a pending element in the buffer and configured time for this element elapsed backpressures differs, depends on OverflowStrategy set completes when upstream completes and buffered elements has been drained","title":"Description"},{"location":"/stream/operators/Source-or-Flow/detach.html","text":"","title":"detach"},{"location":"/stream/operators/Source-or-Flow/detach.html#detach","text":"Detach upstream demand from downstream demand without detaching the stream rates.\nSimple operators","title":"detach"},{"location":"/stream/operators/Source-or-Flow/detach.html#signature","text":"Source.detachSource.detach Flow.detachFlow.detach","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/detach.html#description","text":"Detach upstream demand from downstream demand without detaching the stream rates.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/detach.html#reactive-streams-semantics","text":"emits when the upstream operators has emitted and there is demand backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/divertTo.html","text":"","title":"divertTo"},{"location":"/stream/operators/Source-or-Flow/divertTo.html#divertto","text":"Each upstream element will either be diverted to the given sink, or the downstream consumer according to the predicate function applied to the element.\nFan-out operators","title":"divertTo"},{"location":"/stream/operators/Source-or-Flow/divertTo.html#signature","text":"Source.divertToSource.divertTo Flow.divertToFlow.divertTo","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/divertTo.html#description","text":"Each upstream element will either be diverted to the given sink, or the downstream consumer according to the predicate function applied to the element.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/divertTo.html#reactive-streams-semantics","text":"emits when the chosen output stops backpressuring and there is an input element available backpressures when the chosen output backpressures completes when upstream completes and no output is pending cancels when any of the downstreams cancel","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/drop.html","text":"","title":"drop"},{"location":"/stream/operators/Source-or-Flow/drop.html#drop","text":"Drop n elements and then pass any subsequent element downstream.\nSimple operators","title":"drop"},{"location":"/stream/operators/Source-or-Flow/drop.html#signature","text":"Source.dropSource.drop Flow.dropFlow.drop","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/drop.html#description","text":"Drop n elements and then pass any subsequent element downstream.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/drop.html#example","text":"Given a Source of numbers we can drop the first 3 elements with the drop operator:\nScala copysourceval fiveInts: Source[Int, NotUsed] = Source(1 to 5)\nval droppedThreeInts: Source[Int, NotUsed] = fiveInts.drop(3)\n\ndroppedThreeInts.runForeach(println)\n// 4\n// 5 Java copysourceSource<Integer, NotUsed> fiveIntegers = Source.from(Arrays.asList(1, 2, 3, 4, 5));\nSource<Integer, NotUsed> droppedThreeInts = fiveIntegers.drop(3);\n\ndroppedThreeInts.runForeach(System.out::println, system);\n// 4\n// 5","title":"Example"},{"location":"/stream/operators/Source-or-Flow/drop.html#reactive-streams-semantics","text":"emits when the specified number of elements has been dropped already backpressures when the specified number of elements has been dropped and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/dropWhile.html","text":"","title":"dropWhile"},{"location":"/stream/operators/Source-or-Flow/dropWhile.html#dropwhile","text":"Drop elements as long as a predicate function return true for the element\nSimple operators","title":"dropWhile"},{"location":"/stream/operators/Source-or-Flow/dropWhile.html#signature","text":"Source.dropWhileSource.dropWhile Flow.dropWhileFlow.dropWhile","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/dropWhile.html#description","text":"Drop elements as long as a predicate function return true for the element","title":"Description"},{"location":"/stream/operators/Source-or-Flow/dropWhile.html#example","text":"Given a Source of ordered numbers we can drop all the negative ones with the dropWhile operator. Mind that after the first non negative number is encountered, all the consecutive elements will be emitted despite the predicate provided.\nScala copysourceval droppedWhileNegative = Source(-3 to 3).dropWhile(_ < 0)\n\ndroppedWhileNegative.runForeach(println)\n// 0\n// 1\n// 2\n// 3 Java copysourceSource<Integer, NotUsed> droppedWhileNegative =\n    Source.from(Arrays.asList(-3, -2, -1, 0, 1, 2, 3, -1)).dropWhile(integer -> integer < 0);\n\ndroppedWhileNegative.runForeach(System.out::println, system);\n// 1\n// 2\n// 3\n// -1","title":"Example"},{"location":"/stream/operators/Source-or-Flow/dropWhile.html#reactive-streams-semantics","text":"emits when the predicate returned false and for all following stream elements backpressures predicate returned false and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/dropWithin.html","text":"","title":"dropWithin"},{"location":"/stream/operators/Source-or-Flow/dropWithin.html#dropwithin","text":"Drop elements until a timeout has fired\nTimer driven operators","title":"dropWithin"},{"location":"/stream/operators/Source-or-Flow/dropWithin.html#signature","text":"Source.dropWithinSource.dropWithin Flow.dropWithinFlow.dropWithin","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/dropWithin.html#description","text":"Drop elements until a timeout has fired","title":"Description"},{"location":"/stream/operators/Source-or-Flow/dropWithin.html#reactive-streams-semantics","text":"emits after the timer fired and a new upstream element arrives backpressures when downstream backpressures completes upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/empty.html","text":"","title":"Source.empty"},{"location":"/stream/operators/Source/empty.html#source-empty","text":"Complete right away without ever emitting any elements.\nSource operators\nSource.never a source which emits nothing and never completes.","title":"Source.empty"},{"location":"/stream/operators/Source/empty.html#signature","text":"Source.emptySource.empty","title":"Signature"},{"location":"/stream/operators/Source/empty.html#description","text":"Complete right away without ever emitting any elements. Useful when you have to provide a source to an API but there are no elements to emit.","title":"Description"},{"location":"/stream/operators/Source/empty.html#reactive-streams-semantics","text":"emits never completes directly","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/expand.html","text":"","title":"expand"},{"location":"/stream/operators/Source-or-Flow/expand.html#expand","text":"Like extrapolate, but does not have the initial argument, and the Iterator is also used in lieu of the original element, allowing for it to be rewritten and/or filtered.\nBackpressure aware operators","title":"expand"},{"location":"/stream/operators/Source-or-Flow/expand.html#signature","text":"Source.expandSource.expand Flow.expandFlow.expand","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/expand.html#description","text":"Like extrapolate, but does not have the initial argument, and the Iterator is also used in lieu of the original element, allowing for it to be rewritten and/or filtered.\nSee Understanding extrapolate and expand for more information and examples.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/expand.html#example","text":"Imagine a streaming client decoding a video. It is possible the network bandwidth is a bit unreliable. It’s fine, as long as the audio remains fluent, it doesn’t matter if we can’t decode a frame or two (or more). But we also want to watermark every decoded frame with the name of our colleague. expand provides access to the element flowing through the stream and let’s us create extra frames in case the producer slows down:\nScala copysource// each element flowing through the stream is expanded to a watermark copy\n// of the upstream frame and grayed out copies. The grayed out copies should\n// only be used downstream if the producer is too slow.\nval watermarkerRateControl: Flow[Frame, Frame, NotUsed] =\n  Flow[Frame].expand((frame: Frame) => {\n    val watermarked = frame.withFilter(Watermark)\n    val grayedOut = frame.withFilter(Gray)\n    Iterator.single(watermarked) ++ Iterator.continually(grayedOut)\n  })\n\nval watermarkedVideoSource: Source[Frame, NotUsed] =\n  networkSource.via(decode).via(rateControl)\n\n// let's create a 25fps stream (a Frame every 40.millis)\nval ticks: Source[Tick.type, Cancellable] = Source.tick(0.seconds, 40.millis, Tick)\n\nval watermarkedVideoAt25Fps: Source[Frame, Cancellable] =\n  ticks.zip(watermarkedVideoSource).map(_._2)\n Java copysource// each element flowing through the stream is expanded to a watermark copy\n// of the upstream frame and grayed out copies. The grayed out copies should\n// only be used downstream if the producer is too slow.\nFlow<Frame, Frame, NotUsed> watermarkerRateControl =\n    Flow.of(Frame.class)\n        .expand(\n            lastFrame -> {\n              Frame watermarked =\n                  new Frame(\n                      lastFrame.pixels().$plus$plus(ByteString.fromString(\" - watermark\")));\n              Frame gray =\n                  new Frame(lastFrame.pixels().$plus$plus(ByteString.fromString(\" - gray\")));\n              return Stream.concat(Stream.of(watermarked), Stream.iterate(gray, i -> i))\n                  .iterator();\n            });\n\nSource<Frame, NotUsed> watermakedVideoSource =\n    networkSource.via(decode).via(watermarkerRateControl);\n\n// let's create a 25fps stream (a Frame every 40.millis)\nSource<String, Cancellable> ticks = Source.tick(Duration.ZERO, Duration.ofMillis(40), \"tick\");\n\nSource<Frame, Cancellable> watermarkedVideoAt25Fps =\n    ticks.zip(watermakedVideoSource).map(Pair::second);","title":"Example"},{"location":"/stream/operators/Source-or-Flow/expand.html#reactive-streams-semantics","text":"emits when downstream stops backpressuring backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/extrapolate.html","text":"","title":"extrapolate"},{"location":"/stream/operators/Source-or-Flow/extrapolate.html#extrapolate","text":"Allow for a faster downstream by expanding the last emitted element to an Iterator.\nBackpressure aware operators","title":"extrapolate"},{"location":"/stream/operators/Source-or-Flow/extrapolate.html#signature","text":"Source.extrapolateSource.extrapolate Flow.extrapolateFlow.extrapolate","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/extrapolate.html#description","text":"Allow for a faster downstream by expanding the last emitted element to an Iterator. For example, an Iterator.continually(element) will cause extrapolate to keep repeating the last emitted element.\nAll original elements are always emitted unchanged - the Iterator is only used whenever there is downstream demand before upstream emits a new element.\nIncludes an optional initial argument to prevent blocking the entire stream when there are multiple producers.\nSee Understanding extrapolate and expand for more information and examples.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/extrapolate.html#example","text":"Imagine a videoconference client decoding a video feed from a colleague working remotely. It is possible the network bandwidth is a bit unreliable. It’s fine, as long as the audio remains fluent, it doesn’t matter if we can’t decode a frame or two (or more). When a frame is dropped, though, we want the UI to show the last frame decoded:\nScala copysource// if upstream is too slow, produce copies of the last frame but grayed out.\nval rateControl: Flow[Frame, Frame, NotUsed] =\n  Flow[Frame].extrapolate((frame: Frame) => {\n      val grayedOut = frame.withFilter(Gray)\n      Iterator.continually(grayedOut)\n    }, Some(Frame.blackFrame))\n\nval videoSource: Source[Frame, NotUsed] = networkSource.via(decode).via(rateControl)\n\n// let's create a 25fps stream (a Frame every 40.millis)\nval tickSource: Source[Tick.type, Cancellable] = Source.tick(0.seconds, 40.millis, Tick)\n\nval videoAt25Fps: Source[Frame, Cancellable] =\n  tickSource.zip(videoSource).map(_._2) Java copysource// if upstream is too slow, produce copies of the last frame but grayed out.\nFlow<Frame, Frame, NotUsed> rateControl =\n    Flow.of(Frame.class)\n        .extrapolate(\n            lastFrame -> {\n              Frame gray =\n                  new Frame(\n                      ByteString.fromString(\n                          \"gray frame!! - \" + lastFrame.pixels().utf8String()));\n              return Stream.iterate(gray, i -> i).iterator();\n            },\n            BLACK_FRAME // initial value\n            );\n\nSource<Frame, NotUsed> videoSource = networkSource.via(decode).via(rateControl);\n\n// let's create a 25fps stream (a Frame every 40.millis)\nSource<String, Cancellable> tickSource =\n    Source.tick(Duration.ZERO, Duration.ofMillis(40), \"tick\");\n\nSource<Frame, Cancellable> videoAt25Fps = tickSource.zip(videoSource).map(Pair::second);","title":"Example"},{"location":"/stream/operators/Source-or-Flow/extrapolate.html#reactive-streams-semantics","text":"emits when downstream stops backpressuring backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/failed.html","text":"","title":"Source.failed"},{"location":"/stream/operators/Source/failed.html#source-failed","text":"Fail directly with a user specified exception.\nSource operators","title":"Source.failed"},{"location":"/stream/operators/Source/failed.html#signature","text":"Source.failedSource.failed","title":"Signature"},{"location":"/stream/operators/Source/failed.html#description","text":"Fail directly with a user specified exception.","title":"Description"},{"location":"/stream/operators/Source/failed.html#reactive-streams-semantics","text":"emits never completes fails the stream directly with the given exception","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/filter.html","text":"","title":"filter"},{"location":"/stream/operators/Source-or-Flow/filter.html#filter","text":"Filter the incoming elements using a predicate.\nSimple operators","title":"filter"},{"location":"/stream/operators/Source-or-Flow/filter.html#signature","text":"Source.filterSource.filter Flow.filterFlow.filter","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/filter.html#description","text":"Filter the incoming elements using a predicate. If the predicate returns true the element is passed downstream, if it returns false the element is discarded.\nSee also filterNot.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/filter.html#example","text":"For example, given a Source of words we can select the longer words with the filter operator:\nScala copysourceval words: Source[String, NotUsed] =\n  Source(\n    (\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt \" +\n    \"ut labore et dolore magna aliqua\").split(\" \").toList)\n\nval longWords: Source[String, NotUsed] = words.filter(_.length > 6)\n\nlongWords.runForeach(println)\n// consectetur\n// adipiscing\n// eiusmod\n// incididunt Java copysourceSource<String, NotUsed> words =\n    Source.from(\n        Arrays.asList(\n            (\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt \"\n                    + \"ut labore et dolore magna aliqua\")\n                .split(\" \")));\n\nSource<String, NotUsed> longWords = words.filter(w -> w.length() > 6);\n\nlongWords.runForeach(System.out::println, system);\n// consectetur\n// adipiscing\n// eiusmod\n// incididunt","title":"Example"},{"location":"/stream/operators/Source-or-Flow/filter.html#reactive-streams-semantics","text":"emits when the given predicate returns true for the element backpressures when the given predicate returns true for the element and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/filter.html#api-docs","text":"Flow.filterFlow.filter","title":"API docs"},{"location":"/stream/operators/Source-or-Flow/filterNot.html","text":"","title":"filterNot"},{"location":"/stream/operators/Source-or-Flow/filterNot.html#filternot","text":"Filter the incoming elements using a predicate.\nSimple operators","title":"filterNot"},{"location":"/stream/operators/Source-or-Flow/filterNot.html#signature","text":"Source.filterNotSource.filterNot Flow.filterNotFlow.filterNot","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/filterNot.html#description","text":"Filter the incoming elements using a predicate. If the predicate returns false the element is passed downstream, if it returns true the element is discarded.\nSee also filter.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/filterNot.html#example","text":"For example, given a Source of words we can omit the shorter words with the filterNot operator:\nScala copysourceval words: Source[String, NotUsed] =\n  Source(\n    (\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt \" +\n    \"ut labore et dolore magna aliqua\").split(\" \").toList)\n\nval longWords: Source[String, NotUsed] = words.filterNot(_.length <= 6)\n\nlongWords.runForeach(println)\n// consectetur\n// adipiscing\n// eiusmod\n// incididunt Java copysourceSource<String, NotUsed> words =\n    Source.from(\n        Arrays.asList(\n            (\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt \"\n                    + \"ut labore et dolore magna aliqua\")\n                .split(\" \")));\n\nSource<String, NotUsed> longWords = words.filterNot(w -> w.length() <= 6);\n\nlongWords.runForeach(System.out::println, system);\n// consectetur\n// adipiscing\n// eiusmod\n// incididunt","title":"Example"},{"location":"/stream/operators/Source-or-Flow/filterNot.html#reactive-streams-semantics","text":"emits when the given predicate returns false for the element backpressures when the given predicate returns false for the element and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/filterNot.html#api-docs","text":"Flow.filterNotFlow.filterNot","title":"API docs"},{"location":"/stream/operators/Source-or-Flow/flatMapConcat.html","text":"","title":"flatMapConcat"},{"location":"/stream/operators/Source-or-Flow/flatMapConcat.html#flatmapconcat","text":"Transform each input element into a Source whose elements are then flattened into the output stream through concatenation.\nNesting and flattening operators","title":"flatMapConcat"},{"location":"/stream/operators/Source-or-Flow/flatMapConcat.html#signature","text":"Flow.flatMapConcatFlow.flatMapConcat","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/flatMapConcat.html#description","text":"Transform each input element into a Source whose elements are then flattened into the output stream through concatenation. This means each source is fully consumed before consumption of the next source starts.\nSee also: flatMapMerge, mapConcat","title":"Description"},{"location":"/stream/operators/Source-or-Flow/flatMapConcat.html#example","text":"In the following example flatMapConcat is used to create a Source for each incoming customerId. This could be, for example, a calculation or a query to a database. Each customer is then passed to lookupCustomerEvents which returns a Source. All the events for a customer are delivered before moving to the next customer.\nScala copysourceval source: Source[String, NotUsed] = Source(List(\"customer-1\", \"customer-2\"))\n\n// e.g. could b a query to a database\ndef lookupCustomerEvents(customerId: String): Source[String, NotUsed] = {\n  Source(List(s\"$customerId-event-1\", s\"$customerId-event-2\"))\n}\n\nsource.flatMapConcat(customerId => lookupCustomerEvents(customerId)).runForeach(println)\n\n// prints - events from each customer consecutively\n// customer-1-event-1\n// customer-1-event-2\n// customer-2-event-1\n// customer-2-event-2 Java copysource// e.g. could be a query to a database\nprivate Source<String, NotUsed> lookupCustomerEvents(String customerId) {\n  return Source.from(Arrays.asList(customerId + \"-event-1\", customerId + \"-event-2\"));\n}\n  Source.from(Arrays.asList(\"customer-1\", \"customer-2\"))\n      .flatMapConcat(this::lookupCustomerEvents)\n      .runForeach(System.out::println, system);\n  // prints - events from each customer consecutively\n  // customer-1-event-1\n  // customer-1-event-2\n  // customer-2-event-1\n  // customer-2-event-2","title":"Example"},{"location":"/stream/operators/Source-or-Flow/flatMapConcat.html#reactive-streams-semantics","text":"emits when the current consumed substream has an element available backpressures when downstream backpressures completes when upstream completes and all consumed substreams complete","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/flatMapMerge.html","text":"","title":"flatMapMerge"},{"location":"/stream/operators/Source-or-Flow/flatMapMerge.html#flatmapmerge","text":"Transform each input element into a Source whose elements are then flattened into the output stream through merging.\nNesting and flattening operators","title":"flatMapMerge"},{"location":"/stream/operators/Source-or-Flow/flatMapMerge.html#signature","text":"Flow.flatMapMergeFlow.flatMapMerge","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/flatMapMerge.html#description","text":"Transform each input element into a Source whose elements are then flattened into the output stream through merging. The maximum number of merged sources has to be specified. When this is met flatMapMerge does not request any more elements meaning that it back pressures until one of the existing Sources completes. Order of the elements for each Source is preserved but there is no deterministic order between elements from different active Sources.\nSee also: flatMapConcat, mapConcat","title":"Description"},{"location":"/stream/operators/Source-or-Flow/flatMapMerge.html#example","text":"In the following example flatMapMerge is used to create a Source for each incoming customerId. This could, for example, be a calculation or a query to a database. There can be breadth active sources at any given time so events for different customers could interleave in any order but events for the same customer will be in the order emitted by the underlying Source;\nScala copysourceval source: Source[String, NotUsed] = Source(List(\"customer-1\", \"customer-2\"))\n\n// e.g. could b a query to a database\ndef lookupCustomerEvents(customerId: String): Source[String, NotUsed] = {\n  Source(List(s\"$customerId-evt-1\", s\"$customerId-evt2\"))\n}\n\nsource.flatMapMerge(10, customerId => lookupCustomerEvents(customerId)).runForeach(println)\n\n// prints - events from different customers could interleave\n// customer-1-evt-1\n// customer-2-evt-1\n// customer-1-evt-2\n// customer-2-evt-2 Java copysource// e.g. could be a query to a database\nprivate Source<String, NotUsed> lookupCustomerEvents(String customerId) {\n  return Source.from(Arrays.asList(customerId + \"-evt-1\", customerId + \"-evt-2\"));\n}\n  Source.from(Arrays.asList(\"customer-1\", \"customer-2\"))\n      .flatMapMerge(10, this::lookupCustomerEvents)\n      .runForeach(System.out::println, system);\n  // prints - events from different customers could interleave\n  // customer-1-evt-1\n  // customer-2-evt-1\n  // customer-1-evt-2\n  // customer-2-evt-2","title":"Example"},{"location":"/stream/operators/Source-or-Flow/flatMapMerge.html#reactive-streams-semantics","text":"emits when one of the currently consumed substreams has an element available backpressures when downstream backpressures or the max number of substreams is reached completes when upstream completes and all consumed substreams complete","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/flatMapPrefix.html","text":"","title":"flatMapPrefix"},{"location":"/stream/operators/Source-or-Flow/flatMapPrefix.html#flatmapprefix","text":"Use the first n elements from the stream to determine how to process the rest.\nNesting and flattening operators","title":"flatMapPrefix"},{"location":"/stream/operators/Source-or-Flow/flatMapPrefix.html#signature","text":"Source.flatMapPrefixSource.flatMapPrefix Flow.flatMapPrefixFlow.flatMapPrefix","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/flatMapPrefix.html#description","text":"Take up to n elements from the stream (less than n only if the upstream completes before emitting n elements), then apply f on these elements in order to obtain a flow, this flow is then materialized and the rest of the input is processed by this flow (similar to via). This method returns a flow consuming the rest of the stream producing the materialized flow’s output.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/flatMapPrefix.html#reactive-streams-semantics","text":"emits when the materialized flow emits. Notice the first n elements are buffered internally before materializing the flow and connecting it to the rest of the upstream - producing elements at its own discretion (might ‘swallow’ or multiply elements). backpressures when the materialized flow backpressures completes the materialized flow completes. If upstream completes before producing n elements, f will be applied with the provided elements, the resulting flow will be materialized and signalled for upstream completion, it can then or continue to emit elements at its own discretion.","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/flattenOptional.html","text":"","title":"Flow.flattenOptional"},{"location":"/stream/operators/Flow/flattenOptional.html#flow-flattenoptional","text":"Collect the value of Optional from all the elements passing through this flow , empty Optional is filtered out.\nSimple operators","title":"Flow.flattenOptional"},{"location":"/stream/operators/Flow/flattenOptional.html#signature","text":"Flow.flattenOptionalFlow.flattenOptional","title":"Signature"},{"location":"/stream/operators/Flow/flattenOptional.html#description","text":"Streams the elements through the given future flow once it successfully completes. If the future fails the stream is failed.","title":"Description"},{"location":"/stream/operators/Flow/flattenOptional.html#reactive-streams-semantics","text":"Emits when the current Optional’s value is present. Backpressures when the value of the current Optional is present and downstream backpressures. Completes when upstream completes. Cancels when downstream cancels.","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/fold.html","text":"","title":"fold"},{"location":"/stream/operators/Source-or-Flow/fold.html#fold","text":"Start with current value zero and then apply the current and next value to the given function. When upstream completes, the current value is emitted downstream.\nSimple operators","title":"fold"},{"location":"/stream/operators/Source-or-Flow/fold.html#signature","text":"Source.foldSource.fold Flow.foldFlow.fold","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/fold.html#description","text":"Start with current value zero and then apply the current and next value to the given function. When upstream completes, the current value is emitted downstream.\nWarning Note that the zero value must be immutable, because otherwise the same mutable instance would be shared across different threads when running the stream more than once.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/fold.html#example","text":"fold is typically used to ‘fold up’ the incoming values into an aggregate. For example, you might want to summarize the incoming values into a histogram:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.ActorSystem\nimport pekko.stream.scaladsl.Source\n\ncase class Histogram(low: Long = 0, high: Long = 0) {\n  def add(i: Int): Histogram = if (i < 100) copy(low = low + 1) else copy(high = high + 1)\n}\nSource(1 to 150).fold(Histogram())((acc, n) => acc.add(n)).runForeach(println)\n\n// Prints: Histogram(99,51) Java copysource public CompletionStage<Histogram> addAsync(Integer n) {\n  if (n < 100) {\n    return CompletableFuture.supplyAsync(() -> new Histogram(low + 1L, high));\n  } else {\n    return CompletableFuture.supplyAsync(() -> new Histogram(low, high + 1L));\n  }\n}\n\n// Folding over the numbers from 1 to 150:\nSource.range(1, 150)\n    .fold(Histogram.INSTANCE, Histogram::add)\n    .runForeach(h -> System.out.println(\"Histogram(\" + h.low + \", \" + h.high + \")\"), system);\n\n// Prints: Histogram(99, 51)","title":"Example"},{"location":"/stream/operators/Source-or-Flow/fold.html#reactive-streams-semantics","text":"emits when upstream completes backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/fold.html","text":"","title":"Sink.fold"},{"location":"/stream/operators/Sink/fold.html#sink-fold","text":"Fold over emitted element with a function, where each invocation will get the new element and the result from the previous fold invocation.\nSink operators","title":"Sink.fold"},{"location":"/stream/operators/Sink/fold.html#signature","text":"Sink.foldSink.fold","title":"Signature"},{"location":"/stream/operators/Sink/fold.html#description","text":"Fold over emitted element with a function, where each invocation will get the new element and the result from the previous fold invocation. The first invocation will be provided the zero value.\nMaterializes into a Future CompletionStage that will complete with the last state when the stream has completed.\nThis operator allows combining values into a result without a global mutable state by instead passing the state along between invocations.","title":"Description"},{"location":"/stream/operators/Sink/fold.html#example","text":"This example reads the numbers from a source and do some calculation in the flow part and in the end uses Sink.fold and adds the incoming elements.\nScala copysourceval source = Source(1 to 100)\nval result: Future[Int] = source.runWith(Sink.fold(0)((acc, element) => acc + element))\nresult.map(println)\n// 5050 Java copysourceSource<Integer, NotUsed> source = Source.range(1, 100);\nCompletionStage<Integer> sum =\n    source.runWith(Sink.fold(0, (res, element) -> res + element), system);\nsum.thenAccept(System.out::println);","title":"Example"},{"location":"/stream/operators/Sink/fold.html#reactive-streams-semantics","text":"cancels never backpressures when the previous fold function invocation has not yet completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/foldAsync.html","text":"","title":"foldAsync"},{"location":"/stream/operators/Source-or-Flow/foldAsync.html#foldasync","text":"Just like fold but receives a function that results in a Future CompletionStage to the next value.\nSimple operators","title":"foldAsync"},{"location":"/stream/operators/Source-or-Flow/foldAsync.html#signature","text":"Source.foldAsyncSource.foldAsync Flow.foldAsyncFlow.foldAsync","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/foldAsync.html#description","text":"Just like fold but receives a function that results in a Future CompletionStage to the next value.\nWarning Note that the zero value must be immutable, because otherwise the same mutable instance would be shared across different threads when running the stream more than once.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/foldAsync.html#example","text":"foldAsync is typically used to ‘fold up’ the incoming values into an aggregate asynchronously. For example, you might want to summarize the incoming values into a histogram:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.ActorSystem\nimport pekko.stream.scaladsl.Source\n\nimport scala.concurrent.{ ExecutionContext, Future }\ncase class Histogram(low: Long = 0, high: Long = 0) {\n  def add(i: Int): Future[Histogram] =\n    if (i < 100) Future { copy(low = low + 1) }\n    else Future { copy(high = high + 1) }\n}\n\nSource(1 to 150).foldAsync(Histogram())((acc, n) => acc.add(n)).runForeach(println)\n\n// Prints: Histogram(99,51) Java copysourceclass Histogram {\n  final long low;\n  final long high;\n\n  private Histogram(long low, long high) {\n    this.low = low;\n    this.high = high;\n  }\n\n  // Immutable start value\n  public static Histogram INSTANCE = new Histogram(0L, 0L);\n\n  public CompletionStage<Histogram> addAsync(Integer n) {\n    if (n < 100) {\n      return CompletableFuture.supplyAsync(() -> new Histogram(low + 1L, high));\n    } else {\n      return CompletableFuture.supplyAsync(() -> new Histogram(low, high + 1L));\n    }\n  }\n}\n\n  // Folding over the numbers from 1 to 150:\n  Source.range(1, 150)\n      .foldAsync(Histogram.INSTANCE, Histogram::addAsync)\n      .runForeach(h -> System.out.println(\"Histogram(\" + h.low + \", \" + h.high + \")\"), system);\n\n  // Prints: Histogram(99, 51)","title":"Example"},{"location":"/stream/operators/Source-or-Flow/foldAsync.html#reactive-streams-semantics","text":"emits when upstream completes and the last Future CompletionStage is resolved backpressures when downstream backpressures completes when upstream completes and the last Future CompletionStage is resolved","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/foreach.html","text":"","title":"Sink.foreach"},{"location":"/stream/operators/Sink/foreach.html#sink-foreach","text":"Invoke a given procedure for each element received.\nSink operators","title":"Sink.foreach"},{"location":"/stream/operators/Sink/foreach.html#signature","text":"Sink.foreachSink.foreach","title":"Signature"},{"location":"/stream/operators/Sink/foreach.html#description","text":"Invoke a given procedure for each element received. Note that it is not safe to mutate shared state from the procedure.\nThe sink materializes into a Future[Done] CompletionStage<Done> which completes when the stream completes, or fails if the stream fails.\nNote that it is not safe to mutate state from the procedure.\nSee also:\nforeachAsync Invoke a given procedure asynchronously for each element received. actorRef Send the elements from the stream to an ActorRef.","title":"Description"},{"location":"/stream/operators/Sink/foreach.html#example","text":"This prints out every element to standard out.\nScala copysourceval printlnSink: Sink[Any, Future[Done]] = Sink.foreach(println)\nval f = Source(1 to 4).runWith(printlnSink)\nval done = Await.result(f, 100.millis)\n// will print\n// 1\n// 2\n// 3\n// 4 Java copysourceSink<Integer, CompletionStage<Done>> printlnSink = Sink.foreach(System.out::println);\nCompletionStage<Done> cs = Source.from(Arrays.asList(1, 2, 3, 4)).runWith(printlnSink, system);\nDone done = cs.toCompletableFuture().get(100, TimeUnit.MILLISECONDS);\n// will print\n// 1\n// 2\n// 3\n// 4","title":"Example"},{"location":"/stream/operators/Sink/foreach.html#reactive-streams-semantics","text":"cancels never backpressures when the previous procedure invocation has not yet completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/foreachAsync.html","text":"","title":"Sink.foreachAsync"},{"location":"/stream/operators/Sink/foreachAsync.html#sink-foreachasync","text":"Invoke a given procedure asynchronously for each element received.\nSink operators","title":"Sink.foreachAsync"},{"location":"/stream/operators/Sink/foreachAsync.html#signature","text":"Sink.foreachAsyncSink.foreachAsync","title":"Signature"},{"location":"/stream/operators/Sink/foreachAsync.html#description","text":"Invoke a given procedure asynchronously for each element received. Note that if shared state is mutated from the procedure that must be done in a thread-safe way.\nThe sink materializes into a Future[Done] CompletionStage<Done> which completes when the stream completes, or fails if the stream fails.\nSee also:\nforeach Invoke a given procedure for each element received. actorRef Send the elements from the stream to an ActorRef.","title":"Description"},{"location":"/stream/operators/Sink/foreachAsync.html#example","text":"Scala copysource// def asyncProcessing(value: Int): Future[Unit] = _\n\nSource(1 to 100).runWith(Sink.foreachAsync(10)(asyncProcessing)) Java copysource// final Function<Integer, CompletionStage<Void>> asyncProcessing = _\n\nfinal Source<Integer, NotUsed> numberSource = Source.range(1, 100);\n\nnumberSource.runWith(Sink.foreachAsync(10, asyncProcessing), system);","title":"Example"},{"location":"/stream/operators/Sink/foreachAsync.html#reactive-streams-semantics","text":"cancels when a Future CompletionStage fails backpressures when the number of Futures CompletionStages reaches the configured parallelism","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/foreachParallel.html","text":"","title":"Sink.foreachParallel"},{"location":"/stream/operators/Sink/foreachParallel.html#sink-foreachparallel","text":"Like foreach but allows up to parallellism procedure calls to happen in parallel.\nSink operators","title":"Sink.foreachParallel"},{"location":"/stream/operators/Sink/foreachParallel.html#reactive-streams-semantics","text":"cancels never backpressures when the previous parallel procedure invocations has not yet completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/from.html","text":"","title":"Source.applySource.from"},{"location":"/stream/operators/Source/from.html#","text":"Stream the values of an immutable.SeqIterable.\nSource operators","title":"Source.applySource.from"},{"location":"/stream/operators/Source/from.html#signature","text":"Source.applySource.apply\nSource.fromSource.from","title":"Signature"},{"location":"/stream/operators/Source/from.html#description","text":"Stream the values of an immutable.SeqIterable. Make sure the Iterable is immutable or at least not modified after being used as a source. Otherwise the stream may fail with ConcurrentModificationException or other more subtle errors may occur.","title":"Description"},{"location":"/stream/operators/Source/from.html#examples","text":"Java copysourceimport org.apache.pekko.Done;\nimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.actor.testkit.typed.javadsl.ManualTime;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestKitJunitResource;\nimport org.apache.pekko.stream.javadsl.Source;\n\nimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.stream.OverflowStrategy;\nimport org.apache.pekko.stream.CompletionStrategy;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.testkit.TestProbe;\n\nimport org.apache.pekko.stream.javadsl.RunnableGraph;\nimport java.util.concurrent.CompletableFuture;\n\nimport java.util.Arrays;\nimport java.util.Optional;\n\nSource<Integer, NotUsed> ints = Source.from(Arrays.asList(0, 1, 2, 3, 4, 5));\nints.runForeach(System.out::println, system);\n\nString text =\n    \"Perfection is finally attained not when there is no longer more to add,\"\n        + \"but when there is no longer anything to take away.\";\nSource<String, NotUsed> words = Source.from(Arrays.asList(text.split(\"\\\\s\")));\nwords.runForeach(System.out::println, system);","title":"Examples"},{"location":"/stream/operators/Source/from.html#reactive-streams-semantics","text":"emits the next value of the seq completes when the last element of the seq has been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/fromCompletionStage.html","text":"","title":"Source.fromCompletionStage"},{"location":"/stream/operators/Source/fromCompletionStage.html#source-fromcompletionstage","text":"Deprecated by Source.completionStage.\nSource operators","title":"Source.fromCompletionStage"},{"location":"/stream/operators/Source/fromCompletionStage.html#signature","text":"Source.fromCompletionStageSource.fromCompletionStage","title":"Signature"},{"location":"/stream/operators/Source/fromCompletionStage.html#description","text":"fromCompletionStage has been deprecated in 2.6.0, use completionStage instead.\nSend the single value of the CompletionStage when it completes and there is demand. If the CompletionStage completes with null stage is completed without emitting a value. If the CompletionStage fails the stream is failed with that exception.","title":"Description"},{"location":"/stream/operators/Source/fromCompletionStage.html#reactive-streams-semantics","text":"emits the future completes completes after the future has completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/FileIO/fromFile.html","text":"","title":"FileIO.fromFile"},{"location":"/stream/operators/FileIO/fromFile.html#fileio-fromfile","text":"Emits the contents of a file.\nFile IO Sinks and Sources\nWarning The fromFile operator has been deprecated, use fromPath instead.","title":"FileIO.fromFile"},{"location":"/stream/operators/FileIO/fromFile.html#signature","text":"FileIO.fromFileFileIO.fromFile","title":"Signature"},{"location":"/stream/operators/FileIO/fromFile.html#description","text":"Emits the contents of a file, as ByteStrings, materializes into a Future CompletionStage which will be completed with a IOResult upon reaching the end of the file or if there is a failure.","title":"Description"},{"location":"/stream/operators/Source/fromFuture.html","text":"","title":"Source.fromFuture"},{"location":"/stream/operators/Source/fromFuture.html#source-fromfuture","text":"Deprecated by Source.future.\nSource operators","title":"Source.fromFuture"},{"location":"/stream/operators/Source/fromFuture.html#signature","text":"Source.fromFutureSource.fromFuture","title":"Signature"},{"location":"/stream/operators/Source/fromFuture.html#description","text":"fromFuture has been deprecated in 2.6.0, use future instead.\nSend the single value of the Future when it completes and there is demand. If the future fails the stream is failed with that exception.","title":"Description"},{"location":"/stream/operators/Source/fromFuture.html#reactive-streams-semantics","text":"emits the future completes completes after the future has completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/fromFutureSource.html","text":"","title":"Source.fromFutureSource"},{"location":"/stream/operators/Source/fromFutureSource.html#source-fromfuturesource","text":"Deprecated by Source.futureSource.\nSource operators","title":"Source.fromFutureSource"},{"location":"/stream/operators/Source/fromFutureSource.html#signature","text":"Source.fromFutureSourceSource.fromFutureSource","title":"Signature"},{"location":"/stream/operators/Source/fromFutureSource.html#description","text":"fromFutureSource has been deprecated in 2.6.0, use futureSource instead.\nStreams the elements of the given future source once it successfully completes. If the future fails the stream is failed.","title":"Description"},{"location":"/stream/operators/Source/fromFutureSource.html#reactive-streams-semantics","text":"emits the next value from the future source, once it has completed completes after the future source completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/StreamConverters/fromInputStream.html","text":"","title":"StreamConverters.fromInputStream"},{"location":"/stream/operators/StreamConverters/fromInputStream.html#streamconverters-frominputstream","text":"Create a source that wraps an InputStream.\nAdditional Sink and Source converters","title":"StreamConverters.fromInputStream"},{"location":"/stream/operators/StreamConverters/fromInputStream.html#signature","text":"StreamConverters.fromInputStreamStreamConverters.fromInputStream","title":"Signature"},{"location":"/stream/operators/StreamConverters/fromInputStream.html#description","text":"Creates a Source from a java.io.InputStream created by the given function. Emitted elements are up to chunkSize sized ByteStringByteStrings elements. The actual size of the emitted elements depends on how much data the underlying java.io.InputStream returns on each read invocation. Such chunks will never be larger than chunkSize though.\nYou can configure the default dispatcher for this Source by changing the pekko.stream.materializer.blocking-io-dispatcher or set it for a given Source by using org.apache.pekko.stream.ActorAttributes.\nIt materializes a CompletionStageFuture of IOResult containing the number of bytes read from the source file upon completion, and a possible exception if IO operation was not completed successfully. Note that bytes having been read by the source does not give any guarantee that the bytes were seen by downstream stages.\nThe created InputStream will be closed when the Source is cancelled.\nSee also fromOutputStream","title":"Description"},{"location":"/stream/operators/StreamConverters/fromInputStream.html#example","text":"Here is an example using both fromInputStream and fromOutputStream to read from a java.io.InputStream, uppercase the read content and write back out into a java.io.OutputStream.\nScala copysourceval bytes = \"Some random input\".getBytes\nval inputStream = new ByteArrayInputStream(bytes)\nval outputStream = new ByteArrayOutputStream()\n\nval source: Source[ByteString, Future[IOResult]] = StreamConverters.fromInputStream(() => inputStream)\n\nval toUpperCase: Flow[ByteString, ByteString, NotUsed] = Flow[ByteString].map(_.map(_.toChar.toUpper.toByte))\n\nval sink: Sink[ByteString, Future[IOResult]] = StreamConverters.fromOutputStream(() => outputStream)\n\nval eventualResult = source.via(toUpperCase).runWith(sink)\n Java copysourcejava.io.InputStream inputStream = new ByteArrayInputStream(bytes);\nSource<ByteString, CompletionStage<IOResult>> source =\n    StreamConverters.fromInputStream(() -> inputStream);\n\n// Given a ByteString produces a ByteString with the upperCase representation\n// Removed from the sample for brevity.\n// Flow<ByteString, ByteString, NotUsed> toUpperCase = ...\n\njava.io.OutputStream outputStream = new ByteArrayOutputStream();\nSink<ByteString, CompletionStage<IOResult>> sink =\n    StreamConverters.fromOutputStream(() -> outputStream);\n\nCompletionStage<IOResult> ioResultCompletionStage =\n    source.via(toUpperCase).runWith(sink, system);\n// When the ioResultCompletionStage completes, the byte array backing the outputStream\n// will contain the uppercase representation of the bytes read from the inputStream.","title":"Example"},{"location":"/stream/operators/Source/fromIterator.html","text":"","title":"Source.fromIterator"},{"location":"/stream/operators/Source/fromIterator.html#source-fromiterator","text":"Stream the values from an Iterator, requesting the next value when there is demand.\nSource operators","title":"Source.fromIterator"},{"location":"/stream/operators/Source/fromIterator.html#signature","text":"Source.fromIteratorSource.fromIterator","title":"Signature"},{"location":"/stream/operators/Source/fromIterator.html#description","text":"Stream the values from an Iterator, requesting the next value when there is demand. The iterator will be created anew for each materialization, which is the reason the method factory takes a function Creator rather than an Iterator directly.\nIf the iterator perform blocking operations, make sure to run it on a separate dispatcher.","title":"Description"},{"location":"/stream/operators/Source/fromIterator.html#example","text":"Scala copysourceSource.fromIterator(() => (1 to 3).iterator).runForeach(println)\n// could print\n// 1\n// 2\n// 3 Java copysourceSource.fromIterator(() -> Arrays.asList(1, 2, 3).iterator())\n    .runForeach(System.out::println, system);\n// could print\n// 1\n// 2\n// 3","title":"Example"},{"location":"/stream/operators/Source/fromIterator.html#reactive-streams-semantics","text":"emits the next value returned from the iterator completes when the iterator reaches its end","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/fromJavaStream.html","text":"","title":"fromJavaStream"},{"location":"/stream/operators/Source/fromJavaStream.html#fromjavastream","text":"Stream the values from a Java 8 Stream, requesting the next value when there is demand.\nSource operators","title":"fromJavaStream"},{"location":"/stream/operators/Source/fromJavaStream.html#signature","text":"StreamConverters.fromJavaStreamStreamConverters.fromJavaStream","title":"Signature"},{"location":"/stream/operators/Source/fromJavaStream.html#description","text":"Stream the values from a Java 8 Stream, requesting the next value when there is demand. The iterator will be created anew for each materialization, which is the reason the method factory takes a function Creator rather than an Stream directly.\nYou can use Source.async to create asynchronous boundaries between synchronous java stream and the rest of flow.","title":"Description"},{"location":"/stream/operators/Source/fromJavaStream.html#example","text":"Scala copysourceSource.fromJavaStream(() => IntStream.rangeClosed(1, 3)).runForeach(println)\n// could print\n// 1\n// 2\n// 3 Java copysourceSource.fromJavaStream(() -> IntStream.rangeClosed(1, 3))\n    .runForeach(System.out::println, system);\n// could print\n// 1\n// 2\n// 3","title":"Example"},{"location":"/stream/operators/Source/fromJavaStream.html#reactive-streams-semantics","text":"emits the next value returned from the iterator completes when the iterator reaches its end","title":"Reactive Streams semantics"},{"location":"/stream/operators/StreamConverters/fromJavaStream.html","text":"","title":"StreamConverters.fromJavaStream"},{"location":"/stream/operators/StreamConverters/fromJavaStream.html#streamconverters-fromjavastream","text":"Create a source that wraps a Java 8 java.util.stream.Stream.\nAdditional Sink and Source converters","title":"StreamConverters.fromJavaStream"},{"location":"/stream/operators/StreamConverters/fromJavaStream.html#signature","text":"StreamConvertersStreamConverters","title":"Signature"},{"location":"/stream/operators/StreamConverters/fromJavaStream.html#example","text":"Here is an example of a SourceSource created from a java.util.stream.Stream.\nScala copysourceimport java.util.stream\nimport java.util.stream.IntStream\n\nimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.scaladsl.Keep\nimport pekko.stream.scaladsl.Sink\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.StreamConverters\ndef factory(): IntStream = IntStream.rangeClosed(0, 9)\nval source: Source[Int, NotUsed] = StreamConverters.fromJavaStream(() => factory()).map(_.intValue())\nval sink: Sink[Int, Future[immutable.Seq[Int]]] = Sink.seq[Int]\n\nval futureInts: Future[immutable.Seq[Int]] = source.toMat(sink)(Keep.right).run()\n Java copysourceimport org.apache.pekko.japi.function.Creator;\nimport org.apache.pekko.stream.Materializer;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.StreamConverters;\n\nimport java.util.concurrent.CompletionStage;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.stream.BaseStream;\nimport java.util.stream.IntStream;\nimport java.util.stream.Stream;\n\nCreator<BaseStream<Integer, IntStream>> creator = () -> IntStream.rangeClosed(0, 9);\nSource<Integer, NotUsed> source = StreamConverters.fromJavaStream(creator);\n\nSink<Integer, CompletionStage<Integer>> sink = Sink.last();\n\nCompletionStage<Integer> integerCompletionStage = source.runWith(sink, system);","title":"Example"},{"location":"/stream/operators/Source-or-Flow/fromMaterializer.html","text":"","title":"fromMaterializer"},{"location":"/stream/operators/Source-or-Flow/fromMaterializer.html#frommaterializer","text":"Defer the creation of a Source/Flow until materialization and access Materializer and Attributes\nSimple operators","title":"fromMaterializer"},{"location":"/stream/operators/Source-or-Flow/fromMaterializer.html#signature","text":"Source.fromMaterializerSource.fromMaterializer Flow.fromMaterializerFlow.fromMaterializer","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/fromMaterializer.html#description","text":"Typically used when access to materializer is needed to run a different stream during the construction of a source/flow. Can also be used to access the underlying ActorSystem from Materializer.","title":"Description"},{"location":"/stream/operators/Sink/fromMaterializer.html","text":"","title":"Sink.fromMaterializer"},{"location":"/stream/operators/Sink/fromMaterializer.html#sink-frommaterializer","text":"Defer the creation of a Sink until materialization and access Materializer and Attributes\nSink operators","title":"Sink.fromMaterializer"},{"location":"/stream/operators/Sink/fromMaterializer.html#signature","text":"Sink.fromMaterializerSink.fromMaterializer","title":"Signature"},{"location":"/stream/operators/Sink/fromMaterializer.html#description","text":"Typically used when access to materializer is needed to run a different stream during the construction of a sink. Can also be used to access the underlying ActorSystem from Materializer.","title":"Description"},{"location":"/stream/operators/StreamConverters/fromOutputStream.html","text":"","title":"StreamConverters.fromOutputStream"},{"location":"/stream/operators/StreamConverters/fromOutputStream.html#streamconverters-fromoutputstream","text":"Create a sink that wraps an OutputStream.\nAdditional Sink and Source converters","title":"StreamConverters.fromOutputStream"},{"location":"/stream/operators/StreamConverters/fromOutputStream.html#signature","text":"StreamConverters.fromOutputStreamStreamConverters.fromOutputStream","title":"Signature"},{"location":"/stream/operators/StreamConverters/fromOutputStream.html#description","text":"Creates a Sink which writes incoming ByteStringByteStrings to a java.io.OutputStream created by the given function.\nMaterializes a CompletionStageFuture of IOResult that will be completed with the size of the file (in bytes) on completion, and a possible exception if IO operation was not completed successfully.\nYou can configure the default dispatcher for this Source by changing the org.apache.pekko.stream.materializer.blocking-io-dispatcher or set it for a given Source by using org.apache.pekko.stream.ActorAttributes.\nIf autoFlush is true the OutputStream will be flushed whenever a byte array is written, defaults to false. The OutputStream will be closed when the stream flowing into this Sink is completed. The Sink will cancel the stream when the OutputStream is no longer writable.\nSee also fromInputStream","title":"Description"},{"location":"/stream/operators/StreamConverters/fromOutputStream.html#example","text":"Here is an example using both fromInputStream and fromOutputStream to read from a java.io.InputStream, uppercase the read content and write back out into a java.io.OutputStream.\nScala copysourceval bytes = \"Some random input\".getBytes\nval inputStream = new ByteArrayInputStream(bytes)\nval outputStream = new ByteArrayOutputStream()\n\nval source: Source[ByteString, Future[IOResult]] = StreamConverters.fromInputStream(() => inputStream)\n\nval toUpperCase: Flow[ByteString, ByteString, NotUsed] = Flow[ByteString].map(_.map(_.toChar.toUpper.toByte))\n\nval sink: Sink[ByteString, Future[IOResult]] = StreamConverters.fromOutputStream(() => outputStream)\n\nval eventualResult = source.via(toUpperCase).runWith(sink)\n Java copysourcejava.io.InputStream inputStream = new ByteArrayInputStream(bytes);\nSource<ByteString, CompletionStage<IOResult>> source =\n    StreamConverters.fromInputStream(() -> inputStream);\n\n// Given a ByteString produces a ByteString with the upperCase representation\n// Removed from the sample for brevity.\n// Flow<ByteString, ByteString, NotUsed> toUpperCase = ...\n\njava.io.OutputStream outputStream = new ByteArrayOutputStream();\nSink<ByteString, CompletionStage<IOResult>> sink =\n    StreamConverters.fromOutputStream(() -> outputStream);\n\nCompletionStage<IOResult> ioResultCompletionStage =\n    source.via(toUpperCase).runWith(sink, system);\n// When the ioResultCompletionStage completes, the byte array backing the outputStream\n// will contain the uppercase representation of the bytes read from the inputStream.","title":"Example"},{"location":"/stream/operators/FileIO/fromPath.html","text":"","title":"FileIO.fromPath"},{"location":"/stream/operators/FileIO/fromPath.html#fileio-frompath","text":"Emits the contents of a file from the given path.\nFile IO Sinks and Sources","title":"FileIO.fromPath"},{"location":"/stream/operators/FileIO/fromPath.html#signature","text":"FileIO.fromPathFileIO.fromPath","title":"Signature"},{"location":"/stream/operators/FileIO/fromPath.html#description","text":"Emits the contents of a file from the given path, as ByteStrings, materializes into a Future CompletionStage which will be completed with a IOResult upon reaching the end of the file or if there is a failure.","title":"Description"},{"location":"/stream/operators/FileIO/fromPath.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl._\nval file = Paths.get(\"example.csv\")\n\nval foreach: Future[IOResult] = FileIO.fromPath(file).to(Sink.ignore).run() Java copysourcefinal Path file = Paths.get(\"example.csv\");\n  Sink<ByteString, CompletionStage<Done>> printlnSink =\n      Sink.<ByteString>foreach(chunk -> System.out.println(chunk.utf8String()));\n\n  CompletionStage<IOResult> ioResult = FileIO.fromPath(file).to(printlnSink).run(system);","title":"Example"},{"location":"/stream/operators/Source/fromPublisher.html","text":"","title":"Source.fromPublisher"},{"location":"/stream/operators/Source/fromPublisher.html#source-frompublisher","text":"Integration with Reactive Streams, subscribes to a Publisher.\nSource operators","title":"Source.fromPublisher"},{"location":"/stream/operators/Source/fromPublisher.html#signature","text":"Scala copysourcedef fromPublisher[T](publisher: java.util.concurrent.Flow.Publisher[T]): Source[T, NotUsed] = Java copysourcestatic <T> pekko.stream.javadsl.Source<T, NotUsed> fromPublisher(Publisher<T> publisher)","title":"Signature"},{"location":"/stream/operators/Source/fromPublisher.html#description","text":"If you want to create a SourceSource that gets its elements from another library that supports Reactive Streams, you can use JavaFlowSupport.Source.fromPublisher. This source will produce the elements from the Publisher, and coordinate backpressure as needed.\nIf the API you want to consume elements from accepts a Subscriber instead of providing a Publisher, see asSubscriber.\nNote For JDK 8 users: since java.util.concurrent.Flow was introduced in JDK version 9, if you are still on version 8 you may use the org.reactivestreams library with Source.fromPublisherSource.fromPublisher.","title":"Description"},{"location":"/stream/operators/Source/fromPublisher.html#example","text":"Suppose we use a database client that supports Reactive Streams, we could create a SourceSource that queries the database for its rows. That SourceSource can then be used for further processing, for example creating a SourceSource that contains the names of the rows.\nBecause both the database driver and Pekko Streams support Reactive Streams, backpressure is applied throughout the stream, preventing us from running out of memory when the database rows are consumed slower than they are produced by the database.\nScala copysourceimport java.util.concurrent.Flow.Subscriber\nimport java.util.concurrent.Flow.Publisher\n\nimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.JavaFlowSupport\n\nval names: Source[String, NotUsed] =\n  // A new subscriber will subscribe to the supplied publisher for each\n  // materialization, so depending on whether the database client supports\n  // this the Source can be materialized more than once.\n  JavaFlowSupport.Source.fromPublisher(databaseClient.fetchRows())\n    .map(row => row.name) Java copysourceimport java.util.concurrent.Flow.Publisher;\n\nimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.JavaFlowSupport;\n\nclass Example {\n    public Source<String, NotUsed> names() {\n        // A new subscriber will subscribe to the supplied publisher for each\n        // materialization, so depending on whether the database client supports\n        // this the Source can be materialized more than once.\n        return JavaFlowSupport.Source.<Row>fromPublisher(databaseClient.fetchRows())\n            .map(row -> row.getField(\"name\"));\n    }\n}","title":"Example"},{"location":"/stream/operators/Flow/fromSinkAndSource.html","text":"","title":"Flow.fromSinkAndSource"},{"location":"/stream/operators/Flow/fromSinkAndSource.html#flow-fromsinkandsource","text":"Creates a Flow from a Sink and a Source where the Flow’s input will be sent to the Sink and the Flow ’s output will come from the Source.\nFlow operators composed of Sinks and Sources","title":"Flow.fromSinkAndSource"},{"location":"/stream/operators/Flow/fromSinkAndSource.html#signature","text":"Flow.fromSinkAndSourceFlow.fromSinkAndSource","title":"Signature"},{"location":"/stream/operators/Flow/fromSinkAndSource.html#description","text":"fromSinkAndSource combines a separate Sink and Source into a Flow.\nUseful in many cases where an API requires a Flow but you want to provide a Sink and Source whose flows of elements are decoupled.\nNote that termination events, like completion and cancellation, are not automatically propagated through to the “other-side” of the such-composed Flow. The Source can complete and the sink will continue to accept elements.\nUse fromSinkAndSourceCoupled if you want to couple termination of both of the ends.","title":"Description"},{"location":"/stream/operators/Flow/fromSinkAndSource.html#examples","text":"One use case is constructing a TCP server where requests and responses do not map 1:1 (like it does in the Echo TCP server sample where every incoming test is echoed back) but allows separate flows of elements from the client to the server and from the server to the client.\nThis example cancels the incoming stream, not allowing the client to write more messages, switching the TCP connection to “half-closed”, but keeps streaming periodic output to the client:\nScala copysource// close in immediately\nval sink = Sink.cancelled[ByteString]\n// periodic tick out\nval source =\n  Source.tick(1.second, 1.second, \"tick\").map(_ => ByteString(System.currentTimeMillis().toString + \"\\n\"))\n\nval serverFlow = Flow.fromSinkAndSource(sink, source)\n\nTcp(system).bind(\"127.0.0.1\", 9999, halfClose = true).runForeach { incomingConnection =>\n  incomingConnection.handleWith(serverFlow)\n} Java copysource// close in immediately\nSink<ByteString, NotUsed> sink = Sink.cancelled();\n// periodic tick out\nSource<ByteString, Cancellable> source =\n    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), \"tick\")\n        .map(tick -> ByteString.fromString(System.currentTimeMillis() + \"\\n\"));\n\nFlow<ByteString, ByteString, NotUsed> serverFlow = Flow.fromSinkAndSource(sink, source);\n\nSource<Tcp.IncomingConnection, CompletionStage<Tcp.ServerBinding>> connectionStream =\n    Tcp.get(system)\n        .bind(\n            \"127.0.0.1\", // interface\n            9999, // port\n            100, // backlog\n            Collections.emptyList(), // socket options\n            true, // Important: half close enabled\n            Optional.empty() // idle timeout\n            );\n\nconnectionStream.runForeach(\n    incomingConnection -> incomingConnection.handleWith(serverFlow, system), system);\nWith this server running you could use telnet 127.0.0.1 9999 to see a stream of timestamps being printed, one every second.\nThe following sample is a little bit more advanced and uses the MergeHubMergeHub to dynamically merge incoming messages to a single stream which is then fed into a BroadcastHubBroadcastHub which emits elements over a dynamic set of downstreams allowing us to create a simplistic little TCP chat server in which a text entered from one client is emitted to all connected clients.\nScala copysourceval (sink, source) = MergeHub.source[String].toMat(BroadcastHub.sink[String])(Keep.both).run()\n\nval framing = Framing.delimiter(ByteString(\"\\n\"), 1024)\n\nval sinkWithFraming = framing.map(bytes => bytes.utf8String).to(sink)\nval sourceWithFraming = source.map(text => ByteString(text + \"\\n\"))\n\nval serverFlow = Flow.fromSinkAndSource(sinkWithFraming, sourceWithFraming)\n\nTcp(system).bind(\"127.0.0.1\", 9999).runForeach { incomingConnection =>\n  incomingConnection.handleWith(serverFlow)\n} Java copysourcePair<Sink<String, NotUsed>, Source<String, NotUsed>> pair =\n    MergeHub.of(String.class).toMat(BroadcastHub.of(String.class), Keep.both()).run(system);\nSink<String, NotUsed> sink = pair.first();\nSource<String, NotUsed> source = pair.second();\n\nFlow<ByteString, ByteString, NotUsed> framing =\n    Framing.delimiter(ByteString.fromString(\"\\n\"), 1024);\n\nSink<ByteString, NotUsed> sinkWithFraming =\n    framing.map(bytes -> bytes.utf8String()).to(pair.first());\nSource<ByteString, NotUsed> sourceWithFraming =\n    source.map(text -> ByteString.fromString(text + \"\\n\"));\n\nFlow<ByteString, ByteString, NotUsed> serverFlow =\n    Flow.fromSinkAndSource(sinkWithFraming, sourceWithFraming);\n\nTcp.get(system)\n    .bind(\"127.0.0.1\", 9999)\n    .runForeach(\n        incomingConnection -> incomingConnection.handleWith(serverFlow, system), system);\nThe same patterns can also be applied to Pekko HTTP WebSockets which also have an API accepting a Flow of messages.\nIf we would replace the fromSinkAndSource here with fromSinkAndSourceCoupled it would allow the client to close the connection by closing its outgoing stream.\nfromSinkAndSource can also be useful when testing a component that takes a Flow allowing for complete separate control and assertion of incoming and outgoing elements using stream testkit test probes for sink and source:\nScala copysourceval inProbe = TestSubscriber.probe[String]()\nval outProbe = TestPublisher.probe[String]()\nval testFlow = Flow.fromSinkAndSource(Sink.fromSubscriber(inProbe), Source.fromPublisher(outProbe))\n\nmyApiThatTakesAFlow(testFlow)\ninProbe.expectNext(\"first\")\noutProbe.expectRequest()\noutProbe.sendError(new RuntimeException(\"test error\"))\n// ... Java copysourceTestSubscriber.Probe<String> inProbe = TestSubscriber.probe(system);\nTestPublisher.Probe<String> outProbe = TestPublisher.probe(0, system);\nFlow<String, String, NotUsed> testFlow =\n    Flow.fromSinkAndSource(Sink.fromSubscriber(inProbe), Source.fromPublisher(outProbe));\n\nmyApiThatTakesAFlow(testFlow);\ninProbe.expectNext(\"first\");\noutProbe.expectRequest();\noutProbe.sendError(new RuntimeException(\"test error\"));\n// ...","title":"Examples"},{"location":"/stream/operators/Flow/fromSinkAndSource.html#reactive-streams-semantics","text":"emits when the Source emits backpressures when the Sink backpressures completes when the Source has completed and the Sink has cancelled.","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/fromSinkAndSourceCoupled.html","text":"","title":"Flow.fromSinkAndSourceCoupled"},{"location":"/stream/operators/Flow/fromSinkAndSourceCoupled.html#flow-fromsinkandsourcecoupled","text":"Allows coupling termination (cancellation, completion, erroring) of Sinks and Sources while creating a Flow between them.\nFlow operators composed of Sinks and Sources","title":"Flow.fromSinkAndSourceCoupled"},{"location":"/stream/operators/Flow/fromSinkAndSourceCoupled.html#signature","text":"Flow.fromSinkAndSourceCoupledFlow.fromSinkAndSourceCoupled","title":"Signature"},{"location":"/stream/operators/Flow/fromSinkAndSourceCoupled.html#description","text":"See Flow.fromSinkAndSource for docs on the general workings and examples.\nThis operator only adds coupled termination to what fromSinkAndSource does: If the emitted Flow gets a cancellation, the Source is cancelled, however the Sink will also be completed. The table below illustrates the effects in detail:\nReturned Flow Sink (in) Source (out) cause: upstream (sink-side) receives completion effect: receives completion effect: receives cancel cause: upstream (sink-side) receives error effect: receives error effect: receives cancel cause: downstream (source-side) receives cancel effect: completes effect: receives cancel effect: cancels upstream, completes downstream effect: completes cause: signals complete effect: cancels upstream, errors downstream effect: receives error cause: signals error or throws effect: cancels upstream, completes downstream cause: cancels effect: receives cancel\nThe order in which the in and out sides receive their respective completion signals is not defined, do not rely on its ordering.","title":"Description"},{"location":"/stream/operators/Source/fromSourceCompletionStage.html","text":"","title":"Source.fromSourceCompletionStage"},{"location":"/stream/operators/Source/fromSourceCompletionStage.html#source-fromsourcecompletionstage","text":"Deprecated by Source.completionStageSource.\nSource operators","title":"Source.fromSourceCompletionStage"},{"location":"/stream/operators/Source/fromSourceCompletionStage.html#signature","text":"","title":"Signature"},{"location":"/stream/operators/Source/fromSourceCompletionStage.html#description","text":"fromSourceCompletionStage has been deprecated in 2.6.0, use completionStageSource instead.\nStreams the elements of an asynchronous source once its given completion operator completes. If the completion fails the stream is failed with that exception.","title":"Description"},{"location":"/stream/operators/Source/fromSourceCompletionStage.html#reactive-streams-semantics","text":"emits the next value from the asynchronous source, once its completion operator has completed completes after the asynchronous source completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/fromSubscriber.html","text":"","title":"Sink.fromSubscriber"},{"location":"/stream/operators/Sink/fromSubscriber.html#sink-fromsubscriber","text":"Integration with Reactive Streams, wraps a org.reactivestreams.Subscriber as a sink.\nSink operators","title":"Sink.fromSubscriber"},{"location":"/stream/operators/Sink/fromSubscriber.html#signature","text":"Sink.fromSubscriberSink.fromSubscriber","title":"Signature"},{"location":"/stream/operators/Sink/fromSubscriber.html#description","text":"TODO: We would welcome help on contributing descriptions and examples, see: https://github.com/akka/akka/issues/25646","title":"Description"},{"location":"/stream/operators/Source/future.html","text":"","title":"Source.future"},{"location":"/stream/operators/Source/future.html#source-future","text":"Send the single value of the Future when it completes and there is demand.\nSource operators","title":"Source.future"},{"location":"/stream/operators/Source/future.html#signature","text":"Source.futureSource.future","title":"Signature"},{"location":"/stream/operators/Source/future.html#description","text":"Send the single value of the Future when it completes and there is demand. If the future fails the stream is failed with that exception.\nFor the corresponding operator for the Java standard library CompletionStage see completionStage.","title":"Description"},{"location":"/stream/operators/Source/future.html#example","text":"Scala copysource import org.apache.pekko\nimport pekko.actor.ActorSystem\nimport pekko.stream.scaladsl._\nimport pekko.{ Done, NotUsed }\n\nimport scala.concurrent.Future\n\nval source: Source[Int, NotUsed] = Source.future(Future.successful(10))\nval sink: Sink[Int, Future[Done]] = Sink.foreach((i: Int) => println(i))\n\nval done: Future[Done] = source.runWith(sink) // 10","title":"Example"},{"location":"/stream/operators/Source/future.html#reactive-streams-semantics","text":"emits the future completes completes after the future has completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/futureFlow.html","text":"","title":"Flow.futureFlow"},{"location":"/stream/operators/Flow/futureFlow.html#flow-futureflow","text":"Streams the elements through the given future flow once it successfully completes.\nSimple operators","title":"Flow.futureFlow"},{"location":"/stream/operators/Flow/futureFlow.html#signature","text":"Flow.futureFlowFlow.futureFlow","title":"Signature"},{"location":"/stream/operators/Flow/futureFlow.html#description","text":"Streams the elements through the given future flow once it successfully completes. If the future fails the stream is failed.","title":"Description"},{"location":"/stream/operators/Flow/futureFlow.html#examples","text":"A deferred creation of the stream based on the initial element can be achieved by combining futureFlow with prefixAndTail like so:\nScala copysourcedef processingFlow(id: Int): Future[Flow[Int, String, NotUsed]] =\n  Future {\n    Flow[Int].map(n => s\"id: $id, value: $n\")\n  }\n\nval source: Source[String, NotUsed] =\n  Source(1 to 10).prefixAndTail(1).flatMapConcat {\n    case (List(id: Int), tail) =>\n      // base the Future flow creation on the first element\n      tail.via(Flow.futureFlow(processingFlow(id)))\n  }","title":"Examples"},{"location":"/stream/operators/Flow/futureFlow.html#reactive-streams-semantics","text":"emits when the internal flow is successfully created and it emits backpressures when the internal flow is successfully created and it backpressures completes when upstream completes and all elements have been emitted from the internal flow completes when upstream completes and all futures have been completed and all elements have been emitted cancels when downstream cancels (keep reading) The operator’s default behaviour in case of downstream cancellation before nested flow materialization (future completion) is to cancel immediately. This behaviour can be controlled by setting the org.apache.pekko.stream.Attributes.NestedMaterializationCancellationPolicy.PropagateToNested attribute, this will delay downstream cancellation until nested flow’s materialization which is then immediately cancelled (with the original cancellation cause).","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/futureSink.html","text":"","title":"Sink.futureSink"},{"location":"/stream/operators/Sink/futureSink.html#sink-futuresink","text":"Streams the elements to the given future sink once it successfully completes.\nSink operators","title":"Sink.futureSink"},{"location":"/stream/operators/Sink/futureSink.html#signature","text":"Sink.futureSinkSink.futureSink","title":"Signature"},{"location":"/stream/operators/Sink/futureSink.html#description","text":"Streams the elements through the given future flow once it successfully completes. If the future fails the stream is failed.","title":"Description"},{"location":"/stream/operators/Sink/futureSink.html#reactive-streams-semantics","text":"cancels if the future fails or if the created sink cancels backpressures when initialized and when created sink backpressures","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/futureSource.html","text":"","title":"Source.futureSource"},{"location":"/stream/operators/Source/futureSource.html#source-futuresource","text":"Streams the elements of the given future source once it successfully completes.\nSource operators","title":"Source.futureSource"},{"location":"/stream/operators/Source/futureSource.html#signature","text":"Source.futureSourceSource.futureSource","title":"Signature"},{"location":"/stream/operators/Source/futureSource.html#description","text":"Streams the elements of the given future source once it successfully completes. If the future fails the stream is failed.\nFor the corresponding operator for the Java standard library CompletionStage see completionStageSource.","title":"Description"},{"location":"/stream/operators/Source/futureSource.html#example","text":"Suppose we are accessing a remote service that streams user data over HTTP/2 or a WebSocket. We can model that as a Source<User,NotUsed>Source[User,NotUsed] but that source will only be available once the connection has been established.\nScala copysource import org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.scaladsl.Source\n\nimport scala.concurrent.Future\n\nobject FutureSource {\n  def sourceCompletionStageSource(): Unit = {\n    val userRepository: UserRepository = ??? // an abstraction over the remote service\n    val userFutureSource = Source.futureSource(userRepository.loadUsers)\n    // ...\n  }\n\n  trait UserRepository {\n    def loadUsers: Future[Source[User, NotUsed]]\n  }\n\n  case class User()\n}","title":"Example"},{"location":"/stream/operators/Source/futureSource.html#reactive-streams-semantics","text":"emits the next value from the future source, once it has completed completes after the future source completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/groupBy.html","text":"","title":"groupBy"},{"location":"/stream/operators/Source-or-Flow/groupBy.html#groupby","text":"Demultiplex the incoming stream into separate output streams.\nNesting and flattening operators","title":"groupBy"},{"location":"/stream/operators/Source-or-Flow/groupBy.html#signature","text":"Source.groupBySource.groupBy Flow.groupByFlow.groupBy","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/groupBy.html#description","text":"This operation demultiplexes the incoming stream into separate output streams, one for each element key. The key is computed for each element using the given function. When a new key is encountered for the first time a new substream is opened and subsequently fed with all elements belonging to that key.\nNote: If allowClosedSubstreamRecreation is set to true substream completion and incoming elements are subject to race-conditions. If elements arrive for a stream that is in the process of closing these elements might get lost.\nWarning If allowClosedSubstreamRecreation is set to false (default behavior) the operators keeps track of all keys of streams that have already been closed. If you expect an infinite number of keys this can cause memory issues. Elements belonging to those keys are drained directly and not send to the substream.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/groupBy.html#example","text":"Scala copysourceSource(1 to 10)\n  .groupBy(maxSubstreams = 2, _ % 2 == 0) // create two sub-streams with odd and even numbers\n  .reduce(_ + _) // for each sub-stream, sum its elements\n  .mergeSubstreams // merge back into a stream\n  .runForeach(println)\n// 25\n// 30 Java copysourceSource.range(1, 10)\n    .groupBy(2, i -> i % 2 == 0) // create two sub-streams with odd and even numbers\n    .reduce(Integer::sum) // for each sub-stream, sum its elements\n    .mergeSubstreams() // merge back into a stream\n    .runForeach(System.out::println, system);\n// 25\n// 30","title":"Example"},{"location":"/stream/operators/Source-or-Flow/groupBy.html#reactive-streams-semantics","text":"emits an element for which the grouping function returns a group that has not yet been created. Emits the new group there is an element pending for a group whose substream backpressures completes when upstream completes (Until the end of stream it is not possible to know whether new substreams will be needed or not)","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/grouped.html","text":"","title":"grouped"},{"location":"/stream/operators/Source-or-Flow/grouped.html#grouped","text":"Accumulate incoming events until the specified number of elements have been accumulated and then pass the collection of elements downstream.\nSimple operators","title":"grouped"},{"location":"/stream/operators/Source-or-Flow/grouped.html#signature","text":"Source.groupedSource.grouped Flow.groupedFlow.grouped","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/grouped.html#description","text":"Accumulate incoming events until the specified number of elements have been accumulated and then pass the collection of elements downstream.\nSee also:\ngroupedWeighted for a variant that groups based on element weight groupedWithin for a variant that groups based on number of elements and a time window groupedWeightedWithin for a variant that groups based on element weight and a time window","title":"Description"},{"location":"/stream/operators/Source-or-Flow/grouped.html#examples","text":"The below example demonstrates how grouped groups the accumulated elements into Seq List and maps with other operation.\nScala copysourceSource(1 to 7).grouped(3).runForeach(println)\n// Vector(1, 2, 3)\n// Vector(4, 5, 6)\n// Vector(7)\n\nSource(1 to 7).grouped(3).map(_.sum).runForeach(println)\n// 6   (= 1 + 2 + 3)\n// 15  (= 4 + 5 + 6)\n// 7   (= 7) Java copysourceSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7))\n    .grouped(3)\n    .runForeach(System.out::println, system);\n// [1, 2, 3]\n// [4, 5, 6]\n// [7]\n\nSource.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7))\n    .grouped(3)\n    .map(g -> g.stream().reduce(0, Integer::sum))\n    .runForeach(System.out::println, system);\n// 6   (= 1 + 2 + 3)\n// 15  (= 4 + 5 + 6)\n// 7   (= 7)","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/grouped.html#reactive-streams-semantics","text":"emits when the specified number of elements has been accumulated or upstream completed backpressures when a group has been assembled and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/groupedWeighted.html","text":"","title":"groupedWeighted"},{"location":"/stream/operators/Source-or-Flow/groupedWeighted.html#groupedweighted","text":"Accumulate incoming events until the combined weight of elements is greater than or equal to the minimum weight and then pass the collection of elements downstream.\nSimple operators","title":"groupedWeighted"},{"location":"/stream/operators/Source-or-Flow/groupedWeighted.html#signature","text":"Source.groupedWeightedSource.groupedWeighted Flow.groupedWeightedFlow.groupedWeighted","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/groupedWeighted.html#description","text":"Chunk up this stream into groups of elements that have a cumulative weight greater than or equal to the minWeight, with the last group possibly smaller than requested minWeight due to end-of-stream.\nSee also:\ngrouped for a variant that groups based on number of elements groupedWithin for a variant that groups based on number of elements and a time window groupedWeightedWithin for a variant that groups based on element weight and a time window","title":"Description"},{"location":"/stream/operators/Source-or-Flow/groupedWeighted.html#examples","text":"The below example demonstrates how groupedWeighted groups the accumulated elements into Seq List and maps with other operation.\nScala copysourceval collections = immutable.Iterable(Seq(1, 2), Seq(3, 4), Seq(5, 6))\nSource[Seq[Int]](collections).groupedWeighted(4)(_.length).runForeach(println)\n// Vector(Seq(1, 2), Seq(3, 4))\n// Vector(Seq(5, 6))\n\nSource[Seq[Int]](collections).groupedWeighted(3)(_.length).runForeach(println)\n// Vector(Seq(1, 2), Seq(3, 4))\n// Vector(Seq(5, 6)) Java copysourceSource.from(Arrays.asList(Arrays.asList(1, 2), Arrays.asList(3, 4), Arrays.asList(5, 6)))\n    .groupedWeighted(4, x -> (long) x.size())\n    .runForeach(System.out::println, system);\n// [[1, 2], [3, 4]]\n// [[5, 6]]\n\nSource.from(Arrays.asList(Arrays.asList(1, 2), Arrays.asList(3, 4), Arrays.asList(5, 6)))\n    .groupedWeighted(3, x -> (long) x.size())\n    .runForeach(System.out::println, system);\n// [[1, 2], [3, 4]]\n// [[5, 6]]","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/groupedWeighted.html#reactive-streams-semantics","text":"emits when the cumulative weight of elements is greater than or equal to the minimum weight or upstream completed backpressures when a group has been assembled and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/groupedWeightedWithin.html","text":"","title":"groupedWeightedWithin"},{"location":"/stream/operators/Source-or-Flow/groupedWeightedWithin.html#groupedweightedwithin","text":"Chunk up this stream into groups of elements received within a time window, or limited by the weight of the elements, whatever happens first.\nTimer driven operators","title":"groupedWeightedWithin"},{"location":"/stream/operators/Source-or-Flow/groupedWeightedWithin.html#signature","text":"Source.groupedWeightedWithinSource.groupedWeightedWithin Flow.groupedWeightedWithinFlow.groupedWeightedWithin","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/groupedWeightedWithin.html#description","text":"Chunk up this stream into groups of elements received within a time window, or limited by the weight and number of the elements, whatever happens first. Empty groups will not be emitted if no elements are received from upstream. The last group before end-of-stream will contain the buffered elements since the previously emitted group.\nSee also:\ngrouped for a variant that groups based on number of elements groupedWeighted for a variant that groups based on element weight groupedWithin for a variant that groups based on number of elements and a time window","title":"Description"},{"location":"/stream/operators/Source-or-Flow/groupedWeightedWithin.html#reactive-streams-semantics","text":"emits when the configured time elapses since the last group has been emitted, but not if no elements has been grouped (i.e: no empty groups), or when weight limit has been reached. backpressures downstream backpressures, and buffered group (+ pending element) weighs more than maxWeight completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/groupedWithin.html","text":"","title":"groupedWithin"},{"location":"/stream/operators/Source-or-Flow/groupedWithin.html#groupedwithin","text":"Chunk up this stream into groups of elements received within a time window, or limited by the number of the elements, whatever happens first.\nTimer driven operators","title":"groupedWithin"},{"location":"/stream/operators/Source-or-Flow/groupedWithin.html#signature","text":"Source.groupedWithinSource.groupedWithin Flow.groupedWithinFlow.groupedWithin","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/groupedWithin.html#description","text":"Chunk up this stream into groups of elements received within a time window, or limited by the number of the elements, whatever happens first. Empty groups will not be emitted if no elements are received from upstream. The last group before end-of-stream will contain the buffered elements since the previously emitted group.\nSee also:\ngrouped for a variant that groups based on number of elements groupedWeighted for a variant that groups based on element weight groupedWeightedWithin for a variant that groups based on element weight and a time window","title":"Description"},{"location":"/stream/operators/Source-or-Flow/groupedWithin.html#reactive-streams-semantics","text":"emits when the configured time elapses since the last group has been emitted, but not if no elements has been grouped (i.e: no empty groups), or when limit has been reached. backpressures downstream backpressures, and there are n+1 buffered elements completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Compression/gunzip.html","text":"","title":"Compression.gunzip"},{"location":"/stream/operators/Compression/gunzip.html#compression-gunzip","text":"Creates a flow that gzip-decompresses a stream of ByteStrings.\nCompression operators","title":"Compression.gunzip"},{"location":"/stream/operators/Compression/gunzip.html#signature","text":"Compression.gunzipCompression.gunzip","title":"Signature"},{"location":"/stream/operators/Compression/gunzip.html#description","text":"Creates a flow that gzip-decompresses a stream of ByteStrings. If the input is truncated, uses invalid compression method or is invalid (failed CRC checks) this operator fails with a java.util.zip.ZipException.","title":"Description"},{"location":"/stream/operators/Compression/gunzip.html#reactive-streams-semantics","text":"emits when the decompression algorithm produces output for the received ByteString (the emitted ByteString is of maxBytesPerChunk maximum length) backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Compression/gzip.html","text":"","title":"Compression.gzip"},{"location":"/stream/operators/Compression/gzip.html#compression-gzip","text":"Creates a flow that gzip-compresses a stream of ByteStrings.\nCompression operators","title":"Compression.gzip"},{"location":"/stream/operators/Compression/gzip.html#signature","text":"Compression.gzipCompression.gzip","title":"Signature"},{"location":"/stream/operators/Compression/gzip.html#description","text":"Creates a flow that gzip-compresses a stream of ByteStrings. Note that the compressor will SYNC_FLUSH after every ByteStringByteString so that it is guaranteed that every ByteStringByteString coming out of the flow can be fully decompressed without waiting for additional data. This may come at a compression performance cost for very small chunks.\nUse the overload method to control the compression level.","title":"Description"},{"location":"/stream/operators/Compression/gzip.html#reactive-streams-semantics","text":"emits when the compression algorithm produces output for the received ByteString backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/head.html","text":"","title":"Sink.head"},{"location":"/stream/operators/Sink/head.html#sink-head","text":"Materializes into a Future CompletionStage which completes with the first value arriving, after this the stream is canceled.\nSink operators","title":"Sink.head"},{"location":"/stream/operators/Sink/head.html#signature","text":"Sink.headSink.head","title":"Signature"},{"location":"/stream/operators/Sink/head.html#description","text":"Materializes into a Future CompletionStage which completes with the first value arriving, after this the stream is canceled. If no element is emitted, the Future CompletionStage is failed.","title":"Description"},{"location":"/stream/operators/Sink/head.html#example","text":"Scala copysourceval source = Source(1 to 10)\nval result: Future[Int] = source.runWith(Sink.head)\nresult.map(println)\n// 1 Java copysourceSource<Integer, NotUsed> source = Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));\nCompletionStage<Integer> result = source.runWith(Sink.head(), system);\nresult.thenAccept(System.out::println);\n// 1","title":"Example"},{"location":"/stream/operators/Sink/head.html#reactive-streams-semantics","text":"cancels after receiving one element backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/headOption.html","text":"","title":"Sink.headOption"},{"location":"/stream/operators/Sink/headOption.html#sink-headoption","text":"Materializes into a Future[Option[T]] CompletionStage<Optional<T>> which completes with the first value arriving wrapped in Some Optional, or a None an empty Optional if the stream completes without any elements emitted.\nSink operators","title":"Sink.headOption"},{"location":"/stream/operators/Sink/headOption.html#signature","text":"Sink.headOptionSink.headOption","title":"Signature"},{"location":"/stream/operators/Sink/headOption.html#description","text":"Materializes into a Future[Option[T]] CompletionStage<Optional<T>> which completes with the first value arriving wrapped in Some Optional, or a None an empty Optional if the stream completes without any elements emitted.","title":"Description"},{"location":"/stream/operators/Sink/headOption.html#example","text":"In this example there is an empty source i.e. it does not emit any element and to handle it we have used headOption operator which will complete with None.\nScala copysourceval source = Source.empty\nval result: Future[Option[Int]] = source.runWith(Sink.headOption)\nresult.foreach(println)\n// None Java copysourceSource<Integer, NotUsed> source = Source.empty();\nCompletionStage<Optional<Integer>> result = source.runWith(Sink.headOption(), system);\nresult.thenAccept(System.out::println);\n// Optional.empty","title":"Example"},{"location":"/stream/operators/Sink/headOption.html#reactive-streams-semantics","text":"cancels after receiving one element backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/idleTimeout.html","text":"","title":"idleTimeout"},{"location":"/stream/operators/Source-or-Flow/idleTimeout.html#idletimeout","text":"If the time between two processed elements exceeds the provided timeout, the stream is failed with a TimeoutException.\nTime aware operators","title":"idleTimeout"},{"location":"/stream/operators/Source-or-Flow/idleTimeout.html#signature","text":"Source.idleTimeoutSource.idleTimeout Flow.idleTimeoutFlow.idleTimeout","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/idleTimeout.html#description","text":"If the time between two processed elements exceeds the provided timeout, the stream is failed with a TimeoutException. The timeout is checked periodically, so the resolution of the check is one period (equals to timeout value).","title":"Description"},{"location":"/stream/operators/Source-or-Flow/idleTimeout.html#reactive-streams-semantics","text":"emits when upstream emits an element backpressures when downstream backpressures completes when upstream completes or fails if timeout elapses between two emitted elements cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/ignore.html","text":"","title":"Sink.ignore"},{"location":"/stream/operators/Sink/ignore.html#sink-ignore","text":"Consume all elements but discards them.\nSink operators","title":"Sink.ignore"},{"location":"/stream/operators/Sink/ignore.html#signature","text":"Sink.ignoreSink.ignore","title":"Signature"},{"location":"/stream/operators/Sink/ignore.html#description","text":"Consume all elements but discards them. Useful when a stream has to be consumed but there is no use to actually do anything with the elements in the Sink. Processing of the elements may occur in the Source/Flow.","title":"Description"},{"location":"/stream/operators/Sink/ignore.html#example","text":"This examples reads lines from a file, saves them to a database, and stores the database identifiers in another file. The stream is run with Sink.ignore because all processing of the elements have been performed by the preceding stream operators.\nScala copysourceval lines: Source[String, NotUsed] = readLinesFromFile()\nval databaseIds: Source[UUID, NotUsed] =\n  lines.mapAsync(1)(line => saveLineToDatabase(line))\ndatabaseIds.mapAsync(1)(uuid => writeIdToFile(uuid)).runWith(Sink.ignore) Java copysourceSource<String, NotUsed> lines = readLinesFromFile();\nSource<UUID, NotUsed> databaseIds = lines.mapAsync(1, line -> saveLineToDatabase(line));\ndatabaseIds.mapAsync(1, uuid -> writeIdToFile(uuid)).runWith(Sink.ignore(), system);","title":"Example"},{"location":"/stream/operators/Sink/ignore.html#reactive-streams-semantics","text":"cancels never backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Compression/inflate.html","text":"","title":"Compression.inflate"},{"location":"/stream/operators/Compression/inflate.html#compression-inflate","text":"Creates a flow that deflate-decompresses a stream of ByteStrings.\nCompression operators","title":"Compression.inflate"},{"location":"/stream/operators/Compression/inflate.html#signature","text":"Compression.indeflateCompression.indeflate","title":"Signature"},{"location":"/stream/operators/Compression/inflate.html#description","text":"Creates a flow that deflate-decompresses a stream of ByteStrings.","title":"Description"},{"location":"/stream/operators/Compression/inflate.html#reactive-streams-semantics","text":"emits when the compression algorithm produces output for the received ByteString (the emitted ByteString is of maxBytesPerChunk maximum length) backpressures when downstream backpressures completes when upstream completes (may emit finishing bytes in an extra ByteString )","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/initialDelay.html","text":"","title":"initialDelay"},{"location":"/stream/operators/Source-or-Flow/initialDelay.html#initialdelay","text":"Delays the initial element by the specified duration.\nTimer driven operators","title":"initialDelay"},{"location":"/stream/operators/Source-or-Flow/initialDelay.html#signature","text":"Source.initialDelaySource.initialDelay Flow.initialDelayFlow.initialDelay","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/initialDelay.html#description","text":"Delays the initial element by the specified duration.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/initialDelay.html#reactive-streams-semantics","text":"emits when upstream emits an element if the initial delay is already elapsed backpressures when downstream backpressures or initial delay is not yet elapsed completes when upstream completes cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/initialTimeout.html","text":"","title":"initialTimeout"},{"location":"/stream/operators/Source-or-Flow/initialTimeout.html#initialtimeout","text":"If the first element has not passed through this operators before the provided timeout, the stream is failed with a TimeoutException.\nTime aware operators","title":"initialTimeout"},{"location":"/stream/operators/Source-or-Flow/initialTimeout.html#signature","text":"Source.initialTimeoutSource.initialTimeout Flow.initialTimeoutFlow.initialTimeout","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/initialTimeout.html#description","text":"If the first element has not passed through this operators before the provided timeout, the stream is failed with a TimeoutException.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/initialTimeout.html#reactive-streams-semantics","text":"emits when upstream emits an element backpressures when downstream backpressures completes when upstream completes or fails if timeout elapses before first element arrives cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/interleave.html","text":"","title":"interleave"},{"location":"/stream/operators/Source-or-Flow/interleave.html#interleave","text":"Emits a specifiable number of elements from the original source, then from the provided source and repeats.\nFan-in operators","title":"interleave"},{"location":"/stream/operators/Source-or-Flow/interleave.html#signature","text":"Source.interleaveSource.interleave Flow.interleaveFlow.interleave","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/interleave.html#description","text":"Emits a specifiable number of elements from the original source, then from the provided source and repeats. If one source completes the rest of the other stream will be emitted.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/interleave.html#example","text":"Scala copysourceimport org.apache.pekko\nimport pekko.stream.scaladsl.Sink\nimport pekko.stream.scaladsl.Source\n\nval sourceA = Source(List(1, 2, 3, 4))\nval sourceB = Source(List(10, 20, 30, 40))\n\nsourceA.interleave(sourceB, segmentSize = 2).runWith(Sink.foreach(println))\n// prints 1, 2, 10, 20, 3, 4, 30, 40 Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(10, 20, 30, 40));\nsourceA.interleave(sourceB, 2).runForeach(System.out::println, system);\n// prints 1, 2, 10, 20, 3, 4, 30, 40","title":"Example"},{"location":"/stream/operators/Source-or-Flow/interleave.html#reactive-streams-semantics","text":"emits when element is available from the currently consumed upstream backpressures when upstream backpressures completes when both upstreams have completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/interleaveAll.html","text":"","title":"interleaveAll"},{"location":"/stream/operators/Source-or-Flow/interleaveAll.html#interleaveall","text":"Emits a specifiable number of elements from the original source, then from the provided sources and repeats.\nFan-in operators","title":"interleaveAll"},{"location":"/stream/operators/Source-or-Flow/interleaveAll.html#signature","text":"Source.interleaveAllSource.interleaveAll Flow.interleaveAllFlow.interleaveAll","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/interleaveAll.html#description","text":"Emits a specifiable number of elements from the original source, then from the provided sources and repeats. If one source completes the rest of the other stream will be emitted when eagerClose is false, otherwise the flow is complete.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/interleaveAll.html#example","text":"Scala copysourceval sourceA = Source(List(1, 2, 7, 8))\nval sourceB = Source(List(3, 4, 9))\nval sourceC = Source(List(5, 6))\n\nsourceA\n  .interleaveAll(List(sourceB, sourceC), 2, eagerClose = false)\n  .fold(new StringJoiner(\",\"))((joiner, input) => joiner.add(String.valueOf(input)))\n  .runWith(Sink.foreach(println))\n// prints 1,2,3,4,5,6,7,8,9 Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 7, 8));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(3, 4, 9));\nSource<Integer, NotUsed> sourceC = Source.from(Arrays.asList(5, 6));\nsourceA\n    .interleaveAll(Arrays.asList(sourceB, sourceC), 2, false)\n    .fold(new StringJoiner(\",\"), (joiner, input) -> joiner.add(String.valueOf(input)))\n    .runForeach(System.out::println, system);\n// prints 1,2,3,4,5,6,7,8,9","title":"Example"},{"location":"/stream/operators/Source-or-Flow/interleaveAll.html#reactive-streams-semantics","text":"emits when element is available from the currently consumed upstream backpressures when upstream backpressures completes when all upstreams have completed if eagerClose is false, or any upstream completes if eagerClose is true.","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/intersperse.html","text":"","title":"intersperse"},{"location":"/stream/operators/Source-or-Flow/intersperse.html#intersperse","text":"Intersperse stream with provided element similar to List.mkString.\nSimple operators","title":"intersperse"},{"location":"/stream/operators/Source-or-Flow/intersperse.html#signature","text":"Source.intersperseSource.intersperse Flow.intersperseFlow.intersperse","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/intersperse.html#description","text":"Intersperse stream with provided element similar to List.mkString. It can inject start and end marker elements to stream.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/intersperse.html#example","text":"The following takes a stream of integers, converts them to strings and then adds a [ at the start, , between each element and a ] at the end.\nScala copysourceSource(1 to 4).map(_.toString).intersperse(\"[\", \", \", \"]\").runWith(Sink.foreach(print))\n// prints\n// [1, 2, 3, 4] Java copysourceSource.from(Arrays.asList(1, 2, 3))\n    .map(String::valueOf)\n    .intersperse(\"[\", \", \", \"]\")\n    .runForeach(System.out::print, system);\n// prints\n// [1, 2, 3]","title":"Example"},{"location":"/stream/operators/Source-or-Flow/intersperse.html#reactive-streams-semantics","text":"emits when upstream emits an element or before with the start element if provided backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/StreamConverters/javaCollector.html","text":"","title":"StreamConverters.javaCollector"},{"location":"/stream/operators/StreamConverters/javaCollector.html#streamconverters-javacollector","text":"Create a sink which materializes into a Future CompletionStage which will be completed with a result of the Java 8 Collector transformation and reduction operations.\nAdditional Sink and Source converters","title":"StreamConverters.javaCollector"},{"location":"/stream/operators/StreamConverters/javaCollector.html#signature","text":"StreamConverters.javaCollectorStreamConverters.javaCollector","title":"Signature"},{"location":"/stream/operators/StreamConverters/javaCollector.html#description","text":"TODO: We would welcome help on contributing descriptions and examples, see: https://github.com/akka/akka/issues/25646","title":"Description"},{"location":"/stream/operators/StreamConverters/javaCollectorParallelUnordered.html","text":"","title":"StreamConverters.javaCollectorParallelUnordered"},{"location":"/stream/operators/StreamConverters/javaCollectorParallelUnordered.html#streamconverters-javacollectorparallelunordered","text":"Create a sink which materializes into a Future CompletionStage which will be completed with a result of the Java 8 Collector transformation and reduction operations.\nAdditional Sink and Source converters","title":"StreamConverters.javaCollectorParallelUnordered"},{"location":"/stream/operators/StreamConverters/javaCollectorParallelUnordered.html#signature","text":"StreamConverters.javaCollectorParallelUnorderedStreamConverters.javaCollectorParallelUnordered","title":"Signature"},{"location":"/stream/operators/StreamConverters/javaCollectorParallelUnordered.html#description","text":"TODO: We would welcome help on contributing descriptions and examples, see: https://github.com/akka/akka/issues/25646","title":"Description"},{"location":"/stream/operators/Source-or-Flow/keepAlive.html","text":"","title":"keepAlive"},{"location":"/stream/operators/Source-or-Flow/keepAlive.html#keepalive","text":"Injects additional (configured) elements if upstream does not emit for a configured amount of time.\nTime aware operators","title":"keepAlive"},{"location":"/stream/operators/Source-or-Flow/keepAlive.html#signature","text":"Source.keepAliveSource.keepAlive Flow.keepAliveFlow.keepAlive","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/keepAlive.html#description","text":"Injects additional (configured) elements if upstream does not emit for a configured amount of time.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/keepAlive.html#reactive-streams-semantics","text":"emits when upstream emits an element or if the upstream was idle for the configured period backpressures when downstream backpressures completes when upstream completes cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/last.html","text":"","title":"Sink.last"},{"location":"/stream/operators/Sink/last.html#sink-last","text":"Materializes into a Future CompletionStage which will complete with the last value emitted when the stream completes.\nSink operators","title":"Sink.last"},{"location":"/stream/operators/Sink/last.html#signature","text":"Sink.lastSink.last","title":"Signature"},{"location":"/stream/operators/Sink/last.html#description","text":"Materializes into a Future CompletionStage which will complete with the last value emitted when the stream completes. If the stream completes with no elements the Future CompletionStage is failed.","title":"Description"},{"location":"/stream/operators/Sink/last.html#example","text":"Scala copysourceval source = Source(1 to 10)\nval result: Future[Int] = source.runWith(Sink.last)\nresult.map(println)\n// 10 Java copysourceSource<Integer, NotUsed> source = Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));\nCompletionStage<Integer> result = source.runWith(Sink.last(), system);\nresult.thenAccept(System.out::println);\n// 10","title":"Example"},{"location":"/stream/operators/Sink/last.html#reactive-streams-semantics","text":"cancels never backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/lastOption.html","text":"","title":"Sink.lastOption"},{"location":"/stream/operators/Sink/lastOption.html#sink-lastoption","text":"Materialize a Future[Option[T]] CompletionStage<Optional<T>> which completes with the last value emitted wrapped in an Some Optional when the stream completes.\nSink operators","title":"Sink.lastOption"},{"location":"/stream/operators/Sink/lastOption.html#signature","text":"Sink.lastOptionSink.lastOption","title":"Signature"},{"location":"/stream/operators/Sink/lastOption.html#description","text":"Materialize a Future[Option[T]] CompletionStage<Optional<T>> which completes with the last value emitted wrapped in an Some Optional when the stream completes. if the stream completes with no elements the CompletionStage is completed with None an empty Optional.","title":"Description"},{"location":"/stream/operators/Sink/lastOption.html#example","text":"Scala copysourceval source = Source.empty[Int]\nval result: Future[Option[Int]] = source.runWith(Sink.lastOption)\nresult.map(println)\n// None Java copysourceSource<Integer, NotUsed> source = Source.empty();\nCompletionStage<Optional<Integer>> result = source.runWith(Sink.lastOption(), system);\nresult.thenAccept(System.out::println);\n// Optional.empty","title":"Example"},{"location":"/stream/operators/Sink/lastOption.html#reactive-streams-semantics","text":"cancels never backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/lazily.html","text":"","title":"Source.lazily"},{"location":"/stream/operators/Source/lazily.html#source-lazily","text":"Deprecated by Source.lazySource.\nSource operators","title":"Source.lazily"},{"location":"/stream/operators/Source/lazily.html#signature","text":"Source.lazilySource.lazily","title":"Signature"},{"location":"/stream/operators/Source/lazily.html#description","text":"lazily has been deprecated in 2.6.0, use lazySource instead.\nDefers creation and materialization of a Source until there is demand.","title":"Description"},{"location":"/stream/operators/Source/lazily.html#reactive-streams-semantics","text":"emits depends on the wrapped Source completes depends on the wrapped Source","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/lazilyAsync.html","text":"","title":"Source.lazilyAsync"},{"location":"/stream/operators/Source/lazilyAsync.html#source-lazilyasync","text":"Deprecated by Source.lazyFutureSource.\nSource operators","title":"Source.lazilyAsync"},{"location":"/stream/operators/Source/lazilyAsync.html#signature","text":"","title":"Signature"},{"location":"/stream/operators/Source/lazilyAsync.html#description","text":"lazilyAsync has been deprecated in 2.6.0, use lazyFutureSource instead.\nDefers creation and materialization of a CompletionStage until there is demand.","title":"Description"},{"location":"/stream/operators/Source/lazilyAsync.html#reactive-streams-semantics","text":"emits the future completes completes after the future has completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/lazyCompletionStage.html","text":"","title":"Source.lazyCompletionStage"},{"location":"/stream/operators/Source/lazyCompletionStage.html#source-lazycompletionstage","text":"Defers creation of a future of a single element source until there is demand.\nSource operators","title":"Source.lazyCompletionStage"},{"location":"/stream/operators/Source/lazyCompletionStage.html#description","text":"Invokes the user supplied factory when the first downstream demand arrives. When the returned future completes successfully the value is emitted downstream as a single stream element. If the future or the factory fails the stream is failed.\nNote that asynchronous boundaries (and other operators) in the stream may do pre-fetching which counter acts the laziness and will trigger the factory immediately.","title":"Description"},{"location":"/stream/operators/Source/lazyCompletionStage.html#reactive-streams-semantics","text":"emits when there is downstream demand and the element factory returned future has completed completes after emitting the single element","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/lazyCompletionStageFlow.html","text":"","title":"Flow.lazyCompletionStageFlow"},{"location":"/stream/operators/Flow/lazyCompletionStageFlow.html#flow-lazycompletionstageflow","text":"Defers creation and materialization of a Flow until there is a first element.\nSimple operators","title":"Flow.lazyCompletionStageFlow"},{"location":"/stream/operators/Flow/lazyCompletionStageFlow.html#description","text":"When the first element comes from upstream the actual CompletionStage<Flow> is created and when that completes it is materialized and inserted in the stream. The internal Flow will not be created if there are no elements on completion or failure of up or downstream.\nThe materialized value of the Flow will be the materialized value of the created internal flow if it is materialized and failed with a org.apache.pekko.stream.NeverMaterializedException if the stream fails or completes without the flow being materialized.\nSee also lazyFlow.\nCan be combined with prefixAndTail(1) to base the flow construction on the initial element triggering creation. See lazyFlow for sample.","title":"Description"},{"location":"/stream/operators/Flow/lazyCompletionStageFlow.html#reactive-streams-semantics","text":"emits when the internal flow is successfully created and it emits backpressures when the internal flow is successfully created and it backpressures completes when upstream completes and all elements have been emitted from the internal flow completes when upstream completes and all futures have been completed and all elements have been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/lazyCompletionStageSink.html","text":"","title":"Sink.lazyCompletionStageSink"},{"location":"/stream/operators/Sink/lazyCompletionStageSink.html#sink-lazycompletionstagesink","text":"Defers creation and materialization of a Sink until there is a first element.\nSink operators","title":"Sink.lazyCompletionStageSink"},{"location":"/stream/operators/Sink/lazyCompletionStageSink.html#description","text":"When the first element comes from upstream the CompletionStage<Sink> is created. When that completes successfully with a sink that is materialized and inserted in the stream. The internal Sink will not be created if the stream completes of fails before any element got through.\nThe materialized value of the Sink will be the materialized value of the created internal flow if it is materialized and failed with a org.apache.pekko.stream.NeverMaterializedException if the stream fails or completes without the flow being materialized.\nCan be combined with prefixAndTail to base the sink on the first element.\nSee also lazySink.","title":"Description"},{"location":"/stream/operators/Sink/lazyCompletionStageSink.html#reactive-streams-semantics","text":"cancels if the future fails or if the created sink cancels backpressures when initialized and when created sink backpressures","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/lazyCompletionStageSource.html","text":"","title":"Source.lazyCompletionStageSource"},{"location":"/stream/operators/Source/lazyCompletionStageSource.html#source-lazycompletionstagesource","text":"Defers creation of a future source until there is demand.\nSource operators","title":"Source.lazyCompletionStageSource"},{"location":"/stream/operators/Source/lazyCompletionStageSource.html#description","text":"Invokes the user supplied factory when the first downstream demand arrives. When the returned CompletionStage completes successfully the source switches over to the new source and emits downstream just like if it had been created up front. If the future or the factory fails the stream is failed.\nNote that asynchronous boundaries (and other operators) in the stream may do pre-fetching which counter acts the laziness and will trigger the factory immediately.\nSee also lazySource.","title":"Description"},{"location":"/stream/operators/Source/lazyCompletionStageSource.html#reactive-streams-semantics","text":"emits depends on the wrapped Source completes depends on the wrapped Source","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/lazyFlow.html","text":"","title":"Flow.lazyFlow"},{"location":"/stream/operators/Flow/lazyFlow.html#flow-lazyflow","text":"Defers creation and materialization of a Flow until there is a first element.\nSimple operators","title":"Flow.lazyFlow"},{"location":"/stream/operators/Flow/lazyFlow.html#signature","text":"Flow.lazyFlowFlow.lazyFlow","title":"Signature"},{"location":"/stream/operators/Flow/lazyFlow.html#description","text":"Defers Flow creation and materialization until when the first element arrives at the lazyFlow from upstream. After that the stream behaves as if the nested flow replaced the lazyFlow. The nested Flow will not be created if the outer flow completes or fails before any elements arrive.\nNote that asynchronous boundaries and many other operators in the stream may do pre-fetching or trigger demand and thereby making an early element come throught the stream leading to creation of the inner flow earlier than you would expect.\nThe materialized value of the Flow is a FutureCompletionStage that is completed with the materialized value of the nested flow once that is constructed.\nSee also:\nflatMapPrefix Flow.lazyFutureFlow and Flow.lazyCompletionStageFlow Source.lazySource Sink.lazySink","title":"Description"},{"location":"/stream/operators/Flow/lazyFlow.html#examples","text":"In this sample we produce a short sequence of numbers, mostly to side effect and write to standard out to see in which order things happen. Note how producing the first value in the Source happens before the creation of the flow:\nScala copysourceval numbers = Source\n  .unfold(0) { n =>\n    val next = n + 1\n    println(s\"Source producing $next\")\n    Some((next, next))\n  }\n  .take(3)\n\nval flow = Flow.lazyFlow { () =>\n  println(\"Creating the actual flow\")\n  Flow[Int].map { element =>\n    println(s\"Actual flow mapped $element\")\n    element\n  }\n}\n\nnumbers.via(flow).run()\n// prints:\n// Source producing 1\n// Creating the actual flow\n// Actual flow mapped 1\n// Source producing 2\n// Actual flow mapped 2 Java copysourceSource<Integer, NotUsed> numbers =\n    Source.unfold(\n            0,\n            n -> {\n              int next = n + 1;\n              System.out.println(\"Source producing \" + next);\n              return Optional.of(Pair.create(next, next));\n            })\n        .take(3);\n\nFlow<Integer, Integer, CompletionStage<NotUsed>> flow =\n    Flow.lazyFlow(\n        () -> {\n          System.out.println(\"Creating the actual flow\");\n          return Flow.fromFunction(\n              element -> {\n                System.out.println(\"Actual flow mapped \" + element);\n                return element;\n              });\n        });\n\nnumbers.via(flow).run(system);\n// prints:\n// Source producing 1\n// Creating the actual flow\n// Actual flow mapped 1\n// Source producing 2\n// Actual flow mapped 2\nSince the factory is called once per stream materialization it can be used to safely construct a mutable object to use with the actual deferred Flow. In this example we fold elements into an ArrayList created inside the lazy flow factory:\nScala copysourceval mutableFold = Flow.lazyFlow { () =>\n  val zero = new util.ArrayList[Int]()\n  Flow[Int].fold(zero) { (list, element) =>\n    list.add(element)\n    list\n  }\n}\nval stream =\n  Source(1 to 3).via(mutableFold).to(Sink.foreach(println))\n\nstream.run()\nstream.run()\nstream.run()\n// prints:\n// [1, 2, 3]\n// [1, 2, 3]\n// [1, 2, 3]\n Java copysourceFlow<Integer, List<Integer>, CompletionStage<NotUsed>> mutableFold =\n    Flow.lazyFlow(\n        () -> {\n          List<Integer> zero = new ArrayList<>();\n\n          return Flow.of(Integer.class)\n              .fold(\n                  zero,\n                  (list, element) -> {\n                    list.add(element);\n                    return list;\n                  });\n        });\n\nRunnableGraph<NotUsed> stream =\n    Source.range(1, 3).via(mutableFold).to(Sink.foreach(System.out::println));\n\nstream.run(system);\nstream.run(system);\nstream.run(system);\n// prints:\n// [1, 2, 3]\n// [1, 2, 3]\n// [1, 2, 3]\nIf we instead had used fold directly with an ArrayList we would have shared the same list across all materialization and what is even worse, unsafely across threads.","title":"Examples"},{"location":"/stream/operators/Flow/lazyFlow.html#reactive-streams-semantics","text":"emits when the internal flow is successfully created and it emits backpressures when the internal flow is successfully created and it backpressures completes when upstream completes and all elements have been emitted from the internal flow completes when upstream completes and all futures have been completed and all elements have been emitted cancels when downstream cancels (keep reading) The operator’s default behaviour in case of downstream cancellation before nested flow materialization (future completion) is to cancel immediately. This behaviour can be controlled by setting the org.apache.pekko.stream.Attributes.NestedMaterializationCancellationPolicy.PropagateToNested attribute, this will delay downstream cancellation until nested flow’s materialization which is then immediately cancelled (with the original cancellation cause).","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/lazyFuture.html","text":"","title":"Source.lazyFuture"},{"location":"/stream/operators/Source/lazyFuture.html#source-lazyfuture","text":"Defers creation of a future of a single element source until there is demand.\nSource operators","title":"Source.lazyFuture"},{"location":"/stream/operators/Source/lazyFuture.html#signature","text":"Source.lazyFutureSource.lazyFuture","title":"Signature"},{"location":"/stream/operators/Source/lazyFuture.html#description","text":"Invokes the user supplied factory when the first downstream demand arrives. When the returned future completes successfully the value is emitted downstream as a single stream element. If the future or the factory fails the stream is failed.\nNote that asynchronous boundaries (and other operators) in the stream may do pre-fetching which counter acts the laziness and will trigger the factory immediately.","title":"Description"},{"location":"/stream/operators/Source/lazyFuture.html#reactive-streams-semantics","text":"emits when there is downstream demand and the element factory returned future has completed completes after emitting the single element","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/lazyFutureFlow.html","text":"","title":"Flow.lazyFutureFlow"},{"location":"/stream/operators/Flow/lazyFutureFlow.html#flow-lazyfutureflow","text":"Defers creation and materialization of a Flow until there is a first element.\nSimple operators","title":"Flow.lazyFutureFlow"},{"location":"/stream/operators/Flow/lazyFutureFlow.html#signature","text":"Flow.lazyFutureFlowFlow.lazyFutureFlow","title":"Signature"},{"location":"/stream/operators/Flow/lazyFutureFlow.html#description","text":"When the first element comes from upstream the actual Future[Flow] is created and when that completes it is materialized and inserted in the stream. The internal Flow will not be created if there are no elements on completion or failure of up or downstream.\nThe materialized value of the Flow will be the materialized value of the created internal flow if it is materialized and failed with a org.apache.pekko.stream.NeverMaterializedException if the stream fails or completes without the flow being materialized.\nSee also lazyFlow.\nCan be combined with prefixAndTail(1) to base the flow construction on the initial element triggering creation. See lazyFlow for sample.","title":"Description"},{"location":"/stream/operators/Flow/lazyFutureFlow.html#reactive-streams-semantics","text":"emits when the internal flow is successfully created and it emits backpressures when the internal flow is successfully created and it backpressures completes when upstream completes and all elements have been emitted from the internal flow completes when upstream completes and all futures have been completed and all elements have been emitted cancels when downstream cancels (keep reading) The operator’s default behaviour in case of downstream cancellation before nested flow materialization (future completion) is to cancel immediately. This behaviour can be controlled by setting the org.apache.pekko.stream.Attributes.NestedMaterializationCancellationPolicy.PropagateToNested attribute, this will delay downstream cancellation until nested flow’s materialization which is then immediately cancelled (with the original cancellation cause).","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/lazyFutureSink.html","text":"","title":"Sink.lazyFutureSink"},{"location":"/stream/operators/Sink/lazyFutureSink.html#sink-lazyfuturesink","text":"Defers creation and materialization of a Sink until there is a first element.\nSink operators","title":"Sink.lazyFutureSink"},{"location":"/stream/operators/Sink/lazyFutureSink.html#signature","text":"Sink.lazyFutureSinkSink.lazyFutureSink","title":"Signature"},{"location":"/stream/operators/Sink/lazyFutureSink.html#description","text":"When the first element comes from upstream the Future[Sink] is created. When that completes successfully with a sink that is materialized and inserted in the stream. The internal Sink will not be created if the stream completes of fails before any element got through.\nThe materialized value of the Sink will be the materialized value of the created internal flow if it is materialized and failed with a org.apache.pekko.stream.NeverMaterializedException if the stream fails or completes without the flow being materialized.\nCan be combined with prefixAndTail to base the sink on the first element.\nSee also lazySink.","title":"Description"},{"location":"/stream/operators/Sink/lazyFutureSink.html#reactive-streams-semantics","text":"cancels if the future fails or if the created sink cancels backpressures when initialized and when created sink backpressures","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/lazyFutureSource.html","text":"","title":"Source.lazyFutureSource"},{"location":"/stream/operators/Source/lazyFutureSource.html#source-lazyfuturesource","text":"Defers creation and materialization of a Source until there is demand.\nSource operators","title":"Source.lazyFutureSource"},{"location":"/stream/operators/Source/lazyFutureSource.html#signature","text":"Source.lazyFutureSourceSource.lazyFutureSource","title":"Signature"},{"location":"/stream/operators/Source/lazyFutureSource.html#description","text":"Invokes the user supplied factory when the first downstream demand arrives. When the returned future completes successfully the source switches over to the new source and emits downstream just like if it had been created up front.\nNote that asynchronous boundaries (and other operators) in the stream may do pre-fetching which counter acts the laziness and will trigger the factory immediately.\nSee also lazySource.","title":"Description"},{"location":"/stream/operators/Source/lazyFutureSource.html#reactive-streams-semantics","text":"emits depends on the wrapped Source completes depends on the wrapped Source","title":"Reactive Streams semantics"},{"location":"/stream/operators/Flow/lazyInitAsync.html","text":"","title":"Flow.lazyInitAsync"},{"location":"/stream/operators/Flow/lazyInitAsync.html#flow-lazyinitasync","text":"Deprecated by Flow.lazyFutureFlow in combination with prefixAndTail.\nSimple operators","title":"Flow.lazyInitAsync"},{"location":"/stream/operators/Flow/lazyInitAsync.html#signature","text":"Flow.lazyInitAsyncFlow.lazyInitAsync","title":"Signature"},{"location":"/stream/operators/Flow/lazyInitAsync.html#description","text":"fromCompletionStage has been deprecated in 2.6.0 use lazyFutureFlow in combination with prefixAndTail) instead.\nDefers creation until a first element arrives.","title":"Description"},{"location":"/stream/operators/Flow/lazyInitAsync.html#reactive-streams-semantics","text":"emits when the internal flow is successfully created and it emits backpressures when the internal flow is successfully created and it backpressures completes when upstream completes and all elements have been emitted from the internal flow completes when upstream completes and all futures have been completed and all elements have been emitted cancels when downstream cancels (keep reading) The operator’s default behaviour in case of downstream cancellation before nested flow materialization (future completion) is to cancel immediately. This behaviour can be controlled by setting the org.apache.pekko.stream.Attributes.NestedMaterializationCancellationPolicy.PropagateToNested attribute, this will delay downstream cancellation until nested flow’s materialization which is then immediately cancelled (with the original cancellation cause).","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/lazyInitAsync.html","text":"","title":"Sink.lazyInitAsync"},{"location":"/stream/operators/Sink/lazyInitAsync.html#sink-lazyinitasync","text":"Deprecated by Sink.lazyFutureSink.\nSink operators","title":"Sink.lazyInitAsync"},{"location":"/stream/operators/Sink/lazyInitAsync.html#signature","text":"Flow.lazyInitAsyncFlow.lazyInitAsync Sink.lazyInitAsyncSink.lazyInitAsync","title":"Signature"},{"location":"/stream/operators/Sink/lazyInitAsync.html#description","text":"lazyInitAsync has been deprecated in 2.6.0, use lazyFutureSink instead.\nCreates a real Sink upon receiving the first element. Internal Sink will not be created if there are no elements, because of completion or error.\nIf upstream completes before an element was received then the FutureCompletionStage is completed with Nonean empty Optional. If upstream fails before an element was received, sinkFactory throws an exception, or materialization of the internal sink fails then the FutureCompletionStage is completed with the exception. Otherwise the FutureCompletionStage is completed with the materialized value of the internal sink.","title":"Description"},{"location":"/stream/operators/Sink/lazyInitAsync.html#reactive-streams-semantics","text":"cancels never backpressures when initialized and when created sink backpressures","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/lazySingle.html","text":"","title":"Source.lazySingle"},{"location":"/stream/operators/Source/lazySingle.html#source-lazysingle","text":"Defers creation of a single element source until there is demand.\nSource operators","title":"Source.lazySingle"},{"location":"/stream/operators/Source/lazySingle.html#signature","text":"Source.lazySingleSource.lazySingle","title":"Signature"},{"location":"/stream/operators/Source/lazySingle.html#description","text":"Invokes the user supplied factory when the first downstream demand arrives, then emits the returned single value downstream and completes the stream.\nNote that asynchronous boundaries (and other operators) in the stream may do pre-fetching which counter acts the laziness and will trigger the factory immediately.","title":"Description"},{"location":"/stream/operators/Source/lazySingle.html#reactive-streams-semantics","text":"emits when there is downstream demand and the element factory has completed completes after emitting the single element","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/lazySink.html","text":"","title":"Sink.lazySink"},{"location":"/stream/operators/Sink/lazySink.html#sink-lazysink","text":"Defers creation and materialization of a Sink until there is a first element.\nSink operators","title":"Sink.lazySink"},{"location":"/stream/operators/Sink/lazySink.html#signature","text":"Sink.lazySinkSink.lazySink","title":"Signature"},{"location":"/stream/operators/Sink/lazySink.html#description","text":"Defers Sink creation and materialization until when the first element arrives from upstream to the lazySink. After that the stream behaves as if the nested sink replaced the lazySink. The nested Sink will not be created if upstream completes or fails without any elements arriving at the sink.\nThe materialized value of the Sink is a FutureCompletionStage that is completed with the materialized value of the nested sink once that is constructed.\nCan be combined with prefixAndTail to base the sink on the first element.\nSee also:\nSink.lazyFutureSink and lazyCompletionStageSink. Source.lazySource Flow.lazyFlow","title":"Description"},{"location":"/stream/operators/Sink/lazySink.html#examples","text":"In this example we side effect from Flow.map, the sink factory and Sink.foreach so that the order becomes visible, the nested sink is only created once the element has passed map:\nScala copysourceval matVal =\n  Source\n    .maybe[String]\n    .map { element =>\n      println(s\"mapped $element\")\n      element\n    }\n    .toMat(Sink.lazySink { () =>\n      println(\"Sink created\")\n      Sink.foreach(elem => println(s\"foreach $elem\"))\n    })(Keep.left)\n    .run()\n\n// some time passes\n// nothing has been printed\nmatVal.success(Some(\"one\"))\n// now prints:\n// mapped one\n// Sink created\n// foreach one\n Java copysourceCompletionStage<Optional<String>> matVal =\n    Source.<String>maybe()\n        .map(\n            element -> {\n              System.out.println(\"mapped \" + element);\n              return element;\n            })\n        .toMat(\n            Sink.lazySink(\n                () -> {\n                  System.out.println(\"Sink created\");\n                  return Sink.foreach(elem -> System.out.println(\"foreach \" + elem));\n                }),\n            Keep.left())\n        .run(system);\n\n// some time passes\n// nothing has been printed\nmatVal.toCompletableFuture().complete(Optional.of(\"one\"));\n// now prints:\n// mapped one\n// Sink created\n// foreach one","title":"Examples"},{"location":"/stream/operators/Sink/lazySink.html#reactive-streams-semantics","text":"cancels if the future fails or if the created sink cancels backpressures when initialized and when created sink backpressures","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/lazySource.html","text":"","title":"Source.lazySource"},{"location":"/stream/operators/Source/lazySource.html#source-lazysource","text":"Defers creation and materialization of a Source until there is demand.\nSource operators","title":"Source.lazySource"},{"location":"/stream/operators/Source/lazySource.html#signature","text":"Source.lazySourceSource.lazySource","title":"Signature"},{"location":"/stream/operators/Source/lazySource.html#description","text":"Defers creation and materialization of a Source until there is demand, then emits the elements from the source downstream just like if it had been created up front. If the stream fails or cancels before there is demand the factory will not be invoked.\nNote that asynchronous boundaries and many other operators in the stream may do pre-fetching or trigger demand earlier than you would expect.\nThe materialized value of the lazy is a FutureCompletionStage that is completed with the materialized value of the nested source once that is constructed.\nSee also:\nSource.lazyFutureSource and Source.lazyCompletionStageSource Flow.lazyFlow Sink.lazySink","title":"Description"},{"location":"/stream/operators/Source/lazySource.html#example","text":"In this example you might expect this sample to not construct the expensive source until .pull is called. However, since Sink.queue has a buffer and will ask for that immediately on materialization the expensive source is in created quickly after the stream has been materialized:\nScala copysourceval source = Source.lazySource { () =>\n  println(\"Creating the actual source\")\n  createExpensiveSource()\n}\n\nval queue = source.runWith(Sink.queue())\n\n// ... time passes ...\n// at some point in time we pull the first time\n// but the source creation may already have been triggered\nqueue.pull() Java copysourceSource<String, CompletionStage<NotUsed>> source =\n    Source.lazySource(\n        () -> {\n          System.out.println(\"Creating the actual source\");\n          return createExpensiveSource();\n        });\n\nSinkQueueWithCancel<String> queue = source.runWith(Sink.queue(), system);\n\n// ... time passes ...\n// at some point in time we pull the first time\n// but the source creation may already have been triggered\nqueue.pull();\nInstead the most useful aspect of the operator is that the factory is called once per stream materialization which means that it can be used to safely construct a mutable object to use with the actual deferred source.\nIn this example we make use of that by unfolding a mutable object that works like an iterator with a method to say if there are more elements and one that produces the next and moves to the next element.\nIf the IteratorLikeThing was used directly in a Source.unfold the same instance would end up being unsafely shared across all three materializations of the stream, but wrapping it with Source.lazy ensures we create a separate instance for each of the started streams:\nScala copysourceval stream = Source\n  .lazySource { () =>\n    val iteratorLike = new IteratorLikeThing\n    Source.unfold(iteratorLike) { iteratorLike =>\n      if (iteratorLike.thereAreMore) Some((iteratorLike, iteratorLike.extractNext))\n      else None\n    }\n  }\n  .to(Sink.foreach(println))\n\n// each of the three materializations will have their own instance of IteratorLikeThing\nstream.run()\nstream.run()\nstream.run() Java copysourceRunnableGraph<CompletionStage<NotUsed>> stream =\n    Source.lazySource(\n            () -> {\n              IteratorLikeThing instance = new IteratorLikeThing();\n              return Source.unfold(\n                  instance,\n                  sameInstance -> {\n                    if (sameInstance.thereAreMore())\n                      return Optional.of(Pair.create(sameInstance, sameInstance.extractNext()));\n                    else return Optional.empty();\n                  });\n            })\n        .to(Sink.foreach(System.out::println));\n\n// each of the three materializations will have their own instance of IteratorLikeThing\nstream.run(system);\nstream.run(system);\nstream.run(system);\nNote though that you can often also achieve the same using unfoldResource. If you have an actual Iterator you should prefer fromIterator.","title":"Example"},{"location":"/stream/operators/Source/lazySource.html#reactive-streams-semantics","text":"emits depends on the wrapped Source completes depends on the wrapped Source","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/limit.html","text":"","title":"limit"},{"location":"/stream/operators/Source-or-Flow/limit.html#limit","text":"Limit number of element from upstream to given max number.\nSimple operators","title":"limit"},{"location":"/stream/operators/Source-or-Flow/limit.html#signature","text":"Flow.limitFlow.limit","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/limit.html#description","text":"Limits the number of elements from upstream to a given max number, if the limit is passed the operator fails the stream with a StreamLimitReachedExceptionStreamLimitReachedException.\nSee also limitWeighted which can choose a weight for each element counting to a total max limit weight. take is also closely related but completes the stream instead of failing it after a certain number of elements.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/limit.html#example","text":"limit can protect a stream coming from an untrusted source into an in-memory aggregate that grows with the number of elements from filling the heap and causing an out-of-memory error. In this sample we take at most 10 000 of the untrusted source elements into the aggregated sequence of elements, if the untrusted source emits more elements the stream and the materialized Future[Seq[String]]CompletionStage<List<String>> will be failed:\nScala copysourceval untrustedSource: Source[String, NotUsed] = Source.repeat(\"element\")\n\nval elements: Future[Seq[String]] =\n  untrustedSource.limit(10000).runWith(Sink.seq) Java copysourceSource<String, NotUsed> untrustedSource = Source.repeat(\"element\");\n\nCompletionStage<List<String>> elements =\n    untrustedSource.limit(10000).runWith(Sink.seq(), system);","title":"Example"},{"location":"/stream/operators/Source-or-Flow/limit.html#reactive-streams-semantics","text":"emits when upstream emits and the number of emitted elements has not reached max backpressures when downstream backpressures completes when upstream completes and the number of emitted elements has not reached max","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/limitWeighted.html","text":"","title":"limitWeighted"},{"location":"/stream/operators/Source-or-Flow/limitWeighted.html#limitweighted","text":"Limit the total weight of incoming elements\nSimple operators","title":"limitWeighted"},{"location":"/stream/operators/Source-or-Flow/limitWeighted.html#signature","text":"Flow.limitWeightedFlow.limitWeighted","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/limitWeighted.html#description","text":"A weight function returns the weight of each element, then the total accumulated weight is compared to a max and if it has passed the max the stream is failed with a StreamLimitReachedExceptionStreamLimitReachedException.\nSee also limit which puts a limit on the number of elements instead (the same as always returning 1 from the weight function).","title":"Description"},{"location":"/stream/operators/Source-or-Flow/limitWeighted.html#examples","text":"limitWeighted can protect a stream coming from an untrusted source into an in-memory aggregate that grows with the number of elements from filling the heap and causing an out-of-memory error. In this sample we use the number of bytes in each ByteString element as weight and accept at most a total of 10 000 bytes from the untrusted source elements into the aggregated ByteString of all bytes, if the untrusted source emits more elements the stream and the materialized Future[ByteString]CompletionStage<ByteString> will be failed:\nScala copysourceval untrustedSource: Source[ByteString, NotUsed] = Source.repeat(ByteString(\"element\"))\n\nval allBytes: Future[ByteString] =\n  untrustedSource.limitWeighted(max = 10000)(_.length).runReduce(_ ++ _) Java copysourceSource<ByteString, NotUsed> untrustedSource = Source.repeat(ByteString.fromString(\"element\"));\n\nCompletionStage<ByteString> allBytes =\n    untrustedSource\n        .limitWeighted(\n            10000, // max bytes\n            bytes -> (long) bytes.length() // bytes of each chunk\n            )\n        .runReduce(ByteString::concat, system);","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/limitWeighted.html#reactive-streams-semantics","text":"emits when upstream emits and the number of emitted elements has not reached max backpressures when downstream backpressures completes when upstream completes and the number of emitted elements has not reached max","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/log.html","text":"","title":"log"},{"location":"/stream/operators/Source-or-Flow/log.html#log","text":"Log elements flowing through the stream as well as completion and erroring.\nSimple operators","title":"log"},{"location":"/stream/operators/Source-or-Flow/log.html#signature","text":"Source.logSource.log Flow.logFlow.log","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/log.html#description","text":"Log elements flowing through the stream as well as completion and erroring. By default element and completion signals are logged on debug level, and errors are logged on Error level. This can be changed by calling Attributes.logLevels(...) Attributes.createLogLevels(...) on the given Flow.\nSee also logWithMarker.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/log.html#example","text":"Scala copysourceimport org.apache.pekko.stream.Attributes\n\n.log(name = \"myStream\")\n.addAttributes(\n  Attributes.logLevels(\n    onElement = Attributes.LogLevels.Off,\n    onFinish = Attributes.LogLevels.Info,\n    onFailure = Attributes.LogLevels.Error)) Java copysourceimport org.apache.pekko.event.LogMarker;\nimport org.apache.pekko.stream.Attributes;\n\n.log(\"myStream\")\n.addAttributes(\n    Attributes.createLogLevels(\n        Attributes.logLevelOff(), // onElement\n        Attributes.logLevelInfo(), // onFinish\n        Attributes.logLevelError())) // onFailure","title":"Example"},{"location":"/stream/operators/Source-or-Flow/log.html#reactive-streams-semantics","text":"emits when upstream emits backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/logWithMarker.html","text":"","title":"logWithMarker"},{"location":"/stream/operators/Source-or-Flow/logWithMarker.html#logwithmarker","text":"Log elements flowing through the stream as well as completion and erroring.\nSimple operators","title":"logWithMarker"},{"location":"/stream/operators/Source-or-Flow/logWithMarker.html#signature","text":"Source.logWithMarkerSource.logWithMarker Flow.logWithMarkerFlow.logWithMarker","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/logWithMarker.html#description","text":"Log elements flowing through the stream as well as completion and erroring. By default element and completion signals are logged on debug level, and errors are logged on Error level. This can be changed by calling Attributes.logLevels(...) Attributes.createLogLevels(...) on the given Flow.\nSee also log.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/logWithMarker.html#example","text":"Scala copysourceimport org.apache.pekko\nimport pekko.event.LogMarker\nimport pekko.stream.Attributes\n\n.logWithMarker(name = \"myStream\", e => LogMarker(name = \"myMarker\", properties = Map(\"element\" -> e)))\n.addAttributes(\n  Attributes.logLevels(\n    onElement = Attributes.LogLevels.Off,\n    onFinish = Attributes.LogLevels.Info,\n    onFailure = Attributes.LogLevels.Error)) Java copysource.logWithMarker(\n    \"myStream\", (e) -> LogMarker.create(\"myMarker\", Collections.singletonMap(\"element\", e)))\n.addAttributes(\n    Attributes.createLogLevels(\n        Attributes.logLevelOff(), // onElement\n        Attributes.logLevelInfo(), // onFinish\n        Attributes.logLevelError())) // onFailure","title":"Example"},{"location":"/stream/operators/Source-or-Flow/logWithMarker.html#reactive-streams-semantics","text":"emits when upstream emits backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/map.html","text":"","title":"map"},{"location":"/stream/operators/Source-or-Flow/map.html#map","text":"Transform each element in the stream by calling a mapping function with it and passing the returned value downstream.\nSimple operators","title":"map"},{"location":"/stream/operators/Source-or-Flow/map.html#signature","text":"Source.mapSource.map Flow.mapFlow.map","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/map.html#description","text":"Transform each element in the stream by calling a mapping function with it and passing the returned value downstream.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/map.html#examples","text":"Scala copysourceimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.scaladsl._\n\nval source: Source[Int, NotUsed] = Source(1 to 10)\nval mapped: Source[String, NotUsed] = source.map(elem => elem.toString)","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/map.html#reactive-streams-semantics","text":"emits when the mapping function returns an element backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mapAsync.html","text":"","title":"mapAsync"},{"location":"/stream/operators/Source-or-Flow/mapAsync.html#mapasync","text":"Pass incoming elements to a function that return a Future CompletionStage result.\nAsynchronous operators","title":"mapAsync"},{"location":"/stream/operators/Source-or-Flow/mapAsync.html#signature","text":"Source.mapAsyncSource.mapAsync Flow.mapAsyncFlow.mapAsync","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mapAsync.html#description","text":"Pass incoming elements to a function that return a Future CompletionStage result. When the Future CompletionStage arrives the result is passed downstream. Up to n elements can be processed concurrently, but regardless of their completion time the incoming order will be kept when results complete. For use cases where order does not matter mapAsyncUnordered can be used.\nIf a Future CompletionStage completes with null, element is not passed downstream. If a Future CompletionStage fails, the stream also fails (unless a different supervision strategy is applied)","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mapAsync.html#examples","text":"Imagine you are consuming messages from a broker. These messages represent business events produced on a service upstream. In that case, you want to consume the messages in order and one at a time:\nScala copysource val events: Source[Event, NotUsed] = // ...\n\ndef eventHandler(event: Event): Future[Int] = {\n  println(s\"Processing event $event...\")\n  // ...\n}\n\nevents\n  .mapAsync(1) { in =>\n    eventHandler(in)\n  }\n  .map { in =>\n    println(s\"`mapAsync` emitted event number: $in\")\n  } Java copysource private final Source<Event, NotUsed> events =\n    Source.fromIterator(() -> Stream.iterate(1, i -> i + 1).iterator())\n        .throttle(1, Duration.ofMillis(50))\n        .map(Event::new);\n\npublic CompletionStage<Integer> eventHandler(Event in) throws InterruptedException {\n  System.out.println(\"Processing event number \" + in + \"...\");\n  // ...\n}\n\n  events\n      .mapAsync(1, this::eventHandler)\n      .map(in -> \"`mapSync` emitted event number \" + in.intValue())\n      .runWith(Sink.foreach(str -> System.out.println(str)), system);\nWhen running the stream above the logging output would look like:\n[...]\nProcessing event number Event(33)...\nCompleted processing 33\n`mapAsync` emitted event number: 33\nProcessing event number Event(34)...\nCompleted processing 34\n`mapAsync` emitted event number: 34\n[...]\nIf, instead, you may process information concurrently, but still emit the messages downstream in order, you may increase the parallelism. In this case, the events could some IoT payload with weather metrics, for example, where processing the data in strict ordering is not critical:\nScala copysource val events: Source[Event, NotUsed] = // ...\n\ndef eventHandler(event: Event): Future[Int] = {\n  println(s\"Processing event $event...\")\n  // ...\n}\n\nevents\n  .mapAsync(3) { in =>\n    eventHandler(in)\n  }\n  .map { in =>\n    println(s\"`mapAsync` emitted event number: $in\")\n  } Java copysource private final Source<Event, NotUsed> events =\n    Source.fromIterator(() -> Stream.iterate(1, i -> i + 1).iterator())\n        .throttle(1, Duration.ofMillis(50))\n        .map(Event::new);\n\npublic CompletionStage<Integer> eventHandler(Event in) throws InterruptedException {\n  System.out.println(\"Processing event number \" + in + \"...\");\n  // ...\n}\n\n  events\n      .mapAsync(10, this::eventHandler)\n      .map(in -> \"`mapSync` emitted event number \" + in.intValue())\n      .runWith(Sink.foreach(str -> System.out.println(str)), system);\nIn this case, the logging soon shows how processing of the events happens concurrently which may break the ordering. Still, the stage emits the events back in the correct order:\n[...]\nProcessing event number Event(15)...\nProcessing event number Event(16)...\nCompleted processing 16\nProcessing event number Event(17)...\nCompleted processing 17\nCompleted processing 15\n`mapAsync` emitted event number: 15\n`mapAsync` emitted event number: 16\nProcessing event number Event(18)...\n`mapAsync` emitted event number: 17\n[...]\nSee also mapAsyncUnordered.","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/mapAsync.html#reactive-streams-semantics","text":"emits when the Future CompletionStage returned by the provided function finishes for the next element in sequence backpressures when the number of Future s CompletionStage s reaches the configured parallelism and the downstream backpressures completes when upstream completes and all Future s CompletionStage s has been completed and all elements has been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mapAsyncUnordered.html","text":"","title":"mapAsyncUnordered"},{"location":"/stream/operators/Source-or-Flow/mapAsyncUnordered.html#mapasyncunordered","text":"Like mapAsync but Future CompletionStage results are passed downstream as they arrive regardless of the order of the elements that triggered them.\nAsynchronous operators","title":"mapAsyncUnordered"},{"location":"/stream/operators/Source-or-Flow/mapAsyncUnordered.html#signature","text":"Source.mapAsyncUnorderedSource.mapAsyncUnordered Flow.mapAsyncUnorderedFlow.mapAsyncUnordered","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mapAsyncUnordered.html#description","text":"Like mapAsync but Future CompletionStage results are passed downstream as they arrive regardless of the order of the elements that triggered them.\nIf a Future CompletionStage completes with null, element is not passed downstream. If a Future CompletionStage fails, the stream also fails (unless a different supervision strategy is applied)","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mapAsyncUnordered.html#examples","text":"Imagine you are consuming messages from a source, and you prioritize throughput over order (this could be uncorrelated messages so order is irrelevant). You may use the mapAsyncUnordered (so messages are emitted as soon as they’ve been processed) with some parallelism (so processing happens concurrently) :\nScala copysource val events: Source[Event, NotUsed] = // ...\n\ndef eventHandler(event: Event): Future[Int] = {\n  println(s\"Processing event $event...\")\n  // ...\n}\n\nevents\n  .mapAsyncUnordered(3) { in =>\n    eventHandler(in)\n  }\n  .map { in =>\n    println(s\"`mapAsyncUnordered` emitted event number: $in\")\n  } Java copysource private final Source<Event, NotUsed> events =\n    Source.fromIterator(() -> Stream.iterate(1, i -> i + 1).iterator())\n        .throttle(1, Duration.ofMillis(50))\n        .map(Event::new);\n\npublic CompletionStage<Integer> eventHandler(Event in) throws InterruptedException {\n  System.out.println(\"Processing event number \" + in + \"...\");\n  // ...\n}\n\n  events\n      .mapAsyncUnordered(10, this::eventHandler)\n      .map(in -> \"`mapSync` emitted event number \" + in.intValue())\n      .runWith(Sink.foreach(str -> System.out.println(str)), system);\nWhen running the stream above the logging output would look like:\n[...]\nProcessing event numner Event(27)...\nCompleted processing 27\n`mapAsyncUnordered` emitted event number: 27\nProcessing event numner Event(28)...\nCompleted processing 22\n`mapAsyncUnordered` emitted event number: 22\nProcessing event numner Event(29)...\nCompleted processing 26\n`mapAsyncUnordered` emitted event number: 26\nProcessing event numner Event(30)...\nCompleted processing 30\n`mapAsyncUnordered` emitted event number: 30\nProcessing event numner Event(31)...\nCompleted processing 31\n`mapAsyncUnordered` emitted event number: 31\n[...]\nSee mapAsync for a variant with ordering guarantees.","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/mapAsyncUnordered.html#reactive-streams-semantics","text":"emits any of the Future s CompletionStage s returned by the provided function complete backpressures when the number of Future s CompletionStage s reaches the configured parallelism and the downstream backpressures completes upstream completes and all Future s CompletionStage s has been completed and all elements has been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mapConcat.html","text":"","title":"mapConcat"},{"location":"/stream/operators/Source-or-Flow/mapConcat.html#mapconcat","text":"Transform each element into zero or more elements that are individually passed downstream.\nSimple operators","title":"mapConcat"},{"location":"/stream/operators/Source-or-Flow/mapConcat.html#signature","text":"Source.mapConcatSource.mapConcat Flow.mapConcatFlow.mapConcat","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mapConcat.html#description","text":"Transform each element into zero or more elements that are individually passed downstream. This can be used to flatten collections into individual stream elements. Returning an empty iterable results in zero elements being passed downstream rather than the stream being cancelled.\nSee also statefulMapConcat, flatMapConcat, flatMapMerge","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mapConcat.html#example","text":"The following takes a stream of integers and emits each element twice downstream.\nScala copysourcedef duplicate(i: Int): List[Int] = List(i, i)\n\nSource(1 to 3).mapConcat(i => duplicate(i)).runForeach(println)\n// prints:\n// 1\n// 1\n// 2\n// 2\n// 3\n// 3 Java copysourceIterable<Integer> duplicate(int i) {\n  return Arrays.asList(i, i);\n}\n\n  Source.from(Arrays.asList(1, 2, 3))\n      .mapConcat(i -> duplicate(i))\n      .runForeach(System.out::println, system);\n  // prints:\n  // 1\n  // 1\n  // 2\n  // 2\n  // 3\n  // 3","title":"Example"},{"location":"/stream/operators/Source-or-Flow/mapConcat.html#reactive-streams-semantics","text":"emits when the mapping function returns an element or there are still remaining elements from the previously calculated collection backpressures when downstream backpressures or there are still available elements from the previously calculated collection completes when upstream completes and all remaining elements has been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mapError.html","text":"","title":"mapError"},{"location":"/stream/operators/Source-or-Flow/mapError.html#maperror","text":"While similar to recover this operators can be used to transform an error signal to a different one without logging it as an error in the process.\nError handling","title":"mapError"},{"location":"/stream/operators/Source-or-Flow/mapError.html#signature","text":"Source.mapErrorSource.mapError Flow.mapErrorFlow.mapError","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mapError.html#description","text":"While similar to recover this operators can be used to transform an error signal to a different one without logging it as an error in the process. So in that sense it is NOT exactly equivalent to recover(t => throw t2) since recover would log the t2 error.\nSince the underlying failure signal onError arrives out-of-band, it might jump over existing elements. This operators can recover the failure signal, but not the skipped elements, which will be dropped.\nSimilarly to recover throwing an exception inside mapError will be logged on ERROR level automatically.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mapError.html#example","text":"The following example demonstrates a stream which throws ArithmeticException when the element 0 goes through the map operator. ThemapError is used to transform this exception to UnsupportedOperationException.\nScala copysourceSource(-1 to 1)\n  .map(1 / _)\n  .mapError {\n    case _: ArithmeticException =>\n      new UnsupportedOperationException(\"Divide by Zero Operation is not supported.\") with NoStackTrace\n  }\n  .runWith(Sink.seq)\n  .onComplete {\n    case Success(value) => println(value.mkString)\n    case Failure(ex)    => println(ex.getMessage)\n  }\n\n// prints \"Divide by Zero Operation is not supported.\" Java copysource final ActorSystem system = ActorSystem.create(\"mapError-operator-example\");\nSource.from(Arrays.asList(-1, 0, 1))\n    .map(x -> 1 / x)\n    .mapError(\n        ArithmeticException.class,\n        (ArithmeticException e) ->\n            new UnsupportedOperationException(\"Divide by Zero Operation is not supported.\"))\n    .runWith(Sink.seq(), system)\n    .whenComplete(\n        (result, exception) -> {\n          if (result != null) System.out.println(result.toString());\n          else System.out.println(exception.getMessage());\n        });\n\n// prints \"Divide by Zero Operation is not supported.\"","title":"Example"},{"location":"/stream/operators/Source-or-Flow/mapError.html#reactive-streams-semantics","text":"emits when element is available from the upstream or upstream is failed and pf returns an element backpressures when downstream backpressures completes when upstream completes or upstream failed with exception pf can handle","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/maybe.html","text":"","title":"Source.maybe"},{"location":"/stream/operators/Source/maybe.html#source-maybe","text":"Create a source that emits once the materialized Promise CompletableFuture is completed with a value.\nSource operators","title":"Source.maybe"},{"location":"/stream/operators/Source/maybe.html#signature","text":"Source.maybeSource.maybe","title":"Signature"},{"location":"/stream/operators/Source/maybe.html#description","text":"Create a source with a materialized Promise[Option[T]] CompletableFuture<Optional<T>> which controls what element will be emitted by the Source. This makes it possible to inject a value into a stream after creation.\nIf the materialized promise is completed with a Somenon-empty Optional, that value will be produced downstream, followed by completion. If the materialized promise is completed with a Noneempty Optional, no value will be produced downstream and completion will be signalled immediately. If the materialized promise is completed with a failure, then the source will fail with that error. If the downstream of this source cancels or fails before the promise has been completed, then the promise will be completed with Noneempty Optional.\nSource.maybe has some similarities with Source.fromFutureSource.fromCompletionStage. One difference is that a new PromiseCompletableFuture is materialized from Source.maybe each time the stream is run while the FutureCompletionStage given to Source.fromFutureSource.fromCompletionStage can only be completed once.\nSource.queue is an alternative for emitting more than one element.","title":"Description"},{"location":"/stream/operators/Source/maybe.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl._\nimport scala.concurrent.Promise\n\nval source = Source.maybe[Int].to(Sink.foreach(elem => println(elem)))\n\nval promise1: Promise[Option[Int]] = source.run()\npromise1.success(Some(1)) // prints 1\n\n// a new Promise is returned when the stream is materialized\nval promise2 = source.run()\npromise2.success(Some(2)) // prints 2 Java copysourceimport org.apache.pekko.stream.javadsl.RunnableGraph;\nimport java.util.concurrent.CompletableFuture;\nSource<Integer, CompletableFuture<Optional<Integer>>> source = Source.<Integer>maybe();\nRunnableGraph<CompletableFuture<Optional<Integer>>> runnable =\n    source.to(Sink.foreach(System.out::println));\n\nCompletableFuture<Optional<Integer>> completable1 = runnable.run(system);\ncompletable1.complete(Optional.of(1)); // prints 1\n\nCompletableFuture<Optional<Integer>> completable2 = runnable.run(system);\ncompletable2.complete(Optional.of(2)); // prints 2\nThe Source.maybe[Int] will return a Promise[Option[Int]]CompletableFuture<Optional<Integer>> materialized value. That PromiseCompletableFuture can be completed later. Each time the stream is run a new PromiseCompletableFuture is returned.","title":"Example"},{"location":"/stream/operators/Source/maybe.html#reactive-streams-semantics","text":"emits when the returned promise is completed with some value completes after emitting some value, or directly if the promise is completed with no value","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/merge.html","text":"","title":"merge"},{"location":"/stream/operators/Source-or-Flow/merge.html#merge","text":"Merge multiple sources.\nFan-in operators","title":"merge"},{"location":"/stream/operators/Source-or-Flow/merge.html#signature","text":"Source.mergeSource.merge Flow.mergeFlow.merge","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/merge.html#description","text":"Merge multiple sources. Picks elements randomly if all sources has elements ready.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/merge.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl.{ Sink, Source }\n\nval sourceA = Source(List(1, 2, 3, 4))\nval sourceB = Source(List(10, 20, 30, 40))\n\nsourceA.merge(sourceB).runWith(Sink.foreach(println))\n// merging is not deterministic, can for example print 1, 2, 3, 4, 10, 20, 30, 40 Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(10, 20, 30, 40));\nsourceA.merge(sourceB).runForeach(System.out::println, system);\n// merging is not deterministic, can for example print 1, 2, 3, 4, 10, 20, 30, 40","title":"Example"},{"location":"/stream/operators/Source-or-Flow/merge.html#reactive-streams-semantics","text":"emits when one of the inputs has an element available backpressures when downstream backpressures completes when all upstreams complete (This behavior is changeable to completing when any upstream completes by setting eagerComplete=true.)","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mergeAll.html","text":"","title":"mergeAll"},{"location":"/stream/operators/Source-or-Flow/mergeAll.html#mergeall","text":"Merge multiple sources.\nFan-in operators","title":"mergeAll"},{"location":"/stream/operators/Source-or-Flow/mergeAll.html#signature","text":"Source.mergeAllSource.mergeAll Flow.mergeAllFlow.mergeAll","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mergeAll.html#description","text":"Merge multiple sources. Picks elements randomly if all sources has elements ready.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mergeAll.html#example","text":"Scala copysourceval sourceA = Source(1 to 3)\nval sourceB = Source(4 to 6)\nval sourceC = Source(7 to 10)\nsourceA.mergeAll(List(sourceB, sourceC), eagerComplete = false).runForeach(println)\n// merging is not deterministic, can for example print 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Java copysourceSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(4, 5, 6));\nSource<Integer, NotUsed> sourceC = Source.from(Arrays.asList(7, 8, 9, 10));\nsourceA\n    .mergeAll(Arrays.asList(sourceB, sourceC), false)\n    .runForeach(System.out::println, system);\n// merging is not deterministic, can for example print 1, 2, 3, 4, 5, 6, 7, 8, 9, 10","title":"Example"},{"location":"/stream/operators/Source-or-Flow/mergeAll.html#reactive-streams-semantics","text":"emits when one of the inputs has an element available backpressures when downstream backpressures completes when all upstreams complete (This behavior is changeable to completing when any upstream completes by setting eagerComplete=true.)","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mergeLatest.html","text":"","title":"mergeLatest"},{"location":"/stream/operators/Source-or-Flow/mergeLatest.html#mergelatest","text":"Merge multiple sources.\nFan-in operators","title":"mergeLatest"},{"location":"/stream/operators/Source-or-Flow/mergeLatest.html#signature","text":"Flow.mergeLatestFlow.mergeLatest","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mergeLatest.html#description","text":"MergeLatest joins elements from N input streams into stream of lists of size N. The i-th element in list is the latest emitted element from i-th input stream. MergeLatest emits list for each element emitted from some input stream, but only after each input stream emitted at least one element If eagerComplete is set to true then it completes as soon as the first upstream completes otherwise when all upstreams complete.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mergeLatest.html#example","text":"This example takes a stream of prices and quantities and emits the price each time the price of quantity changes:\nScala copysourceval prices = Source(List(100, 101, 99, 103))\nval quantity = Source(List(1, 3, 4, 2))\n\nprices\n  .mergeLatest(quantity)\n  .map {\n    case price :: quantity :: Nil => price * quantity\n  }\n  .runForeach(println)\n\n// prints something like:\n// 100\n// 101\n// 303\n// 297\n// 396\n// 412\n// 206 Java copysourceSource<Integer, NotUsed> prices = Source.from(Arrays.asList(100, 101, 99, 103));\nSource<Integer, NotUsed> quantities = Source.from(Arrays.asList(1, 3, 4, 2));\n\nprices\n    .mergeLatest(quantities, true)\n    .map(priceAndQuantity -> priceAndQuantity.get(0) * priceAndQuantity.get(1))\n    .runForeach(System.out::println, system);\n\n// prints something like:\n// 100\n// 101\n// 303\n// 297\n// 396\n// 412\n// 206","title":"Example"},{"location":"/stream/operators/Source-or-Flow/mergeLatest.html#reactive-streams-semantics","text":"emits when element is available from some input and each input emits at least one element from stream start completes all upstreams complete (eagerClose=false) or one upstream completes (eagerClose=true)","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mergePreferred.html","text":"","title":"mergePreferred"},{"location":"/stream/operators/Source-or-Flow/mergePreferred.html#mergepreferred","text":"Merge multiple sources.\nFan-in operators","title":"mergePreferred"},{"location":"/stream/operators/Source-or-Flow/mergePreferred.html#signature","text":"Source.mergePreferredSource.mergePreferred Flow.mergePreferredFlow.mergePreferred","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mergePreferred.html#description","text":"Merge multiple sources. If all sources have elements ready, emit the preferred source first. Then emit the preferred source again if another element is pushed. Otherwise, emit all the secondary sources. Repeat until streams are empty. For the case with two sources, when preferred is set to true then prefer the right source, otherwise prefer the left source (see examples).","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mergePreferred.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl.{ Sink, Source }\n\nval sourceA = Source(List(1, 2, 3, 4))\nval sourceB = Source(List(10, 20, 30, 40))\n\nsourceA.mergePreferred(sourceB, false).runWith(Sink.foreach(println))\n// prints 1, 10, ... since both sources have their first element ready and the left source is preferred\n\nsourceA.mergePreferred(sourceB, true).runWith(Sink.foreach(println))\n// prints 10, 1, ... since both sources have their first element ready and the right source is preferred Java copysourceSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(10, 20, 30, 40));\n\nsourceA.mergePreferred(sourceB, false, false).runForeach(System.out::println, system);\n// prints 1, 10, ... since both sources have their first element ready and the left source is\n// preferred\n\nsourceA.mergePreferred(sourceB, true, false).runForeach(System.out::println, system);\n// prints 10, 1, ... since both sources have their first element ready and the right source is\n// preferred","title":"Example"},{"location":"/stream/operators/Source-or-Flow/mergePreferred.html#reactive-streams-semantics","text":"emits when one of the inputs has an element available, preferring a defined input if multiple have elements available backpressures when downstream backpressures completes when all upstreams complete (This behavior is changeable to completing when any upstream completes by setting eagerComplete=true.)","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mergePrioritized.html","text":"","title":"mergePrioritized"},{"location":"/stream/operators/Source-or-Flow/mergePrioritized.html#mergeprioritized","text":"Merge multiple sources.\nFan-in operators","title":"mergePrioritized"},{"location":"/stream/operators/Source-or-Flow/mergePrioritized.html#signature","text":"Source.mergePrioritizedSource.mergePrioritized Flow.mergePrioritizedFlow.mergePrioritized","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mergePrioritized.html#description","text":"Merge multiple sources. Prefer sources depending on priorities if all sources have elements ready. If a subset of all sources have elements ready the relative priorities for those sources are used to prioritize. For example, when used with only two sources, the left source has a probability of (leftPriority) / (leftPriority + rightPriority) of being prioritized and similarly for the right source. The priorities for each source must be positive integers.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mergePrioritized.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl.{ Sink, Source }\n\nval sourceA = Source(List(1, 2, 3, 4))\nval sourceB = Source(List(10, 20, 30, 40))\n\nsourceA.mergePrioritized(sourceB, 99, 1).runWith(Sink.foreach(println))\n// prints e.g. 1, 10, 2, 3, 4, 20, 30, 40 since both sources have their first element ready and the left source\n// has higher priority – if both sources have elements ready, sourceA has a 99% chance of being picked next\n// while sourceB has a 1% chance Java copysourceSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(10, 20, 30, 40));\n\nsourceA.mergePrioritized(sourceB, 99, 1, false).runForeach(System.out::println, system);\n// prints e.g. 1, 10, 2, 3, 4, 20, 30, 40 since both sources have their first element ready and\n// the left source has higher priority – if both sources have elements ready, sourceA has a\n// 99% chance of being picked next while sourceB has a 1% chance","title":"Example"},{"location":"/stream/operators/Source-or-Flow/mergePrioritized.html#reactive-streams-semantics","text":"emits when one of the inputs has an element available, preferring inputs based on their priorities if multiple have elements available backpressures when downstream backpressures completes when all upstreams complete (This behavior is changeable to completing when any upstream completes by setting eagerComplete=true.)","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/mergePrioritizedN.html","text":"","title":"mergePrioritizedN"},{"location":"/stream/operators/Source/mergePrioritizedN.html#mergeprioritizedn","text":"Merge multiple sources with priorities.\nFan-in operators","title":"mergePrioritizedN"},{"location":"/stream/operators/Source/mergePrioritizedN.html#signature","text":"","title":"Signature"},{"location":"/stream/operators/Source/mergePrioritizedN.html#description","text":"Merge multiple sources. Prefer sources depending on priorities if all sources have elements ready. If a subset of all sources have elements ready the relative priorities for those sources are used to prioritize. For example, when used with only three sources sourceA, sourceB and sourceC, the sourceA has a probability of (priorityOfA) / (priorityOfA + priorityOfB + priorityOfC) of being prioritized and similarly for the rest of the sources. The priorities for each source must be positive integers.","title":"Description"},{"location":"/stream/operators/Source/mergePrioritizedN.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl.{ Sink, Source }\n\nval sourceA = Source(List(1, 2, 3, 4))\nval sourceB = Source(List(10, 20, 30, 40))\nval sourceC = Source(List(100, 200, 300, 400))\n\nSource\n  .mergePrioritizedN(List((sourceA, 9900), (sourceB, 99), (sourceC, 1)), eagerComplete = false)\n  .runWith(Sink.foreach(println))\n// prints e.g. 1, 100, 2, 3, 4, 10, 20, 30, 40, 200, 300, 400  since both sources have their first element ready and\n// the left sourceA has higher priority - if both sources have elements ready, sourceA has a 99% chance of being picked next\n// while sourceB has a 0.99% chance and sourceC has a 0.01% chance Java copysourceSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(10, 20, 30, 40));\nSource<Integer, NotUsed> sourceC = Source.from(Arrays.asList(100, 200, 300, 400));\nList<Pair<Source<Integer, ?>, Integer>> sourcesAndPriorities =\n    Arrays.asList(new Pair<>(sourceA, 9900), new Pair<>(sourceB, 99), new Pair<>(sourceC, 1));\nSource.mergePrioritizedN(sourcesAndPriorities, false).runForeach(System.out::println, system);\n// prints e.g. 1, 100, 2, 3, 4, 10, 20, 30, 40, 200, 300, 400  since both sources have their\n// first element ready and\n// the left sourceA has higher priority - if both sources have elements ready, sourceA has a 99%\n// chance of being picked next\n// while sourceB has a 0.99% chance and sourceC has a 0.01% chance","title":"Example"},{"location":"/stream/operators/Source/mergePrioritizedN.html#reactive-streams-semantics","text":"emits when one of the inputs has an element available, preferring inputs based on their priorities if multiple have elements available backpressures when downstream backpressures completes when all upstreams complete (or when any upstream completes if eagerComplete=true.) Cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/MergeSequence.html","text":"","title":"MergeSequence"},{"location":"/stream/operators/MergeSequence.html#mergesequence","text":"Merge a linear sequence partitioned across multiple sources.\nFan-in operators","title":"MergeSequence"},{"location":"/stream/operators/MergeSequence.html#signature","text":"MergeSequenceMergeSequence","title":"Signature"},{"location":"/stream/operators/MergeSequence.html#description","text":"Merge a linear sequence partitioned across multiple sources. Each element from upstream must have a defined index, starting from 0. There must be no gaps in the sequence, nor may there be any duplicates. Each upstream source must be ordered by the sequence.","title":"Description"},{"location":"/stream/operators/MergeSequence.html#example","text":"MergeSequence is most useful when used in combination with Partition, to merge the partitioned stream back into a single stream, while maintaining the order of the original elements. zipWithIndex can be used before partitioning the stream to generate the index.\nThe example below shows partitioning a stream of messages into one stream for elements that must be processed by a given processing flow, and another stream for elements for which no processing will be done, and then merges them back together so that the messages can be acknowledged in order.\nScala copysourceimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.ClosedShape\nimport pekko.stream.scaladsl.{ Flow, GraphDSL, MergeSequence, Partition, RunnableGraph, Sink, Source }\n\nval subscription: Source[Message, NotUsed] = createSubscription()\nval messageProcessor: Flow[(Message, Long), (Message, Long), NotUsed] =\n  createMessageProcessor()\nval messageAcknowledger: Sink[Message, NotUsed] = createMessageAcknowledger()\n\nRunnableGraph\n  .fromGraph(GraphDSL.create() { implicit builder =>\n    import GraphDSL.Implicits._\n    // Partitions stream into messages that should or should not be processed\n    val partition = builder.add(Partition[(Message, Long)](2,\n      {\n        case (message, _) if shouldProcess(message) => 0\n        case _                                      => 1\n      }))\n    // Merges stream by the index produced by zipWithIndex\n    val merge = builder.add(MergeSequence[(Message, Long)](2)(_._2))\n\n    subscription.zipWithIndex ~> partition.in\n    // First goes through message processor\n    partition.out(0) ~> messageProcessor ~> merge\n    // Second partition bypasses message processor\n    partition.out(1)    ~> merge\n    merge.out.map(_._1) ~> messageAcknowledger\n    ClosedShape\n  })\n  .run()\n Java copysourceimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.japi.Pair;\nimport org.apache.pekko.stream.ClosedShape;\nimport org.apache.pekko.stream.UniformFanInShape;\nimport org.apache.pekko.stream.UniformFanOutShape;\nimport org.apache.pekko.stream.javadsl.Flow;\nimport org.apache.pekko.stream.javadsl.GraphDSL;\nimport org.apache.pekko.stream.javadsl.MergeSequence;\nimport org.apache.pekko.stream.javadsl.Partition;\nimport org.apache.pekko.stream.javadsl.RunnableGraph;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.Source;\n\nSource<Message, NotUsed> subscription = createSubscription();\nFlow<Pair<Message, Long>, Pair<Message, Long>, NotUsed> messageProcessor =\n    createMessageProcessor();\nSink<Message, NotUsed> messageAcknowledger = createMessageAcknowledger();\n\nRunnableGraph.fromGraph(\n        GraphDSL.create(\n            builder -> {\n              // Partitions stream into messages that should or should not be processed\n              UniformFanOutShape<Pair<Message, Long>, Pair<Message, Long>> partition =\n                  builder.add(\n                      Partition.create(2, element -> shouldProcess(element.first()) ? 0 : 1));\n              // Merges stream by the index produced by zipWithIndex\n              UniformFanInShape<Pair<Message, Long>, Pair<Message, Long>> merge =\n                  builder.add(MergeSequence.create(2, Pair::second));\n\n              builder.from(builder.add(subscription.zipWithIndex())).viaFanOut(partition);\n              // First goes through message processor\n              builder.from(partition.out(0)).via(builder.add(messageProcessor)).viaFanIn(merge);\n              // Second partition bypasses message processor\n              builder.from(partition.out(1)).viaFanIn(merge);\n\n              // Unwrap message index pairs and send to acknowledger\n              builder\n                  .from(merge.out())\n                  .to(\n                      builder.add(\n                          Flow.<Pair<Message, Long>>create()\n                              .map(Pair::first)\n                              .to(messageAcknowledger)));\n\n              return ClosedShape.getInstance();\n            }))\n    .run(system);","title":"Example"},{"location":"/stream/operators/MergeSequence.html#reactive-streams-semantics","text":"emits when one of the upstreams has the next expected element in the sequence available. backpressures when downstream backpressures completes when all upstreams complete cancels downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/mergeSorted.html","text":"","title":"mergeSorted"},{"location":"/stream/operators/Source-or-Flow/mergeSorted.html#mergesorted","text":"Merge multiple sources.\nFan-in operators","title":"mergeSorted"},{"location":"/stream/operators/Source-or-Flow/mergeSorted.html#signature","text":"Source.mergeSortedSource.mergeSorted Flow.mergeSortedFlow.mergeSorted","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/mergeSorted.html#description","text":"Merge multiple sources. Waits for one element to be ready from each input stream and emits the smallest element.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/mergeSorted.html#example","text":"Scala copysourceimport org.apache.pekko.stream.scaladsl.{ Sink, Source }\n\nval sourceA = Source(List(1, 3, 5, 7))\nval sourceB = Source(List(2, 4, 6, 8))\n\nsourceA.mergeSorted(sourceB).runWith(Sink.foreach(println))\n// prints 1, 2, 3, 4, 5, 6, 7, 8\n\nval sourceC = Source(List(20, 1, 1, 1))\n\nsourceA.mergeSorted(sourceC).runWith(Sink.foreach(println))\n// prints 1, 3, 5, 7, 20, 1, 1, 1 Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<Integer, NotUsed> sourceA = Source.from(Arrays.asList(1, 3, 5, 7));\nSource<Integer, NotUsed> sourceB = Source.from(Arrays.asList(2, 4, 6, 8));\nsourceA\n    .mergeSorted(sourceB, Comparator.<Integer>naturalOrder())\n    .runForeach(System.out::println, system);\n// prints 1, 2, 3, 4, 5, 6, 7, 8\n\nSource<Integer, NotUsed> sourceC = Source.from(Arrays.asList(20, 1, 1, 1));\nsourceA\n    .mergeSorted(sourceC, Comparator.<Integer>naturalOrder())\n    .runForeach(System.out::println, system);\n// prints 1, 3, 5, 7, 20, 1, 1, 1","title":"Example"},{"location":"/stream/operators/Source-or-Flow/mergeSorted.html#reactive-streams-semantics","text":"emits when all of the inputs have an element available backpressures when downstream backpressures completes when all upstreams complete","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/monitor.html","text":"","title":"monitor"},{"location":"/stream/operators/Source-or-Flow/monitor.html#monitor","text":"Materializes to a FlowMonitor that monitors messages flowing through or completion of the operators.\nWatching status operators","title":"monitor"},{"location":"/stream/operators/Source-or-Flow/monitor.html#signature","text":"Source.monitorSource.monitor Flow.monitorFlow.monitor","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/monitor.html#description","text":"Materializes to a FlowMonitor that monitors messages flowing through or completion of the stream. Elements pass through unchanged. Note that the FlowMonitor inserts a memory barrier every time it processes an event, and may therefore affect performance. The provided FlowMonitor contains a state field you can use to peek and get information about the stream.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/monitor.html#example","text":"The example below uses the monitorMat variant of monitor. The only difference between the two operators is that monitorMat has a combine argument so we can decide which materialization value to keep. In the sample below be Keep.right so only the FlowMonitor[Int] is returned.\nScala copysourceval source: Source[Int, NotUsed] =\n  Source.fromIterator(() => Iterator.from(0))\n\ndef printMonitorState(flowMonitor: FlowMonitor[Int]) =\n  flowMonitor.state match {\n    case FlowMonitorState.Initialized =>\n      println(\"Stream is initialized but hasn't processed any element\")\n    case FlowMonitorState.Received(msg) =>\n      println(s\"Last element processed: $msg\")\n    case FlowMonitorState.Failed(cause) =>\n      println(s\"Stream failed with cause $cause\")\n    case FlowMonitorState.Finished => println(s\"Stream completed already\")\n  }\n\nval monitoredSource: Source[Int, FlowMonitor[Int]] = source.take(6).throttle(5, 1.second).monitorMat(Keep.right)\nval (flowMonitor, futureDone) =\n  monitoredSource.toMat(Sink.foreach(println))(Keep.both).run()\n\n// If we peek in the monitor too early, it's possible it was not initialized yet.\nprintMonitorState(flowMonitor)\n\n// Periodically check the monitor\nSource.tick(200.millis, 400.millis, \"\").runForeach(_ => printMonitorState(flowMonitor))\n Java copysourceprivate static <T> void printMonitorState(FlowMonitorState.StreamState<T> state) {\n  if (state == FlowMonitorState.finished()) {\n    System.out.println(\"Stream is initialized but hasn't processed any element\");\n  } else if (state instanceof FlowMonitorState.Received) {\n    FlowMonitorState.Received msg = (FlowMonitorState.Received) state;\n    System.out.println(\"Last message received: \" + msg.msg());\n  } else if (state instanceof FlowMonitorState.Failed) {\n    Throwable cause = ((FlowMonitorState.Failed) state).cause();\n    System.out.println(\"Stream failed with cause: \" + cause.getMessage());\n  } else {\n    System.out.println(\"Stream completed already\");\n  }\n}\n  Source<Integer, FlowMonitor<Integer>> monitoredSource =\n      Source.fromIterator(() -> Arrays.asList(0, 1, 2, 3, 4, 5).iterator())\n          .throttle(5, Duration.ofSeconds(1))\n          .monitorMat(Keep.right());\n\n  Pair<FlowMonitor<Integer>, CompletionStage<Done>> run =\n      monitoredSource.toMat(Sink.foreach(System.out::println), Keep.both()).run(actorSystem);\n\n  FlowMonitor<Integer> monitor = run.first();\n\n  // If we peek in the monitor too early, it's possible it was not initialized yet.\n  printMonitorState(monitor.state());\n\n  // Periodically check the monitor\n  Source.tick(Duration.ofMillis(200), Duration.ofMillis(400), \"\")\n      .runForeach(__ -> printMonitorState(monitor.state()), actorSystem);\nWhen run, the sample code will produce something similar to:\nStream is initialized but hasn't processed any element\n0\n1\n2\nLast element processed: 2\n3\n4\n5\nStream completed already","title":"Example"},{"location":"/stream/operators/Source-or-Flow/monitor.html#reactive-streams-semantics","text":"emits when upstream emits an element backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/never.html","text":"","title":"never"},{"location":"/stream/operators/Source/never.html#never","text":"Never emit any elements, never complete and never fail.\nSource operators\nSource.empty, a source which emits nothing and completes immediately.","title":"never"},{"location":"/stream/operators/Source/never.html#signature","text":"Source.neverSource.never","title":"Signature"},{"location":"/stream/operators/Source/never.html#description","text":"Create a source which never emits any elements, never completes and never failes. Useful for tests.","title":"Description"},{"location":"/stream/operators/Source/never.html#reactive-streams-semantics","text":"emits never completes never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/never.html","text":"","title":"Sink.never"},{"location":"/stream/operators/Sink/never.html#sink-never","text":"Always backpressure never cancel and never consume any elements from the stream.\nSink operators","title":"Sink.never"},{"location":"/stream/operators/Sink/never.html#signature","text":"Sink.neverSink.never Sink.neverSink.never","title":"Signature"},{"location":"/stream/operators/Sink/never.html#description","text":"A Sink that will always backpressure never cancel and never consume any elements from the stream.","title":"Description"},{"location":"/stream/operators/Sink/never.html#reactive-streams-semantics","text":"cancels never backpressures always","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/onComplete.html","text":"","title":"Sink.onComplete"},{"location":"/stream/operators/Sink/onComplete.html#sink-oncomplete","text":"Invoke a callback when the stream has completed or failed.\nSink operators","title":"Sink.onComplete"},{"location":"/stream/operators/Sink/onComplete.html#signature","text":"Sink.onCompleteSink.onComplete","title":"Signature"},{"location":"/stream/operators/Sink/onComplete.html#description","text":"Invoke a callback when the stream has completed or failed.","title":"Description"},{"location":"/stream/operators/Sink/onComplete.html#reactive-streams-semantics","text":"cancels never backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/RestartSource/onFailuresWithBackoff.html","text":"","title":"RestartSource.onFailuresWithBackoff"},{"location":"/stream/operators/RestartSource/onFailuresWithBackoff.html#restartsource-onfailureswithbackoff","text":"Wrap the given SourceSource with a SourceSource that will restart it when it fails using an exponential backoff. Notice that this SourceSource will not restart on completion of the wrapped flow.\nError handling","title":"RestartSource.onFailuresWithBackoff"},{"location":"/stream/operators/RestartSource/onFailuresWithBackoff.html#signature","text":"RestartSource.onFailuresWithBackoffRestartSource.onFailuresWithBackoff","title":"Signature"},{"location":"/stream/operators/RestartSource/onFailuresWithBackoff.html#description","text":"Wraps the given SourceSource with a SourceSource that will restart it when it fails using exponential backoff. The backoff resets back to minBackoff if there hasn’t been a restart within maxRestartsWithin (which defaults to minBackoff).\nThis SourceSource will not emit a failure as long as maxRestarts is not reached. The failure of the wrapped SourceSource is handled by restarting it. However, the wrapped SourceSource can be cancelled by cancelling this SourceSource. When that happens, the wrapped SourceSource, if currently running will, be cancelled and not restarted. This can be triggered by the downstream cancelling, or externally by introducing a KillSwitch right after this SourceSource in the graph.\nThis uses the same exponential backoff algorithm as BackoffOptsBackoffOpts.\nSee also:\nRestartSource.withBackoff RestartFlow.onFailuresWithBackoff RestartFlow.withBackoff RestartSink.withBackoff","title":"Description"},{"location":"/stream/operators/RestartSource/onFailuresWithBackoff.html#examples","text":"This shows that a Source is not restarted if it completes, only if it fails. Tick is only printed three times as the take(3) means the inner source completes successfully after emitting the first 3 elements.\nScala copysourceval finiteSource = Source.tick(1.second, 1.second, \"tick\").take(3)\nval forever = RestartSource.onFailuresWithBackoff(RestartSettings(1.second, 10.seconds, 0.1))(() => finiteSource)\nforever.runWith(Sink.foreach(println))\n// prints\n// tick\n// tick\n// tick Java copysourceSource<String, Cancellable> finiteSource =\n    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), \"tick\").take(3);\nSource<String, NotUsed> forever =\n    RestartSource.onFailuresWithBackoff(\n        RestartSettings.create(Duration.ofSeconds(1), Duration.ofSeconds(10), 0.1),\n        () -> finiteSource);\nforever.runWith(Sink.foreach(System.out::println), system);\n// prints\n// tick\n// tick\n// tick\nIf the inner source instead fails, it will be restarted with an increasing backoff. The source emits 1, 2, 3, and then throws an exception. The first time the exception is thrown the source is restarted after 1s, then 2s etc, until the maxBackoff of 10s.\nScala copysource// could throw if for example it used a database connection to get rows\nval flakySource: Source[() => Int, NotUsed] =\n  Source(List(() => 1, () => 2, () => 3, () => throw CantConnectToDatabase(\"darn\")))\nval forever =\n  RestartSource.onFailuresWithBackoff(\n    RestartSettings(minBackoff = 1.second, maxBackoff = 10.seconds, randomFactor = 0.1))(() => flakySource)\nforever.runWith(Sink.foreach(nr => system.log.info(\"{}\", nr())))\n// logs\n// [INFO] [12/10/2019 13:51:58.300] [default-pekko.test.stream-dispatcher-7] [pekko.actor.ActorSystemImpl(default)] 1\n// [INFO] [12/10/2019 13:51:58.301] [default-pekko.test.stream-dispatcher-7] [pekko.actor.ActorSystemImpl(default)] 2\n// [INFO] [12/10/2019 13:51:58.302] [default-pekko.test.stream-dispatcher-7] [pekko.actor.ActorSystemImpl(default)] 3\n// [WARN] [12/10/2019 13:51:58.310] [default-pekko.test.stream-dispatcher-7] [RestartWithBackoffSource(pekko://default)] Restarting graph due to failure. stack_trace:  (docs.stream.operators.source.Restart$CantConnectToDatabase: darn)\n// --> 1 second gap\n// [INFO] [12/10/2019 13:51:59.379] [default-pekko.test.stream-dispatcher-8] [pekko.actor.ActorSystemImpl(default)] 1\n// [INFO] [12/10/2019 13:51:59.382] [default-pekko.test.stream-dispatcher-8] [pekko.actor.ActorSystemImpl(default)] 2\n// [INFO] [12/10/2019 13:51:59.383] [default-pekko.test.stream-dispatcher-8] [pekko.actor.ActorSystemImpl(default)] 3\n// [WARN] [12/10/2019 13:51:59.386] [default-pekko.test.stream-dispatcher-8] [RestartWithBackoffSource(pekko://default)] Restarting graph due to failure. stack_trace:  (docs.stream.operators.source.Restart$CantConnectToDatabase: darn)\n// --> 2 second gap\n// [INFO] [12/10/2019 13:52:01.594] [default-pekko.test.stream-dispatcher-8] [pekko.actor.ActorSystemImpl(default)] 1\n// [INFO] [12/10/2019 13:52:01.595] [default-pekko.test.stream-dispatcher-8] [pekko.actor.ActorSystemImpl(default)] 2\n// [INFO] [12/10/2019 13:52:01.595] [default-pekko.test.stream-dispatcher-8] [pekko.actor.ActorSystemImpl(default)] 3\n// [WARN] [12/10/2019 13:52:01.596] [default-pekko.test.stream-dispatcher-8] [RestartWithBackoffSource(pekko://default)] Restarting graph due to failure. stack_trace:  (docs.stream.operators.source.Restart$CantConnectToDatabase: darn) Java copysource// could throw if for example it used a database connection to get rows\nSource<Creator<Integer>, NotUsed> flakySource =\n    Source.from(\n        Arrays.<Creator<Integer>>asList(\n            () -> 1,\n            () -> 2,\n            () -> 3,\n            () -> {\n              throw new RuntimeException(\"darn\");\n            }));\nSource<Creator<Integer>, NotUsed> forever =\n    RestartSource.onFailuresWithBackoff(\n        RestartSettings.create(Duration.ofSeconds(1), Duration.ofSeconds(10), 0.1),\n        () -> flakySource);\nforever.runWith(\n    Sink.foreach((Creator<Integer> nr) -> system.log().info(\"{}\", nr.create())), system);\n// logs\n// [INFO] [12/10/2019 13:51:58.300] [default-pekko.test.stream-dispatcher-7]\n// [pekko.actor.ActorSystemImpl(default)] 1\n// [INFO] [12/10/2019 13:51:58.301] [default-pekko.test.stream-dispatcher-7]\n// [pekko.actor.ActorSystemImpl(default)] 2\n// [INFO] [12/10/2019 13:51:58.302] [default-pekko.test.stream-dispatcher-7]\n// [pekko.actor.ActorSystemImpl(default)] 3\n// [WARN] [12/10/2019 13:51:58.310] [default-pekko.test.stream-dispatcher-7]\n// [RestartWithBackoffSource(pekko://default)] Restarting graph due to failure. stack_trace:\n// (RuntimeException: darn)\n// --> 1 second gap\n// [INFO] [12/10/2019 13:51:59.379] [default-pekko.test.stream-dispatcher-8]\n// [pekko.actor.ActorSystemImpl(default)] 1\n// [INFO] [12/10/2019 13:51:59.382] [default-pekko.test.stream-dispatcher-8]\n// [pekko.actor.ActorSystemImpl(default)] 2\n// [INFO] [12/10/2019 13:51:59.383] [default-pekko.test.stream-dispatcher-8]\n// [pekko.actor.ActorSystemImpl(default)] 3\n// [WARN] [12/10/2019 13:51:59.386] [default-pekko.test.stream-dispatcher-8]\n// [RestartWithBackoffSource(pekko://default)] Restarting graph due to failure. stack_trace:\n// (RuntimeException: darn)\n// --> 2 second gap\n// [INFO] [12/10/2019 13:52:01.594] [default-pekko.test.stream-dispatcher-8]\n// [pekko.actor.ActorSystemImpl(default)] 1\n// [INFO] [12/10/2019 13:52:01.595] [default-pekko.test.stream-dispatcher-8]\n// [pekko.actor.ActorSystemImpl(default)] 2\n// [INFO] [12/10/2019 13:52:01.595] [default-pekko.test.stream-dispatcher-8]\n// [pekko.actor.ActorSystemImpl(default)] 3\n// [WARN] [12/10/2019 13:52:01.596] [default-pekko.test.stream-dispatcher-8]\n// [RestartWithBackoffSource(pekko://default)] Restarting graph due to failure. stack_trace:\n// (RuntimeException: darn)\nFinally, to be able to stop the restarting, a kill switch can be used. The kill switch is inserted right after the restart source. The inner source is the same as above so emits 3 elements and then fails. A killswitch is used to be able to stop the source being restarted:\nScala copysourceval flakySource: Source[() => Int, NotUsed] =\n  Source(List(() => 1, () => 2, () => 3, () => throw CantConnectToDatabase(\"darn\")))\nval stopRestarting: UniqueKillSwitch =\n  RestartSource\n    .onFailuresWithBackoff(RestartSettings(1.second, 10.seconds, 0.1))(() => flakySource)\n    .viaMat(KillSwitches.single)(Keep.right)\n    .toMat(Sink.foreach(nr => println(s\"Nr ${nr()}\")))(Keep.left)\n    .run()\n// ... from some where else\n// stop the source from restarting\nstopRestarting.shutdown() Java copysourceSource<Creator<Integer>, NotUsed> flakySource =\n    Source.from(\n        Arrays.<Creator<Integer>>asList(\n            () -> 1,\n            () -> 2,\n            () -> 3,\n            () -> {\n              throw new RuntimeException(\"darn\");\n            }));\nUniqueKillSwitch stopRestarting =\n    RestartSource.onFailuresWithBackoff(\n            RestartSettings.create(Duration.ofSeconds(1), Duration.ofSeconds(10), 0.1),\n            () -> flakySource)\n        .viaMat(KillSwitches.single(), Keep.right())\n        .toMat(Sink.foreach(nr -> System.out.println(\"nr \" + nr.create())), Keep.left())\n        .run(system);\n// ... from some where else\n// stop the source from restarting\nstopRestarting.shutdown();","title":"Examples"},{"location":"/stream/operators/RestartSource/onFailuresWithBackoff.html#reactive-streams-semantics","text":"emits when the wrapped source emits backpressures during backoff and when downstream backpressures completes when the wrapped source completes or maxRestarts are reached within the given time limit cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/RestartFlow/onFailuresWithBackoff.html","text":"","title":"RestartFlow.onFailuresWithBackoff"},{"location":"/stream/operators/RestartFlow/onFailuresWithBackoff.html#restartflow-onfailureswithbackoff","text":"Wrap the given FlowFlow with a FlowFlow that will restart it when it fails using an exponential backoff. Notice that this FlowFlow will not restart on completion of the wrapped flow.\nError handling","title":"RestartFlow.onFailuresWithBackoff"},{"location":"/stream/operators/RestartFlow/onFailuresWithBackoff.html#signature","text":"RestartFlow.onFailuresWithBackoffRestartFlow.onFailuresWithBackoff","title":"Signature"},{"location":"/stream/operators/RestartFlow/onFailuresWithBackoff.html#description","text":"Wrap the given FlowFlow with a FlowFlow that will restart it when it fails using exponential backoff. The backoff resets back to minBackoff if there hasn’t been a restart within maxRestartsWithin (which defaults to minBackoff if max restarts).\nThis FlowFlow will not emit any failure as long as maxRestarts is not reached. The failure of the wrapped FlowFlow will be handled by restarting it. However, any termination signals sent to this FlowFlow will terminate the wrapped FlowFlow, if it’s running, and then the FlowFlow will be allowed to terminate without being restarted.\nThe restart process is inherently lossy, since there is no coordination between cancelling and the sending of messages. A termination signal from either end of the wrapped FlowFlow will cause the other end to be terminated, and any in transit messages will be lost. During backoff, this FlowFlow will backpressure.\nThis uses the same exponential backoff algorithm as BackoffOptsBackoffOpts.\nSee also:\nRestartSource.withBackoff RestartSource.onFailuresWithBackoff RestartFlow.withBackoff RestartSink.withBackoff","title":"Description"},{"location":"/stream/operators/RestartFlow/onFailuresWithBackoff.html#reactive-streams-semantics","text":"emits when the wrapped flow emits backpressures during backoff and when the wrapped flow backpressures completes when the wrapped flow completes or maxRestarts are reached within the given time limit","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/orElse.html","text":"","title":"orElse"},{"location":"/stream/operators/Source-or-Flow/orElse.html#orelse","text":"If the primary source completes without emitting any elements, the elements from the secondary source are emitted.\nFan-in operators","title":"orElse"},{"location":"/stream/operators/Source-or-Flow/orElse.html#signature","text":"Source.orElseSource.orElse Flow.orElseFlow.orElse","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/orElse.html#description","text":"If the primary source completes without emitting any elements, the elements from the secondary source are emitted. If the primary source emits any elements the secondary source is cancelled.\nNote that both sources are materialized directly and the secondary source is backpressured until it becomes the source of elements or is cancelled.\nSignal errors downstream, regardless which of the two sources emitted the error.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/orElse.html#example","text":"Scala copysourceval source1 = Source(List(\"First source\"))\nval source2 = Source(List(\"Second source\"))\nval emptySource = Source.empty[String]\n\nsource1.orElse(source2).runWith(Sink.foreach(println))\n// this will print \"First source\"\n\nemptySource.orElse(source2).runWith(Sink.foreach(println))\n// this will print \"Second source\" Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<String, NotUsed> source1 = Source.from(Arrays.asList(\"First source\"));\nSource<String, NotUsed> source2 = Source.from(Arrays.asList(\"Second source\"));\nSource<String, NotUsed> emptySource = Source.empty();\n\nsource1.orElse(source2).runForeach(System.out::println, system);\n// this will print \"First source\"\n\nemptySource.orElse(source2).runForeach(System.out::println, system);\n// this will print \"Second source\"","title":"Example"},{"location":"/stream/operators/Source-or-Flow/orElse.html#reactive-streams-semantics","text":"emits when an element is available from first stream or first stream closed without emitting any elements and an element is available from the second stream backpressures when downstream backpressures completes the primary stream completes after emitting at least one element, when the primary stream completes without emitting and the secondary stream already has completed or when the secondary stream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Partition.html","text":"","title":"Partition"},{"location":"/stream/operators/Partition.html#partition","text":"Fan-out the stream to several streams.\nFan-out operators","title":"Partition"},{"location":"/stream/operators/Partition.html#signature","text":"PartitionPartition","title":"Signature"},{"location":"/stream/operators/Partition.html#description","text":"Fan-out the stream to several streams. Each upstream element is emitted to one downstream consumer according to the partitioner function applied to the element.","title":"Description"},{"location":"/stream/operators/Partition.html#example","text":"Here is an example of using Partition to split a Source of integers to one Sink for the even numbers and another Sink for the odd numbers.\nScala copysourceimport org.apache.pekko\nimport pekko.NotUsed\nimport pekko.stream.Attributes\nimport pekko.stream.Attributes.LogLevels\nimport pekko.stream.ClosedShape\nimport pekko.stream.scaladsl.Flow\nimport pekko.stream.scaladsl.GraphDSL\nimport pekko.stream.scaladsl.Partition\nimport pekko.stream.scaladsl.RunnableGraph\nimport pekko.stream.scaladsl.Sink\nimport pekko.stream.scaladsl.Source\n\nval source: Source[Int, NotUsed] = Source(1 to 10)\n\nval even: Sink[Int, NotUsed] =\n  Flow[Int].log(\"even\").withAttributes(Attributes.logLevels(onElement = LogLevels.Info)).to(Sink.ignore)\nval odd: Sink[Int, NotUsed] =\n  Flow[Int].log(\"odd\").withAttributes(Attributes.logLevels(onElement = LogLevels.Info)).to(Sink.ignore)\n\nRunnableGraph\n  .fromGraph(GraphDSL.create() { implicit builder =>\n    import GraphDSL.Implicits._\n    val partition = builder.add(Partition[Int](2, element => if (element % 2 == 0) 0 else 1))\n    source           ~> partition.in\n    partition.out(0) ~> even\n    partition.out(1) ~> odd\n    ClosedShape\n  })\n  .run()\n Java copysourceimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.stream.Attributes;\nimport org.apache.pekko.stream.ClosedShape;\nimport org.apache.pekko.stream.UniformFanOutShape;\nimport org.apache.pekko.stream.javadsl.Flow;\nimport org.apache.pekko.stream.javadsl.GraphDSL;\nimport org.apache.pekko.stream.javadsl.Partition;\nimport org.apache.pekko.stream.javadsl.RunnableGraph;\nimport org.apache.pekko.stream.javadsl.Sink;\nimport org.apache.pekko.stream.javadsl.Source;\n\nSource<Integer, NotUsed> source = Source.range(1, 10);\n\nSink<Integer, NotUsed> even =\n    Flow.of(Integer.class)\n        .log(\"even\")\n        .withAttributes(Attributes.createLogLevels(Attributes.logLevelInfo()))\n        .to(Sink.ignore());\nSink<Integer, NotUsed> odd =\n    Flow.of(Integer.class)\n        .log(\"odd\")\n        .withAttributes(Attributes.createLogLevels(Attributes.logLevelInfo()))\n        .to(Sink.ignore());\n\nRunnableGraph.fromGraph(\n        GraphDSL.create(\n            builder -> {\n              UniformFanOutShape<Integer, Integer> partition =\n                  builder.add(\n                      Partition.create(\n                          Integer.class, 2, element -> (element % 2 == 0) ? 0 : 1));\n              builder.from(builder.add(source)).viaFanOut(partition);\n              builder.from(partition.out(0)).to(builder.add(even));\n              builder.from(partition.out(1)).to(builder.add(odd));\n              return ClosedShape.getInstance();\n            }))\n    .run(system);","title":"Example"},{"location":"/stream/operators/Partition.html#reactive-streams-semantics","text":"emits when the chosen output stops backpressuring and there is an input element available backpressures when the chosen output backpressures completes when upstream completes and no output is pending cancels depends on the eagerCancel flag. If it is true, when any downstream cancels, if false, when all downstreams cancel.","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/prefixAndTail.html","text":"","title":"prefixAndTail"},{"location":"/stream/operators/Source-or-Flow/prefixAndTail.html#prefixandtail","text":"Take up to n elements from the stream (less than n only if the upstream completes before emitting n elements) and returns a pair containing a strict sequence of the taken element and a stream representing the remaining elements.\nNesting and flattening operators","title":"prefixAndTail"},{"location":"/stream/operators/Source-or-Flow/prefixAndTail.html#signature","text":"Source.prefixAndTailSource.prefixAndTail Flow.prefixAndTailFlow.prefixAndTail","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/prefixAndTail.html#description","text":"Take up to n elements from the stream (less than n only if the upstream completes before emitting n elements) and returns a pair containing a strict sequence of the taken element and a stream representing the remaining elements.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/prefixAndTail.html#reactive-streams-semantics","text":"emits when the configured number of prefix elements are available. Emits this prefix, and the rest as a substream backpressures when downstream backpressures or substream backpressures completes when prefix elements has been consumed and substream has been consumed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/preMaterialize.html","text":"","title":"preMaterialize"},{"location":"/stream/operators/Source-or-Flow/preMaterialize.html#prematerialize","text":"Materializes this Graph, immediately returning (1) its materialized value, and (2) a new pre-materialized Graph.\nSimple operators","title":"preMaterialize"},{"location":"/stream/operators/Source-or-Flow/preMaterialize.html#signature","text":"Source.preMaterializeSource.preMaterialize Flow.preMaterializeFlow.preMaterialize","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/preMaterialize.html#description","text":"Materializes this Graph, immediately returning (1) its materialized value, and (2) a new pre-materialized Graph.","title":"Description"},{"location":"/stream/operators/Sink/preMaterialize.html","text":"","title":"Sink.preMaterialize"},{"location":"/stream/operators/Sink/preMaterialize.html#sink-prematerialize","text":"Materializes this Sink, immediately returning (1) its materialized value, and (2) a new Sink that can be consume elements ‘into’ the pre-materialized one.\nSink operators","title":"Sink.preMaterialize"},{"location":"/stream/operators/Sink/preMaterialize.html#signature","text":"Sink.preMaterializeSink.preMaterialize","title":"Signature"},{"location":"/stream/operators/Sink/preMaterialize.html#description","text":"Materializes this Sink, immediately returning (1) its materialized value, and (2) a new Sink that can be consume elements ‘into’ the pre-materialized one. Useful for when you need a materialized value of a Sink when handing it out to someone to materialize it for you.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/prepend.html","text":"","title":"prepend"},{"location":"/stream/operators/Source-or-Flow/prepend.html#prepend","text":"Prepends the given source to the flow, consuming it until completion before the original source is consumed.\nFan-in operators","title":"prepend"},{"location":"/stream/operators/Source-or-Flow/prepend.html#signature","text":"Source.prependSource.prepend Flow.prependFlow.prepend","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/prepend.html#description","text":"Prepends the given source to the flow, consuming it until completion before the original source is consumed.\nNote The `prepend` operator is for backwards compatibility reasons \"detached\" and will eagerly\ndemand an element from both upstreams when the stream is materialized and will then have a\none element buffer for each of the upstreams, this is most often not what you want, instead\nuse @ref(prependLazy)[prependLazy.md]\nIf materialized values needs to be collected prependMat is available.\nNote The prepend operator is for backwards compatibility reasons “detached” and will eagerly demand an element from both upstreams when the stream is materialized and will then have a one element buffer for each of the upstreams, this is not always what you want, if not, use @ref(prependLazy)[prependLazy.md]","title":"Description"},{"location":"/stream/operators/Source-or-Flow/prepend.html#example","text":"Scala copysource val ladies = Source(List(\"Emma\", \"Emily\"))\nval gentlemen = Source(List(\"Liam\", \"William\"))\n\ngentlemen.prepend(ladies).runWith(Sink.foreach(println))\n// this will print \"Emma\", \"Emily\", \"Liam\", \"William\" Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<String, NotUsed> ladies = Source.from(Arrays.asList(\"Emma\", \"Emily\"));\nSource<String, NotUsed> gentlemen = Source.from(Arrays.asList(\"Liam\", \"William\"));\ngentlemen.prepend(ladies).runForeach(System.out::println, system);\n// this will print \"Emma\", \"Emily\", \"Liam\", \"William\"\n\nSource<String, NotUsed> ladies = Source.from(Arrays.asList(\"Emma\", \"Emily\"));\nSource<String, NotUsed> gentlemen = Source.from(Arrays.asList(\"Liam\", \"William\"));\ngentlemen.prependLazy(ladies).runForeach(System.out::println, system);\n// this will print \"Emma\", \"Emily\", \"Liam\", \"William\"","title":"Example"},{"location":"/stream/operators/Source-or-Flow/prepend.html#reactive-streams-semantics","text":"emits when the given stream has an element available; if the given input completes, it tries the current one backpressures when downstream backpressures completes when all upstreams complete","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/prependLazy.html","text":"","title":"prependLazy"},{"location":"/stream/operators/Source-or-Flow/prependLazy.html#prependlazy","text":"Prepends the given source to the flow, consuming it until completion before the original source is consumed.\nFan-in operators","title":"prependLazy"},{"location":"/stream/operators/Source-or-Flow/prependLazy.html#signature","text":"Source.prependSource.prepend Flow.prependFlow.prepend","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/prependLazy.html#description","text":"Prepends the given source to the flow, consuming it until completion before the original source is consumed.\nBoth streams will be materialized together, however, the original stream will be pulled for the first time only after the prepended upstream was completed. (In contrast, @ref(prepend)[prepend.md], introduces single-element buffers after both, original and given sources so that the original source is also pulled once immediately.)\nIf materialized values needs to be collected prependLazyMat is available.\nSee also prepend which is detached.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/prependLazy.html#example","text":"Scala copysourceval ladies = Source(List(\"Emma\", \"Emily\"))\nval gentlemen = Source(List(\"Liam\", \"William\"))\n\ngentlemen.prependLazy(ladies).runWith(Sink.foreach(println))\n// this will print \"Emma\", \"Emily\", \"Liam\", \"William\" Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;","title":"Example"},{"location":"/stream/operators/Source-or-Flow/prependLazy.html#reactive-streams-semantics","text":"emits when the given stream has an element available; if the given input completes, it tries the current one backpressures when downstream backpressures completes when all upstreams complete","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/queue.html","text":"","title":"Source.queue"},{"location":"/stream/operators/Source/queue.html#source-queue","text":"Materialize a BoundedSourceQueue or SourceQueue onto which elements can be pushed for emitting from the source.\nSource operators","title":"Source.queue"},{"location":"/stream/operators/Source/queue.html#signature-boundedsourcequeue-","text":"Source.queueSource.queue","title":"Signature (BoundedSourceQueue)"},{"location":"/stream/operators/Source/queue.html#description-boundedsourcequeue-","text":"The BoundedSourceQueue is an optimized variant of the SourceQueue with OverflowStrategy.dropNew. The BoundedSourceQueue will give immediate, synchronous feedback whether an element was accepted or not and is therefore recommended for situations where overload and dropping elements is expected and needs to be handled quickly.\nIn contrast, the SourceQueue offers more variety of OverflowStrategies but feedback is only asynchronously provided through a FutureCompletionStage value. In cases where elements need to be discarded quickly at times of overload to avoid out-of-memory situations, delivering feedback asynchronously can itself become a problem. This happens if elements come in faster than the feedback can be delivered in which case the feedback mechanism itself is part of the reason that an out-of-memory situation arises.\nIn summary, prefer BoundedSourceQueue over SourceQueue with OverflowStrategy.dropNew especially in high-load scenarios. Use SourceQueue if you need one of the other OverflowStrategies.\nThe BoundedSourceQueue contains a buffer that can be used by many producers on different threads. When the buffer is full, the BoundedSourceQueue will not accept more elements. The return value of BoundedSourceQueue.offer() immediately returns a QueueOfferResult (as opposed to an asynchronous value returned by SourceQueue). A synchronous result is important in order to avoid situations where offer acknowledgements are handled slower than the rate of which elements are offered, which will eventually lead to an Out Of Memory error.","title":"Description (BoundedSourceQueue)"},{"location":"/stream/operators/Source/queue.html#example-boundedsourcequeue-","text":"Scala copysourceval bufferSize = 1000\n\nval queue = Source\n  .queue[Int](bufferSize)\n  .map(x => x * x)\n  .toMat(Sink.foreach(x => println(s\"completed $x\")))(Keep.left)\n  .run()\n\nval fastElements = 1 to 10\n\nimplicit val ec = system.dispatcher\nfastElements.foreach { x =>\n  queue.offer(x) match {\n    case QueueOfferResult.Enqueued    => println(s\"enqueued $x\")\n    case QueueOfferResult.Dropped     => println(s\"dropped $x\")\n    case QueueOfferResult.Failure(ex) => println(s\"Offer failed ${ex.getMessage}\")\n    case QueueOfferResult.QueueClosed => println(\"Source Queue closed\")\n  }\n} Java copysourceint bufferSize = 10;\nint elementsToProcess = 5;\n\nBoundedSourceQueue<Integer> sourceQueue =\n    Source.<Integer>queue(bufferSize)\n        .throttle(elementsToProcess, Duration.ofSeconds(3))\n        .map(x -> x * x)\n        .to(Sink.foreach(x -> System.out.println(\"got: \" + x)))\n        .run(system);\n\nList<Integer> fastElements = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);\n\nfastElements.stream()\n    .forEach(\n        x -> {\n          QueueOfferResult result = sourceQueue.offer(x);\n          if (result == QueueOfferResult.enqueued()) {\n            System.out.println(\"enqueued \" + x);\n          } else if (result == QueueOfferResult.dropped()) {\n            System.out.println(\"dropped \" + x);\n          } else if (result instanceof QueueOfferResult.Failure) {\n            QueueOfferResult.Failure failure = (QueueOfferResult.Failure) result;\n            System.out.println(\"Offer failed \" + failure.cause().getMessage());\n          } else if (result instanceof QueueOfferResult.QueueClosed$) {\n            System.out.println(\"Bounded Source Queue closed\");\n          }\n        });","title":"Example (BoundedSourceQueue)"},{"location":"/stream/operators/Source/queue.html#signature-sourcequeue-","text":"Source.queueSource.queue Source.queueSource.queue","title":"Signature (SourceQueue)"},{"location":"/stream/operators/Source/queue.html#description-sourcequeue-","text":"Materialize a SourceQueue onto which elements can be pushed for emitting from the source. The queue contains a buffer, if elements are pushed onto the queue faster than the source is consumed the overflow will be handled with a strategy specified by the user. Functionality for tracking when an element has been emitted is available through SourceQueue.offer.\nUsing Source.queue you can push elements to the queue and they will be emitted to the stream if there is demand from downstream, otherwise they will be buffered until request for demand is received. Elements in the buffer will be discarded if downstream is terminated.\nIn combination with the queue, the throttle operator can be used to control the processing to a given limit, e.g. 5 elements per 3 seconds.","title":"Description (SourceQueue)"},{"location":"/stream/operators/Source/queue.html#example-sourcequeue-","text":"Scala copysourceval bufferSize = 10\nval elementsToProcess = 5\n\nval queue = Source\n  .queue[Int](bufferSize)\n  .throttle(elementsToProcess, 3.second)\n  .map(x => x * x)\n  .toMat(Sink.foreach(x => println(s\"completed $x\")))(Keep.left)\n  .run()\n\nval source = Source(1 to 10)\n\nimplicit val ec = system.dispatcher\nsource\n  .map(x => {\n    queue.offer(x).map {\n      case QueueOfferResult.Enqueued    => println(s\"enqueued $x\")\n      case QueueOfferResult.Dropped     => println(s\"dropped $x\")\n      case QueueOfferResult.Failure(ex) => println(s\"Offer failed ${ex.getMessage}\")\n      case QueueOfferResult.QueueClosed => println(\"Source Queue closed\")\n    }\n  })\n  .runWith(Sink.ignore) Java copysourceint bufferSize = 10;\nint elementsToProcess = 5;\n\nBoundedSourceQueue<Integer> sourceQueue =\n    Source.<Integer>queue(bufferSize)\n        .throttle(elementsToProcess, Duration.ofSeconds(3))\n        .map(x -> x * x)\n        .to(Sink.foreach(x -> System.out.println(\"got: \" + x)))\n        .run(system);\n\nSource<Integer, NotUsed> source = Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));\n\nsource.map(x -> sourceQueue.offer(x)).runWith(Sink.ignore(), system);","title":"Example (SourceQueue)"},{"location":"/stream/operators/Source/queue.html#reactive-streams-semantics","text":"emits when there is demand and the queue contains elements completes when downstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/queue.html","text":"","title":"Sink.queue"},{"location":"/stream/operators/Sink/queue.html#sink-queue","text":"Materialize a SinkQueue that can be pulled to trigger demand through the sink.\nSink operators","title":"Sink.queue"},{"location":"/stream/operators/Sink/queue.html#signature","text":"Sink.queueSink.queue","title":"Signature"},{"location":"/stream/operators/Sink/queue.html#description","text":"Materialize a SinkQueue that can be pulled to trigger demand through the sink. The queue contains a buffer in case stream emitting elements faster than queue pulling them.","title":"Description"},{"location":"/stream/operators/Sink/queue.html#reactive-streams-semantics","text":"cancels when SinkQueue.cancel is called backpressures when buffer has some space","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/range.html","text":"","title":"Source.range"},{"location":"/stream/operators/Source/range.html#source-range","text":"Emit each integer in a range, with an option to take bigger steps than 1.\nSource operators","title":"Source.range"},{"location":"/stream/operators/Source/range.html#dependency","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/Source/range.html#description","text":"Emit each integer in a range, with an option to take bigger steps than 1. In Scala, use the apply method to generate a sequence of integers.","title":"Description"},{"location":"/stream/operators/Source/range.html#examples","text":"Define the range of integers.\nJava copysourceimport org.apache.pekko.Done;\nimport org.apache.pekko.NotUsed;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.actor.testkit.typed.javadsl.ManualTime;\nimport org.apache.pekko.actor.testkit.typed.javadsl.TestKitJunitResource;\nimport org.apache.pekko.stream.javadsl.Source;\n\nSource<Integer, NotUsed> source = Source.range(1, 100);\n\nSource<Integer, NotUsed> sourceStepFive = Source.range(1, 100, 5);\n\nSource<Integer, NotUsed> sourceStepNegative = Source.range(100, 1, -1);\nPrint out the stream of integers.\nJava copysourcesource.runForeach(i -> System.out.println(i), system);","title":"Examples"},{"location":"/stream/operators/Source/range.html#reactive-streams-semantics","text":"emits when there is demand, the next value completes when the end of the range has been reached","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/recover.html","text":"","title":"recover"},{"location":"/stream/operators/Source-or-Flow/recover.html#recover","text":"Allow sending of one last element downstream when a failure has happened upstream.\nError handling","title":"recover"},{"location":"/stream/operators/Source-or-Flow/recover.html#signature","text":"Source.recoverSource.recover Flow.recoverFlow.recover","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/recover.html#description","text":"recover allows you to emit a final element and then complete the stream on an upstream failure. Deciding which exceptions should be recovered is done through a PartialFunction. If an exception does not have a matching case match defined the stream is failed.\nRecovering can be useful if you want to gracefully complete a stream on failure while letting downstream know that there was a failure.\nThrowing an exception inside recover will be logged on ERROR level automatically.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/recover.html#reactive-streams-semantics","text":"emits when the element is available from the upstream or upstream is failed and pf returns an element backpressures when downstream backpressures, not when failure happened completes when upstream completes or upstream failed with exception pf can handle\nBelow example demonstrates how recover gracefully complete a stream on failure.\nScala copysourceSource(0 to 6)\n  .map(n =>\n    // assuming `4` and `5` are unexpected values that could throw exception\n    if (List(4, 5).contains(n)) throw new RuntimeException(s\"Boom! Bad value found: $n\")\n    else n.toString)\n  .recover {\n    case e: RuntimeException => e.getMessage\n  }\n  .runForeach(println) Java copysourceSource.from(Arrays.asList(0, 1, 2, 3, 4, 5, 6))\n    .map(\n        n -> {\n          // assuming `4` and `5` are unexpected values that could throw exception\n          if (Arrays.asList(4, 5).contains(n))\n            throw new RuntimeException(String.format(\"Boom! Bad value found: %s\", n));\n          else return n.toString();\n        })\n    .recover(\n        new PFBuilder<Throwable, String>()\n            .match(RuntimeException.class, Throwable::getMessage)\n            .build())\n    .runForeach(System.out::println, system);\nThis will output:\nScala copysource0\n1\n2\n3                         // last element before failure\nBoom! Bad value found: 4  // first element on failure Java copysource0\n1\n2\n3                         // last element before failure\nBoom! Bad value found: 4  // first element on failure\nThe output in the line before failure denotes the last successful element available from the upstream, and the output in the line on failure denotes the element returns by partial function when upstream is failed.","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/recoverWith.html","text":"","title":"recoverWith"},{"location":"/stream/operators/Source-or-Flow/recoverWith.html#recoverwith","text":"Allow switching to alternative Source when a failure has happened upstream.\nError handling","title":"recoverWith"},{"location":"/stream/operators/Source-or-Flow/recoverWith.html#signature","text":"Source.recoverWithSource.recoverWith Flow.recoverWithFlow.recoverWith","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/recoverWith.html#description","text":"Allow switching to alternative Source when a failure has happened upstream.\nThrowing an exception inside recoverWith will be logged on ERROR level automatically.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/recoverWith.html#reactive-streams-semantics","text":"emits the element is available from the upstream or upstream is failed and pf returns alternative Source backpressures downstream backpressures, after failure happened it backprssures to alternative Source completes upstream completes or upstream failed with exception pf can handle","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/recoverWithRetries.html","text":"","title":"recoverWithRetries"},{"location":"/stream/operators/Source-or-Flow/recoverWithRetries.html#recoverwithretries","text":"RecoverWithRetries allows to switch to alternative Source on flow failure.\nError handling","title":"recoverWithRetries"},{"location":"/stream/operators/Source-or-Flow/recoverWithRetries.html#signature","text":"Source.recoverWithRetriesSource.recoverWithRetries Flow.recoverWithRetriesFlow.recoverWithRetries","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/recoverWithRetries.html#description","text":"RecoverWithRetries allows to switch to alternative Source on flow failure. It will stay in effect after a failure has been recovered up to attempts number of times so that each time there is a failure it is fed into the pf and a new Source may be materialized. Note that if you pass in 0, this won’t attempt to recover at all. A negative attempts number is interpreted as “infinite”, which results in the exact same behavior as recoverWith.\nSince the underlying failure signal onError arrives out-of-band, it might jump over existing elements. This operators can recover the failure signal, but not the skipped elements, which will be dropped.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/recoverWithRetries.html#reactive-streams-semantics","text":"emits when element is available from the upstream or upstream is failed and element is available from alternative Source backpressures when downstream backpressures completes when upstream completes or upstream failed with exception pf can handle","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/reduce.html","text":"","title":"reduce"},{"location":"/stream/operators/Source-or-Flow/reduce.html#reduce","text":"Start with first element and then apply the current and next value to the given function, when upstream complete the current value is emitted downstream.\nSimple operators","title":"reduce"},{"location":"/stream/operators/Source-or-Flow/reduce.html#signature","text":"Source.reduceSource.reduce Flow.reduceFlow.reduce","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/reduce.html#description","text":"Start with first element and then apply the current and next value to the given function, when upstream complete the current value is emitted downstream. Similar to fold.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/reduce.html#example","text":"reduce will take a function and apply it on the incoming elements in the Stream and only emits its result when upstream completes. Here, it will add the incoming elements.\nScala copysourceval source = Source(1 to 100).reduce((acc, element) => acc + element)\nval result: Future[Int] = source.runWith(Sink.head)\nresult.map(println)\n// 5050 Java copysourceSource<Integer, NotUsed> source = Source.range(1, 100).reduce((acc, element) -> acc + element);\nCompletionStage<Integer> result = source.runWith(Sink.head(), system);\nresult.thenAccept(System.out::println);\n// 5050","title":"Example"},{"location":"/stream/operators/Source-or-Flow/reduce.html#reactive-streams-semantics","text":"emits when upstream completes backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/reduce.html","text":"","title":"Sink.reduce"},{"location":"/stream/operators/Sink/reduce.html#sink-reduce","text":"Apply a reduction function on the incoming elements and pass the result to the next invocation.\nSink operators","title":"Sink.reduce"},{"location":"/stream/operators/Sink/reduce.html#signature","text":"Sink.reduceSink.reduce","title":"Signature"},{"location":"/stream/operators/Sink/reduce.html#description","text":"Apply a reduction function on the incoming elements and pass the result to the next invocation. The first invocation receives the two first elements of the flow.\nMaterializes into a Future CompletionStage that will be completed by the last result of the reduction function.","title":"Description"},{"location":"/stream/operators/Sink/reduce.html#example","text":"Scala copysourceval source = Source(1 to 10)\nval result = source.runWith(Sink.reduce[Int]((a, b) => a + b))\nresult.map(println)(system.dispatcher)\n// will print\n// 55 Java copysourceSource<Integer, NotUsed> ints = Source.from(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10));\nCompletionStage<Integer> sum = ints.runWith(Sink.reduce((a, b) -> a + b), system);\nsum.thenAccept(System.out::println);\n// 55","title":"Example"},{"location":"/stream/operators/Sink/reduce.html#reactive-streams-semantics","text":"cancels never backpressures when the previous reduction function invocation has not yet completed","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/repeat.html","text":"","title":"Source.repeat"},{"location":"/stream/operators/Source/repeat.html#source-repeat","text":"Stream a single object repeatedly.\nSource operators","title":"Source.repeat"},{"location":"/stream/operators/Source/repeat.html#signature","text":"Source.repeatSource.repeat","title":"Signature"},{"location":"/stream/operators/Source/repeat.html#description","text":"This source emits a single element repeatedly. It never completes, if you want the stream to be finite you will need to limit it by combining with another operator\nSee also:\nsingle Stream a single object once. tick A periodical repetition of an arbitrary object. cycle Stream iterator in cycled manner.","title":"Description"},{"location":"/stream/operators/Source/repeat.html#example","text":"This example prints the first 4 elements emitted by Source.repeat.\nScala copysourceval source: Source[Int, NotUsed] = Source.repeat(42)\nval f = source.take(4).runWith(Sink.foreach(println))\n// 42\n// 42\n// 42\n// 42 Java copysourceSource<Integer, NotUsed> source = Source.repeat(42);\nCompletionStage<Done> f = source.take(4).runWith(Sink.foreach(System.out::println), system);\n// 42\n// 42\n// 42\n// 42","title":"Example"},{"location":"/stream/operators/Source/repeat.html#reactive-streams-semantics","text":"emits the same value repeatedly when there is demand completes never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/scan.html","text":"","title":"scan"},{"location":"/stream/operators/Source-or-Flow/scan.html#scan","text":"Emit its current value, which starts at zero, and then apply the current and next value to the given function, emitting the next current value.\nSimple operators","title":"scan"},{"location":"/stream/operators/Source-or-Flow/scan.html#signature","text":"Source.scanSource.scan Flow.scanFlow.scan","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/scan.html#description","text":"Emit its current value, which starts at zero, and then apply the current and next value to the given function, emitting the next current value. This means that scan emits one element downstream before, and upstream elements will not be requested until, the second element is required from downstream.\nWarning Note that the zero value must be immutable, because otherwise the same mutable instance would be shared across different threads when running the stream more than once.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/scan.html#examples","text":"Below example demonstrates how scan is similar to fold, but it keeps value from every iteration.\nScala copysourceval source = Source(1 to 5)\nsource.scan(0)((acc, x) => acc + x).runForeach(println)\n// 0  (= 0)\n// 1  (= 0 + 1)\n// 3  (= 0 + 1 + 2)\n// 6  (= 0 + 1 + 2 + 3)\n// 10 (= 0 + 1 + 2 + 3 + 4)\n// 15 (= 0 + 1 + 2 + 3 + 4 + 5) Java copysourceSource<Integer, NotUsed> source = Source.range(1, 5);\nsource.scan(0, (acc, x) -> acc + x).runForeach(System.out::println, system);\n// 0  (= 0)\n// 1  (= 0 + 1)\n// 3  (= 0 + 1 + 2)\n// 6  (= 0 + 1 + 2 + 3)\n// 10 (= 0 + 1 + 2 + 3 + 4)\n// 15 (= 0 + 1 + 2 + 3 + 4 + 5)","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/scan.html#reactive-streams-semantics","text":"emits when the function scanning the element returns a new element backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/scanAsync.html","text":"","title":"scanAsync"},{"location":"/stream/operators/Source-or-Flow/scanAsync.html#scanasync","text":"Just like scan but receives a function that results in a Future CompletionStage to the next value.\nSimple operators","title":"scanAsync"},{"location":"/stream/operators/Source-or-Flow/scanAsync.html#signature","text":"Source.scanAsyncSource.scanAsync Flow.scanAsyncFlow.scanAsync","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/scanAsync.html#description","text":"Just like scan but receives a function that results in a Future CompletionStage to the next value.\nWarning Note that the zero value must be immutable, because otherwise the same mutable instance would be shared across different threads when running the stream more than once.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/scanAsync.html#example","text":"Below example demonstrates how scanAsync is similar to fold, but it keeps value from every iteration.\nScala copysourcedef asyncFunction(acc: Int, next: Int): Future[Int] = Future {\n  acc + next\n}\n\nval source = Source(1 to 5)\nsource.scanAsync(0)((acc, x) => asyncFunction(acc, x)).runForeach(println)\n// 0  (= 0)\n// 1  (= 0 + 1)\n// 3  (= 0 + 1 + 2)\n// 6  (= 0 + 1 + 2 + 3)\n// 10 (= 0 + 1 + 2 + 3 + 4)\n// 15 (= 0 + 1 + 2 + 3 + 4 + 5) Java copysourceCompletionStage<Integer> asyncFunction(int acc, int next) {\n  return CompletableFuture.supplyAsync(() -> acc + next);\n}\n  Source<Integer, NotUsed> source = Source.range(1, 5);\n  source.scanAsync(0, (acc, x) -> asyncFunction(acc, x)).runForeach(System.out::println, system);\n  // 0  (= 0)\n  // 1  (= 0 + 1)\n  // 3  (= 0 + 1 + 2)\n  // 6  (= 0 + 1 + 2 + 3)\n  // 10 (= 0 + 1 + 2 + 3 + 4)\n  // 15 (= 0 + 1 + 2 + 3 + 4 + 5)\nWarning In an actual application the future would probably involve some external API that returns a Future CompletionStage rather than an immediately completed value.","title":"Example"},{"location":"/stream/operators/Source-or-Flow/scanAsync.html#reactive-streams-semantics","text":"emits when the Future CompletionStage resulting from the function scanning the element resolves to the next value backpressures when downstream backpressures completes when upstream completes and the last Future CompletionStage is resolved","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/seq.html","text":"","title":"Sink.seq"},{"location":"/stream/operators/Sink/seq.html#sink-seq","text":"Collect values emitted from the stream into a collection.\nSink operators","title":"Sink.seq"},{"location":"/stream/operators/Sink/seq.html#signature","text":"Sink.seqSink.seq","title":"Signature"},{"location":"/stream/operators/Sink/seq.html#description","text":"Collect values emitted from the stream into a collection, the collection is available through a Future CompletionStage or which completes when the stream completes. Note that the collection is bounded to Int.MaxValue Integer.MAX_VALUE, if more element are emitted the sink will cancel the stream","title":"Description"},{"location":"/stream/operators/Sink/seq.html#example","text":"Given a stream of numbers we can collect the numbers into a collection with the seq operator\nScala copysourceval source = Source(1 to 3)\nval result = source.runWith(Sink.seq[Int])\nval seq = result.futureValue\nseq.foreach(println)\n// will print\n// 1\n// 2\n// 3 Java copysourceSource<Integer, NotUsed> ints = Source.from(Arrays.asList(1, 2, 3));\nCompletionStage<List<Integer>> result = ints.runWith(Sink.seq(), system);\nresult.thenAccept(list -> list.forEach(System.out::println));\n// 1\n// 2\n// 3","title":"Example"},{"location":"/stream/operators/Sink/seq.html#reactive-streams-semantics","text":"cancels If too many values are collected","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/setup.html","text":"","title":"setup"},{"location":"/stream/operators/Source-or-Flow/setup.html#setup","text":"Defer the creation of a Source/Flow until materialization and access Materializer and Attributes\nSimple operators\nWarning The setup operator has been deprecated, use fromMaterializer instead.","title":"setup"},{"location":"/stream/operators/Source-or-Flow/setup.html#signature","text":"Source.setupSource.setup Flow.setupFlow.setup","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/setup.html#description","text":"Typically used when access to materializer is needed to run a different stream during the construction of a source/flow. Can also be used to access the underlying ActorSystem from ActorMaterializer.","title":"Description"},{"location":"/stream/operators/Sink/setup.html","text":"","title":"Sink.setup"},{"location":"/stream/operators/Sink/setup.html#sink-setup","text":"Defer the creation of a Sink until materialization and access ActorMaterializer and Attributes\nSink operators\nWarning The setup operator has been deprecated, use fromMaterializer instead.","title":"Sink.setup"},{"location":"/stream/operators/Sink/setup.html#signature","text":"Sink.setupSink.setup","title":"Signature"},{"location":"/stream/operators/Sink/setup.html#description","text":"Typically used when access to materializer is needed to run a different stream during the construction of a sink. Can also be used to access the underlying ActorSystem from ActorMaterializer.","title":"Description"},{"location":"/stream/operators/Source/single.html","text":"","title":"Source.single"},{"location":"/stream/operators/Source/single.html#source-single","text":"Stream a single object once.\nSource operators","title":"Source.single"},{"location":"/stream/operators/Source/single.html#signature","text":"Source.singleSource.single","title":"Signature"},{"location":"/stream/operators/Source/single.html#description","text":"Stream a single object once and complete after thereafter.\nSee also:\nrepeat Stream a single object repeatedly. tick A periodical repetition of an arbitrary object. cycle Stream iterator in cycled manner.","title":"Description"},{"location":"/stream/operators/Source/single.html#examples","text":"Scala copysourceimport org.apache.pekko.stream._\n\nval s: Future[immutable.Seq[Int]] = Source.single(1).runWith(Sink.seq)\ns.foreach(list => println(s\"Collected elements: $list\")) // prints: Collected elements: List(1)\n Java copysourceimport org.apache.pekko.stream.*;\nCompletionStage<List<String>> future = Source.single(\"A\").runWith(Sink.seq(), system);\nCompletableFuture<List<String>> completableFuture = future.toCompletableFuture();\ncompletableFuture.thenAccept(result -> System.out.printf(\"collected elements: %s\\n\", result));\n// result list will contain exactly one element \"A\"","title":"Examples"},{"location":"/stream/operators/Source/single.html#reactive-streams-semantics","text":"emits the value once completes when the single value has been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/PubSub/sink.html","text":"","title":"PubSub.sink"},{"location":"/stream/operators/PubSub/sink.html#pubsub-sink","text":"A sink that will publish emitted messages to a TopicTopic.\nActor interop operators\nNote that there is no backpressure from the topic, so care must be taken to not publish messages at a higher rate than that can be handled by subscribers.\nIf the topic does not have any subscribers when a message is published, or the topic actor is stopped, the message is sent to dead letters.","title":"PubSub.sink"},{"location":"/stream/operators/PubSub/sink.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/PubSub/sink.html#signature","text":"PubSub.sinkPubSub.sink","title":"Signature"},{"location":"/stream/operators/PubSub/sink.html#reactive-streams-semantics","text":"cancels never backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/sliding.html","text":"","title":"sliding"},{"location":"/stream/operators/Source-or-Flow/sliding.html#sliding","text":"Provide a sliding window over the incoming stream and pass the windows as groups of elements downstream.\nSimple operators","title":"sliding"},{"location":"/stream/operators/Source-or-Flow/sliding.html#signature","text":"Flow.slidingFlow.sliding","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/sliding.html#description","text":"Provide a sliding window over the incoming stream and pass the windows as groups of elements downstream.\nNote: the last window might be smaller than the requested size due to end of stream.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/sliding.html#examples","text":"In this first sample we just see the behavior of the operator itself, first with a window of 2 elements and the default step which is 1a step value of 1.\nScala copysourceval source = Source(1 to 4)\nsource.sliding(2).runForeach(println)\n// prints:\n// Vector(1, 2)\n// Vector(2, 3)\n// Vector(3, 4) Java copysourceSource<Integer, NotUsed> source = Source.range(1, 4);\nsource.sliding(2, 1).runForeach(n -> System.out.println(n), system);\n// prints:\n// [1, 2]\n// [2, 3]\n// [3, 4]\nIf the stream stops without having seen enough elements to fill a window, the last window will have as many elements was emitted before the stream ended. Here we also provide a step to move two elements forward for each window:\nScala copysourceval source = Source(1 to 4)\nsource.sliding(n = 3, step = 2).runForeach(println)\n// prints:\n// Vector(1, 2, 3)\n// Vector(3, 4) - shorter because stream ended before we got 3 elements Java copysourceSource<Integer, NotUsed> source = Source.range(1, 4);\nsource.sliding(3, 2).runForeach(n -> System.out.println(n), system);\n// prints:\n// Vector(1, 2, 3)\n// [1, 2, 3]\n// [3, 4] - shorter because stream ended before we got 3 elements\nOne use case for sliding is to implement a moving average, here we do that with a “period” of 5:\nScala copysourceval numbers = Source(1 :: 3 :: 10 :: 2 :: 3 :: 4 :: 2 :: 10 :: 11 :: Nil)\nval movingAverage = numbers.sliding(5).map(window => window.sum.toFloat / window.size)\nmovingAverage.runForeach(println)\n// prints\n// 3.8 = average of 1, 3, 10, 2, 3\n// 4.4 = average of 3, 10, 2, 3, 4\n// 4.2 = average of 10, 2, 3, 4, 2\n// 4.2 = average of 2, 3, 4, 2, 10\n// 6.0 = average of 3, 4, 2, 10, 11 Java copysourceSource<Integer, NotUsed> numbers = Source.from(Arrays.asList(1, 3, 10, 2, 3, 4, 2, 10, 11));\nSource<Float, NotUsed> movingAverage =\n    numbers\n        .sliding(5, 1)\n        .map(window -> ((float) window.stream().mapToInt(i -> i).sum()) / window.size());\nmovingAverage.runForeach(n -> System.out.println(n), system);\n// prints\n// 3.8 = average of 1, 3, 10, 2, 3\n// 4.4 = average of 3, 10, 2, 3, 4\n// 4.2 = average of 10, 2, 3, 4, 2\n// 4.2 = average of 2, 3, 4, 2, 10\n// 6.0 = average of 3, 4, 2, 10, 11\nSliding can also be used to do simple windowing, see splitAfter.","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/sliding.html#reactive-streams-semantics","text":"emits the specified number of elements has been accumulated or upstream completed backpressures when a group has been assembled and downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/PubSub/source.html","text":"","title":"PubSub.source"},{"location":"/stream/operators/PubSub/source.html#pubsub-source","text":"A source that will subscribe to a TopicTopic and stream messages published to the topic.\nActor interop operators\nThe source can be materialized multiple times, each materialized stream will stream messages published to the topic after the stream has started.\nNote that it is not possible to propagate the backpressure from the running stream to the pub sub topic, if the stream is backpressuring published messages are buffered up to a limit and if the limit is hit the configurable OverflowStrategy decides what happens. It is not possible to use the Backpressure strategy.","title":"PubSub.source"},{"location":"/stream/operators/PubSub/source.html#dependency","text":"This operator is included in:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-stream-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-stream-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-stream-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/stream/operators/PubSub/source.html#signature","text":"PubSub.sourcePubSub.source","title":"Signature"},{"location":"/stream/operators/PubSub/source.html#reactive-streams-semantics","text":"emits a message published to the topic is emitted as soon as there is demand from downstream completes when the topic actor terminates","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/splitAfter.html","text":"","title":"splitAfter"},{"location":"/stream/operators/Source-or-Flow/splitAfter.html#splitafter","text":"End the current substream whenever a predicate returns true, starting a new substream for the next element.\nNesting and flattening operators","title":"splitAfter"},{"location":"/stream/operators/Source-or-Flow/splitAfter.html#signature","text":"Source.splitAfterSource.splitAfter Flow.splitAfterFlow.splitAfter","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/splitAfter.html#description","text":"End the current substream whenever a predicate returns true, starting a new substream for the next element.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/splitAfter.html#example","text":"Given some time series data source we would like to split the stream into sub-streams for each second. By using sliding we can compare the timestamp of the current and next element to decide when to split.\nScala copysourceSource(1 to 100)\n  .throttle(1, 100.millis)\n  .map(elem => (elem, Instant.now()))\n  .sliding(2)\n  .splitAfter { slidingElements =>\n    if (slidingElements.size == 2) {\n      val current = slidingElements.head\n      val next = slidingElements.tail.head\n      val currentBucket = LocalDateTime.ofInstant(current._2, ZoneOffset.UTC).withNano(0)\n      val nextBucket = LocalDateTime.ofInstant(next._2, ZoneOffset.UTC).withNano(0)\n      currentBucket != nextBucket\n    } else {\n      false\n    }\n  }\n  .map(_.head._1)\n  .fold(0)((acc, _) => acc + 1) // sum\n  .to(Sink.foreach(println))\n  .run()\n// 3\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 6\n// note that the very last element is never included due to sliding,\n// but that would not be problem for an infinite stream Java copysourceSource.range(1, 100)\n    .throttle(1, Duration.ofMillis(100))\n    .map(elem -> new Pair<>(elem, Instant.now()))\n    .sliding(2, 1)\n    .splitAfter(\n        slidingElements -> {\n          if (slidingElements.size() == 2) {\n            Pair<Integer, Instant> current = slidingElements.get(0);\n            Pair<Integer, Instant> next = slidingElements.get(1);\n            LocalDateTime currentBucket =\n                LocalDateTime.ofInstant(current.second(), ZoneOffset.UTC).withNano(0);\n            LocalDateTime nextBucket =\n                LocalDateTime.ofInstant(next.second(), ZoneOffset.UTC).withNano(0);\n            return !currentBucket.equals(nextBucket);\n          } else {\n            return false;\n          }\n        })\n    .map(slidingElements -> slidingElements.get(0).first())\n    .fold(0, (acc, notUsed) -> acc + 1) // sum\n    .to(Sink.foreach(System.out::println))\n    .run(system);\n// 3\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 6\n// note that the very last element is never included due to sliding,\n// but that would not be problem for an infinite stream\nAn alternative way of implementing this is shown in splitWhen example.","title":"Example"},{"location":"/stream/operators/Source-or-Flow/splitAfter.html#reactive-streams-semantics","text":"emits when an element passes through. When the provided predicate is true it emits the element * and opens a new substream for subsequent element backpressures when there is an element pending for the next substream, but the previous is not fully consumed yet, or the substream backpressures completes when upstream completes (Until the end of stream it is not possible to know whether new substreams will be needed or not)","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/splitWhen.html","text":"","title":"splitWhen"},{"location":"/stream/operators/Source-or-Flow/splitWhen.html#splitwhen","text":"Split off elements into a new substream whenever a predicate function return true.\nNesting and flattening operators","title":"splitWhen"},{"location":"/stream/operators/Source-or-Flow/splitWhen.html#signature","text":"Source.splitWhenSource.splitWhen Flow.splitWhenFlow.splitWhen","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/splitWhen.html#description","text":"Split off elements into a new substream whenever a predicate function return true.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/splitWhen.html#example","text":"Given some time series data source we would like to split the stream into sub-streams for each second. We need to compare the timestamp of the previous and current element to decide when to split. This decision can be implemented in a statefulMapConcat operator preceding the splitWhen.\nScala copysourceSource(1 to 100)\n  .throttle(1, 100.millis)\n  .map(elem => (elem, Instant.now()))\n  .statefulMapConcat(() => {\n    // stateful decision in statefulMapConcat\n    // keep track of time bucket (one per second)\n    var currentTimeBucket = LocalDateTime.ofInstant(Instant.ofEpochMilli(0), ZoneOffset.UTC)\n\n    {\n      case (elem, timestamp) =>\n        val time = LocalDateTime.ofInstant(timestamp, ZoneOffset.UTC)\n        val bucket = time.withNano(0)\n        val newBucket = bucket != currentTimeBucket\n        if (newBucket)\n          currentTimeBucket = bucket\n        List((elem, newBucket))\n    }\n  })\n  .splitWhen(_._2) // split when time bucket changes\n  .map(_._1)\n  .fold(0)((acc, _) => acc + 1) // sum\n  .to(Sink.foreach(println))\n  .run()\n// 3\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 7 Java copysourceSource.range(1, 100)\n    .throttle(1, Duration.ofMillis(100))\n    .map(elem -> new Pair<>(elem, Instant.now()))\n    .statefulMapConcat(\n        () -> {\n          return new Function<Pair<Integer, Instant>, Iterable<Pair<Integer, Boolean>>>() {\n            // stateful decision in statefulMapConcat\n            // keep track of time bucket (one per second)\n            LocalDateTime currentTimeBucket =\n                LocalDateTime.ofInstant(Instant.ofEpochMilli(0), ZoneOffset.UTC);\n\n            @Override\n            public Iterable<Pair<Integer, Boolean>> apply(\n                Pair<Integer, Instant> elemTimestamp) {\n              LocalDateTime time =\n                  LocalDateTime.ofInstant(elemTimestamp.second(), ZoneOffset.UTC);\n              LocalDateTime bucket = time.withNano(0);\n              boolean newBucket = !bucket.equals(currentTimeBucket);\n              if (newBucket) currentTimeBucket = bucket;\n              return Collections.singleton(new Pair<>(elemTimestamp.first(), newBucket));\n            }\n          };\n        })\n    .splitWhen(elemDecision -> elemDecision.second()) // split when time bucket changes\n    .map(elemDecision -> elemDecision.first())\n    .fold(0, (acc, notUsed) -> acc + 1) // sum\n    .to(Sink.foreach(System.out::println))\n    .run(system);\n// 3\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 10\n// 7\nAn alternative way of implementing this is shown in splitAfter example.","title":"Example"},{"location":"/stream/operators/Source-or-Flow/splitWhen.html#reactive-streams-semantics","text":"emits an element for which the provided predicate is true, opening and emitting a new substream for subsequent elements backpressures when there is an element pending for the next substream, but the previous is not fully consumed yet, or the substream backpressures completes when upstream completes (Until the end of stream it is not possible to know whether new substreams will be needed or not)","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/statefulMap.html","text":"","title":"statefulMap"},{"location":"/stream/operators/Source-or-Flow/statefulMap.html#statefulmap","text":"Transform each stream element with the help of a state.\nSimple operators","title":"statefulMap"},{"location":"/stream/operators/Source-or-Flow/statefulMap.html#signature","text":"Flow.statefulMapFlow.statefulMap","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/statefulMap.html#description","text":"Transform each stream element with the help of a state.\nThe state creation function is invoked once when the stream is materialized and the returned state is passed to the mapping function for mapping the first element.\nThe mapping function returns a mapped element to emit downstream and a state to pass to the next mapping function. The state can be the same for each mapping return, be a new immutable state but it is also safe to use a mutable state.\nThe on complete function is called, once, when the first of upstream completion, downstream cancellation or stream failure happens. If the cause is upstream completion and the downstream is still accepting elements, the returned value from the function is passed downstream before completing the operator itself, for the other cases the returned value is ignored.\nThe statefulMap operator adheres to the ActorAttributes.SupervisionStrategy attribute.\nFor mapping stream elements without keeping a state see map.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/statefulMap.html#examples","text":"In the first example we implement an zipWithIndex operator like zipWithIndex, it always associates a unique index with each element of the stream, the index starts from 0.\nScala copysourceSource(List(\"A\", \"B\", \"C\", \"D\"))\n  .statefulMap(() => 0L)((index, elem) => (index + 1, (elem, index)), _ => None)\n  .runForeach(println)\n// prints\n// (A,0)\n// (B,1)\n// (C,2)\n// (D,3) Java copysourceSource.from(Arrays.asList(\"A\", \"B\", \"C\", \"D\"))\n    .statefulMap(\n        () -> 0L,\n        (index, element) -> Pair.create(index + 1, Pair.create(element, index)),\n        indexOnComplete -> Optional.empty())\n    .runForeach(System.out::println, system);\n// prints\n// Pair(A,0)\n// Pair(B,1)\n// Pair(C,2)\n// Pair(D,3)\nIn the second example, the elements are buffered until the incoming element is different, and then emitted downstream. When upstream completes, if there are buffered elements, they are emitted before completing.\nScala copysourceSource(\"A\" :: \"B\" :: \"B\" :: \"C\" :: \"C\" :: \"C\" :: \"D\" :: Nil)\n  .statefulMap(() => List.empty[String])(\n    (buffer, element) =>\n      buffer match {\n        case head :: _ if head != element => (element :: Nil, buffer)\n        case _                            => (element :: buffer, Nil)\n      },\n    buffer => Some(buffer))\n  .filter(_.nonEmpty)\n  .runForeach(println)\n// prints\n// List(A)\n// List(B, B)\n// List(C, C, C)\n// List(D) Java copysourceSource.from(Arrays.asList(\"A\", \"B\", \"B\", \"C\", \"C\", \"C\", \"D\"))\n    .statefulMap(\n        () -> (List<String>) new LinkedList<String>(),\n        (buffer, element) -> {\n          if (buffer.size() > 0 && (!buffer.get(0).equals(element))) {\n            return Pair.create(\n                new LinkedList<>(Collections.singletonList(element)),\n                Collections.unmodifiableList(buffer));\n          } else {\n            buffer.add(element);\n            return Pair.create(buffer, Collections.<String>emptyList());\n          }\n        },\n        Optional::ofNullable)\n    .filterNot(List::isEmpty)\n    .runForeach(System.out::println, system);\n// prints\n// [A]\n// [B, B]\n// [C, C, C]\n// [D]\nIn the forth example, repeated incoming elements are only emitted once and then dropped.\nScala copysourceSource(\"A\" :: \"B\" :: \"B\" :: \"C\" :: \"C\" :: \"C\" :: \"D\" :: Nil)\n  .statefulMap(() => Option.empty[String])(\n    (lastElement, elem) =>\n      lastElement match {\n        case Some(head) if head == elem => (Some(elem), None)\n        case _                          => (Some(elem), Some(elem))\n      },\n    _ => None)\n  .collect { case Some(elem) => elem }\n  .runForeach(println)\n// prints\n// A\n// B\n// C\n// D Java copysourceSource.from(Arrays.asList(\"A\", \"B\", \"B\", \"C\", \"C\", \"C\", \"D\"))\n    .statefulMap(\n        Optional::<String>empty,\n        (lastElement, element) -> {\n          if (lastElement.isPresent() && lastElement.get().equals(element)) {\n            return Pair.create(lastElement, Optional.<String>empty());\n          } else {\n            return Pair.create(Optional.of(element), Optional.of(element));\n          }\n        },\n        listOnComplete -> Optional.empty())\n    .via(Flow.flattenOptional())\n    .runForeach(System.out::println, system);\n// prints\n// A\n// B\n// C\n// D\nIn the fifth example, we combine the statefulMap and mapConcat to implement a statefulMapConcat like behavior.\nScala copysourceSource(1 to 10)\n  .statefulMap(() => List.empty[Int])(\n    (state, elem) => {\n      // grouped 3 elements into a list\n      val newState = elem :: state\n      if (newState.size == 3)\n        (Nil, newState.reverse)\n      else\n        (newState, Nil)\n    },\n    state => Some(state.reverse))\n  .mapConcat(identity)\n  .runForeach(println)\n// prints\n// 1\n// 2\n// 3\n// 4\n// 5\n// 6\n// 7\n// 8\n// 9\n// 10 Java copysourceSource.fromJavaStream(() -> IntStream.rangeClosed(1, 10))\n    .statefulMap(\n        () -> new ArrayList<Integer>(3),\n        (list, element) -> {\n          list.add(element);\n          if (list.size() == 3) {\n            return Pair.create(new ArrayList<Integer>(3), Collections.unmodifiableList(list));\n          } else {\n            return Pair.create(list, Collections.<Integer>emptyList());\n          }\n        },\n        Optional::ofNullable)\n    .mapConcat(list -> list)\n    .runForeach(System.out::println, system);\n// prints\n// 1\n// 2\n// 3\n// 4\n// 5\n// 6\n// 7\n// 8\n// 9\n// 10","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/statefulMap.html#reactive-streams-semantics","text":"emits the mapping function returns an element and downstream is ready to consume it backpressures downstream backpressures completes upstream completes cancels downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/statefulMapConcat.html","text":"","title":"statefulMapConcat"},{"location":"/stream/operators/Source-or-Flow/statefulMapConcat.html#statefulmapconcat","text":"Transform each element into zero or more elements that are individually passed downstream.\nSimple operators","title":"statefulMapConcat"},{"location":"/stream/operators/Source-or-Flow/statefulMapConcat.html#signature","text":"Flow.statefulMapConcatFlow.statefulMapConcat","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/statefulMapConcat.html#description","text":"Transform each element into zero or more elements that are individually passed downstream. The difference to mapConcat is that the transformation function is created from a factory for every materialization of the flow. This makes it possible to create and use mutable state for the operation, each new materialization of the stream will have its own state.\nFor cases where no state is needed but only a way to emit zero or more elements for every incoming element you can use mapConcat","title":"Description"},{"location":"/stream/operators/Source-or-Flow/statefulMapConcat.html#examples","text":"In this first sample we keep a counter, and combine each element with an id that is unique for the stream materialization (replicating the zipWithIndex operator):\nScala copysourceval letterAndIndex = Source(\"a\" :: \"b\" :: \"c\" :: \"d\" :: Nil).statefulMapConcat { () =>\n  var index = 0L\n\n  // we return the function that will be invoked for each element\n  { element =>\n    val zipped = (element, index)\n    index += 1\n    // we return an iterable with the single element\n    zipped :: Nil\n  }\n}\n\nletterAndIndex.runForeach(println)\n// prints\n// (a,0)\n// (b,1)\n// (c,2)\n// (d,3) Java copysourceSource<Pair<String, Long>, NotUsed> letterAndIndex =\n    Source.from(Arrays.asList(\"a\", \"b\", \"c\", \"d\"))\n        .statefulMapConcat(\n            () -> {\n              // variables we close over with lambdas must be final, so we use a container,\n              // a 1 element array, for the actual value.\n              final long[] index = {0L};\n\n              // we return the function that will be invoked for each element\n              return (element) -> {\n                final Pair<String, Long> zipped = new Pair<>(element, index[0]);\n                index[0] += 1;\n                // we return an iterable with the single element\n                return Collections.singletonList(zipped);\n              };\n            });\n\nletterAndIndex.runForeach(System.out::println, system);\n// prints\n// Pair(a,0)\n// Pair(b,1)\n// Pair(c,2)\n// Pair(d,3)\nIn this sample we let the value of the elements have an effect on the following elements, if an element starts with deny:word we add it to a deny list and filter out any subsequent entries of word:\nScala copysourceval fruitsAndDeniedCommands = Source(\n  \"banana\" :: \"pear\" :: \"orange\" :: \"deny:banana\" :: \"banana\" :: \"pear\" :: \"banana\" :: Nil)\n\nval denyFilterFlow = Flow[String].statefulMapConcat { () =>\n  var denyList = Set.empty[String]\n\n  { element =>\n    if (element.startsWith(\"deny:\")) {\n      denyList += element.drop(\"deny:\".size)\n      Nil // no element downstream when adding a deny listed keyword\n    } else if (denyList(element)) {\n      Nil // no element downstream if element is deny listed\n    } else {\n      element :: Nil\n    }\n  }\n}\n\nfruitsAndDeniedCommands.via(denyFilterFlow).runForeach(println)\n// prints\n// banana\n// pear\n// orange\n// pear Java copysourceSource<String, NotUsed> fruitsAndDenyCommands =\n    Source.from(\n        Arrays.asList(\"banana\", \"pear\", \"orange\", \"deny:banana\", \"banana\", \"pear\", \"banana\"));\n\nFlow<String, String, NotUsed> denyFilterFlow =\n    Flow.of(String.class)\n        .statefulMapConcat(\n            () -> {\n              Set<String> denyList = new HashSet<>();\n\n              return (element) -> {\n                if (element.startsWith(\"deny:\")) {\n                  denyList.add(element.substring(\"deny:\".length()));\n                  return Collections\n                      .emptyList(); // no element downstream when adding a deny listed keyword\n                } else if (denyList.contains(element)) {\n                  return Collections\n                      .emptyList(); // no element downstream if element is deny listed\n                } else {\n                  return Collections.singletonList(element);\n                }\n              };\n            });\n\nfruitsAndDenyCommands.via(denyFilterFlow).runForeach(System.out::println, system);\n// prints\n// banana\n// pear\n// orange\n// pear\nFor cases where there is a need to emit elements based on the state when the stream ends, it is possible to add an extra element signalling the end of the stream before the statefulMapConcat operator.\nIn this sample we collect all elements starting with the letter b and emit those once we have reached the end of the stream using a special end element. The end element is a special string to keep the sample concise, in a real application it may make sense to use types instead.\nScala copysourceval words = Source(\"baboon\" :: \"crocodile\" :: \"bat\" :: \"flamingo\" :: \"hedgehog\" :: \"beaver\" :: Nil)\n\nval bWordsLast = Flow[String].concat(Source.single(\"-end-\")).statefulMapConcat { () =>\n  var stashedBWords: List[String] = Nil\n\n  { element =>\n    if (element.startsWith(\"b\")) {\n      // prepend to stash and emit no element\n      stashedBWords = element :: stashedBWords\n      Nil\n    } else if (element.equals(\"-end-\")) {\n      // return in the stashed words in the order they got stashed\n      stashedBWords.reverse\n    } else {\n      // emit the element as is\n      element :: Nil\n    }\n  }\n}\n\nwords.via(bWordsLast).runForeach(println)\n// prints\n// crocodile\n// flamingo\n// hedgehog\n// baboon\n// bat\n// beaver Java copysourceSource<String, NotUsed> words =\n    Source.from(Arrays.asList(\"baboon\", \"crocodile\", \"bat\", \"flamingo\", \"hedgehog\", \"beaver\"));\n\nFlow<String, String, NotUsed> bWordsLast =\n    Flow.of(String.class)\n        .concat(Source.single(\"-end-\"))\n        .statefulMapConcat(\n            () -> {\n              List<String> stashedBWords = new ArrayList<>();\n\n              return (element) -> {\n                if (element.startsWith(\"b\")) {\n                  // add to stash and emit no element\n                  stashedBWords.add(element);\n                  return Collections.emptyList();\n                } else if (element.equals(\"-end-\")) {\n                  // return in the stashed words in the order they got stashed\n                  return stashedBWords;\n                } else {\n                  // emit the element as is\n                  return Collections.singletonList(element);\n                }\n              };\n            });\n\nwords.via(bWordsLast).runForeach(System.out::println, system);\n// prints\n// crocodile\n// flamingo\n// hedgehog\n// baboon\n// bat\n// beaver\nWhen defining aggregates like this you should consider if it is safe to let the state grow without bounds or if you should rather drop elements or throw an exception if the collected set of elements grows too big.\nFor even more fine grained capabilities than can be achieved with statefulMapConcat take a look at stream customization.","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/statefulMapConcat.html#reactive-streams-semantics","text":"emits when the mapping function returns an element or there are still remaining elements from the previously calculated collection backpressures when downstream backpressures or there are still available elements from the previously calculated collection completes when upstream completes and all remaining elements has been emitted","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/take.html","text":"","title":"take"},{"location":"/stream/operators/Source-or-Flow/take.html#take","text":"Pass n incoming elements downstream and then complete\nSimple operators","title":"take"},{"location":"/stream/operators/Source-or-Flow/take.html#signature","text":"Source.takeSource.take Flow.takeFlow.take","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/take.html#description","text":"Pass n incoming elements downstream and then complete","title":"Description"},{"location":"/stream/operators/Source-or-Flow/take.html#example","text":"Scala copysourceSource(1 to 5).take(3).runForeach(println)\n// 1\n// 2\n// 3 Java copysourceSource.from(Arrays.asList(1, 2, 3, 4, 5)).take(3).runForeach(System.out::println, system);\n// this will print:\n// 1\n// 2\n// 3","title":"Example"},{"location":"/stream/operators/Source-or-Flow/take.html#reactive-streams-semantics","text":"emits while the specified number of elements to take has not yet been reached backpressures when downstream backpressures completes when the defined number of elements has been taken or upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Sink/takeLast.html","text":"","title":"Sink.takeLast"},{"location":"/stream/operators/Sink/takeLast.html#sink-takelast","text":"Collect the last n values emitted from the stream into a collection.\nSink operators","title":"Sink.takeLast"},{"location":"/stream/operators/Sink/takeLast.html#signature","text":"Sink.takeLastSink.takeLast","title":"Signature"},{"location":"/stream/operators/Sink/takeLast.html#description","text":"Materializes into a Future CompletionStage of immutable.Seq[T] List<In> containing the last n collected elements when the stream completes. If the stream completes before signaling at least n elements, the Future CompletionStage will complete with the number of elements taken at that point. If the stream never completes, the Future CompletionStage will never complete. If there is a failure signaled in the stream the Future CompletionStage will be completed with failure.","title":"Description"},{"location":"/stream/operators/Sink/takeLast.html#example","text":"Scala copysourcecase class Student(name: String, gpa: Double)\n\nval students = List(\n  Student(\"Alison\", 4.7),\n  Student(\"Adrian\", 3.1),\n  Student(\"Alexis\", 4),\n  Student(\"Benita\", 2.1),\n  Student(\"Kendra\", 4.2),\n  Student(\"Jerrie\", 4.3)).sortBy(_.gpa)\n\nval sourceOfStudents = Source(students)\n\nval result: Future[Seq[Student]] = sourceOfStudents.runWith(Sink.takeLast(3))\n\nresult.foreach { topThree =>\n  println(\"#### Top students ####\")\n  topThree.reverse.foreach { s =>\n    println(s\"Name: ${s.name}, GPA: ${s.gpa}\")\n  }\n}\n/*\n    #### Top students ####\n    Name: Alison, GPA: 4.7\n    Name: Jerrie, GPA: 4.3\n    Name: Kendra, GPA: 4.2\n */\n Java copysourceimport org.apache.pekko.japi.Pair;\nimport org.reactivestreams.Publisher;\n// pair of (Name, GPA)\nList<Pair<String, Double>> sortedStudents =\n    Arrays.asList(\n        new Pair<>(\"Benita\", 2.1),\n        new Pair<>(\"Adrian\", 3.1),\n        new Pair<>(\"Alexis\", 4.0),\n        new Pair<>(\"Kendra\", 4.2),\n        new Pair<>(\"Jerrie\", 4.3),\n        new Pair<>(\"Alison\", 4.7));\n\nSource<Pair<String, Double>, NotUsed> studentSource = Source.from(sortedStudents);\n\nCompletionStage<List<Pair<String, Double>>> topThree =\n    studentSource.runWith(Sink.takeLast(3), system);\n\ntopThree.thenAccept(\n    result -> {\n      System.out.println(\"#### Top students ####\");\n      for (int i = result.size() - 1; i >= 0; i--) {\n        Pair<String, Double> s = result.get(i);\n        System.out.println(\"Name: \" + s.first() + \", \" + \"GPA: \" + s.second());\n      }\n    });\n/*\n  #### Top students ####\n  Name: Alison, GPA: 4.7\n  Name: Jerrie, GPA: 4.3\n  Name: Kendra, GPA: 4.2\n*/","title":"Example"},{"location":"/stream/operators/Sink/takeLast.html#reactive-streams-semantics","text":"cancels never backpressures never","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/takeWhile.html","text":"","title":"takeWhile"},{"location":"/stream/operators/Source-or-Flow/takeWhile.html#takewhile","text":"Pass elements downstream as long as a predicate function returns true and then complete.\nSimple operators","title":"takeWhile"},{"location":"/stream/operators/Source-or-Flow/takeWhile.html#signature","text":"Source.takeWhileSource.takeWhile Flow.takeWhileFlow.takeWhile","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/takeWhile.html#description","text":"Pass elements downstream as long as a predicate function returns true and then complete. The element for which the predicate returns false is not emitted.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/takeWhile.html#example","text":"Scala copysourceSource(1 to 10).takeWhile(_ < 3).runForeach(println)\n// prints\n// 1\n// 2 Java copysourceSource.from(Arrays.asList(1, 2, 3, 4, 5))\n    .takeWhile(i -> i < 3)\n    .runForeach(System.out::println, system);\n// this will print:\n// 1\n// 2","title":"Example"},{"location":"/stream/operators/Source-or-Flow/takeWhile.html#reactive-streams-semantics","text":"emits while the predicate is true and until the first false result backpressures when downstream backpressures completes when predicate returned false or upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/takeWithin.html","text":"","title":"takeWithin"},{"location":"/stream/operators/Source-or-Flow/takeWithin.html#takewithin","text":"Pass elements downstream within a timeout and then complete.\nTimer driven operators","title":"takeWithin"},{"location":"/stream/operators/Source-or-Flow/takeWithin.html#signature","text":"Source.takeWithinSource.takeWithin Flow.takeWithinFlow.takeWithin","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/takeWithin.html#description","text":"Pass elements downstream within a timeout and then complete.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/takeWithin.html#reactive-streams-semantics","text":"emits when an upstream element arrives backpressures downstream backpressures completes upstream completes or timer fires","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/throttle.html","text":"","title":"throttle"},{"location":"/stream/operators/Source-or-Flow/throttle.html#throttle","text":"Limit the throughput to a specific number of elements per time unit, or a specific total cost per time unit, where a function has to be provided to calculate the individual cost of each element.\nSimple operators","title":"throttle"},{"location":"/stream/operators/Source-or-Flow/throttle.html#signature","text":"Source.throttleSource.throttle Flow.throttleFlow.throttle","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/throttle.html#description","text":"Limit the throughput to a specific number of elements per time unit, or a specific total cost per time unit, where a function has to be provided to calculate the individual cost of each element.\nThe throttle operator combines well with the queue operator to adapt the speeds on both ends of the queue-throttle pair.\nSee also Buffers and working with rate for related operators.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/throttle.html#example","text":"Imagine the server end of a streaming platform. When a client connects and request a video content, the server should return the content. Instead of serving a complete video as fast as bandwith allows, throttle can be used to limit the network usage to 24 frames per second (let’s imagine this streaming platform stores frames, not bytes).\nScala copysourceval framesPerSecond = 24\n\n// val frameSource: Source[Frame,_]\nval videoThrottling = frameSource.throttle(framesPerSecond, 1.second)\n// serialize `Frame` and send over the network. Java copysourceint framesPerSecond = 24;\n\nSource<Frame, NotUsed> videoThrottling =\n    frameSource.throttle(framesPerSecond, Duration.ofSeconds(1));\n// serialize `Frame` and send over the network.\nThe problem in the example above is that when there’s a network hiccup, the video playback will interrupt. It can be improved by sending more content than the necessary ahead of time and let the client buffer that. So, throttle can be used to burst the first 30 seconds and then send a constant of 24 frames per second. This way, when a request comes in a good chunk of content will be downloaded and after that the server will activate the throttling.\nScala copysource// val frameSource: Source[Frame,_]\nval videoThrottlingWithBurst = frameSource.throttle(\n  framesPerSecond,\n  1.second,\n  framesPerSecond * 30, // maximumBurst\n  ThrottleMode.Shaping)\n// serialize `Frame` and send over the network. Java copysourceSource<Frame, NotUsed> throttlingWithBurst =\n    frameSource.throttle(\n        framesPerSecond, Duration.ofSeconds(1), framesPerSecond * 30, ThrottleMode.shaping());\n// serialize `Frame` and send over the network.\nThe extra argument to set the ThrottleMode to shaping tells throttle to make pauses to avoid exceeding the maximum rate. Alternatively we could set the throttling mode to cause a stream failure when upstream is faster than the throttle rate.\nThe examples above don’t cover all the parameters supported by throttle (e.g. cost-based throttling). See the api documentationapi documentation for all the details.","title":"Example"},{"location":"/stream/operators/Source-or-Flow/throttle.html#reactive-streams-semantics","text":"emits when upstream emits an element and configured time per each element elapsed backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/tick.html","text":"","title":"Source.tick"},{"location":"/stream/operators/Source/tick.html#source-tick","text":"A periodical repetition of an arbitrary object.\nSource operators","title":"Source.tick"},{"location":"/stream/operators/Source/tick.html#signature","text":"Source.tickSource.tick","title":"Signature"},{"location":"/stream/operators/Source/tick.html#description","text":"A periodical repetition of an arbitrary object. Delay of first tick is specified separately from interval of the following ticks.\nIf downstream is applying backpressure when the time period has passed the tick is dropped.\nThe source materializes a CancellableCancellable that can be used to complete the source.\nNote The element must be immutable as the source can be materialized several times and may pass it between threads, see the second example for achieving a periodical element that changes over time.\nSee also:\nrepeat Stream a single object repeatedly. cycle Stream iterator in cycled manner.","title":"Description"},{"location":"/stream/operators/Source/tick.html#examples","text":"This first example prints to standard out periodically:\nScala copysourceSource\n  .tick(\n    1.second, // delay of first tick\n    1.second, // delay of subsequent ticks\n    \"tick\" // element emitted each tick\n  )\n  .runForeach(println) Java copysourceSource.tick(\n        Duration.ofSeconds(1), // delay of first tick\n        Duration.ofSeconds(1), // delay of subsequent ticks\n        \"tick\" // element emitted each tick\n        )\n    .runForeach(System.out::println, system);\nYou can also use the tick to periodically emit a value, in this sample we use the tick to trigger a query to an actor using ask and emit the response downstream. For this usage, what is important is that it was emitted, not the actual tick value.\nScala copysourceval periodicActorResponse: Source[String, Cancellable] = Source\n  .tick(1.second, 1.second, \"tick\")\n  .mapAsync(1) { _ =>\n    implicit val timeout: Timeout = 3.seconds\n    val response: Future[MyActor.Response] = myActor.ask(MyActor.Query(_))\n    response\n  }\n  .map(_.text); Java copysourceSource<String, Cancellable> periodicActorResponse =\n    Source.tick(Duration.ofSeconds(1), Duration.ofSeconds(1), \"tick\")\n        .mapAsync(\n            1,\n            notUsed -> {\n              CompletionStage<MyActor.Response> response =\n                  AskPattern.ask(\n                      myActor, MyActor.Query::new, Duration.ofSeconds(3), system.scheduler());\n              return response;\n            })\n        .map(response -> response.text);\nA neat trick is to combine this with zipLatest to combine a stream of elements with a value that is updated periodically instead of having to trigger a query for each element:\nScala copysourceval zipWithLatestResponse: Flow[Int, (Int, String), NotUsed] =\n  Flow[Int].zipLatest(periodicActorResponse); Java copysourceFlow<Integer, Pair<Integer, String>, NotUsed> zipWithLatestResponse =\n    Flow.of(Integer.class).zipLatest(periodicActorResponse);","title":"Examples"},{"location":"/stream/operators/Source/tick.html#reactive-streams-semantics","text":"emits periodically, if there is downstream backpressure ticks are skipped completes when the materialized Cancellable is cancelled","title":"Reactive Streams semantics"},{"location":"/stream/operators/FileIO/toFile.html","text":"","title":"FileIO.toFile"},{"location":"/stream/operators/FileIO/toFile.html#fileio-tofile","text":"Create a sink which will write incoming ByteString s to a given file.\nFile IO Sinks and Sources\nWarning The toFile operator has been deprecated, use toPath instead.","title":"FileIO.toFile"},{"location":"/stream/operators/FileIO/toFile.html#signature","text":"FileIO.toFileFileIO.toFile","title":"Signature"},{"location":"/stream/operators/FileIO/toFile.html#description","text":"Creates a Sink which writes incoming ByteString elements to the given file path. Overwrites existing files by truncating their contents as default. Materializes a Future CompletionStage of IOResult that will be completed with the size of the file (in bytes) at the streams completion, and a possible exception if IO operation was not completed successfully.","title":"Description"},{"location":"/stream/operators/FileIO/toPath.html","text":"","title":"FileIO.toPath"},{"location":"/stream/operators/FileIO/toPath.html#fileio-topath","text":"Create a sink which will write incoming ByteString s to a given file path.\nFile IO Sinks and Sources","title":"FileIO.toPath"},{"location":"/stream/operators/FileIO/toPath.html#signature","text":"FileIO.toPathFileIO.toPath","title":"Signature"},{"location":"/stream/operators/FileIO/toPath.html#description","text":"Creates a Sink which writes incoming ByteString elements to the given file path. Overwrites existing files by truncating their contents as default. Materializes a Future CompletionStage of IOResult that will be completed with the size of the file (in bytes) at the streams completion, and a possible exception if IO operation was not completed successfully.","title":"Description"},{"location":"/stream/operators/FileIO/toPath.html#example","text":"Scala copysourceval file = Paths.get(\"greeting.txt\")\nval text = Source.single(\"Hello Pekko Stream!\")\nval result: Future[IOResult] = text.map(t => ByteString(t)).runWith(FileIO.toPath(file)) Java copysourcefinal Path file = Paths.get(\"greeting.txt\");\n  Sink<ByteString, CompletionStage<IOResult>> fileSink = FileIO.toPath(file);\n  Source<String, NotUsed> textSource = Source.single(\"Hello Pekko Stream!\");\n\n  CompletionStage<IOResult> ioResult =\n      textSource.map(ByteString::fromString).runWith(fileSink, system);","title":"Example"},{"location":"/stream/operators/Source/unfold.html","text":"","title":"Source.unfold"},{"location":"/stream/operators/Source/unfold.html#source-unfold","text":"Stream the result of a function as long as it returns a Some non empty Optional.\nSource operators","title":"Source.unfold"},{"location":"/stream/operators/Source/unfold.html#signature","text":"Source.unfoldSource.unfold","title":"Signature"},{"location":"/stream/operators/Source/unfold.html#description","text":"Stream the result of a function as long as it returns a Some non empty Optional. The value inside the option consists of a tuple pair where the first value is a state passed back into the next call to the function allowing to pass a state. The first invocation of the provided fold function will receive the zero state.\nWarning The same zero state object will be used for every materialization of the Source so it is mandatory that the state is immutable. For example a java.util.Iterator, Array or Java standard library collection would not be safe as the fold operation could mutate the value. If you must use a mutable value, combining with Source.lazySource to make sure a new mutable zero value is created for each materialization is one solution.\nNote that for unfolding a source of elements through a blocking API, such as a network or filesystem resource you should prefer using unfoldResource.","title":"Description"},{"location":"/stream/operators/Source/unfold.html#examples","text":"This first sample starts at a user provided integer and counts down to zero using unfold :\nScala copysourcedef countDown(from: Int): Source[Int, NotUsed] =\n  Source.unfold(from) { current =>\n    if (current == 0) None\n    else Some((current - 1, current))\n  } Java copysourcepublic static Source<Integer, NotUsed> countDown(Integer from) {\n  return Source.unfold(\n      from,\n      current -> {\n        if (current == 0) return Optional.empty();\n        else return Optional.of(Pair.create(current - 1, current));\n      });\n}\nIt is also possible to express unfolds that don’t have an end, which will never return None Optional.empty and must be combined with for example .take(n) to not produce infinite streams. Here we have implemented the Fibonacci numbers (0, 1, 1, 2, 3, 5, 8, 13, etc.) with unfold:\nScala copysourcedef fibonacci: Source[BigInt, NotUsed] =\n  Source.unfold((BigInt(0), BigInt(1))) {\n    case (a, b) =>\n      Some(((b, a + b), a))\n  } Java copysourcepublic static Source<BigInteger, NotUsed> fibonacci() {\n  return Source.unfold(\n      Pair.create(BigInteger.ZERO, BigInteger.ONE),\n      current -> {\n        BigInteger a = current.first();\n        BigInteger b = current.second();\n        Pair<BigInteger, BigInteger> next = Pair.create(b, a.add(b));\n        return Optional.of(Pair.create(next, a));\n      });\n}","title":"Examples"},{"location":"/stream/operators/Source/unfold.html#reactive-streams-semantics","text":"emits when there is demand and the unfold function over the previous state returns non empty value completes when the unfold function returns an empty value","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/unfoldAsync.html","text":"","title":"Source.unfoldAsync"},{"location":"/stream/operators/Source/unfoldAsync.html#source-unfoldasync","text":"Just like unfold but the fold function returns a Future CompletionStage.\nSource operators","title":"Source.unfoldAsync"},{"location":"/stream/operators/Source/unfoldAsync.html#signature","text":"Source.unfoldAsyncSource.unfoldAsync","title":"Signature"},{"location":"/stream/operators/Source/unfoldAsync.html#description","text":"Just like unfold but the fold function returns a Future CompletionStage which will cause the source to complete or emit when it completes.\nCan be used to implement many stateful sources without having to touch the more low level GraphStage API.","title":"Description"},{"location":"/stream/operators/Source/unfoldAsync.html#examples","text":"In this example we are asking an imaginary actor for chunks of bytes from an offset with a protocol like this:\nScala copysourceobject DataActor {\n  sealed trait Command\n  case class FetchChunk(offset: Long, replyTo: ActorRef[Chunk]) extends Command\n  case class Chunk(bytes: ByteString) Java copysourceclass DataActor {\n  interface Command {}\n\n  static final class FetchChunk implements Command {\n    public final long offset;\n    public final ActorRef<Chunk> replyTo;\n\n    public FetchChunk(long offset, ActorRef<Chunk> replyTo) {\n      this.offset = offset;\n      this.replyTo = replyTo;\n    }\n  }\n\n  static final class Chunk {\n    public final ByteString bytes;\n\n    public Chunk(ByteString bytes) {\n      this.bytes = bytes;\n    }\n  }\nThe actor will reply with the Chunk message, if we ask for an offset outside of the end of the data the actor will respond with an empty ByteString\nWe want to represent this as a stream of ByteStrings that complete when we reach the end, to achieve this we use the offset as the state passed between unfoldAsync invocations:\nScala copysource// actor we can query for data with an offset\nval dataActor: ActorRef[DataActor.Command] = ???\nimport system.executionContext\n\nimplicit val askTimeout: Timeout = 3.seconds\nval startOffset = 0L\nval byteSource: Source[ByteString, NotUsed] =\n  Source.unfoldAsync(startOffset) { currentOffset =>\n    // ask for next chunk\n    val nextChunkFuture: Future[DataActor.Chunk] =\n      dataActor.ask(DataActor.FetchChunk(currentOffset, _))\n\n    nextChunkFuture.map { chunk =>\n      val bytes = chunk.bytes\n      if (bytes.isEmpty) None // end of data\n      else Some((currentOffset + bytes.length, bytes))\n    }\n  } Java copysourceActorRef<DataActor.Command> dataActor = null; // let's say we got it from somewhere\n\nDuration askTimeout = Duration.ofSeconds(3);\nlong startOffset = 0L;\nSource<ByteString, NotUsed> byteSource =\n    Source.unfoldAsync(\n        startOffset,\n        currentOffset -> {\n          // ask for next chunk\n          CompletionStage<DataActor.Chunk> nextChunkCS =\n              AskPattern.ask(\n                  dataActor,\n                  (ActorRef<DataActor.Chunk> ref) ->\n                      new DataActor.FetchChunk(currentOffset, ref),\n                  askTimeout,\n                  system.scheduler());\n\n          return nextChunkCS.thenApply(\n              chunk -> {\n                ByteString bytes = chunk.bytes;\n                if (bytes.isEmpty()) return Optional.empty();\n                else return Optional.of(Pair.create(currentOffset + bytes.size(), bytes));\n              });\n        });","title":"Examples"},{"location":"/stream/operators/Source/unfoldAsync.html#reactive-streams-semantics","text":"emits when there is demand and unfold state returned future completes with some value completes when the future CompletionStage returned by the unfold function completes with an empty value","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/unfoldResource.html","text":"","title":"Source.unfoldResource"},{"location":"/stream/operators/Source/unfoldResource.html#source-unfoldresource","text":"Wrap any resource that can be opened, queried for next element (in a blocking way) and closed using three distinct functions into a source.\nSource operators","title":"Source.unfoldResource"},{"location":"/stream/operators/Source/unfoldResource.html#signature","text":"Source.unfoldResourceSource.unfoldResource","title":"Signature"},{"location":"/stream/operators/Source/unfoldResource.html#description","text":"Source.unfoldResource allows us to safely extract stream elements from blocking resources by providing it with three functions:\ncreate: Open or create the resource read: Fetch the next element or signal that we reached the end of the stream by returning a Optional.emptyNone close: Close the resource, invoked on end of stream or if the stream fails\nThe functions are by default called on Pekko’s dispatcher for blocking IO to avoid interfering with other stream operations. See Blocking Needs Careful Management for an explanation on why this is important.\nNote that there are pre-built unfoldResource-like operators to wrap java.io.InputStreams in Additional Sink and Source converters, Iterator in fromIterator and File IO in File IO Sinks and Sources. Additional prebuilt technology specific connectors can also be found in Pekko Connectors.","title":"Description"},{"location":"/stream/operators/Source/unfoldResource.html#examples","text":"Imagine we have a database API which may potentially block both when we initially perform a query and on retrieving each result from the query. It also gives us an iterator like way to determine if we have reached the end of the result and a close method that must be called to free resources:\nScala copysourcetrait Database {\n  // blocking query\n  def doQuery(): QueryResult\n}\ntrait QueryResult {\n  def hasMore: Boolean\n  // potentially blocking retrieval of each element\n  def nextEntry(): DatabaseEntry\n  def close(): Unit\n}\ntrait DatabaseEntry Java copysourceinterface Database {\n  // blocking query\n  QueryResult doQuery();\n}\n\ninterface QueryResult {\n  boolean hasMore();\n  // potentially blocking retrieval of each element\n  DatabaseEntry nextEntry();\n\n  void close();\n}\n\ninterface DatabaseEntry {}\nLet’s see how we use the API above safely through unfoldResource:\nScala copysource// we don't actually have one, it was just made up for the sample\nval database: Database = ???\n\nval queryResultSource: Source[DatabaseEntry, NotUsed] =\n  Source.unfoldResource[DatabaseEntry, QueryResult](\n    // open\n    { () =>\n      database.doQuery()\n    },\n    // read\n    { query =>\n      if (query.hasMore)\n        Some(query.nextEntry())\n      else\n        // signals end of resource\n        None\n    },\n    // close\n    query => query.close())\n\n// process each element\nqueryResultSource.runForeach(println) Java copysource// we don't actually have one, it was just made up for the sample\nDatabase database = null;\n\nSource<DatabaseEntry, NotUsed> queryResultSource =\n    Source.unfoldResource(\n        // open\n        () -> database.doQuery(),\n        // read\n        (queryResult) -> {\n          if (queryResult.hasMore()) return Optional.of(queryResult.nextEntry());\n          else return Optional.empty();\n        },\n        // close\n        QueryResult::close);\n\nqueryResultSource.runForeach(entry -> System.out.println(entry.toString()), system);\nIf the resource produces more than one element at a time, combining unfoldResource with mapConcat(identity)mapConcat(elems -> elems) will give you a stream of individual elements. See mapConcat) for details.","title":"Examples"},{"location":"/stream/operators/Source/unfoldResource.html#reactive-streams-semantics","text":"emits when there is demand and the read function returns a value completes when the read function returns Nonean empty Optional","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/unfoldResourceAsync.html","text":"","title":"Source.unfoldResourceAsync"},{"location":"/stream/operators/Source/unfoldResourceAsync.html#source-unfoldresourceasync","text":"Wrap any resource that can be opened, queried for next element and closed in an asynchronous way.\nSource operators","title":"Source.unfoldResourceAsync"},{"location":"/stream/operators/Source/unfoldResourceAsync.html#signature","text":"Source.unfoldResourceAsyncSource.unfoldResourceAsync","title":"Signature"},{"location":"/stream/operators/Source/unfoldResourceAsync.html#description","text":"Wrap any resource that can be opened, queried for next element and closed in an asynchronous way with three distinct functions into a source. This operator is the equivalent of unfoldResource but for resources with asynchronous APIs.\nSource.unfoldResourceAsync allows us to safely extract stream elements from a resource with an async API by providing it with three functions that all return a FutureCompletionStage:\ncreate: Open or create the resource read: Fetch the next element or signal that we reached the end of the stream by completing the FutureCompletionStage with a Optional.emptyNone close: Close the resource, invoked on end of stream or if the stream fails\nAll exceptions thrown by create and close as well as the FutureCompletionStages completing with failure will fail the stream. The supervision strategy is used to handle exceptions from read, create and from the FutureCompletionStages.\nNote that there are pre-built unfoldResourceAsync-like operators to wrap java.io.InputStreams in Additional Sink and Source converters, Iterator in fromIterator and File IO in File IO Sinks and Sources. Additional prebuilt technology specific connectors can also be found in Pekko Connectors.","title":"Description"},{"location":"/stream/operators/Source/unfoldResourceAsync.html#examples","text":"Imagine we have an async database API which we initially perform an async query and then can check if there are more results in an asynchronous way.\nScala copysourcetrait Database {\n  // blocking query\n  def doQuery(): Future[QueryResult]\n}\ntrait QueryResult {\n  def hasMore(): Future[Boolean]\n  def nextEntry(): Future[DatabaseEntry]\n  def close(): Future[Unit]\n}\ntrait DatabaseEntry Java copysourceinterface Database {\n  // async query\n  CompletionStage<QueryResult> doQuery();\n}\n\ninterface QueryResult {\n\n  // are there more results\n  CompletionStage<Boolean> hasMore();\n\n  // async retrieval of each element\n  CompletionStage<DatabaseEntry> nextEntry();\n\n  CompletionStage<Void> close();\n}\n\ninterface DatabaseEntry {}\nLet’s see how we use the API above safely through unfoldResourceAsync:\nScala copysource// we don't actually have one, it was just made up for the sample\nval database: Database = ???\n\nval queryResultSource: Source[DatabaseEntry, NotUsed] =\n  Source.unfoldResourceAsync[DatabaseEntry, QueryResult](\n    // open\n    () => database.doQuery(),\n    // read\n    query =>\n      query.hasMore().flatMap {\n        case false => Future.successful(None)\n        case true  => query.nextEntry().map(dbEntry => Some(dbEntry))\n      },\n    // close\n    query => query.close().map(_ => Done))\n\n// process each element\nqueryResultSource.runForeach(println) Java copysource  // we don't actually have one, it was just made up for the sample\n  Database database = null;\n\n  Source<DatabaseEntry, NotUsed> queryResultSource =\n      Source.unfoldResourceAsync(\n          // open\n          database::doQuery,\n          // read\n          this::readQueryResult,\n          // close\n          queryResult -> queryResult.close().thenApply(__ -> Done.done()));\n\n  queryResultSource.runForeach(entry -> System.out.println(entry.toString()), system);\nprivate CompletionStage<Optional<DatabaseEntry>> readQueryResult(QueryResult queryResult) {\n  return queryResult\n      .hasMore()\n      .thenCompose(\n          more -> {\n            if (more) {\n              return queryResult.nextEntry().thenApply(Optional::of);\n            } else {\n              return CompletableFuture.completedFuture(Optional.empty());\n            }\n          });\n}","title":"Examples"},{"location":"/stream/operators/Source/unfoldResourceAsync.html#reactive-streams-semantics","text":"emits when there is demand and Future CompletionStage from read function returns value completes when Future CompletionStage from read function returns None","title":"Reactive Streams semantics"},{"location":"/stream/operators/Unzip.html","text":"","title":"Unzip"},{"location":"/stream/operators/Unzip.html#unzip","text":"Takes a stream of two element tuples and unzips the two elements ino two different downstreams.\nFan-out operators","title":"Unzip"},{"location":"/stream/operators/Unzip.html#signature","text":"","title":"Signature"},{"location":"/stream/operators/Unzip.html#description","text":"Takes a stream of two element tuples and unzips the two elements ino two different downstreams.","title":"Description"},{"location":"/stream/operators/Unzip.html#reactive-streams-semantics","text":"emits when all of the outputs stops backpressuring and there is an input element available backpressures when any of the outputs backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/UnzipWith.html","text":"","title":"UnzipWith"},{"location":"/stream/operators/UnzipWith.html#unzipwith","text":"Splits each element of input into multiple downstreams using a function\nFan-out operators","title":"UnzipWith"},{"location":"/stream/operators/UnzipWith.html#signature","text":"","title":"Signature"},{"location":"/stream/operators/UnzipWith.html#description","text":"Splits each element of input into multiple downstreams using a function","title":"Description"},{"location":"/stream/operators/UnzipWith.html#reactive-streams-semantics","text":"emits when all of the outputs stops backpressuring and there is an input element available backpressures when any of the outputs backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/watch.html","text":"","title":"watch"},{"location":"/stream/operators/Source-or-Flow/watch.html#watch","text":"Watch a specific ActorRef and signal a failure downstream once the actor terminates.\nActor interop operators","title":"watch"},{"location":"/stream/operators/Source-or-Flow/watch.html#signature","text":"Source.watchSource.watch Flow.watchFlow.watch","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/watch.html#description","text":"Watch a specific ActorRef and signal a failure downstream once the actor terminates. The signaled failure will be an WatchedActorTerminatedException WatchedActorTerminatedException.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/watch.html#example","text":"An ActorRef can be can be watched and the stream will fail with WatchedActorTerminatedException when the actor terminates.\nScala copysourceval ref: ActorRef = someActor()\nval flow: Flow[String, String, NotUsed] =\n  Flow[String].watch(ref).recover {\n    case _: WatchedActorTerminatedException => s\"$ref terminated\"\n  } Java copysourcefinal ActorRef ref = someActor();\nFlow<String, String, NotUsed> flow =\n    Flow.of(String.class)\n        .watch(ref)\n        .recover(\n            org.apache.pekko.stream.WatchedActorTerminatedException.class,\n            () -> ref + \" terminated\");","title":"Example"},{"location":"/stream/operators/Source-or-Flow/watch.html#reactive-streams-semantics","text":"emits when upstream emits backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/watchTermination.html","text":"","title":"watchTermination"},{"location":"/stream/operators/Source-or-Flow/watchTermination.html#watchtermination","text":"Materializes to a Future CompletionStage that will be completed with Done or failed depending whether the upstream of the operators has been completed or failed.\nWatching status operators","title":"watchTermination"},{"location":"/stream/operators/Source-or-Flow/watchTermination.html#signature","text":"Source.watchTerminationSource.watchTermination Flow.watchTerminationFlow.watchTermination","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/watchTermination.html#description","text":"Materializes to a Future CompletionStage that will be completed with Done or failed depending whether the upstream of the operators has been completed or failed. The operators otherwise passes through elements unchanged.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/watchTermination.html#examples","text":"Scala copysourceSource(1 to 5)\n  .watchTermination()((prevMatValue, future) =>\n    // this function will be run when the stream terminates\n    // the Future provided as a second parameter indicates whether the stream completed successfully or failed\n    future.onComplete {\n      case Failure(exception) => println(exception.getMessage)\n      case Success(_)         => println(s\"The stream materialized $prevMatValue\")\n    })\n  .runForeach(println)\n/*\nPrints:\n1\n2\n3\n4\n5\nThe stream materialized NotUsed\n */\n\nSource(1 to 5)\n  .watchTermination()((prevMatValue, future) =>\n    future.onComplete {\n      case Failure(exception) => println(exception.getMessage)\n      case Success(_)         => println(s\"The stream materialized $prevMatValue\")\n    })\n  .runForeach(e => if (e == 3) throw new Exception(\"Boom\") else println(e))\n/*\nPrints:\n1\n2\nBoom\n */ Java copysourceSource.range(1, 5)\n    .watchTermination(\n        (prevMatValue, completionStage) -> {\n          completionStage.whenComplete(\n              (done, exc) -> {\n                if (done != null)\n                  System.out.println(\"The stream materialized \" + prevMatValue.toString());\n                else System.out.println(exc.getMessage());\n              });\n          return prevMatValue;\n        })\n    .runForeach(System.out::println, system);\n\n/*\nPrints:\n1\n2\n3\n4\n5\nThe stream materialized NotUsed\n */\n\nSource.range(1, 5)\n    .watchTermination(\n        (prevMatValue, completionStage) -> {\n          // this function will be run when the stream terminates\n          // the CompletionStage provided as a second parameter indicates whether\n          // the stream completed successfully or failed\n          completionStage.whenComplete(\n              (done, exc) -> {\n                if (done != null)\n                  System.out.println(\"The stream materialized \" + prevMatValue.toString());\n                else System.out.println(exc.getMessage());\n              });\n          return prevMatValue;\n        })\n    .runForeach(\n        element -> {\n          if (element == 3) throw new Exception(\"Boom\");\n          else System.out.println(element);\n        },\n        system);\n/*\nPrints:\n1\n2\nBoom\n */\nYou can also use the lambda function expected by watchTermination to map the materialized value of the stream. Additionally, the completion of the FutureCompletionStage provided as a second parameter of the lambda can be used to perform cleanup operations of the resources used by the stream itself.","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/watchTermination.html#reactive-streams-semantics","text":"emits when input has an element available backpressures when output backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/wireTap.html","text":"","title":"wireTap"},{"location":"/stream/operators/Source-or-Flow/wireTap.html#wiretap","text":"Attaches the given Sink to this Flow as a wire tap, meaning that elements that pass through will also be sent to the wire-tap Sink, without the latter affecting the mainline flow.\nFan-out operators","title":"wireTap"},{"location":"/stream/operators/Source-or-Flow/wireTap.html#signature","text":"Source.wireTapSource.wireTap Flow.wireTapFlow.wireTap","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/wireTap.html#description","text":"Attaches the given Sink to this Flow as a wire tap, meaning that elements that pass through will also be sent to the wire-tap Sink, without the latter affecting the mainline flow. If the wire-tap Sink backpressures, elements that would’ve been sent to it will be dropped instead.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/wireTap.html#reactive-streams-semantics","text":"emits element is available and demand exists from the downstream; the element will also be sent to the wire-tap Sink if there is demand. backpressures downstream backpressures completes upstream completes cancels downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/RestartSource/withBackoff.html","text":"","title":"RestartSource.withBackoff"},{"location":"/stream/operators/RestartSource/withBackoff.html#restartsource-withbackoff","text":"Wrap the given SourceSource with a SourceSource that will restart it when it fails or completes using an exponential backoff.\nError handling","title":"RestartSource.withBackoff"},{"location":"/stream/operators/RestartSource/withBackoff.html#signature","text":"RestartSource.withBackoffRestartSource.withBackoff","title":"Signature"},{"location":"/stream/operators/RestartSource/withBackoff.html#description","text":"Wrap the given SourceSource with a SourceSource that will restart it when it completes or fails using exponential backoff. The backoff resets back to minBackoff if there hasn’t been a restart within maxRestartsWithin (which defaults to minBackoff).\nThis SourceSource will not emit a complete or fail as long as maxRestarts is not reached, since the completion or failure of the wrapped SourceSource is handled by restarting it. The wrapped SourceSource can however be cancelled by cancelling this SourceSource. When that happens, the wrapped SourceSource, if currently running, will be cancelled, and it will not be restarted. This can be triggered simply by the downstream cancelling, or externally by introducing a KillSwitch right after this SourceSource in the graph.\nThis uses the same exponential backoff algorithm as BackoffOptsBackoffOpts.\nSee also:\nRestartSource.onFailuresWithBackoff RestartFlow.onFailuresWithBackoff RestartFlow.withBackoff RestartSink.withBackoff","title":"Description"},{"location":"/stream/operators/RestartSource/withBackoff.html#reactive-streams-semantics","text":"emits when the wrapped source emits backpressures during backoff and when downstream backpressures completes when maxRestarts are reached within the given time limit cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/RestartFlow/withBackoff.html","text":"","title":"RestartFlow.withBackoff"},{"location":"/stream/operators/RestartFlow/withBackoff.html#restartflow-withbackoff","text":"Wrap the given FlowFlow with a FlowFlow that will restart it when it fails or complete using an exponential backoff.\nError handling","title":"RestartFlow.withBackoff"},{"location":"/stream/operators/RestartFlow/withBackoff.html#signature","text":"RestartFlow.withBackoffRestartFlow.withBackoff","title":"Signature"},{"location":"/stream/operators/RestartFlow/withBackoff.html#description","text":"Wrap the given FlowFlow with a FlowFlow that will restart it when it completes or fails using exponential backoff. The backoff resets back to minBackoff if there hasn’t been a restart within maxRestartsWithin (which defaults to minBackoff).\nThis FlowFlow will not cancel, complete or emit a failure, until the opposite end of it has been cancelled or completed. Any termination by the FlowFlow before that time will be handled by restarting it as long as maxRestarts is not reached. Any termination signals sent to this FlowFlow however will terminate the wrapped FlowFlow, if it’s running, and then the FlowFlow will be allowed to terminate without being restarted.\nThe restart process is inherently lossy, since there is no coordination between cancelling and the sending of messages. A termination signal from either end of the wrapped FlowFlow will cause the other end to be terminated, and any in transit messages will be lost. During backoff, this FlowFlow will backpressure.\nThis uses the same exponential backoff algorithm as BackoffOptsBackoffOpts.\nSee also:\nRestartSource.withBackoff RestartSource.onFailuresWithBackoff RestartFlow.onFailuresWithBackoff RestartSink.withBackoff","title":"Description"},{"location":"/stream/operators/RestartFlow/withBackoff.html#reactive-streams-semantics","text":"emits when the wrapped flow emits backpressures during backoff and when the wrapped flow backpressures completes when maxRestarts are reached within the given time limit","title":"Reactive Streams semantics"},{"location":"/stream/operators/RestartSink/withBackoff.html","text":"","title":"RestartSink.withBackoff"},{"location":"/stream/operators/RestartSink/withBackoff.html#restartsink-withbackoff","text":"Wrap the given SinkSink with a SinkSink that will restart it when it fails or complete using an exponential backoff.\nError handling","title":"RestartSink.withBackoff"},{"location":"/stream/operators/RestartSink/withBackoff.html#signature","text":"RestartSink.withBackoffRestartSink.withBackoff","title":"Signature"},{"location":"/stream/operators/RestartSink/withBackoff.html#description","text":"Wrap the given SinkSink with a SinkSink that will restart it when it completes or fails using exponential backoff. The backoff resets back to minBackoff if there hasn’t been a restart within maxRestartsWithin (which defaults to minBackoff).\nThis SinkSink will not cancel as long as maxRestarts is not reached, since cancellation by the wrapped SinkSink is handled by restarting it. The wrapped SinkSink can however be completed by feeding a completion or error into this SinkSink. When that happens, the SinkSink, if currently running, will terminate and will not be restarted. This can be triggered simply by the upstream completing, or externally by introducing a KillSwitch right before this SinkSink in the graph.\nThe restart process is inherently lossy, since there is no coordination between cancelling and the sending of messages. When the wrapped SinkSink does cancel, this SinkSink will backpressure, however any elements already sent may have been lost.\nThis uses the same exponential backoff algorithm as BackoffOptsBackoffOpts.\nSee also:\nRestartSource.withBackoff RestartSource.onFailuresWithBackoff RestartFlow.onFailuresWithBackoff RestartFlow.withBackoff","title":"Description"},{"location":"/stream/operators/RestartSink/withBackoff.html#reactive-streams-semantics","text":"backpressures during backoff and when the wrapped sink backpressures completes when upstream completes or when maxRestarts are reached within the given time limit","title":"Reactive Streams semantics"},{"location":"/stream/operators/RetryFlow/withBackoff.html","text":"","title":"RetryFlow.withBackoff"},{"location":"/stream/operators/RetryFlow/withBackoff.html#retryflow-withbackoff","text":"Wrap the given FlowFlow and retry individual elements in that stream with an exponential backoff. A decider function tests every emitted element and can return a new element to be sent to the wrapped flow for another try.\nError handling","title":"RetryFlow.withBackoff"},{"location":"/stream/operators/RetryFlow/withBackoff.html#signature","text":"RetryFlow.withBackoffRetryFlow.withBackoff","title":"Signature"},{"location":"/stream/operators/RetryFlow/withBackoff.html#description","text":"When an element is emitted by the wrapped flow it is passed to the decideRetry function, which may return an element to retry in the flow.\nThe retry backoff is controlled by the minBackoff, maxBackoff and randomFactor parameters. At most maxRetries will be made after the initial try.\nThe wrapped flow must have one-in one-out semantics. It may not filter, nor duplicate elements. The RetryFlow will fail if two elements are emitted from the flow, it will be stuck “forever” if nothing is emitted. Just one element will be emitted into the flow at any time. The flow needs to emit an element before the next will be emitted to it.\nElements are retried as long as maxRetries is not reached and the decideRetry function returns a new element to be sent to flow. The decideRetry function gets passed in the original element sent to the flow and the element emitted by it. When decideRetry returns NoneOptional.empty, no retries will be issued, and the response will be emitted downstream.\nNote This API was added in Pekko 2.6.0 and may be changed in further patch releases.\nThis example wraps a flow handling IntsIntegers, and retries elements unless the result is 0 or negative, or maxRetries is hit.\nScala copysourceval flow: Flow[Int, Int, NotUsed] = // ???\n\nval retryFlow: Flow[Int, Int, NotUsed] =\n  RetryFlow.withBackoff(minBackoff = 10.millis, maxBackoff = 5.seconds, randomFactor = 0d, maxRetries = 3, flow)(\n    decideRetry = {\n      case (_, result) if result > 0 => Some(result)\n      case _                         => None\n    }) Java copysourceFlow<Integer, Integer, NotUsed> flow = // ...\n    // the wrapped flow\n\nFlow<Integer, Integer, NotUsed> retryFlow =\n    RetryFlow.withBackoff(\n        minBackoff,\n        maxBackoff,\n        randomFactor,\n        maxRetries,\n        flow,\n        (in, out) -> {\n          if (out > 0) return Optional.of(out);\n          else return Optional.empty();\n        });","title":"Description"},{"location":"/stream/operators/RetryFlow/withBackoff.html#reactive-streams-semantics","text":"emits when the wrapped flow emits, and either maxRetries is reached or decideRetry returns NoneOptional.empty backpressures during backoff, when the wrapped flow backpressures, or when downstream backpressures completes when upstream or the wrapped flow completes cancels when downstream or the wrapped flow cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/RetryFlow/withBackoffAndContext.html","text":"","title":"RetryFlow.withBackoffAndContext"},{"location":"/stream/operators/RetryFlow/withBackoffAndContext.html#retryflow-withbackoffandcontext","text":"Wrap the given FlowWithContextFlowWithContext and retry individual elements in that stream with an exponential backoff. A decider function tests every emitted element and can return a new element to be sent to the wrapped flow for another try.\nError handling","title":"RetryFlow.withBackoffAndContext"},{"location":"/stream/operators/RetryFlow/withBackoffAndContext.html#signature","text":"RetryFlow.withBackoffAndContextRetryFlow.withBackoffAndContext","title":"Signature"},{"location":"/stream/operators/RetryFlow/withBackoffAndContext.html#description","text":"When an element is emitted by the wrapped flow it is passed to the decideRetry function, which may return an element to retry in the flow.\nThe retry backoff is controlled by the minBackoff, maxBackoff and randomFactor parameters. At most maxRetries will be made after the initial try.\nThe wrapped flow must have one-in one-out semantics. It may not filter, nor duplicate elements. The RetryFlow will fail if two elements are emitted from the flow, it will be stuck “forever” if nothing is emitted. Just one element will be emitted into the flow at any time. The flow needs to emit an element before the next will be emitted to it.\nElements are retried as long as maxRetries is not reached and the decideRetry function returns a new element to be sent to flow. The decideRetry function gets passed in the original element sent to the flow and the element emitted by it together with their contexts as tuplesorg.apache.pekko.japi.Pairs. When decideRetry returns NoneOptional.empty, no retries will be issued, and the response will be emitted downstream.\nNote This API was added in Pekko 2.6.0 and may be changed in further patch releases.\nThis example wraps a flow handling IntsIntegers with SomeContext in context, and retries elements unless the result is 0 or negative, or maxRetries is hit.\nScala copysourceval flow: FlowWithContext[Int, SomeContext, Int, SomeContext, NotUsed] = // ???\n\nval retryFlow: FlowWithContext[Int, SomeContext, Int, SomeContext, NotUsed] =\n  RetryFlow.withBackoffAndContext(\n    minBackoff = 10.millis,\n    maxBackoff = 5.seconds,\n    randomFactor = 0d,\n    maxRetries = 3,\n    flow)(decideRetry = {\n    case ((_, _), (result, ctx)) if result > 0 => Some(result -> ctx)\n    case _                                     => None\n  }) Java copysourceFlowWithContext<Integer, SomeContext, Integer, SomeContext, NotUsed> flow = // ...\n    // the wrapped flow\n\nFlowWithContext<Integer, SomeContext, Integer, SomeContext, NotUsed> retryFlow =\n    RetryFlow.withBackoffAndContext(\n        minBackoff,\n        maxBackoff,\n        randomFactor,\n        maxRetries,\n        flow,\n        (in, out) -> {\n          Integer value = out.first();\n          SomeContext context = out.second();\n          if (value > 0) {\n            return Optional.of(Pair.create(value, context));\n          } else {\n            return Optional.empty();\n          }\n        });","title":"Description"},{"location":"/stream/operators/RetryFlow/withBackoffAndContext.html#reactive-streams-semantics","text":"emits when the wrapped flow emits, and either maxRetries is reached or decideRetry returns NoneOptional.empty backpressures during backoff, when the wrapped flow backpressures, or when downstream backpressures completes when upstream or the wrapped flow completes cancels when downstream or the wrapped flow cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/zip.html","text":"","title":"zip"},{"location":"/stream/operators/Source-or-Flow/zip.html#zip","text":"Combines elements from each of multiple sources into tuples Pair and passes the tuples pairs downstream.\nFan-in operators","title":"zip"},{"location":"/stream/operators/Source-or-Flow/zip.html#signature","text":"Source.zipSource.zip Flow.zipFlow.zip","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/zip.html#description","text":"Combines elements from each of multiple sources into tuples Pair and passes the tuples pairs downstream.\nSee also:\nzipAll zipWith zipWithIndex","title":"Description"},{"location":"/stream/operators/Source-or-Flow/zip.html#examples","text":"Scala copysourceimport org.apache.pekko\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.Sink\n\n    val sourceFruits = Source(List(\"apple\", \"orange\", \"banana\"))\n    val sourceFirstLetters = Source(List(\"A\", \"O\", \"B\"))\n    sourceFruits.zip(sourceFirstLetters).runWith(Sink.foreach(println))\n    // this will print ('apple', 'A'), ('orange', 'O'), ('banana', 'B') Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<String, NotUsed> sourceFruits = Source.from(Arrays.asList(\"apple\", \"orange\", \"banana\"));\nSource<String, NotUsed> sourceFirstLetters = Source.from(Arrays.asList(\"A\", \"O\", \"B\"));\nsourceFruits.zip(sourceFirstLetters).runForeach(System.out::println, system);\n// this will print ('apple', 'A'), ('orange', 'O'), ('banana', 'B')","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/zip.html#reactive-streams-semantics","text":"emits when both of the inputs have an element available backpressures both upstreams when downstream backpressures but also on an upstream that has emitted an element until the other upstream has emitted an element completes when either upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/zipAll.html","text":"","title":"zipAll"},{"location":"/stream/operators/Source-or-Flow/zipAll.html#zipall","text":"Combines elements from two sources into tuples Pair handling early completion of either source.\nFan-in operators","title":"zipAll"},{"location":"/stream/operators/Source-or-Flow/zipAll.html#signature","text":"Source.zipAllSource.zipAll Flow.zipAllFlow.zipAll","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/zipAll.html#description","text":"Combines elements from two sources into tuples Pair and passes downstream. If either source completes, a default value is combined with each value from the other source until it completes.\nSee also:\nzip zipWith zipWith zipWithIndex","title":"Description"},{"location":"/stream/operators/Source-or-Flow/zipAll.html#example","text":"Scala copysourceval numbers = Source(1 :: 2 :: 3 :: 4 :: Nil)\nval letters = Source(\"a\" :: \"b\" :: \"c\" :: Nil)\n\nnumbers.zipAll(letters, -1, \"default\").runForeach(println)\n// prints:\n// (1,a)\n// (2,b)\n// (3,c)\n// (4,default) Java copysource Source<Integer, NotUsed> numbers = Source.from(Arrays.asList(1, 2, 3, 4));\nSource<String, NotUsed> letters = Source.from(Arrays.asList(\"a\", \"b\", \"c\"));\n\nnumbers.zipAll(letters, -1, \"default\").runForeach(System.out::println, system);\n// prints:\n// Pair(1,a)\n// Pair(2,b)\n// Pair(3,c)\n// Pair(4,default)","title":"Example"},{"location":"/stream/operators/Source-or-Flow/zipAll.html#reactive-streams-semantics","text":"emits at first emits when both inputs emit, and then as long as any input emits (coupled to the default value of the completed input) backpressures both upstreams when downstream backpressures but also on an upstream that has emitted an element until the other upstream has emitted an element completes when both upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/zipLatest.html","text":"","title":"zipLatest"},{"location":"/stream/operators/Source-or-Flow/zipLatest.html#ziplatest","text":"Combines elements from each of multiple sources into tuples Pair and passes the tuples pairs downstream, picking always the latest element of each.\nFan-in operators","title":"zipLatest"},{"location":"/stream/operators/Source-or-Flow/zipLatest.html#signature","text":"Source.zipLatestSource.zipLatest Flow.zipLatestFlow.zipLatest","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/zipLatest.html#description","text":"Combines elements from each of multiple sources into tuples Pair and passes the tuples pairs downstream, picking always the latest element of each.\nNo element is emitted until at least one element from each Source becomes available.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/zipLatest.html#reactive-streams-semantics","text":"emits when all of the inputs have at least an element available, and then each time an element becomes available on either of the inputs backpressures when downstream backpressures completes when any upstream completes cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/zipLatestWith.html","text":"","title":"zipLatestWith"},{"location":"/stream/operators/Source-or-Flow/zipLatestWith.html#ziplatestwith","text":"Combines elements from multiple sources through a combine function and passes the returned value downstream, picking always the latest element of each.\nFan-in operators","title":"zipLatestWith"},{"location":"/stream/operators/Source-or-Flow/zipLatestWith.html#signature","text":"Source.zipLatestWithSource.zipLatestWith Flow.zipLatestWithFlow.zipLatestWith","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/zipLatestWith.html#description","text":"Combines elements from each of multiple sources into tuples Pair and passes the tuples pairs downstream, picking always the latest element of each.\nNo element is emitted until at least one element from each Source becomes available. Whenever a new element appears, the zipping function is invoked with a tuple containing the new element and the last seen element of the other stream.","title":"Description"},{"location":"/stream/operators/Source-or-Flow/zipLatestWith.html#reactive-streams-semantics","text":"emits all of the inputs have at least an element available, and then each time an element becomes available on either of the inputs backpressures when downstream backpressures completes when any upstream completes cancels when downstream cancels","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/zipN.html","text":"","title":"Source.zipN"},{"location":"/stream/operators/Source/zipN.html#source-zipn","text":"Combine the elements of multiple sources into a source of sequences of value.\nSource operators","title":"Source.zipN"},{"location":"/stream/operators/Source/zipN.html#signature","text":"Source.zipNSource.zipN","title":"Signature"},{"location":"/stream/operators/Source/zipN.html#description","text":"Collects one element for every upstream and when all upstreams has emitted one element all of them are emitted downstream as a collection. The element order in the downstream collection will be the same order as the sources were listed.\nSince the sources are provided as a list the individual types are lost and the downstream sequences will end up containing the closest supertype shared by all sourcesyou may have to make sure to have sources type casted to the same common supertype of all stream elements to use zipN.\nSee also:\nzipWithN zip zipAll zipWith zipWithIndex","title":"Description"},{"location":"/stream/operators/Source/zipN.html#example","text":"In this sample we zip a stream of characters, a stream of numbers and a stream of colours. Into a single Source where each element is a VectorList of [character, number, color]:\nScala copysourceval chars = Source(\"a\" :: \"b\" :: \"c\" :: \"e\" :: \"f\" :: Nil)\nval numbers = Source(1 :: 2 :: 3 :: 4 :: 5 :: 6 :: Nil)\nval colors = Source(\"red\" :: \"green\" :: \"blue\" :: \"yellow\" :: \"purple\" :: Nil)\n\nSource.zipN(chars :: numbers :: colors :: Nil).runForeach(println)\n// prints:\n// Vector(a, 1, red)\n// Vector(b, 2, green)\n// Vector(c, 3, blue)\n// Vector(e, 4, yellow)\n// Vector(f, 5, purple)\n Java copysourceSource<Object, NotUsed> chars = Source.from(Arrays.asList(\"a\", \"b\", \"c\", \"e\", \"f\"));\nSource<Object, NotUsed> numbers = Source.from(Arrays.asList(1, 2, 3, 4, 5, 6));\nSource<Object, NotUsed> colors =\n    Source.from(Arrays.asList(\"red\", \"green\", \"blue\", \"yellow\", \"purple\"));\n\nSource.zipN(Arrays.asList(chars, numbers, colors)).runForeach(System.out::println, system);\n// prints:\n// [a, 1, red]\n// [b, 2, green]\n// [c, 3, blue]\n// [e, 4, yellow]\n// [f, 5, purple]\nNote how it stops as soon as any of the original sources reaches its end.","title":"Example"},{"location":"/stream/operators/Source/zipN.html#reactive-streams-semantics","text":"emits when all of the inputs has an element available completes when any upstream completes backpressures all upstreams when downstream backpressures but also on an upstream that has emitted an element until all other upstreams has emitted elements","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/zipWith.html","text":"","title":"zipWith"},{"location":"/stream/operators/Source-or-Flow/zipWith.html#zipwith","text":"Combines elements from multiple sources through a combine function and passes the returned value downstream.\nFan-in operators","title":"zipWith"},{"location":"/stream/operators/Source-or-Flow/zipWith.html#signature","text":"Source.zipWithSource.zipWith Flow.zipWithFlow.zipWith","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/zipWith.html#description","text":"Combines elements from multiple sources through a combine function and passes the returned value downstream.\nSee also:\nzip zipAll zipWithIndex","title":"Description"},{"location":"/stream/operators/Source-or-Flow/zipWith.html#examples","text":"Scala copysourceimport org.apache.pekko\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.Sink\n\n    val sourceCount = Source(List(\"one\", \"two\", \"three\"))\n    val sourceFruits = Source(List(\"apple\", \"orange\", \"banana\"))\n\n    sourceCount\n      .zipWith(sourceFruits) { (countStr, fruitName) =>\n        s\"$countStr $fruitName\"\n      }\n      .runWith(Sink.foreach(println))\n    // this will print 'one apple', 'two orange', 'three banana' Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource<String, NotUsed> sourceCount = Source.from(Arrays.asList(\"one\", \"two\", \"three\"));\nSource<String, NotUsed> sourceFruits = Source.from(Arrays.asList(\"apple\", \"orange\", \"banana\"));\nsourceCount\n    .zipWith(\n        sourceFruits,\n        (Function2<String, String, String>) (countStr, fruitName) -> countStr + \" \" + fruitName)\n    .runForeach(System.out::println, system);\n// this will print 'one apple', 'two orange', 'three banana'","title":"Examples"},{"location":"/stream/operators/Source-or-Flow/zipWith.html#reactive-streams-semantics","text":"emits when all of the inputs have an element available backpressures both upstreams when downstream backpressures but also on an upstream that has emitted an element until the other upstream has emitted an element completes when any upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source-or-Flow/zipWithIndex.html","text":"","title":"zipWithIndex"},{"location":"/stream/operators/Source-or-Flow/zipWithIndex.html#zipwithindex","text":"Zips elements of current flow with its indices.\nFan-in operators","title":"zipWithIndex"},{"location":"/stream/operators/Source-or-Flow/zipWithIndex.html#signature","text":"Source.zipWithIndexSource.zipWithIndex Flow.zipWithIndexFlow.zipWithIndex","title":"Signature"},{"location":"/stream/operators/Source-or-Flow/zipWithIndex.html#description","text":"Zips elements of current flow with its indices.\nSee also:\nzip zipAll zipWith","title":"Description"},{"location":"/stream/operators/Source-or-Flow/zipWithIndex.html#example","text":"Scala copysourceimport org.apache.pekko\nimport pekko.stream.scaladsl.Source\nimport pekko.stream.scaladsl.Sink\n\n    Source(List(\"apple\", \"orange\", \"banana\")).zipWithIndex.runWith(Sink.foreach(println))\n    // this will print ('apple', 0), ('orange', 1), ('banana', 2) Java copysourceimport org.apache.pekko.stream.javadsl.Keep;\nimport org.apache.pekko.stream.javadsl.Source;\nimport org.apache.pekko.stream.javadsl.Sink;\n\nimport java.util.*;\n\nSource.from(Arrays.asList(\"apple\", \"orange\", \"banana\"))\n    .zipWithIndex()\n    .runForeach(System.out::println, system);\n// this will print ('apple', 0), ('orange', 1), ('banana', 2)","title":"Example"},{"location":"/stream/operators/Source-or-Flow/zipWithIndex.html#reactive-streams-semantics","text":"emits upstream emits an element and is paired with their index backpressures when downstream backpressures completes when upstream completes","title":"Reactive Streams semantics"},{"location":"/stream/operators/Source/zipWithN.html","text":"","title":"Source.zipWithN"},{"location":"/stream/operators/Source/zipWithN.html#source-zipwithn","text":"Combine the elements of multiple streams into a stream of sequences using a combiner function.\nSource operators","title":"Source.zipWithN"},{"location":"/stream/operators/Source/zipWithN.html#signature","text":"Source.zipWithNSource.zipWithN","title":"Signature"},{"location":"/stream/operators/Source/zipWithN.html#description","text":"Combine the elements of multiple streams into a stream of sequences using a combiner function.\nThis operator is essentially the same as using zipN followed by map to turn the zipped sequence into an arbitrary object to emit downstream.\nSee also:\nzipN zip zipAll zipWith zipWithIndex","title":"Description"},{"location":"/stream/operators/Source/zipWithN.html#example","text":"In this sample we zip three streams of integers and for each zipped sequence of numbers we calculate the max value and send downstream:\nScala copysourceval numbers = Source(1 :: 2 :: 3 :: 4 :: 5 :: 6 :: Nil)\nval otherNumbers = Source(5 :: 2 :: 1 :: 4 :: 10 :: 4 :: Nil)\nval andSomeOtherNumbers = Source(3 :: 7 :: 2 :: 1 :: 1 :: Nil)\n\nSource\n  .zipWithN((seq: Seq[Int]) => seq.max)(numbers :: otherNumbers :: andSomeOtherNumbers :: Nil)\n  .runForeach(println)\n// prints:\n// 5\n// 7\n// 3\n// 4\n// 10\n Java copysourceSource<Integer, NotUsed> numbers = Source.from(Arrays.asList(1, 2, 3, 4, 5, 6));\nSource<Integer, NotUsed> otherNumbers = Source.from(Arrays.asList(5, 2, 1, 4, 10, 4));\nSource<Integer, NotUsed> andSomeOtherNumbers = Source.from(Arrays.asList(3, 7, 2, 1, 1));\n\nSource.zipWithN(\n        (List<Integer> seq) -> seq.stream().mapToInt(i -> i).max().getAsInt(),\n        Arrays.asList(numbers, otherNumbers, andSomeOtherNumbers))\n    .runForeach(System.out::println, system);\n// prints:\n// 5\n// 7\n// 3\n// 4\n// 10\nNote how it stops as soon as any of the original sources reaches its end.","title":"Example"},{"location":"/stream/operators/Source/zipWithN.html#reactive-streams-semantics","text":"emits when all of the inputs has an element available completes when any upstream completes backpressures all upstreams when downstream backpressures but also on an upstream that has emitted an element until all other upstreams has emitted elements","title":"Reactive Streams semantics"},{"location":"/discovery/index.html","text":"","title":"Discovery"},{"location":"/discovery/index.html#discovery","text":"The Pekko Discovery API enables service discovery to be provided by different technologies. It allows to delegate endpoint lookup so that services can be configured depending on the environment by other means than configuration files.\nImplementations provided by the Pekko Discovery module are\nConfiguration (HOCON) DNS (SRV records) Aggregate multiple discovery methods\nIn addition, the Pekko Management toolbox contains Pekko Discovery implementations for\nKubernetes API AWS API: EC2 Tag-Based Discovery AWS API: ECS Discovery Consul Marathon API","title":"Discovery"},{"location":"/discovery/index.html#module-info","text":"sbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-discovery\" % PekkoVersion Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-discovery_${versions.ScalaBinary}\"\n} Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-discovery_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies>\nProject Info: Pekko Discovery Artifact org.apache.pekko pekko-discovery 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.discovery License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/discovery/index.html#how-it-works","text":"Loading the extension:\nScala copysourceimport org.apache.pekko.discovery.Discovery\n\nval system = ActorSystem()\nval serviceDiscovery = Discovery(system).discovery Java copysourceActorSystem as = ActorSystem.create();\nServiceDiscovery serviceDiscovery = Discovery.get(as).discovery();\nA Lookup contains a mandatory serviceName and an optional portName and protocol. How these are interpreted is discovery method dependent e.g.DNS does an A/AAAA record query if any of the fields are missing and an SRV query for a full look up:\nScala copysourceimport org.apache.pekko.discovery.Lookup\n\nserviceDiscovery.lookup(Lookup(\"pekko.io\"), 1.second)\n// Convenience for a Lookup with only a serviceName\nserviceDiscovery.lookup(\"pekko.io\", 1.second) Java copysourceserviceDiscovery.lookup(Lookup.create(\"pekko.io\"), Duration.ofSeconds(1));\n// convenience for a Lookup with only a serviceName\nserviceDiscovery.lookup(\"pekko.io\", Duration.ofSeconds(1));\nportName and protocol are optional and their meaning is interpreted by the method.\nScala copysourceimport org.apache.pekko\nimport pekko.discovery.Lookup\nimport pekko.discovery.ServiceDiscovery.Resolved\n\nval lookup: Future[Resolved] =\n  serviceDiscovery.lookup(Lookup(\"pekko.io\").withPortName(\"remoting\").withProtocol(\"tcp\"), 1.second) Java copysourceCompletionStage<ServiceDiscovery.Resolved> lookup =\n    serviceDiscovery.lookup(\n        Lookup.create(\"pekko.io\").withPortName(\"remoting\").withProtocol(\"tcp\"),\n        Duration.ofSeconds(1));\nPort can be used when a service opens multiple ports e.g. a HTTP port and a Pekko remoting port.","title":"How it works"},{"location":"/discovery/index.html#discovery-method-dns","text":"Async DNS Pekko Discovery with DNS does always use the Pekko-native “async-dns” implementation (it is independent of the pekko.io.dns.resolver setting).\nDNS discovery maps Lookup queries as follows:\nserviceName, portName and protocol set: SRV query in the form: _port._protocol.name Where the _s are added. Any query missing any of the fields is mapped to a A/AAAA query for the serviceName\nThe mapping between Pekko service discovery terminology and SRV terminology:\nSRV service = port SRV name = serviceName SRV protocol = protocol\nConfigure pekko-dns to be used as the discovery implementation in your application.conf:\ncopysourcepekko {\n  discovery {\n    method = pekko-dns\n  }\n}\nFrom there on, you can use the generic API that hides the fact which discovery method is being used by calling:\nScala copysourceimport org.apache.pekko\nimport pekko.discovery.Discovery\nimport pekko.discovery.ServiceDiscovery\n\nval discovery: ServiceDiscovery = Discovery(system).discovery\n// ...\nval result: Future[ServiceDiscovery.Resolved] = discovery.lookup(\"pekko.io\", resolveTimeout = 3.seconds) Java copysourceimport org.apache.pekko.discovery.Discovery;\nimport org.apache.pekko.discovery.ServiceDiscovery;\n\nServiceDiscovery discovery = Discovery.get(system).discovery();\n// ...\nCompletionStage<ServiceDiscovery.Resolved> result =\n    discovery.lookup(\"foo\", Duration.ofSeconds(3));","title":"Discovery Method: DNS"},{"location":"/discovery/index.html#dns-records-used","text":"DNS discovery will use either A/AAAA records or SRV records depending on whether a Simple or Full lookup is issued. The advantage of SRV records is that they can include a port.","title":"DNS records used"},{"location":"/discovery/index.html#srv-records","text":"Lookups with all the fields set become SRV queries. For example:\ndig srv _service._tcp.pekko.test\n\n; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> srv service.tcp.pekko.test\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60023\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 5\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n; COOKIE: 5ab8dd4622e632f6190f54de5b28bb8fb1b930a5333c3862 (good)\n;; QUESTION SECTION:\n;service.tcp.pekko.test.         IN      SRV\n\n;; ANSWER SECTION:\n_service._tcp.pekko.test.  86400   IN      SRV     10 60 5060 a-single.pekko.test.\n_service._tcp.pekko.test.  86400   IN      SRV     10 40 5070 a-double.pekko.test.\nIn this case service.tcp.pekko.test resolves to a-single.pekko.test on port 5060 and a-double.pekko.test on port 5070. Currently discovery does not support the weightings.","title":"SRV records"},{"location":"/discovery/index.html#a-aaaa-records","text":"Lookups with any fields missing become A/AAAA record queries. For example:\ndig a-double.pekko.test\n\n; <<>> DiG 9.11.3-RedHat-9.11.3-6.fc28 <<>> a-double.pekko.test\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11983\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n; COOKIE: 16e9815d9ca2514d2f3879265b28bad05ff7b4a82721edd0 (good)\n;; QUESTION SECTION:\n;a-double.pekko.test.            IN      A\n\n;; ANSWER SECTION:\na-double.pekko.test.     86400   IN      A       192.168.1.21\na-double.pekko.test.     86400   IN      A       192.168.1.22\nIn this case a-double.pekko.test would resolve to 192.168.1.21 and 192.168.1.22.","title":"A/AAAA records"},{"location":"/discovery/index.html#discovery-method-configuration","text":"Configuration currently ignores all fields apart from service name.\nFor simple use cases configuration can be used for service discovery. The advantage of using Pekko Discovery with configuration rather than your own configuration values is that applications can be migrated to a more sophisticated discovery method without any code changes.\nConfigure it to be used as discovery method in your application.conf\npekko {\n  discovery.method = config\n}\nBy default the services discoverable are defined in pekko.discovery.config.services and have the following format:\npekko.discovery.config.services = {\n  service1 = {\n    endpoints = [\n      {\n        host = \"cat\"\n        port = 1233\n      },\n      {\n        host = \"dog\"\n        port = 1234\n      }\n    ]\n  },\n  service2 = {\n    endpoints = []\n  }\n}\nWhere the above block defines two services, service1 and service2. Each service can have multiple endpoints.","title":"Discovery Method: Configuration"},{"location":"/discovery/index.html#discovery-method-aggregate-multiple-discovery-methods","text":"Aggregate discovery allows multiple discovery methods to be aggregated e.g. try and resolve via DNS and fall back to configuration.\nTo use aggregate discovery add its dependency as well as all of the discovery that you want to aggregate.\nConfigure aggregate as pekko.discovery.method and which discovery methods are tried and in which order.\npekko {\n  discovery {\n    method = aggregate\n    aggregate {\n      discovery-methods = [\"pekko-dns\", \"config\"]\n    }\n    config {\n      services {\n        service1 {\n          endpoints = [\n            {\n              host = \"host1\"\n              port = 1233\n            },\n            {\n              host = \"host2\"\n              port = 1234\n            }\n          ]\n        }\n      }\n    }\n  }\n}\nThe above configuration will result in pekko-dns first being checked and if it fails or returns no targets for the given service name then config is queried which i configured with one service called service1 which two hosts host1 and host2.","title":"Discovery Method: Aggregate multiple discovery methods"},{"location":"/discovery/index.html#migrating-from-pekko-management-discovery-before-1-0-0-","text":"Pekko Discovery started out as a submodule of Pekko Management, before 1.0.0 of Pekko Management. Pekko Discovery is not compatible with those versions of Pekko Management Discovery.\nAt least version 1.0.0 of any Pekko Management module should be used if also using Pekko Discovery.\nMigration steps:\nAny custom discovery method should now implement org.apache.pekko.discovery.ServiceDiscovery discovery-method now has to be a configuration location under pekko.discovery with at minimum a property class specifying the fully qualified name of the implementation of org.apache.pekko.discovery.ServiceDiscovery. Previous versions allowed this to be a class name or a fully qualified config location e.g. pekko.discovery.kubernetes-api rather than just kubernetes-api","title":"Migrating from Pekko Management Discovery (before 1.0.0)"},{"location":"/index-utilities.html","text":"","title":"Utilities"},{"location":"/index-utilities.html#utilities","text":"Logging Dependency Introduction How to log MDC SLF4J backend Internal logging by Pekko Logging in tests Circuit Breaker Why are they used? What do they do? Examples Futures patterns Dependency After Retry Extending Apache Pekko Building an extension Loading from configuration","title":"Utilities"},{"location":"/typed/logging.html","text":"","title":"Logging"},{"location":"/typed/logging.html#logging","text":"You are viewing the documentation for the new actor APIs, to view the Pekko Classic documentation, see Classic Logging.","title":"Logging"},{"location":"/typed/logging.html#dependency","text":"To use Logging, you must at least use the Pekko actors dependency in your project, and configure logging via the SLF4J backend, such as Logback configuration.\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor-typed\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor-typed_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor-typed_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/typed/logging.html#introduction","text":"SLF4J is used for logging and Pekko provides access to an org.slf4j.Logger for a specific actor via the ActorContextActorContext. You may also retrieve a Logger with the ordinary org.slf4j.LoggerFactory.\nTo ensure that logging has minimal performance impact it’s important that you configure an asynchronous appender for the SLF4J backend. Logging generally means IO and locks, which can slow down the operations of your code if it was performed synchronously.","title":"Introduction"},{"location":"/typed/logging.html#how-to-log","text":"The ActorContextActorContext provides access to an org.slf4j.Logger for a specific actor.\nScala copysourceBehaviors.receive[String] { (context, message) =>\n  context.log.info(\"Received message: {}\", message)\n  Behaviors.same\n} Java copysourcepublic class MyLoggingBehavior extends AbstractBehavior<String> {\n\n  public static Behavior<String> create() {\n    return Behaviors.setup(MyLoggingBehavior::new);\n  }\n\n  private MyLoggingBehavior(ActorContext<String> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder().onMessage(String.class, this::onReceive).build();\n  }\n\n  private Behavior<String> onReceive(String message) {\n    getContext().getLog().info(\"Received message: {}\", message);\n    return this;\n  }\n}\nThe Logger via the ActorContext will automatically have a name that corresponds to the BehaviorBehavior of the actor when the log is accessed the first time. The class name when using AbstractBehaviorAbstractBehavior or the class or object name where the Behavior is defined when using the functional style. You can set a custom logger name with the setLoggerNamesetLoggerName of the ActorContext.\nScala copysourceBehaviors.setup[String] { context =>\n  context.setLoggerName(\"com.myservice.BackendManager\")\n  context.log.info(\"Starting up\")\n\n  Behaviors.receiveMessage { message =>\n    context.log.debug(\"Received message: {}\", message)\n    Behaviors.same\n  }\n} Java copysourcepublic class BackendManager extends AbstractBehavior<String> {\n\n  public static Behavior<String> create() {\n    return Behaviors.setup(\n        context -> {\n          context.setLoggerName(BackendManager.class);\n          context.getLog().info(\"Starting up\");\n          return new BackendManager(context);\n        });\n  }\n\n  private BackendManager(ActorContext<String> context) {\n    super(context);\n  }\n\n  @Override\n  public Receive<String> createReceive() {\n    return newReceiveBuilder().onMessage(String.class, this::onReceive).build();\n  }\n\n  private Behavior<String> onReceive(String message) {\n    getContext().getLog().debug(\"Received message: {}\", message);\n    return this;\n  }\n}\nThe convention is to use logger names like fully qualified class names. The parameter to setLoggerName can be a String or a Class, where the latter is convenience for the class name.\nWhen logging via the ActorContext the path of the actor will automatically be added as pekkoSource Mapped Diagnostic Context (MDC) value. MDC is typically implemented with a ThreadLocal by the SLF4J backend. To reduce performance impact, this MDC value is set when you access the loggetLog() method so you shouldn’t cache the returned Logger in your own field. That is handled by ActorContext and retrieving the Logger repeatedly with the loggetLog method has low overhead. The MDC is cleared automatically after processing of current message has finished.\nNote The Logger is thread-safe but the loggetLog method in ActorContext is not thread-safe and should not be accessed from threads other than the ordinary actor message processing thread, such as FutureCompletionStage callbacks.\nIt’s also perfectly fine to use a Logger retrieved via org.slf4j.LoggerFactory, but then the logging events will not include the pekkoSource MDC value. This is the recommended way when logging outside of an actor, including logging from FutureCompletionStage callbacks.\nScala copysourceval log = LoggerFactory.getLogger(\"com.myservice.BackendTask\")\n\nFuture {\n  // some work\n  \"result\"\n}.onComplete {\n  case Success(result) => log.info(\"Task completed: {}\", result)\n  case Failure(exc)    => log.error(\"Task failed\", exc)\n} Java copysourceclass BackendTask {\n  private final Logger log = LoggerFactory.getLogger(getClass());\n\n  void run() {\n    CompletableFuture<String> task =\n        CompletableFuture.supplyAsync(\n            () -> {\n              // some work\n              return \"result\";\n            });\n    task.whenComplete(\n        (result, exc) -> {\n          if (exc == null) log.error(\"Task failed\", exc);\n          else log.info(\"Task completed: {}\", result);\n        });\n  }\n}","title":"How to log"},{"location":"/typed/logging.html#placeholder-arguments","text":"The log message may contain argument placeholders {}, which will be substituted if the log level is enabled. Compared to constructing a full string for the log message this has the advantage of avoiding superfluous string concatenation and object allocations when the log level is disabled. Some logging backends may also use these message templates before argument substitution to group and filter logging events.\nIt can be good to know that 3 or more arguments will result in the relatively small cost of allocating an array (vararg parameter) also when the log level is disabled. The methods with 1 or 2 arguments don’t allocate the vararg array.\nWhen using the methods for 2 argument placeholders the compiler will often not be able to select the right method and report compiler error “ambiguous reference to overloaded definition”. To work around this problem you can use the trace2, debug2, info2, warn2 or error2 extension methods that are added by import org.apache.pekko.actor.typed.scaladsl.LoggerOps or import org.apache.pekko.actor.typed.scaladsl._. Scala copysourceimport org.apache.pekko.actor.typed.scaladsl.LoggerOps\n\nBehaviors.receive[String] { (context, message) =>\n  context.log.info2(\"{} received message: {}\", context.self.path.name, message)\n  Behaviors.same\n} When using the methods for 3 or more argument placeholders, the compiler will not be able to convert the method parameters to the vararg array when they contain primitive values such as Int, and report compiler error “overloaded method value info with alternatives”. To work around this problem you can use the traceN, debugN, infoN, warnN or errorN extension methods that are added by the same LoggerOps import. Scala copysourceimport org.apache.pekko.actor.typed.scaladsl.LoggerOps\n\nBehaviors.receive[String] { (context, message) =>\n  context.log.infoN(\n    \"{} received message of size {} starting with: {}\",\n    context.self.path.name,\n    message.length,\n    message.take(10))\n  Behaviors.same\n} If you find it tedious to add the import of LoggerOps at many places you can make those additional methods available with a single implicit conversion placed in a root package object of your code: Scala copysourceimport scala.language.implicitConversions\nimport org.apache.pekko.actor.typed.scaladsl.LoggerOps\nimport org.slf4j.Logger\n\npackage object myapp {\n\n  implicit def loggerOps(logger: Logger): LoggerOps =\n    LoggerOps(logger)\n\n}","title":"Placeholder arguments"},{"location":"/typed/logging.html#behaviors-logmessages","text":"If you want very detailed logging of messages and signals you can decorate a BehaviorBehavior with Behaviors.logMessagesBehaviors.logMessages.\nScala copysourceimport org.apache.pekko.actor.typed.LogOptions\nimport org.slf4j.event.Level\n\nBehaviors.logMessages(LogOptions().withLevel(Level.TRACE), BackendManager()) Java copysourceimport org.slf4j.event.Level;\n\nBehaviors.logMessages(LogOptions.create().withLevel(Level.TRACE), BackendManager.create());","title":"Behaviors.logMessages"},{"location":"/typed/logging.html#mdc","text":"MDC allows for adding additional context dependent attributes to log entries. Out of the box, Pekko will place the path of the actor in the the MDC attribute pekkoSource.\nOne or more tags can also be added to the MDC using the ActorTagsActorTags props. The tags will be rendered as a comma separated list and be put in the MDC attribute pekkoTags. This can be used to categorize log entries from a set of different actors to allow easier filtering of logs:\nScala copysourcecontext.spawn(myBehavior, \"MyActor\", ActorTags(\"processing\")) Java copysourcecontext.spawn(myBehavior, \"MyActor\", ActorTags.create(\"processing\"));\nIn addition to these two built in MDC attributes you can also decorate a BehaviorBehavior with Behaviors.withMdcBehaviors.withMdc or use the org.slf4j.MDC API directly.\nThe Behaviors.withMdc decorator has two parts. A static Map of MDC attributes that are not changed, and a dynamic Map that can be constructed for each message.\nScala copysourceval staticMdc = Map(\"startTime\" -> system.startTime.toString)\nBehaviors.withMdc[BackendManager.Command](\n  staticMdc,\n  mdcForMessage =\n    (msg: BackendManager.Command) => Map(\"identifier\" -> msg.identifier, \"upTime\" -> system.uptime.toString)) {\n  BackendManager()\n} Java copysourceMap<String, String> staticMdc = new HashMap<>();\nstaticMdc.put(\"startTime\", String.valueOf(system.startTime()));\n\nBehaviors.withMdc(\n    BackendManager2.Command.class,\n    staticMdc,\n    message -> {\n      Map<String, String> msgMdc = new HashMap<>();\n      msgMdc.put(\"identifier\", message.identifier());\n      msgMdc.put(\"upTime\", String.valueOf(system.uptime()));\n      return msgMdc;\n    },\n    BackendManager2.create());\nIf you use the MDC API directly, be aware that MDC is typically implemented with a ThreadLocal by the SLF4J backend. Pekko clears the MDC if logging is performed via the loggetLog() of the ActorContext and it is cleared automatically after processing of current message has finished, but only if you accessed loggetLog(). The entire MDC is cleared, including attributes that you add yourself to the MDC. MDC is not cleared automatically if you use a Logger via LoggerFactory or not touch loggetLog() in the ActorContext.","title":"MDC"},{"location":"/typed/logging.html#slf4j-backend","text":"To ensure that logging has minimal performance impact it’s important that you configure an asynchronous appender for the SLF4J backend. Logging generally means IO and locks, which can slow down the operations of your code if it was performed synchronously.\nWarning For production the SLF4J backend should be configured with an asynchronous appender as described here. Otherwise there is a risk of reduced performance and thread starvation problems of the dispatchers that are running actors and other tasks.","title":"SLF4J backend"},{"location":"/typed/logging.html#logback","text":"pekko-actor-typed includes a dependency to the slf4j-api. In your runtime, you also need a SLF4J backend. We recommend Logback:\nsbt libraryDependencies += \"ch.qos.logback\" % \"logback-classic\" % \"1.2.11\" Maven <dependencies>\n  <dependency>\n    <groupId>ch.qos.logback</groupId>\n    <artifactId>logback-classic</artifactId>\n    <version>1.2.11</version>\n  </dependency>\n</dependencies> Gradle dependencies {\n  implementation \"ch.qos.logback:logback-classic:1.2.11\"\n}\nLogback has flexible configuration options and details can be found in the Logback manual and other external resources.\nOne part that is important to highlight is the importance of configuring an AsyncAppender, because it offloads rendering of logging events to a background thread, increasing performance. It doesn’t block the threads of the ActorSystemActorSystem while the underlying infrastructure writes the log messages to disk or other configured destination. It also contains a feature which will drop INFO and DEBUG messages if the logging load is high.\nA starting point for configuration of logback.xml for production:\ncopysource<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n\n    <appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <file>myapp.log</file>\n        <immediateFlush>false</immediateFlush>\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n            <fileNamePattern>myapp_%d{yyyy-MM-dd}.log</fileNamePattern>\n        </rollingPolicy>\n        <encoder>\n            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n        </encoder>\n    </appender>\n\n    <appender name=\"ASYNC\" class=\"ch.qos.logback.classic.AsyncAppender\">\n        <queueSize>8192</queueSize>\n        <neverBlock>true</neverBlock>\n        <appender-ref ref=\"FILE\" />\n    </appender>\n\n    <root level=\"INFO\">\n        <appender-ref ref=\"ASYNC\"/>\n    </root>\n</configuration>\nNote that the AsyncAppender may drop log events if the queue becomes full, which may happen if the logging backend can’t keep up with the throughput of produced log events. Dropping log events is necessary if you want to gracefully degrade your application if only your logging backend or filesystem is experiencing issues.\nAn alternative of the Logback AsyncAppender with better performance is the Logstash async appender.\nThe ELK-stack is commonly used as logging infrastructure for production:\nLogstash Logback encoder Logstash Elasticsearch Kibana\nFor development you might want to log to standard out, but also have all debug level logging to file, like in this example:\ncopysource<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n\n    <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\">\n            <level>INFO</level>\n        </filter>\n        <encoder>\n            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n        </encoder>\n    </appender>\n\n    <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\">\n        <file>target/myapp-dev.log</file>\n        <encoder>\n            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n        </encoder>\n    </appender>\n\n    <root level=\"DEBUG\">\n        <appender-ref ref=\"STDOUT\"/>\n        <appender-ref ref=\"FILE\"/>\n    </root>\n</configuration>\nPlace the logback.xml file in src/main/resources/logback.xml. For tests you can define different logging configuration in src/test/resources/logback-test.xml.","title":"Logback"},{"location":"/typed/logging.html#mdc-values","text":"When logging via the loggetLog() of the ActorContext, as described in How to log, Pekko includes a few MDC properties:\npekkoSource: the actor’s path pekkoAddress: the full address of the ActorSystem, including hostname and port if Cluster is enabled pekkoTags: tags defined in the PropsProps of the actor sourceActorSystem: the name of the ActorSystem\nThese MDC properties can be included in the Logback output with for example %X{pekkoSource} specifier within the pattern layout configuration:\n<encoder>\n    <pattern>%date{ISO8601} %-5level %logger{36} %X{pekkoSource} - %msg%n</pattern>\n  </encoder>\nAll MDC properties as key-value entries can be included with %mdc:\n<encoder>\n    <pattern>%date{ISO8601} %-5level %logger{36} - %msg MDC: {%mdc}%n</pattern>\n  </encoder>","title":"MDC values"},{"location":"/typed/logging.html#internal-logging-by-pekko","text":"","title":"Internal logging by Pekko"},{"location":"/typed/logging.html#event-bus","text":"For historical reasons logging by the Pekko internals and by classic actors are performed asynchronously through an event bus. Such log events are processed by an event handler actor, which then emits them to SLF4J or directly to standard out.\nWhen pekko-actor-typed and pekko-slf4j are on the classpath this event handler actor will emit the events to SLF4J. The event.slf4j.Slf4jLoggerevent.slf4j.Slf4jLogger and event.slf4j.Slf4jLoggingFilterevent.slf4j.Slf4jLoggingFilter are enabled automatically without additional configuration. This can be disabled by pekko.use-slf4j=off configuration property.\nIn other words, you don’t have to do anything for the Pekko internal logging to end up in your configured SLF4J backend.","title":"Event bus"},{"location":"/typed/logging.html#log-level","text":"Ultimately the log level defined in the SLF4J backend is used. For the Pekko internal logging it will also check the level defined by the SLF4J backend before constructing the final log message and emitting it to the event bus.\nHowever, there is an additional pekko.loglevel configuration property that defines if logging events with lower log level should be discarded immediately without consulting the SLF4J backend. By default this is at INFO level, which means that DEBUG level logging from the Pekko internals will not reach the SLF4J backend even if DEBUG is enabled in the backend.\nYou can enable DEBUG level for pekko.loglevel and control the actual level in the SLF4j backend without any significant overhead, also for production.\npekko.loglevel = \"DEBUG\"\nTo turn off all Pekko internal logging (not recommended) you can configure the log levels to be OFF like this.\npekko {\n  stdout-loglevel = \"OFF\"\n  loglevel = \"OFF\"\n}\nThe stdout-loglevel is only in effect during system startup and shutdown, and setting it to OFF as well, ensures that nothing gets logged during system startup or shutdown.\nSee Logger names for configuration of log level in SLF4J backend for certain modules of Pekko.","title":"Log level"},{"location":"/typed/logging.html#logging-to-stdout-during-startup-and-shutdown","text":"When the actor system is starting up and shutting down the configured loggers are not used. Instead log messages are printed to stdout (System.out). The default log level for this stdout logger is WARNING and it can be silenced completely by setting pekko.stdout-loglevel=OFF.","title":"Logging to stdout during startup and shutdown"},{"location":"/typed/logging.html#logging-of-dead-letters","text":"By default messages sent to dead letters are logged at info level. Existence of dead letters does not necessarily indicate a problem, but they are logged by default for the sake of caution. After a few messages this logging is turned off, to avoid flooding the logs. You can disable this logging completely or adjust how many dead letters are logged. During system shutdown it is likely that you see dead letters, since pending messages in the actor mailboxes are sent to dead letters. You can also disable logging of dead letters during shutdown.\npekko {\n  log-dead-letters = 10\n  log-dead-letters-during-shutdown = on\n}\nTo customize the logging further or take other actions for dead letters you can subscribe to the Event Stream.","title":"Logging of Dead Letters"},{"location":"/typed/logging.html#auxiliary-logging-options","text":"Pekko has a few configuration options for very low level debugging. These make more sense in development than in production.\nYou almost definitely need to have logging set to DEBUG to use any of the options below:\npekko {\n  loglevel = \"DEBUG\"\n}\nThis config option is very good if you want to know what config settings are loaded by Pekko:\npekko {\n  # Log the complete configuration at INFO level when the actor system is started.\n  # This is useful when you are uncertain of what configuration is used.\n  log-config-on-start = on\n}\nIf you want unhandled messages logged at DEBUG:\npekko {\n  actor {\n    debug {\n      # enable DEBUG logging of unhandled messages\n      unhandled = on\n    }\n  }\n}\nIf you want to monitor subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream:\npekko {\n  actor {\n    debug {\n      # enable DEBUG logging of subscription changes on the eventStream\n      event-stream = on\n    }\n  }\n}","title":"Auxiliary logging options"},{"location":"/typed/logging.html#auxiliary-remote-logging-options","text":"If you want to see all messages that are sent through remoting at DEBUG log level, use the following config option. Note that this logs the messages as they are sent by the transport layer, not by an actor.\npekko.remote.artery {\n  # If this is \"on\", Pekko will log all outbound messages at DEBUG level,\n  # if off then they are not logged\n  log-sent-messages = on\n}\nIf you want to see all messages that are received through remoting at DEBUG log level, use the following config option. Note that this logs the messages as they are received by the transport layer, not by an actor.\npekko.remote.artery {\n  # If this is \"on\", Pekko will log all inbound messages at DEBUG level,\n  # if off then they are not logged\n  log-received-messages = on\n}\nLogging of message types with payload size in bytes larger than the configured log-frame-size-exceeding.\npekko.remote.artery {\n  log-frame-size-exceeding = 10000b\n}","title":"Auxiliary remote logging options"},{"location":"/typed/logging.html#mdc-values-from-pekko-internal-logging","text":"Since the logging is done asynchronously, the thread in which the logging was performed is captured in MDC with attribute name sourceThread.\nThe path of the actor in which the logging was performed is available in the MDC with attribute name pekkoSource.\nThe actor system name in which the logging was performed is available in the MDC with attribute name sourceActorSystem, but that is typically also included in the pekkoSource attribute.\nThe address of the actor system, containing host and port if the system is using cluster, is available through pekkoAddress.\nFor typed actors the log event timestamp is taken when the log call was made but for Pekko’s internal logging as well as the classic actor logging is asynchronous which means that the timestamp of a log entry is taken from when the underlying logger implementation is called, which can be surprising at first. If you want to more accurately output the timestamp for such loggers, use the MDC attribute pekkoTimestamp. Note that the MDC key will not have any value for a typed actor.","title":"MDC values from Pekko internal logging"},{"location":"/typed/logging.html#markers","text":"Pekko is logging some events with markers. Some of these events also include structured MDC properties.\nThe “SECURITY” marker is used for highlighting security related events or incidents. Pekko Actor is using the markers defined in ActorLogMarkerActorLogMarker. Pekko Cluster is using the markers defined in ClusterLogMarkerClusterLogMarker. Pekko Remoting is using the markers defined in RemoteLogMarkerRemoteLogMarker. Pekko Cluster Sharding is using the markers defined in ShardingLogMarkerShardingLogMarker.\nMarkers and MDC properties are automatically picked up by the Logstash Logback encoder.\nThe marker can be included in the Logback output with %marker and all MDC properties as key-value entries with %mdc.\n<encoder>\n    <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n  </encoder>","title":"Markers"},{"location":"/typed/logging.html#logger-names","text":"It can be useful to enable debug level or other SLF4J backend configuration for certain modules of Pekko when troubleshooting. Those logger names are typically prefixed with the package name of the classes in that module. For example, in Logback the configuration may look like this to enable debug logging for Cluster Sharding:\n<logger name=\"pekko.cluster.sharding\" level=\"DEBUG\" />\n\n    <root level=\"INFO\">\n        <appender-ref ref=\"ASYNC\"/>\n    </root>\nOther examples of logger names or prefixes:\npekko.cluster\npekko.cluster.Cluster\npekko.cluster.ClusterHeartbeat\npekko.cluster.ClusterGossip\npekko.cluster.ddata\npekko.cluster.pubsub\npekko.cluster.singleton\npekko.cluster.sharding\npekko.coordination.lease\npekko.discovery\npekko.persistence\npekko.remote","title":"Logger names"},{"location":"/typed/logging.html#logging-in-tests","text":"Testing utilities are described in Testing.","title":"Logging in tests"},{"location":"/common/circuitbreaker.html","text":"","title":"Circuit Breaker"},{"location":"/common/circuitbreaker.html#circuit-breaker","text":"","title":"Circuit Breaker"},{"location":"/common/circuitbreaker.html#why-are-they-used-","text":"A circuit breaker is used to provide stability and prevent cascading failures in distributed systems. These should be used in conjunction with judicious timeouts at the interfaces between remote systems to prevent the failure of a single component from bringing down all components.\nAs an example, we have a web application interacting with a remote third party web service. Let’s say the third party has oversold their capacity and their database melts down under load. Assume that the database fails in such a way that it takes a very long time to hand back an error to the third party web service. This in turn makes calls fail after a long period of time. Back to our web application, the users have noticed that their form submissions take much longer seeming to hang. Well the users do what they know to do which is use the refresh button, adding more requests to their already running requests. This eventually causes the failure of the web application due to resource exhaustion. This will affect all users, even those who are not using functionality dependent on this third party web service.\nIntroducing circuit breakers on the web service call would cause the requests to begin to fail-fast, letting the user know that something is wrong and that they need not refresh their request. This also confines the failure behavior to only those users that are using functionality dependent on the third party, other users are no longer affected as there is no resource exhaustion. Circuit breakers can also allow savvy developers to mark portions of the site that use the functionality unavailable, or perhaps show some cached content as appropriate while the breaker is open.\nThe Apache Pekko library provides an implementation of a circuit breaker called CircuitBreakerCircuitBreaker which has the behavior described below.","title":"Why are they used?"},{"location":"/common/circuitbreaker.html#what-do-they-do-","text":"During normal operation, a circuit breaker is in the Closed state: Exceptions or calls exceeding the configured callTimeout increment a failure counter Successes reset the failure count to zero When the failure counter reaches a maxFailures count, the breaker is tripped into Open state While in Open state: All calls fail-fast with a CircuitBreakerOpenExceptionCircuitBreakerOpenException After the configured resetTimeout, the circuit breaker enters a Half-Open state In Half-Open state: The first call attempted is allowed through without failing fast All other calls fail-fast with an exception just as in Open state If the first call succeeds, the breaker is reset back to Closed state and the resetTimeout is reset If the first call fails, the breaker is tripped again into the Open state (as for exponential backoff circuit breaker, the resetTimeout is multiplied by the exponential backoff factor) State transition listeners: Callbacks can be provided for every state entry via onOpenonOpen, onCloseonClose, and onHalfOpenonHalfOpen These are executed in the ExecutionContext provided. Calls result listeners: Callbacks can be used eg. to collect statistics about all invocations or to react on specific call results like success, failures or timeouts. Supported callbacks are: onCallSuccessonCallSuccess, onCallFailureonCallFailure, onCallTimeoutonCallTimeout, onCallBreakerOpenonCallBreakerOpen. These are executed in the ExecutionContext provided.","title":"What do they do?"},{"location":"/common/circuitbreaker.html#examples","text":"","title":"Examples"},{"location":"/common/circuitbreaker.html#initialization","text":"Here’s how a CircuitBreakerCircuitBreaker would be configured for:\n5 maximum failures a call timeout of 10 seconds a reset timeout of 1 minute\nScala copysourceimport scala.concurrent.duration._\nimport org.apache.pekko\nimport pekko.pattern.CircuitBreaker\nimport pekko.pattern.pipe\nimport pekko.actor.{ Actor, ActorLogging, ActorRef }\n\nimport scala.concurrent.Future\nimport scala.util.{ Failure, Success, Try }\n\nclass DangerousActor extends Actor with ActorLogging {\n  import context.dispatcher\n\n  val breaker =\n    new CircuitBreaker(context.system.scheduler, maxFailures = 5, callTimeout = 10.seconds, resetTimeout = 1.minute)\n      .onOpen(notifyMeOnOpen())\n\n  def notifyMeOnOpen(): Unit =\n    log.warning(\"My CircuitBreaker is now open, and will not close for one minute\") Java copysource import org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.event.LoggingAdapter;\nimport java.time.Duration;\nimport org.apache.pekko.pattern.CircuitBreaker;\nimport org.apache.pekko.event.Logging;\n\nimport static org.apache.pekko.pattern.Patterns.pipe;\n\nimport java.util.concurrent.CompletableFuture;\n\npublic class DangerousJavaActor extends AbstractActor {\n\n  private final CircuitBreaker breaker;\n  private final LoggingAdapter log = Logging.getLogger(getContext().system(), this);\n\n  public DangerousJavaActor() {\n    this.breaker =\n        new CircuitBreaker(\n                getContext().getDispatcher(),\n                getContext().getSystem().getScheduler(),\n                5,\n                Duration.ofSeconds(10),\n                Duration.ofMinutes(1))\n            .addOnOpenListener(this::notifyMeOnOpen);\n  }\n\n  public void notifyMeOnOpen() {\n    log.warning(\"My CircuitBreaker is now open, and will not close for one minute\");\n  }","title":"Initialization"},{"location":"/common/circuitbreaker.html#future-synchronous-based-api","text":"Once a circuit breaker actor has been initialized, interacting with that actor is done by either using the Future based API or the synchronous API. Both of these APIs are considered Call Protection because whether synchronously or asynchronously, the purpose of the circuit breaker is to protect your system from cascading failures while making a call to another service. In the future based API, we use the withCircuitBreakercallWithCircuitBreakerCS which takes an asynchronous method (some method wrapped in a FutureCompletionState), for instance a call to retrieve data from a database, and we pipe the result back to the sender. If for some reason the database in this example isn’t responding, or there is another issue, the circuit breaker will open and stop trying to hit the database again and again until the timeout is over.\nThe Synchronous API would also wrap your call with the circuit breaker logic, however, it uses the withSyncCircuitBreakercallWithSyncCircuitBreaker and receives a method that is not wrapped in a FutureCompletionState.\nScala copysourcedef dangerousCall: String = \"This really isn't that dangerous of a call after all\"\n\ndef receive = {\n  case \"is my middle name\" =>\n    breaker.withCircuitBreaker(Future(dangerousCall)).pipeTo(sender())\n  case \"block for me\" =>\n    sender() ! breaker.withSyncCircuitBreaker(dangerousCall)\n} Java copysourcepublic String dangerousCall() {\n  return \"This really isn't that dangerous of a call after all\";\n}\n\n@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n      .match(\n          String.class,\n          \"is my middle name\"::equals,\n          m ->\n              pipe(\n                      breaker.callWithCircuitBreakerCS(\n                          () -> CompletableFuture.supplyAsync(this::dangerousCall)),\n                      getContext().getDispatcher())\n                  .to(sender()))\n      .match(\n          String.class,\n          \"block for me\"::equals,\n          m -> {\n            sender().tell(breaker.callWithSyncCircuitBreaker(this::dangerousCall), self());\n          })\n      .build();\n}\nNote Using the CircuitBreakerCircuitBreaker’s companion object applyCircuitBreaker.create method will return a CircuitBreakerCircuitBreaker where callbacks are executed in the caller’s thread. This can be useful if the asynchronous FutureCompletionState behavior is unnecessary, for example invoking a synchronous-only API.","title":"Future & Synchronous based API"},{"location":"/common/circuitbreaker.html#control-failure-count-explicitly","text":"By default, the circuit breaker treats Exception as failure in synchronized API, or failed FutureCompletionState as failure in future based API. On failure, the failure count will increment. If the failure count reaches the maxFailures, the circuit breaker will be opened. However, some applications may require certain exceptions to not increase the failure count. In other cases one may want to increase the failure count even if the call succeeded. Pekko circuit breaker provides a way to achieve such use cases: withCircuitBreaker and withSyncCircuitBreakercallWithCircuitBreaker, callWithSyncCircuitBreaker and callWithCircuitBreakerCS.\nAll methods above accept an argument defineFailureFn\nType of defineFailureFn: Try[T] => BooleanBiFunction[Optional[T], Optional[Throwable], Boolean]\nThis is a function which takes in a Try[T] and returns a Boolean. The Try[T] correspond to the Future[T] of the protected call. The response of a protected call is modelled using Optional[T] for a successful return value and Optional[Throwable] for exceptions. This function should return true if the call should increase failure count, else false.\nScala copysourcedef luckyNumber(): Future[Int] = {\n  val evenNumberAsFailure: Try[Int] => Boolean = {\n    case Success(n) => n % 2 == 0\n    case Failure(_) => true\n  }\n\n  val breaker =\n    new CircuitBreaker(context.system.scheduler, maxFailures = 5, callTimeout = 10.seconds, resetTimeout = 1.minute)\n\n  // this call will return 8888 and increase failure count at the same time\n  breaker.withCircuitBreaker(Future(8888), evenNumberAsFailure)\n} Java copysourceprivate final CircuitBreaker breaker;\n\npublic EvenNoFailureJavaExample() {\n  this.breaker =\n      new CircuitBreaker(\n          getContext().getDispatcher(),\n          getContext().getSystem().getScheduler(),\n          5,\n          Duration.ofSeconds(10),\n          Duration.ofMinutes(1));\n}\n\npublic int luckyNumber() {\n  BiFunction<Optional<Integer>, Optional<Throwable>, Boolean> evenNoAsFailure =\n      (result, err) -> (result.isPresent() && result.get() % 2 == 0);\n\n  // this will return 8888 and increase failure count at the same time\n  return this.breaker.callWithSyncCircuitBreaker(() -> 8888, evenNoAsFailure);\n}","title":"Control failure count explicitly"},{"location":"/common/circuitbreaker.html#low-level-api","text":"The low-level API allows you to describe the behavior of the CircuitBreakerCircuitBreaker in detail, including deciding what to return to the calling ActorActor in case of success or failure. This is especially useful when expecting the remote call to send a reply. CircuitBreakerCircuitBreaker doesn’t support Tell Protection (protecting against calls that expect a reply) natively at the moment. Thus you need to use the low-level power-user APIs, succeedsucceed and failfail methods, as well as isClosedisClosed, isOpenisOpen, isHalfOpenisHalfOpen to implement it.\nAs can be seen in the examples below, a Tell Protection pattern could be implemented by using the succeedsucceed and failfail methods, which would count towards the CircuitBreakerCircuitBreaker counts. In the example, a call is made to the remote service if the breaker.isClosedbreaker.isClosed. Once a response is received, the succeedsucceed method is invoked, which tells the CircuitBreakerCircuitBreaker to keep the breaker closed. On the other hand, if an error or timeout is received we trigger a failfail, and the breaker accrues this failure towards its count for opening the breaker.\nNote The below example doesn’t make a remote call when the state is HalfOpen. Using the power-user APIs, it is your responsibility to judge when to make remote calls in HalfOpen.\nScala copysourceimport org.apache.pekko.actor.ReceiveTimeout\n\ndef receive = {\n  case \"call\" if breaker.isClosed => {\n    recipient ! \"message\"\n  }\n  case \"response\" => {\n    breaker.succeed()\n  }\n  case err: Throwable => {\n    breaker.fail()\n  }\n  case ReceiveTimeout => {\n    breaker.fail()\n  }\n} Java copysource@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n      .match(\n          String.class,\n          payload -> \"call\".equals(payload) && breaker.isClosed(),\n          payload -> target.tell(\"message\", self()))\n      .matchEquals(\"response\", payload -> breaker.succeed())\n      .match(Throwable.class, t -> breaker.fail())\n      .match(ReceiveTimeout.class, t -> breaker.fail())\n      .build();\n}","title":"Low level API"},{"location":"/futures.html","text":"","title":"Futures patterns"},{"location":"/futures.html#futures-patterns","text":"","title":"Futures patterns"},{"location":"/futures.html#dependency","text":"Pekko offers tiny helpers for use with FuturesCompletionStage. These are part of Pekko’s core module:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/futures.html#after","text":"org.apache.pekko.pattern.afterorg.apache.pekko.pattern.Patterns.after makes it easy to complete a FutureCompletionStage with a value or exception after a timeout.\nScala copysourceval delayed =\n  pekko.pattern.after(200.millis)(Future.failed(new IllegalStateException(\"OHNOES\")))\n\nval future = Future { Thread.sleep(1000); \"foo\" }\nval result = Future.firstCompletedOf(Seq(future, delayed)) Java copysource CompletionStage<String> failWithException =\n    CompletableFuture.supplyAsync(\n        () -> {\n          throw new IllegalStateException(\"OHNOES1\");\n        });\nCompletionStage<String> delayed =\n    Patterns.after(Duration.ofMillis(200), system, () -> failWithException);","title":"After"},{"location":"/futures.html#retry","text":"org.apache.pekko.pattern.retryorg.apache.pekko.pattern.Patterns.retry will retry a FutureCompletionStage some number of times with a delay between each attempt.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.typed.scaladsl.adapter._\nimplicit val scheduler: pekko.actor.Scheduler = system.scheduler.toClassic\nimplicit val ec: ExecutionContext = system.executionContext\n\n// Given some future that will succeed eventually\n@volatile var failCount = 0\ndef futureToAttempt() = {\n  if (failCount < 5) {\n    failCount += 1\n    Future.failed(new IllegalStateException(failCount.toString))\n  } else Future.successful(5)\n}\n\n// Return a new future that will retry up to 10 times\nval retried: Future[Int] = pekko.pattern.retry(() => futureToAttempt(), attempts = 10, 100 milliseconds) Java copysource Callable<CompletionStage<String>> attempt = () -> CompletableFuture.completedFuture(\"test\");\n\nCompletionStage<String> retriedFuture =\n    Patterns.retry(attempt, 3, java.time.Duration.ofMillis(200), system);","title":"Retry"},{"location":"/typed/extending.html","text":"","title":"Extending Apache Pekko"},{"location":"/typed/extending.html#extending-apache-pekko","text":"Apache Pekko extensions can be used for almost anything, they provide a way to create an instance of a class only once for the whole ActorSystem and be able to access it from anywhere. Pekko features such as Cluster, Serialization and Sharding are all Pekko extensions. Below is the use-case of managing an expensive database connection pool and accessing it from various places in your application.\nYou can choose to have your Extension loaded on-demand or at ActorSystem creation time through the Pekko configuration. Details on how to make that happens are below, in the Loading from Configuration section.\nWarning Since an extension is a way to hook into Pekko itself, the implementor of the extension needs to ensure the thread safety and that it is non-blocking.","title":"Extending Apache Pekko"},{"location":"/typed/extending.html#building-an-extension","text":"Let’s build an extension to manage a shared database connection pool.\nScala copysourceclass ExpensiveDatabaseConnection {\n  def executeQuery(query: String): Future[Any] = ???\n} Java copysourcepublic class ExpensiveDatabaseConnection {\n  public CompletionStage<Object> executeQuery(String query) {\n    throw new RuntimeException(\"I should do a database query\");\n  }\n  // ...\n}\nFirst create an ExtensionExtension, this will be created only once per ActorSystem:\nScala copysourceclass DatabasePool(system: ActorSystem[_]) extends Extension {\n  // database configuration can be loaded from config\n  // from the actor system\n  private val _connection = new ExpensiveDatabaseConnection()\n\n  def connection(): ExpensiveDatabaseConnection = _connection\n} Java copysourcepublic class DatabaseConnectionPool implements Extension {\n\n  private final ExpensiveDatabaseConnection _connection;\n\n  private DatabaseConnectionPool(ActorSystem<?> system) {\n    // database configuration can be loaded from config\n    // from the actor system\n    _connection = new ExpensiveDatabaseConnection();\n  }\n\n  public ExpensiveDatabaseConnection connection() {\n    return _connection;\n  }\n}\nThis is the public API of your extension. Internally in this example we instantiate our expensive database connection.\nThen create an ExtensionIdExtensionId to identify the extension. A good convention is to let the companion object of the Extension be the ExtensionId.A good convention is to define the ExtensionId as a static inner class of the Extension.\nScala copysourceobject DatabasePool extends ExtensionId[DatabasePool] {\n  // will only be called once\n  def createExtension(system: ActorSystem[_]): DatabasePool = new DatabasePool(system)\n\n  // Java API\n  def get(system: ActorSystem[_]): DatabasePool = apply(system)\n} Java copysourcepublic static class Id extends ExtensionId<DatabaseConnectionPool> {\n\n  private static final Id instance = new Id();\n\n  private Id() {}\n\n  // called once per ActorSystem\n  @Override\n  public DatabaseConnectionPool createExtension(ActorSystem<?> system) {\n    return new DatabaseConnectionPool(system);\n  }\n\n  public static DatabaseConnectionPool get(ActorSystem<?> system) {\n    return instance.apply(system);\n  }\n}\nThen finally to use the extension it can be looked up:\nScala copysourceBehaviors.setup[Any] { ctx =>\n  DatabasePool(ctx.system).connection().executeQuery(\"insert into...\")\n  initialBehavior\n} Java copysourceBehaviors.setup(\n    (context) -> {\n      DatabaseConnectionPool.Id.get(context.getSystem())\n          .connection()\n          .executeQuery(\"insert into...\");\n      return initialBehavior();\n    });\nThe DatabaseConnectionPool can be looked up in this way any number of times and it will return the same instance.","title":"Building an extension"},{"location":"/typed/extending.html#loading-from-configuration","text":"To be able to load extensions from your Pekko configuration you must add FQCNs of implementations of the ExtensionId in the pekko.actor.typed.extensions section of the config you provide to your ActorSystem.\nScala copysourcepekko.actor.typed.extensions = [\"org.apache.pekko.pekko.extensions.DatabasePool\"] Java ruby pekko.actor.typed { extensions = [\"jdocs.org.apache.pekko.typed.extensions.ExtensionDocTest$DatabaseConnectionPool$Id\"] }","title":"Loading from configuration"},{"location":"/common/other-modules.html","text":"","title":"Other Apache Pekko modules"},{"location":"/common/other-modules.html#other-apache-pekko-modules","text":"This page describes modules that compliment libraries from the Pekko core. See this overview instead for a guide on the core modules.","title":"Other Apache Pekko modules"},{"location":"/common/other-modules.html#","text":"A full server- and client-side HTTP stack on top of pekko-actor and pekko-stream.","title":"Pekko HTTP"},{"location":"/common/other-modules.html#","text":"Pekko gRPC provides support for building streaming gRPC servers and clients on top of Pekko Streams.","title":"Pekko gRPC"},{"location":"/common/other-modules.html#","text":"Pekko Connectors is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Pekko.","title":"Pekko Connectors"},{"location":"/common/other-modules.html#","text":"The Pekko Kafka Connector connects Apache Kafka with Pekko Streams.","title":"Pekko Kafka Connector"},{"location":"/common/other-modules.html#","text":"Pekko Projections let you process a stream of events or records from a source to a projected model or external system.","title":"Pekko Projections"},{"location":"/common/other-modules.html#","text":"A Pekko Persistence journal and snapshot store backed by Apache Cassandra.","title":"Cassandra Plugin for Pekko Persistence"},{"location":"/common/other-modules.html#","text":"A Pekko Persistence journal and snapshot store for use with JDBC-compatible databases. This implementation relies on Slick.","title":"JDBC Plugin for Pekko Persistence"},{"location":"/common/other-modules.html#","text":"A Pekko Persistence journal and snapshot store for use with R2DBC-compatible databases. This implementation relies on R2DBC.","title":"R2DBC Plugin for Pekko Persistence"},{"location":"/common/other-modules.html#","text":"Use Google Cloud Spanner as Pekko Persistence journal and snapshot store. This integration relies on Pekko gRPC.","title":"Google Cloud Spanner Plugin for Pekko Persistence"},{"location":"/common/other-modules.html#apache-pekko-management","text":"Pekko Management provides a central HTTP endpoint for Pekko management extensions. Pekko Cluster Bootstrap helps bootstrapping a Pekko cluster using Pekko Discovery. Pekko Management Cluster HTTP provides HTTP endpoints for introspecting and managing Pekko clusters. Pekko Discovery for Kubernetes, Consul, Marathon, and AWS Kubernetes Lease","title":"Apache Pekko Management"},{"location":"/additional/deploy.html","text":"","title":"Package, Deploy and Run"},{"location":"/additional/deploy.html#package-deploy-and-run","text":"Packaging sbt: Native Packager Maven: jarjar, onejar or assembly Gradle: the Jar task from the Java plugin Operating a Cluster Starting Stopping Cluster Management Deploying Deploying to Kubernetes Deploying to Docker containers Rolling Updates Serialization Compatibility Cluster Sharding Cluster Singleton Cluster Shutdown Configuration Compatibility Checks When Shutdown Startup Is Required","title":"Package, Deploy and Run"},{"location":"/additional/packaging.html","text":"","title":"Packaging"},{"location":"/additional/packaging.html#packaging","text":"The simplest way to use Apache Pekko is as a regular library, adding the Pekko jars you need to your classpath (in case of a web app, in WEB-INF/lib).\nIn many cases, such as deploying to an analytics cluster, building your application into a single ‘fat jar’ is needed. When building fat jars, some additional configuration is needed to merge Pekko config files, because each Pekko jar contains a reference.conf resource with default values.\nThe method for ensuring reference.conf and other *.conf resources are merged depends on the tooling you use to create the fat jar:\nsbt: as an application packaged with sbt-native-packager Maven: as an application packaged with a bundler such as jarjar, onejar or assembly Gradle: using the Jar task from the Java plugin","title":"Packaging"},{"location":"/additional/packaging.html#sbt-native-packager","text":"sbt-native-packager is a tool for creating distributions of any type of application, including Pekko applications.\nDefine sbt version in project/build.properties file:\nsbt.version=1.3.12\nAdd sbt-native-packager in project/plugins.sbt file:\naddSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.1.5\")\nFollow the instructions for the JavaAppPackaging in the sbt-native-packager plugin documentation.","title":"sbt: Native Packager"},{"location":"/additional/packaging.html#maven-jarjar-onejar-or-assembly","text":"You can use the Apache Maven Shade Plugin support for Resource Transformers to merge all the reference.confs on the build classpath into one.\nThe plugin configuration might look like this:\n<plugin>\n <groupId>org.apache.maven.plugins</groupId>\n <artifactId>maven-shade-plugin</artifactId>\n <version>1.5</version>\n <executions>\n  <execution>\n   <id>shade-my-jar</id>\n   <phase>package</phase>\n   <goals>\n    <goal>shade</goal>\n   </goals>\n   <configuration>\n    <shadedArtifactAttached>true</shadedArtifactAttached>\n    <shadedClassifierName>allinone</shadedClassifierName>\n    <artifactSet>\n     <includes>\n      <include>*:*</include>\n     </includes>\n    </artifactSet>\n    <transformers>\n      <transformer\n       implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">\n       <resource>reference.conf</resource>\n      </transformer>\n      <transformer\n       implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">\n       <resource>version.conf</resource>\n      </transformer>\n      <transformer\n       implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n       <manifestEntries>\n        <Main-Class>myapp.Main</Main-Class>\n       </manifestEntries>\n      </transformer>\n    </transformers>\n   </configuration>\n  </execution>\n </executions>\n</plugin>","title":"Maven: jarjar, onejar or assembly"},{"location":"/additional/packaging.html#gradle-the-jar-task-from-the-java-plugin","text":"When using Gradle, you would typically use the Jar task from the Java plugin to create the fat jar.\nTo make sure the reference.conf resources are correctly merged, you might use the Shadow plugin, which might look something like this:\nimport com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer\n\nplugins {\n    id 'java'\n    id \"com.github.johnrengelman.shadow\" version \"7.0.0\"\n}\n\nshadowJar {\n    append 'reference.conf'\n    append 'version.conf'\n    with jar\n}\nOr when you use the Kotlin DSL:\ntasks.withType<ShadowJar> {\n    val newTransformer = AppendingTransformer()\n    newTransformer.resource = \"reference.conf\"\n    transformers.add(newTransformer)\n}","title":"Gradle: the Jar task from the Java plugin"},{"location":"/additional/operations.html","text":"","title":"Operating a Cluster"},{"location":"/additional/operations.html#operating-a-cluster","text":"This documentation discusses how to operate a cluster. For related, more specific guides see Packaging, Deploying and Rolling Updates.","title":"Operating a Cluster"},{"location":"/additional/operations.html#starting","text":"","title":"Starting"},{"location":"/additional/operations.html#cluster-bootstrap","text":"When starting clusters on cloud systems such as Kubernetes, AWS, Google Cloud, Azure, Mesos or others, you may want to automate the discovery of nodes for the cluster joining process, using your cloud providers, cluster orchestrator, or other form of service discovery (such as managed DNS).\nThe open source Pekko Management library includes the Cluster Bootstrap module which handles just that. Please refer to its documentation for more details.\nNote If you are running Pekko in a Docker container or the nodes for some other reason have separate internal and external ip addresses you must configure remoting according to Pekko behind NAT or in a Docker container","title":"Cluster Bootstrap"},{"location":"/additional/operations.html#stopping","text":"See Rolling Updates, Cluster Shutdown and Coordinated Shutdown.","title":"Stopping"},{"location":"/additional/operations.html#cluster-management","text":"There are several management tools for the cluster. Complete information on running and managing Pekko applications can be found in the project documentation.","title":"Cluster Management"},{"location":"/additional/operations.html#http","text":"Information and management of the cluster is available with a HTTP API. See documentation of Pekko Management.","title":"HTTP"},{"location":"/additional/operations.html#jmx","text":"Information and management of the cluster is available as JMX MBeans with the root name org.apache.pekko.Cluster. The JMX information can be displayed with an ordinary JMX console such as JConsole or JVisualVM.\nFrom JMX you can:\nsee what members that are part of the cluster see status of this node see roles of each member join this node to another node in cluster mark any node in the cluster as down tell any node in the cluster to leave\nMember nodes are identified by their address, in format pekko://actor-system-name@hostname:port.","title":"JMX"},{"location":"/additional/deploying.html","text":"","title":"Deploying"},{"location":"/additional/deploying.html#deploying","text":"","title":"Deploying"},{"location":"/additional/deploying.html#deploying-to-kubernetes","text":"Deploy to Kubernetes according to the guide and example project for Deploying a Pekko Cluster to Kubernetes, but that requires more expertise of Kubernetes.","title":"Deploying to Kubernetes"},{"location":"/additional/deploying.html#cluster-bootstrap","text":"To take advantage of running inside Kubernetes while forming a cluster, Pekko Cluster Bootstrap helps forming or joining a cluster using Pekko Discovery to discover peer nodes. with the Kubernetes API or Kubernetes via DNS.\nYou can look at the Cluster with Kubernetes example project to see what this looks like in practice.","title":"Cluster bootstrap"},{"location":"/additional/deploying.html#resource-limits","text":"To avoid CFS scheduler limits, it is best not to use resources.limits.cpu limits, but use resources.requests.cpu configuration instead.","title":"Resource limits"},{"location":"/additional/deploying.html#deploying-to-docker-containers","text":"You can use both Pekko remoting and Pekko Cluster inside Docker containers. Note that you will need to take special care with the network configuration when using Docker, described here: Pekko behind NAT or in a Docker container\nYou can look at the Cluster with docker-compse example project Cluster with docker-compose example project to see what this looks like in practice.\nFor the JVM to run well in a Docker container, there are some general (not Pekko specific) parameters that might need tuning:","title":"Deploying to Docker containers"},{"location":"/additional/deploying.html#resource-constraints","text":"Docker allows constraining each containers’ resource usage.","title":"Resource constraints"},{"location":"/additional/deploying.html#memory","text":"You may want to look into using -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap options for your JVM later than 8u131, which makes it understand c-group memory limits. On JVM 10 and later, the -XX:+UnlockExperimentalVMOptions option is no longer needed.","title":"Memory"},{"location":"/additional/deploying.html#cpu","text":"For multi-threaded applications such as the JVM, the CFS scheduler limits are an ill fit, because they will restrict the allowed CPU usage even when more CPU cycles are available from the host system. This means your application may be starved of CPU time, but your system appears idle.\nFor this reason, it is best to avoid --cpus and --cpu-quota entirely, and instead specify relative container weights using --cpu-shares instead.","title":"CPU"},{"location":"/additional/rolling-updates.html","text":"","title":"Rolling Updates"},{"location":"/additional/rolling-updates.html#rolling-updates","text":"Note There are a few instances when a full cluster restart is required versus being able to do a rolling update.\nA rolling update is the process of replacing one version of the system with another without downtime. The changes can be new code, changed dependencies such as new Pekko version, or modified configuration.\nIn Apache Pekko, rolling updates are typically used for a stateful Pekko Cluster where you can’t run two separate clusters in parallel during the update, for example in blue green deployments.\nFor rolling updates related to Pekko dependency version upgrades and the migration guides, please see Rolling Updates and Pekko versions","title":"Rolling Updates"},{"location":"/additional/rolling-updates.html#serialization-compatibility","text":"There are two parts of Pekko that need careful consideration when performing an rolling update.\nCompatibility of remote message protocols. Old nodes may send messages to new nodes and vice versa. Serialization format of persisted events and snapshots. New nodes must be able to read old data, and during the update old nodes must be able to read data stored by new nodes.\nThere are many more application specific aspects for serialization changes during rolling updates to consider. For example based on the use case and requirements, whether to allow dropped messages or tear down the TCP connection when the manifest is unknown. When some message loss during a rolling update is acceptable versus a full shutdown and restart, assuming the application recovers afterwards * If a java.io.NotSerializableException is thrown in fromBinary this is treated as a transient problem, the issue logged and the message is dropped * If other exceptions are thrown it can be an indication of corrupt bytes from the underlying transport, and the connection is broken\nFor more zero-impact rolling updates, it is important to consider a strategy for serialization that can be evolved. One approach to retiring a serializer without downtime is described in two rolling update steps to switch to the new serializer. Additionally you can find advice on Persistence - Schema Evolution which also applies to remote messages when deploying with rolling updates.","title":"Serialization Compatibility"},{"location":"/additional/rolling-updates.html#cluster-sharding","text":"During a rolling update, sharded entities receiving traffic may be moved, based on the pluggable allocation strategy and settings. When an old node is stopped the shards that were running on it are moved to one of the other remaining nodes in the cluster when messages are sent to those shards.\nTo make rolling updates as smooth as possible there is a configuration property that defines the version of the application. This is used by rolling update features to distinguish between old and new nodes. For example, the default LeastShardAllocationStrategy avoids allocating shards to old nodes during a rolling update. The LeastShardAllocationStrategy sees that there is rolling update in progress when there are members with different configured app-version.\nTo make use of this feature you need to define the app-version and increase it for each rolling update.\npekko.cluster.app-version = 1.2.3\nTo understand which is old and new it compares the version numbers using normal conventions, see VersionVersion for more details.\nRebalance is also disabled during rolling updates, since shards from stopped nodes are anyway supposed to be started on new nodes. Messages to shards that were stopped on the old nodes will allocate corresponding shards on the new nodes, without waiting for rebalance actions.\nYou should also enable the health check for Cluster Sharding if you use Pekko Management. The readiness check will delay incoming traffic to the node until Sharding has been initialized and can accept messages.\nThe ShardCoordinator is itself a cluster singleton. To minimize downtime of the shard coordinator, see the strategies about ClusterSingleton rolling updates below.\nA few specific changes to sharding configuration require a full cluster restart.","title":"Cluster Sharding"},{"location":"/additional/rolling-updates.html#cluster-singleton","text":"Cluster singletons are always running on the oldest node. To avoid moving cluster singletons more than necessary during a rolling update, it is recommended to upgrade the oldest node last. This way cluster singletons are only moved once during a full rolling update. Otherwise, in the worst case cluster singletons may be migrated from node to node which requires coordination and initialization overhead several times.\nKubernetes Deployments with RollingUpdate strategy will roll out updates in this preferred order, from newest to oldest.","title":"Cluster Singleton"},{"location":"/additional/rolling-updates.html#cluster-shutdown","text":"","title":"Cluster Shutdown"},{"location":"/additional/rolling-updates.html#graceful-shutdown","text":"For rolling updates it is best to leave the Cluster gracefully via Coordinated Shutdown, which will run automatically on SIGTERM, when the Cluster node sees itself as Exiting. Environments such as Kubernetes send a SIGTERM, however if the JVM is wrapped with a script ensure that it forwards the signal. Graceful shutdown of Cluster Singletons and Cluster Sharding similarly happen automatically.","title":"Graceful shutdown"},{"location":"/additional/rolling-updates.html#ungraceful-shutdown","text":"In case of network failures it may still be necessary to set the node’s status to Down in order to complete the removal. Cluster Downing details downing nodes and downing providers. Split Brain Resolver can be used to ensure the cluster continues to function during network partitions and node failures. For example if there is an unreachability problem Split Brain Resolver would make a decision based on the configured downing strategy.","title":"Ungraceful shutdown"},{"location":"/additional/rolling-updates.html#configuration-compatibility-checks","text":"During rolling updates the configuration from existing nodes should pass the Cluster configuration compatibility checks. For example, it is possible to migrate Cluster Sharding from Classic to Typed Actors in a rolling update using a two step approach:\nDeploy with the new nodes set to pekko.cluster.configuration-compatibility-check.enforce-on-join = off and ensure all nodes are in this state Deploy again and with the new nodes set to pekko.cluster.configuration-compatibility-check.enforce-on-join = on.\nFull documentation about enforcing these checks on joining nodes and optionally adding custom checks can be found in Pekko Cluster configuration compatibility checks.","title":"Configuration Compatibility Checks"},{"location":"/additional/rolling-updates.html#when-shutdown-startup-is-required","text":"There are a few instances when a full shutdown and startup is required versus being able to do a rolling update.","title":"When Shutdown Startup Is Required"},{"location":"/additional/rolling-updates.html#cluster-sharding-configuration-change","text":"If you need to change any of the following aspects of sharding it will require a full cluster restart versus a rolling update:\nThe extractShardId function The role that the shard regions run on The persistence mode - It’s important to use the same mode on all nodes in the cluster The number-of-shards - Note: changing the number of nodes in the cluster does not require changing the number of shards.","title":"Cluster Sharding configuration change"},{"location":"/additional/rolling-updates.html#cluster-configuration-change","text":"A full restart is required if you change the SBR strategy","title":"Cluster configuration change"},{"location":"/additional/rolling-updates.html#migrating-from-persistentfsm-to-eventsourcedbehavior","text":"If you’ve migrated from PersistentFSM to EventSourcedBehavior and are using PersistenceFSM with Cluster Sharding, a full shutdown is required as shards can move between new and old nodes.","title":"Migrating from PersistentFSM to EventSourcedBehavior"},{"location":"/additional/rolling-updates.html#migrating-from-classic-remoting-to-artery","text":"If you’ve migrated from classic remoting to Artery which has a completely different protocol, a rolling update is not supported.","title":"Migrating from classic remoting to Artery"},{"location":"/additional/rolling-updates.html#changing-remoting-transport","text":"Rolling update is not supported when changing the remoting transport.","title":"Changing remoting transport"},{"location":"/additional/rolling-updates.html#migrating-from-classic-sharding-to-typed-sharding","text":"If you have been using classic sharding it is possible to do a rolling update to typed sharding using a 3 step procedure. The steps along with example commits are detailed in this sample PR","title":"Migrating from Classic Sharding to Typed Sharding"},{"location":"/project/index.html","text":"","title":"Project Information"},{"location":"/project/index.html#project-information","text":"Binary Compatibility Rules Binary compatibility rules explained Change in versioning scheme, stronger compatibility since 2.4 Mixed versioning is not allowed The meaning of “may change” API stability annotations and comments Binary Compatibility Checking Toolchain Serialization compatibility across Scala versions Scala 3 support Using 2.13 artifacts in Scala 3 Scala 3 artifacts Downstream upgrade strategy Patch versions Minor versions Modules marked “May Change” IDE Tips Configure the auto-importer in IntelliJ / Eclipse Immutability using Lombok Adding Lombok to your project Using lombok Apache Pekko in OSGi Dependency Background Core Components and Structure of OSGi Applications Notable Behavior Changes Configuring the OSGi Framework Intended Use Activator Migration Guides Migration to Pekko Rolling Updates and Versions Pekko upgrades Issue Tracking Browsing Creating tickets Submitting Pull Requests Licenses Frequently Asked Questions Apache Pekko Project Resources with Explicit Lifecycle Actors Cluster Debugging Books and Videos Example projects Quickstart FSM Cluster Distributed Data Cluster Sharding Persistence CQRS Replicated Event Sourcing Cluster with Docker Cluster with Kubernetes Distributed workers Kafka to Cluster Sharding Project Source Code Releases Repository Snapshots Repository","title":"Project Information"},{"location":"/common/binary-compatibility-rules.html","text":"","title":"Binary Compatibility Rules"},{"location":"/common/binary-compatibility-rules.html#binary-compatibility-rules","text":"Apache Pekko maintains and verifies backwards binary compatibility across versions of modules.\nIn the rest of this document whenever binary compatibility is mentioned “backwards binary compatibility” is meant (as opposed to forward compatibility).\nThis means that the new JARs are a drop-in replacement for the old one (but not the other way around) as long as your build does not enable the inliner (Scala-only restriction).\nBecause of this approach applications can upgrade to the latest version of Pekko even when intermediate satellite projects are not yet upgraded","title":"Binary Compatibility Rules"},{"location":"/common/binary-compatibility-rules.html#binary-compatibility-rules-explained","text":"Binary compatibility is maintained between:\nminor and patch versions - please note that the meaning of “minor” has shifted to be more restrictive with Pekko 2.4.0, read Change in versioning scheme for details.\nBinary compatibility is NOT maintained between:\nmajor versions any versions of may change modules – read Modules marked “May Change” for details a few notable exclusions explained below\nSpecific examples (please read Change in versioning scheme to understand the difference in “before 2.4 era” and “after 2.4 era”):\n# [epoch.major.minor] era\nOK:  2.2.0 --> 2.2.1 --> ... --> 2.2.x\nNO:  2.2.y --x 2.3.y\nOK:  2.3.0 --> 2.3.1 --> ... --> 2.3.x\nOK:  2.3.x --> 2.4.x (special case, migration to new versioning scheme)\n# [major.minor.patch] era\nOK:  2.4.0 --> 2.5.x\nOK:  2.5.0 --> 2.6.x\nNO:  2.x.y --x 3.x.y\nOK:  3.0.0 --> 3.0.1 --> ... --> 3.0.n\nOK:  3.0.n --> 3.1.0 --> ... --> 3.1.n\nOK:  3.1.n --> 3.2.0 ...\n     ...","title":"Binary compatibility rules explained"},{"location":"/common/binary-compatibility-rules.html#cases-where-binary-compatibility-is-not-retained","text":"If a security vulnerability is reported in Pekko or a transient dependency of Pekko and it cannot be solved without breaking binary compatibility then fixing the security issue is more important. In such cases binary compatibility might not be retained when releasing a minor version. Such exception is always noted in the release announcement.\nWe do not guarantee binary compatibility with versions that are EOL, though in practice this does not make a big difference: only in rare cases would a change be binary compatible with recent previous releases but not with older ones.\nSome modules are excluded from the binary compatibility guarantees, such as:\n*-testkit modules - since these are to be used only in tests, which usually are re-compiled and run on demand *-tck modules - since they may want to add new tests (or force configuring something), in order to discover possible failures in an existing implementation that the TCK is supposed to be testing. Compatibility here is not guaranteed, however it is attempted to make the upgrade process as smooth as possible. all may change modules - which by definition are subject to rapid iteration and change. Read more about that in Modules marked “May Change”","title":"Cases where binary compatibility is not retained"},{"location":"/common/binary-compatibility-rules.html#when-will-a-deprecated-method-be-removed-entirely","text":"Once a method has been deprecated then the guideline* is that it will be kept, at minimum, for one full minor version release. For example, if it is deprecated in version 2.5.2 then it will remain through the rest of 2.5, as well as the entirety of 2.6.\n*This is a guideline because in rare instances, after careful consideration, an exception may be made and the method removed earlier.","title":"When will a deprecated method be removed entirely"},{"location":"/common/binary-compatibility-rules.html#change-in-versioning-scheme-stronger-compatibility-since-2-4","text":"Since the release of Pekko 2.4.0 a new versioning scheme is in effect.\nHistorically, Pekko has been following the Java or Scala style of versioning in which the first number would mean “epoch”, the second one would mean major, and third be the minor, thus: epoch.major.minor (versioning scheme followed until and during 2.3.x).\nCurrently, since Pekko 2.4.0, the new versioning applies which is closer to semantic versioning many have come to expect, in which the version number is deciphered as major.minor.patch. This also means that Pekko 2.5.x is binary compatible with the 2.4 series releases (with the exception of “may change” APIs).\nIn addition to that, Pekko 2.4.x has been made binary compatible with the 2.3.x series, so there is no reason to remain on Pekko 2.3.x, since upgrading is completely compatible (and many issues have been fixed ever since).","title":"Change in versioning scheme, stronger compatibility since 2.4"},{"location":"/common/binary-compatibility-rules.html#mixed-versioning-is-not-allowed","text":"Modules that are released together under the Pekko project are intended to be upgraded together. For example, it is not legal to mix Pekko Actor 2.6.2 with Pekko Cluster 2.6.5 even though “Pekko 2.6.2” and “Pekko 2.6.5” are binary compatible.\nThis is because modules may assume internals changes across module boundaries, for example some feature in Clustering may have required an internals change in Actor, however it is not public API, thus such change is considered safe.\nIf you accidentally mix Pekko versions, for example through transitive dependencies, you might get a warning at run time such as:\nYou are using version 2.6.6 of Pekko, but it appears you (perhaps indirectly) also depend on older versions \nof related artifacts. You can solve this by adding an explicit dependency on version 2.6.6 of the \n[pekko-persistence-query] artifacts to your project. Here's a complete collection of detected \nartifacts: (2.5.3, [pekko-persistence-query]), (2.6.6, [pekko-actor, pekko-cluster]).\nSee also: https://pekko.apache.org/docs/pekko/current/common/binary-compatibility-rules.html#mixed-versioning-is-not-allowed\nThe fix is typically to pick the highest Pekko version, and add explicit dependencies to your project as needed. For example, in the example above you might want to add pekko-persistence-query dependency for 2.6.6.\nNote We recommend keeping an pekkoVersion variable in your build file, and re-use it for all included modules, so when you upgrade you can simply change it in this one place.\nThe warning includes a full list of Pekko runtime dependencies in the classpath, and the version detected. You can use that information to include an explicit list of Pekko artifacts you depend on into your build. If you use Maven or Gradle, you can include the Pekko Maven BOM (bill of materials) to help you keep all the versions of your Pekko dependencies in sync.","title":"Mixed versioning is not allowed"},{"location":"/common/binary-compatibility-rules.html#the-meaning-of-","text":"May change is used in module descriptions and docs in order to signify that the API that they contain is subject to change without any prior warning and is not covered by the binary compatibility promise. Read more in Modules marked “May Change”.","title":"The meaning of “may change”"},{"location":"/common/binary-compatibility-rules.html#api-stability-annotations-and-comments","text":"Pekko gives a very strong binary compatibility promise to end-users. However some parts of Pekko are excluded from these rules, for example internal or known evolving APIs may be marked as such and shipped as part of an overall stable module. As general rule any breakage is avoided and handled via deprecation and method addition, however certain APIs which are known to not yet be fully frozen (or are fully internal) are marked as such and subject to change at any time (even if best-effort is taken to keep them compatible).","title":"API stability annotations and comments"},{"location":"/common/binary-compatibility-rules.html#the-internal-api-and-marker","text":"When browsing the source code and/or looking for methods available to be called, especially from Java which does not have as rich of an access protection system as Scala has, you may sometimes find methods or classes annotated with the /** INTERNAL API */ comment or the @InternalApi annotation.\nNo compatibility guarantees are given about these classes. They may change or even disappear in minor versions, and user code is not supposed to call them.\nSide-note on JVM representation details of the Scala private[pekko] pattern that Pekko is using extensively in its internals: Such methods or classes, which act as “accessible only from the given package” in Scala, are compiled down to public (!) in raw Java bytecode. The access restriction, that Scala understands is carried along as metadata stored in the classfile. Thus, such methods are safely guarded from being accessed from Scala, however Java users will not be warned about this fact by the javac compiler. Please be aware of this and do not call into Internal APIs, as they are subject to change without any warning.","title":"The INTERNAL API and @InternalAPI marker"},{"location":"/common/binary-compatibility-rules.html#the-donotinherit-and-apimaychange-markers","text":"In addition to the special internal API marker two annotations exist in Pekko and specifically address the following use cases:\n@ApiMayChange – which marks APIs which are known to be not fully stable yet. Read more in Modules marked “May Change” @DoNotInherit – which marks APIs that are designed under a closed-world assumption, and thus must not be extended outside Pekko itself (or such code will risk facing binary incompatibilities). E.g. an interface may be marked using this annotation, and while the type is public, it is not meant for extension by user-code. This allows adding new methods to these interfaces without risking to break client code. Examples of such API are the FlowOps trait or the Pekko HTTP domain model.\nPlease note that a best-effort approach is always taken when having to change APIs and breakage is avoided as much as possible, however these markers allow to experiment, gather feedback and stabilize the best possible APIs we could build.","title":"The @DoNotInherit and @ApiMayChange markers"},{"location":"/common/binary-compatibility-rules.html#binary-compatibility-checking-toolchain","text":"Pekko uses the Lightbend maintained MiMa, for enforcing binary compatibility is kept where it was promised.\nAll Pull Requests must pass MiMa validation (which happens automatically), and if failures are detected, manual exception overrides may be put in place if the change happened to be in an Internal API for example.","title":"Binary Compatibility Checking Toolchain"},{"location":"/common/binary-compatibility-rules.html#serialization-compatibility-across-scala-versions","text":"Scala does not maintain serialization compatibility across major versions. This means that if Java serialization is used there is no guarantee objects can be cleanly deserialized if serialized with a different version of Scala.","title":"Serialization compatibility across Scala versions"},{"location":"/project/scala3.html","text":"","title":"Scala 3 support"},{"location":"/project/scala3.html#scala-3-support","text":"Pekko has experimental support for Scala 3.","title":"Scala 3 support"},{"location":"/project/scala3.html#using-2-13-artifacts-in-scala-3","text":"You can use CrossVersion.for3Use2_13 to use the regular 2.13 Pekko artifacts in a Scala 3 project. This has been shown to be successful for Streams, HTTP and gRPC-heavy applications.","title":"Using 2.13 artifacts in Scala 3"},{"location":"/project/scala3.html#scala-3-artifacts","text":"We are publishing experimental Scala 3 artifacts that can be used ‘directly’ (without CrossVersion) with Scala 3.\nWe do not promise binary compatibility for these artifacts yet.","title":"Scala 3 artifacts"},{"location":"/project/downstream-upgrade-strategy.html","text":"","title":"Downstream upgrade strategy"},{"location":"/project/downstream-upgrade-strategy.html#downstream-upgrade-strategy","text":"When a new Pekko version is released, downstream projects (such as Pekko Management, Pekko HTTP and Pekko gRPC) do not need to update immediately: because of our binary compatibility approach, applications can take advantage of the latest version of Pekko without having to wait for intermediate libraries to update.","title":"Downstream upgrade strategy"},{"location":"/project/downstream-upgrade-strategy.html#patch-versions","text":"When releasing a new patch version of Pekko (e.g. 2.5.22), we typically don’t immediately bump the Pekko version in satellite projects.\nThe reason for this is this will make it more low-friction for users to update those satellite projects: say their project is on Pekko 2.5.22 and Pekko Management 1.0.0, and we release Pekko Management 1.0.1 (still built with Pekko 2.5.22) and Pekko 2.5.23. They can safely update to Pekko Management 1.0.1 without also updating to Pekko 2.5.23, or update to Pekko 2.5.23 without updating to Pekko Management 1.0.1.\nWhen there is reason for a satellite project to upgrade the Pekko patch version, they are free to do so at any time.","title":"Patch versions"},{"location":"/project/downstream-upgrade-strategy.html#minor-versions","text":"When releasing a new minor version of Pekko (e.g. 2.6.0), satellite projects are also usually not updated immediately, but as needed.\nWhen a satellite project does update to a new minor version of Pekko, it will also increase its own minor version. The previous stable branch will enter the usual end-of-support lifecycle and only important bugfixes will be backported to the previous version and released.","title":"Minor versions"},{"location":"/common/may-change.html","text":"","title":"Modules marked May Change"},{"location":"/common/may-change.html#modules-marked-","text":"To be able to introduce new modules and APIs without freezing them the moment they are released because of our binary compatibility guarantees we have introduced the term may change.\nConcretely may change means that an API or module is in early access mode and that it:\nis not guaranteed to be binary compatible in minor releases may have its API change in breaking ways in minor releases may be entirely dropped from Apache Pekko in a minor release\nComplete modules can be marked as may change, which will be stated in the module’s description and in the docs.\nIndividual public APIs can be annotated with ApiMayChange to signal that it has less guarantees than the rest of the module it lives in. For example, when while introducing “new” Java 8 APIs into existing stable modules, these APIs may be marked with this annotation to signal that they are not frozen yet. Please use such methods and classes with care, however if you see such APIs that is the best point in time to try them out and provide feedback before they are frozen as fully stable API.\nBest effort migration guides may be provided, but this is decided on a case-by-case basis for may change modules.\nThe purpose of this is to be able to release features early and make them easily available and improve based on feedback, or even discover that the module or API wasn’t useful.\nThese are the current complete modules marked as may change:\nMulti Node Testing Reliable Delivery","title":"Modules marked “May Change”"},{"location":"/additional/ide.html","text":"","title":"IDE Tips"},{"location":"/additional/ide.html#ide-tips","text":"","title":"IDE Tips"},{"location":"/additional/ide.html#configure-the-auto-importer-in-intellij-eclipse","text":"For a smooth development experience, when using an IDE such as Eclipse or IntelliJ, you can disable the auto-importer from suggesting javadsl imports when working in Scala, or viceversa.\nIn IntelliJ, the auto-importer settings are under “Editor” / “General” / “Auto Import”. Use a name mask such as org.apache.pekko.stream.javadsl* or org.apache.pekko.stream.scaladsl* or *javadsl* or *scaladsl* to indicate the DSL you want to exclude from import/completion. See screenshot below:\nEclipse users can configure this aspect of the IDE by going to “Window” / “Preferences” / “Java” / “Appearance” / “Type Filters”.","title":"Configure the auto-importer in IntelliJ / Eclipse"},{"location":"/project/immutable.html","text":"","title":"Immutability using Lombok"},{"location":"/project/immutable.html#immutability-using-lombok","text":"A preferred best practice in Apache Pekko is to have immutable messages. Scala provides case class which makes it extremely easy to have short and clean classes for creating immutable objects, but no such facility is easily available in Java. We can make use of several third party libraries which help is achieving this. One good example is Lombok.\nProject Lombok is a java library that automatically plugs into your editor and build tools, and helps get rid of much of the boilerplate code for java development.\nLombok handles the following details for you. It:\nmodifies fields to be private and final creates getters for each field creates correct equals, hashCode and a human-friendly toString creates a constructor requiring all fields.","title":"Immutability using Lombok"},{"location":"/project/immutable.html#adding-lombok-to-your-project","text":"To add Lombok to a Maven project, declare it as a simple dependency:\nMaven <dependencies>\n  <dependency>\n    <groupId>org.projectlombok</groupId>\n    <artifactId>lombok</artifactId>\n    <version>1.18.10</version>\n  </dependency>\n</dependencies> Gradle dependencies {\n  implementation \"org.projectlombok:lombok:1.18.10\"\n}","title":"Adding Lombok to your project"},{"location":"/project/immutable.html#using-lombok","text":"@Value\npublic class LombokUser {\n\n  String name;\n\n  String email;\n}\nThe example does not demonstrate other useful Lombok features like @Builder or @With which will help you create builder and copy methods. Be aware that Lombok is not an immutability library but a code generation library which means some setups might not create immutable objects. For example, Lombok’s @Data is equivalent to Lombok’s @Value but will also synthesize mutable methods. Don’t use Lombok’s @Data when creating immutable classes.\nUsing Lombok for creating a message class for actors is quite simple. In following example, Message class just defines the member variable and Lombok annotation @Value takes care of creating methods like getter, toString, hashCode, equals.\npublic class MyActor extends AbstractActor {\n        private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n        public Receive createReceive() {\n            return receiveBuilder()\n            .match(Message.class, message -> {\n                System.out.println(message.getMessage());\n            })\n            .matchAny(o -> log.info(\"Received unknown message\"))\n            .build();\n        }\n\n        @Value\n        public static class Message {\n            private String message;\n\n            public Message(String message) {\n                this.message = message;\n            }\n        }\n}","title":"Using lombok"},{"location":"/project/immutable.html#integrating-lombok-with-an-ide","text":"Lombok integrates with popular IDEs:\nTo use Lombok in IntelliJ IDEA you’ll need the Lombok Plugin for IntelliJ IDEA and you’ll also need to enable Annotation Processing (Settings / Build,Execution,Deployment / Compiler / Annotation Processors and tick Enable annotation processing) To Use Lombok in Eclipse, run java -jar lombok.jar (see the video at Project Lombok).","title":"Integrating Lombok with an IDE"},{"location":"/additional/osgi.html","text":"","title":"Apache Pekko in OSGi"},{"location":"/additional/osgi.html#apache-pekko-in-osgi","text":"","title":"Apache Pekko in OSGi"},{"location":"/additional/osgi.html#dependency","text":"To use Apache Pekko in OSGi, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-osgi\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-osgi_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-osgi_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/additional/osgi.html#background","text":"OSGi is a mature packaging and deployment standard for component-based systems. It has similar capabilities as Project Jigsaw (originally scheduled for JDK 1.8), but has far stronger facilities to support legacy Java code. This is to say that while Jigsaw-ready modules require significant changes to most source files and on occasion to the structure of the overall application, OSGi can be used to modularize almost any Java code as far back as JDK 1.2, usually with no changes at all to the binaries.\nThese legacy capabilities are OSGi’s major strength and its major weakness. The creators of OSGi realized early on that implementors would be unlikely to rush to support OSGi metadata in existing JARs. There were already a handful of new concepts to learn in the JRE and the added value to teams that were managing well with straight J2EE was not obvious. Facilities emerged to “wrap” binary JARs so they could be used as bundles, but this functionality was only used in limited situations. An application of the “80/20 Rule” here would have that “80% of the complexity is with 20% of the configuration”, but it was enough to give OSGi a reputation that has stuck with it to this day.\nThis document aims to the productivity basics folks need to use it with Pekko, the 20% that users need to get 80% of what they want. For more information than is provided here, OSGi In Action is worth exploring.","title":"Background"},{"location":"/additional/osgi.html#core-components-and-structure-of-osgi-applications","text":"The fundamental unit of deployment in OSGi is the Bundle. A bundle is a Java JAR with additional entries https://docs.osgi.org/reference/bundle-headers.html in MANIFEST.MF that minimally expose the name and version of the bundle and packages for import and export. Since these manifest entries are ignored outside OSGi deployments, a bundle can interchangeably be used as a JAR in the JRE.\nWhen a bundle is loaded, a specialized implementation of the Java ClassLoader is instantiated for each bundle. Each classloader reads the manifest entries and publishes both capabilities (in the form of the Bundle-Exports) and requirements (as Bundle-Imports) in a container singleton for discovery by other bundles. The process of matching imports to exports across bundles through these classloaders is the process of resolution, one of six discrete steps in the lifecycle FSM of a bundle in an OSGi container:\nINSTALLED: A bundle that is installed has been loaded from disk and a classloader instantiated with its capabilities. Bundles are iteratively installed manually or through container-specific descriptors. For those familiar with legacy packging such as EJB, the modular nature of OSGi means that bundles may be used by multiple applications with overlapping dependencies. By resolving them individually from repositories, these overlaps can be de-duplicated across multiple deployments to the same container. RESOLVED: A bundle that has been resolved is one that has had its requirements (imports) satisfied. Resolution does mean that a bundle can be started. STARTING: A bundle that is started can be used by other bundles. For an otherwise complete application closure of resolved bundles, the implication here is they must be started in the order directed by a depth-first search for all to be started. When a bundle is starting, any exposed lifecycle interfaces in the bundle are called, giving the bundle the opportunity to start its own service endpoints and threads. ACTIVE: Once a bundle’s lifecycle interfaces return without error, a bundle is marked as active. STOPPING: A bundle that is stopping is in the process of calling the bundle’s stop lifecycle and transitions back to the RESOLVED state when complete. Any long running services or threads that were created while STARTING should be shut down when the bundle’s stop lifecycle is called. UNINSTALLED: A bundle can only transition to this state from the INSTALLED state, meaning it cannot be uninstalled before it is stopped.\nNote the dependency in this FSM on lifecycle interfaces. While there is no requirement that a bundle publishes these interfaces or accepts such callbacks, the lifecycle interfaces provide the semantics of a main() method and allow the bundle to start and stop long-running services such as REST web services, ActorSystems, Clusters, etc.\nSecondly, note when considering requirements and capabilities, it’s a common misconception to equate these with repository dependencies as might be found in Maven or Ivy. While they provide similar practical functionality, OSGi has several parallel type of dependency (such as Blueprint Services) that cannot be easily mapped to repository capabilities. In fact, the core specification leaves these facilities up to the container in use. In turn, some containers have tooling to generate application load descriptors from repository metadata.","title":"Core Components and Structure of OSGi Applications"},{"location":"/additional/osgi.html#notable-behavior-changes","text":"Combined with understanding the bundle lifecycle, the OSGi developer must pay attention to sometimes unexpected behaviors that are introduced. These are generally within the JVM specification, but are unexpected and can lead to frustration.\nBundles should not export overlapping package spaces. It is not uncommon for legacy JVM frameworks to expect plugins in an application composed of multiple JARs to reside under a single package name. For example, a frontend application might scan all classes from com.example.plugins for specific service implementations with that package existing in several contributed JARs. While it is possible to support overlapping packages with complex manifest headers, it’s much better to use non-overlapping package spaces and facilities such as Pekko Cluster for service discovery. Stylistically, many organizations opt to use the root package path as the name of the bundle distribution file. Resources are not shared across bundles unless they are explicitly exported, as with classes. The common case of this is expecting that getClass().getClassLoader().getResources(\"foo\") will return all files on the classpath named foo. The getResources() method only returns resources from the current classloader, and since there are separate classloaders for every bundle, resource files such as configurations are no longer searchable in this manner.","title":"Notable Behavior Changes"},{"location":"/additional/osgi.html#configuring-the-osgi-framework","text":"To use Pekko in an OSGi environment, the container must be configured such that the org.osgi.framework.bootdelegation property delegates the sun.misc package to the boot classloader instead of resolving it through the normal OSGi class space.","title":"Configuring the OSGi Framework"},{"location":"/additional/osgi.html#intended-use","text":"Pekko only supports the usage of an ActorSystem strictly confined to a single OSGi bundle, where that bundle contains or imports all of the actor system’s requirements. This means that the approach of offering an ActorSystem as a service to which Actors can be deployed dynamically via other bundles is not recommended — an ActorSystem and its contained actors are not meant to be dynamic in this way. ActorRefs may safely be exposed to other bundles.","title":"Intended Use"},{"location":"/additional/osgi.html#activator","text":"To bootstrap Pekko inside an OSGi environment, you can use the osgi.ActorSystemActivatorosgi.ActorSystemActivator class to conveniently set up the ActorSystemActorSystem.\ncopysourceimport org.apache.pekko\nimport pekko.actor.{ ActorSystem, Props }\nimport org.osgi.framework.BundleContext\nimport pekko.osgi.ActorSystemActivator\n\nclass Activator extends ActorSystemActivator {\n\n  def configure(context: BundleContext, system: ActorSystem): Unit = {\n    // optionally register the ActorSystem in the OSGi Service Registry\n    registerService(context, system)\n\n    val someActor = system.actorOf(Props[SomeActor](), name = \"someName\")\n    someActor ! SomeMessage\n  }\n\n}\nThe goal here is to map the OSGi lifecycle more directly to the Pekko lifecycle. The ActorSystemActivatorActorSystemActivator creates the actor system with a class loader that finds resources (application.conf and reference.conf files) and classes from the application bundle and all transitive dependencies.","title":"Activator"},{"location":"/project/migration-guides.html","text":"","title":"Migration Guides"},{"location":"/project/migration-guides.html#migration-guides","text":"Apache Pekko is based on the latest version of Akka in the v2.6.x series. If migrating from an earlier version of Akka, please migrate to Akka 2.6 before migrating to Pekko.","title":"Migration Guides"},{"location":"/project/migration-guides.html#migration-to-pekko","text":"TODO","title":"Migration to Pekko"},{"location":"/project/rolling-update.html","text":"","title":"Rolling Updates and Versions"},{"location":"/project/rolling-update.html#rolling-updates-and-versions","text":"","title":"Rolling Updates and Versions"},{"location":"/project/rolling-update.html#pekko-upgrades","text":"Pekko supports rolling updates between two consecutive patch versions unless an exception is mentioned on this page. For example updating from 2.5.15 to 2.5.16. Many times it is also possible to skip several versions and exceptions to that are also described here. For example it’s possible to update from 2.5.14 to 2.5.16 without intermediate 2.5.15.\nIt’s not supported to have a cluster with more than two different versions. Roll out the first update completely before starting next update.\nNote Rolling update from classic remoting to Artery is not supported since the protocol is completely different. It will require a full cluster shutdown and new startup.","title":"Pekko upgrades"},{"location":"/project/issue-tracking.html","text":"","title":"Issue Tracking"},{"location":"/project/issue-tracking.html#issue-tracking","text":"Apache Pekko is using GitHub Issues as its issue tracking system.","title":"Issue Tracking"},{"location":"/project/issue-tracking.html#browsing","text":"","title":"Browsing"},{"location":"/project/issue-tracking.html#tickets","text":"Before filing a ticket, please check the existing Apache Pekko tickets for earlier reports of the same problem. You are very welcome to comment on existing tickets, especially if you have reproducible test cases that you can share.","title":"Tickets"},{"location":"/project/issue-tracking.html#creating-tickets","text":"Please include the versions of Scala and Apache Pekko and relevant configuration files.\nYou can create a new ticket if you have registered a GitHub user account.\nThanks a lot for reporting bugs and suggesting features!","title":"Creating tickets"},{"location":"/project/issue-tracking.html#submitting-pull-requests","text":"Note A pull request is worth a thousand +1’s. – Old Klangian Proverb\nPull Requests fixing issues or adding functionality are very welcome. Please read CONTRIBUTING.md for more information about contributing to Apache Pekko.","title":"Submitting Pull Requests"},{"location":"/project/licenses.html","text":"","title":"Licenses"},{"location":"/project/licenses.html#licenses","text":"Apache License 2.0\nContributor License Agreements","title":"Licenses"},{"location":"/additional/faq.html","text":"","title":"Frequently Asked Questions"},{"location":"/additional/faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"/additional/faq.html#apache-pekko-project","text":"","title":"Apache Pekko Project"},{"location":"/additional/faq.html#where-does-the-name-pekko-come-from-","text":"The former name of this project, Akka, is a goddess in the Sámi (the native Swedish population) mythology. She is the goddess that stands for all the beauty and good in the world. Pekko builds on this foundation and is the Finnish god of farming & protector of the crops.","title":"Where does the name Pekko come from?"},{"location":"/additional/faq.html#resources-with-explicit-lifecycle","text":"Actors, ActorSystems, Materializers (for streams), all these types of objects bind resources that must be released explicitly. The reason is that Actors are meant to have a life of their own, existing independently of whether messages are currently en route to them. Therefore you should always make sure that for every creation of such an object you have a matching stop, terminate, or shutdown call implemented.\nIn particular you typically want to bind such values to immutable references, i.e. final ActorSystem system in Java or val system: ActorSystem in Scala.","title":"Resources with Explicit Lifecycle"},{"location":"/additional/faq.html#jvm-application-or-scala-repl-hanging-","text":"Due to an ActorSystem’s explicit lifecycle the JVM will not exit until it is stopped. Therefore it is necessary to shutdown all ActorSystems within a running application or Scala REPL session in order to allow these processes to terminate.\nShutting down an ActorSystem will properly terminate all Actors and Materializers that were created within it.","title":"JVM application or Scala REPL “hanging”"},{"location":"/additional/faq.html#actors","text":"","title":"Actors"},{"location":"/additional/faq.html#why-outofmemoryerror-","text":"It can be many reasons for OutOfMemoryError. For example, in a pure push based system with message consumers that are potentially slower than corresponding message producers you must add some kind of message flow control. Otherwise messages will be queued in the consumers’ mailboxes and thereby filling up the heap memory.","title":"Why OutOfMemoryError?"},{"location":"/additional/faq.html#cluster","text":"","title":"Cluster"},{"location":"/additional/faq.html#how-reliable-is-the-message-delivery-","text":"The general rule is at-most-once delivery, i.e. no guaranteed delivery. Stronger reliability can be built on top, and Pekko provides tools to do so.\nRead more in Message Delivery Reliability.","title":"How reliable is the message delivery?"},{"location":"/additional/faq.html#debugging","text":"","title":"Debugging"},{"location":"/additional/faq.html#how-do-i-turn-on-debug-logging-","text":"To turn on debug logging in your actor system add the following to your configuration:\npekko.loglevel = DEBUG\nRead more about it in the docs for Logging.","title":"How do I turn on debug logging?"},{"location":"/additional/books.html","text":"","title":"Books and Videos"},{"location":"/additional/books.html#books-and-videos","text":"It is a bit early in the lifecycle of Apache Pekko for there to be books and videos. Please ping us if you would like to have anything linked here.\nIn the meantime, the Akka Books and Videos page is worth a look.","title":"Books and Videos"},{"location":"/project/examples.html","text":"","title":"Example projects"},{"location":"/project/examples.html#example-projects","text":"The following example projects can be downloaded. They contain build files and have instructions of how to run.","title":"Example projects"},{"location":"/project/examples.html#quickstart","text":"Quickstart Guide Quickstart Guide\nThe Quickstart guide walks you through example code that introduces how to define actor systems, actors, and messages as well as how to use the test module and logging.","title":"Quickstart"},{"location":"/project/examples.html#fsm","text":"FSM example project FSM example project\nThis project contains a Dining Hakkers sample illustrating how to model a Finite State Machine (FSM) with actors.","title":"FSM"},{"location":"/project/examples.html#cluster","text":"Cluster example project Cluster example project\nThis project contains samples illustrating different Cluster features, such as subscribing to cluster membership events, and sending messages to actors running on nodes in the cluster with Cluster aware routers.\nIt also includes Multi JVM Testing with the sbt-multi-jvm plugin.","title":"Cluster"},{"location":"/project/examples.html#distributed-data","text":"Distributed Data example project Distributed Data example project\nThis project contains several samples illustrating how to use Distributed Data.","title":"Distributed Data"},{"location":"/project/examples.html#cluster-sharding","text":"Sharding example project Sharding example project\nThis project contains a KillrWeather sample illustrating how to use Cluster Sharding.","title":"Cluster Sharding"},{"location":"/project/examples.html#persistence","text":"Persistence example project Persistence example project\nThis project contains a Shopping Cart sample illustrating how to use Pekko Persistence.","title":"Persistence"},{"location":"/project/examples.html#cqrs","text":"The Microservices with Pekko tutorial contains a Shopping Cart sample illustrating how to use Event Sourcing and Projections together. The events are tagged to be consumed by even processors to build other representations from the events, or publish the events to other services.","title":"CQRS"},{"location":"/project/examples.html#replicated-event-sourcing","text":"Multi-DC Persistence example project Multi-DC Persistence example project\nIllustrates how to use Replicated Event Sourcing that supports active-active persistent entities across data centers.","title":"Replicated Event Sourcing"},{"location":"/project/examples.html#cluster-with-docker","text":"Cluster with docker-compose example project Cluster with docker-compose example project\nIllustrates how to use Pekko Cluster with Docker compose.","title":"Cluster with Docker"},{"location":"/project/examples.html#cluster-with-kubernetes","text":"Cluster with Kubernetes example project\nThis sample illustrates how to form a Pekko Cluster with Pekko Bootstrap when running in Kubernetes.","title":"Cluster with Kubernetes"},{"location":"/project/examples.html#distributed-workers","text":"Distributed workers example project\nThis project demonstrates the work pulling pattern using Pekko Cluster.","title":"Distributed workers"},{"location":"/project/examples.html#kafka-to-cluster-sharding","text":"Kafka to Cluster Sharding example project\nThis project demonstrates how to use the External Shard Allocation strategy to co-locate the consumption of Kafka partitions with the shard that processes the messages.","title":"Kafka to Cluster Sharding"},{"location":"/project/links.html","text":"","title":"Project"},{"location":"/project/links.html#project","text":"","title":"Project"},{"location":"/project/links.html#source-code","text":"Pekko uses Git and is hosted at Github apache/pekko.","title":"Source Code"},{"location":"/project/links.html#releases-repository","text":"All Pekko releases are published via Sonatype to Maven Central, see search.maven.org","title":"Releases Repository"},{"location":"/project/links.html#snapshots-repository","text":"Snapshot builds are available at https://oss.sonatype.org/content/repositories/snapshots/org/apache/pekko/. All Pekko modules that belong to the same build have the same version.\nWarning The use of Pekko SNAPSHOTs, nightlies and milestone releases is discouraged unless you know what you are doing.","title":"Snapshots Repository"},{"location":"/project/links.html#sbt-definition-of-snapshot-repository","text":"Make sure that you add the repository to the sbt resolvers:\nresolvers ++= \"Apache Pekko Snapshots\" at \"https://nightlies.apache.org/pekko/snapshots\"\nDefine the library dependencies with the complete version. For example:\nlibraryDependencies += \"org.apache.pekko\" % \"pekko-remote_2.13\" % \"2.6.14+72-53943d99-SNAPSHOT\"","title":"sbt definition of snapshot repository"},{"location":"/project/links.html#maven-definition-of-snapshot-repository","text":"Make sure that you add the repository to the Maven repositories in pom.xml:\n<repositories>\n  <repository>\n    <id>apache-pekko-snapshots</id>\n    <url>https://nightlies.apache.org/pekko/snapshots</url>\n    <layout>default</layout>\n    <snapshots>\n      <enabled>true</enabled>\n    </snapshots>\n  </repository>\n</repositories>\nDefine the library dependencies with the timestamp as version. For example:\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-remote_2.13</artifactId>\n    <version>2.6.14+72-53943d99-SNAPSHOT</version>\n  </dependency>\n</dependencies>","title":"Maven definition of snapshot repository"},{"location":"/index-classic.html","text":"","title":"Pekko Classic"},{"location":"/index-classic.html#pekko-classic","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nClassic Actors Dependency Classic Actors Classic Supervision Classic Fault Tolerance Classic Dispatchers Classic Mailboxes Classic Routing Classic FSM Classic Persistence Classic Persistent FSM Testing Classic Actors Classic Clustering Classic Cluster Usage Classic Cluster Aware Routers Classic Cluster Singleton Classic Distributed Publish Subscribe in Cluster Classic Cluster Client Classic Cluster Sharding Classic Cluster Metrics Extension Classic Distributed Data Classic Multi-DC Cluster Classic Serialization Classic Networking I/O Using TCP Using UDP DNS Extension Classic Utilities Dependency Classic Event Bus Classic Logging Classic Scheduler Classic Apache Pekko Extensions","title":"Pekko Classic"},{"location":"/index-actors.html","text":"","title":"Classic Actors"},{"location":"/index-actors.html#classic-actors","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.","title":"Classic Actors"},{"location":"/index-actors.html#dependency","text":"To use Classic Pekko Actors, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-testkit_${versions.ScalaBinary}\"\n}\nClassic Actors Module info Introduction Creating Actors Actor API Identifying Actors via Actor Selection Messages and immutability Send messages Receive messages Reply to messages Receive timeout Timers, scheduled messages Stopping actors Become/Unbecome Stash Extending Actors using PartialFunction chaining Initialization patterns Classic Supervision What Supervision Means The Top-Level Supervisors One-For-One Strategy vs. All-For-One Strategy Classic Fault Tolerance Dependency Introduction Fault Handling in Practice Creating a Supervisor Strategy Supervision of Top-Level Actors Test Application Delayed restarts for classic actors Classic Dispatchers Dependency Looking up a Dispatcher Setting the dispatcher for an Actor Classic Mailboxes Dependency Introduction Mailbox Selection Mailbox configuration examples Special Semantics of system.actorOf Classic Routing Dependency Introduction A Simple Router A Router Actor Router usage Specially Handled Messages Dynamically Resizable Pool How Routing is Designed within Pekko Custom Router Configuring Dispatchers Classic FSM Dependency Overview A Simple Example Reference Testing and Debugging Finite State Machines Classic Persistence Module info Introduction Example Snapshots Scaling out At-Least-Once Delivery Event Adapters Custom serialization Testing with LevelDB journal Configuration Multiple persistence plugin configurations Give persistence plugin configurations at runtime See also Classic Persistent FSM Dependency Migration to EventSourcedBehavior Testing Classic Actors Module info Introduction Asynchronous Testing: TestKit CallingThreadDispatcher Tracing Actor Invocations Different Testing Frameworks Configuration Example Synchronous Testing: TestActorRef","title":"Dependency"},{"location":"/actors.html","text":"","title":"Classic Actors"},{"location":"/actors.html#classic-actors","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.","title":"Classic Actors"},{"location":"/actors.html#module-info","text":"To use Classic Actors, add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-testkit_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Actors (classic) Artifact org.apache.pekko pekko-actor 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.actor License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/actors.html#introduction","text":"The Actor Model provides a higher level of abstraction for writing concurrent and distributed systems. It alleviates the developer from having to deal with explicit locking and thread management, making it easier to write correct concurrent and parallel systems. Actors were defined in the 1973 paper by Carl Hewitt but have been popularized by the Erlang language and used for example at Ericsson with great success to build highly concurrent and reliable telecom systems.\nThe API of Pekko’s Actors is similar to Scala Actors which has borrowed some of its syntax from Erlang.","title":"Introduction"},{"location":"/actors.html#creating-actors","text":"Note Since Pekko enforces parental supervision every actor is supervised and (potentially) the supervisor of its children, it is advisable to familiarize yourself with Actor Systems, supervision and handling exceptions as well as Actor References, Paths and Addresses.","title":"Creating Actors"},{"location":"/actors.html#defining-an-actor-class","text":"Actors are implemented by extending the Actor base trait and implementing the receive method. The receive method should define a series of case statements (which has the type PartialFunction[Any, Unit]) that define which messages your Actor can handle, using standard Scala pattern matching, along with the implementation of how the messages should be processed.\nActor classes are implemented by extending the AbstractActor class and setting the “initial behavior” in the createReceive method. The createReceive method has no arguments and returns AbstractActor.Receive. It defines which messages your Actor can handle, along with the implementation of how the messages should be processed. You can build such behavior with a builder named ReceiveBuilder. This build has a convenient factory in AbstractActor called receiveBuilder.\nHere is an example:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.Actor\nimport pekko.actor.Props\nimport pekko.event.Logging\n\nclass MyActor extends Actor {\n  val log = Logging(context.system, this)\n\n  def receive = {\n    case \"test\" => log.info(\"received test\")\n    case _      => log.info(\"received unknown message\")\n  }\n} Java copysourceimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\n\npublic class MyActor extends AbstractActor {\n  private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            s -> {\n              log.info(\"Received String message: {}\", s);\n            })\n        .matchAny(o -> log.info(\"received unknown message\"))\n        .build();\n  }\n}\nPlease note that the Pekko Actor receive message loop is exhaustive, which is different compared to Erlang and the late Scala Actors. This means that you need to provide a pattern match for all messages that it can accept and if you want to be able to handle unknown messages then you need to have a default case as in the example above. Otherwise an UnhandledMessageUnhandledMessage will be published to the ActorSystemActorSystem’s EventStreamEventStream.\nNote further that the return type of the behavior defined above is Unit; if the actor shall reply to the received message then this must be done explicitly as explained below.\nThe result of the receive method is a partial function object, which is createReceive method is AbstractActor.Receive which is a wrapper around partial scala function object. It is stored within the actor as its “initial behavior”, see Become/Unbecome for further information on changing the behavior of an actor after its construction.\nHere is another example that you can edit and run in the browser: import org.apache.pekko.actor.{ Actor, ActorRef, ActorSystem, PoisonPill, Props }\nimport language.postfixOps\nimport scala.concurrent.duration._\n\ncase object Ping\ncase object Pong\n\nclass Pinger extends Actor {\n  var countDown = 100\n\n  def receive = {\n    case Pong =>\n      println(s\"${self.path} received pong, count down $countDown\")\n\n      if (countDown > 0) {\n        countDown -= 1\n        sender() ! Ping\n      } else {\n        sender() ! PoisonPill\n        self ! PoisonPill\n      }\n  }\n}\n\nclass Ponger(pinger: ActorRef) extends Actor {\n  def receive = {\n    case Ping =>\n      println(s\"${self.path} received ping\")\n      pinger ! Pong\n  }\n}\n\n    val system = ActorSystem(\"pingpong\")\n\n    val pinger = system.actorOf(Props[Pinger](), \"pinger\")\n\n    val ponger = system.actorOf(Props(classOf[Ponger], pinger), \"ponger\")\n\n    import system.dispatcher\n    system.scheduler.scheduleOnce(500 millis) {\n      ponger ! Ping\n    }","title":"Defining an Actor class"},{"location":"/actors.html#props","text":"PropsProps is a configuration class to specify options for the creation of actors, think of it as an immutable and thus freely shareable recipe for creating an actor including associated deployment information (e.g., which dispatcher to use, see more below). Here are some examples of how to create a Props instance.\nScala copysourceimport org.apache.pekko.actor.Props\n\nval props1 = Props[MyActor]()\nval props2 = Props(new ActorWithArgs(\"arg\")) // careful, see below\nval props3 = Props(classOf[ActorWithArgs], \"arg\") // no support for value class arguments Java copysourceimport org.apache.pekko.actor.Props;\nProps props1 = Props.create(MyActor.class);\nProps props2 =\n    Props.create(ActorWithArgs.class, () -> new ActorWithArgs(\"arg\")); // careful, see below\nProps props3 = Props.create(ActorWithArgs.class, \"arg\");\nThe second variant shows how to pass constructor arguments to the ActorActor being created, but it should only be used outside of actors as explained below.\nThe last line shows a possibility to pass constructor arguments regardless of the context it is being used in. The presence of a matching constructor is verified during construction of the Props object, resulting in an IllegalArgumentException if no or multiple matching constructors are found.\nNote The recommended approach to create the actor PropsProps is not supported for cases when the actor constructor takes value classes as arguments.","title":"Props"},{"location":"/actors.html#dangerous-variants","text":"Scala copysource// NOT RECOMMENDED within another actor:\n// encourages to close over enclosing class\nval props7 = Props(new MyActor) Java copysource// NOT RECOMMENDED within another actor:\n// encourages to close over enclosing class\nProps props7 = Props.create(ActorWithArgs.class, () -> new ActorWithArgs(\"arg\"));\nThis method is not recommended being used within another actor because it encourages to close over the enclosing scope, resulting in non-serializable Props and possibly race conditions (breaking the actor encapsulation). On the other hand, using this variant in a Props factory in the actor’s companion object as documented under “Recommended Practices” below is completely fine.\nThere were two use-cases for these methods: passing constructor arguments to the actor—which is solved by the newly introduced Props.apply(clazz, args) Props.create(clazz, args) method above or the recommended practice below—and creating actors “on the spot” as anonymous classes. The latter should be solved by making these actors named classes instead (if they are not declared within a top-level object then the enclosing instance’s this reference needs to be passed as the first argument).\nWarning Declaring one actor within another is very dangerous and breaks actor encapsulation. Never pass an actor’s this reference into Props!\nEdge cases There are two edge cases in actor creation with actor.Props: An actor with AnyVal arguments. copysourcecase class MyValueClass(v: Int) extends AnyVal\n copysourceclass ValueActor(value: MyValueClass) extends Actor {\n  def receive = {\n    case multiplier: Long => sender() ! (value.v * multiplier)\n  }\n}\nval valueClassProp = Props(classOf[ValueActor], MyValueClass(5)) // Unsupported An actor with default constructor values. copysourceclass DefaultValueActor(a: Int, b: Int = 5) extends Actor {\n  def receive = {\n    case x: Int => sender() ! ((a + x) * b)\n  }\n}\n\nval defaultValueProp1 = Props(classOf[DefaultValueActor], 2.0) // Unsupported\n\nclass DefaultValueActor2(b: Int = 5) extends Actor {\n  def receive = {\n    case x: Int => sender() ! (x * b)\n  }\n}\nval defaultValueProp2 = Props[DefaultValueActor2]() // Unsupported\nval defaultValueProp3 = Props(classOf[DefaultValueActor2]) // Unsupported In both cases, an IllegalArgumentException will be thrown stating no matching constructor could be found. The next section explains the recommended ways to create Actor props in a way, which simultaneously safe-guards against these edge cases.","title":"Dangerous Variants"},{"location":"/actors.html#recommended-practices","text":"It is a good idea to provide factory methods on the companion object of each ActorActor static factory methods for each ActorActor which help keeping the creation of suitable Props as close to the actor definition as possible. This also avoids the pitfalls associated with using the Props.apply(...) method which takes a by-name argument, since within a companion object Props.create(...) method which takes arguments as constructor parameters, since within static method the given code block will not retain a reference to its enclosing scope:\nScala copysourceobject DemoActor {\n\n  /**\n   * Create Props for an actor of this type.\n   *\n   * @param magicNumber The magic number to be passed to this actor’s constructor.\n   * @return a Props for creating this actor, which can then be further configured\n   *         (e.g. calling `.withDispatcher()` on it)\n   */\n  def props(magicNumber: Int): Props = Props(new DemoActor(magicNumber))\n}\n\nclass DemoActor(magicNumber: Int) extends Actor {\n  def receive = {\n    case x: Int => sender() ! (x + magicNumber)\n  }\n}\n\nclass SomeOtherActor extends Actor {\n  // Props(new DemoActor(42)) would not be safe\n  context.actorOf(DemoActor.props(42), \"demo\")\n  // ...\n} Java copysourcestatic class DemoActor extends AbstractActor {\n  /**\n   * Create Props for an actor of this type.\n   *\n   * @param magicNumber The magic number to be passed to this actor’s constructor.\n   * @return a Props for creating this actor, which can then be further configured (e.g. calling\n   *     `.withDispatcher()` on it)\n   */\n  static Props props(Integer magicNumber) {\n    // You need to specify the actual type of the returned actor\n    // since Java 8 lambdas have some runtime type information erased\n    return Props.create(DemoActor.class, () -> new DemoActor(magicNumber));\n  }\n\n  private final Integer magicNumber;\n\n  public DemoActor(Integer magicNumber) {\n    this.magicNumber = magicNumber;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Integer.class,\n            i -> {\n              getSender().tell(i + magicNumber, getSelf());\n            })\n        .build();\n  }\n}\n\nstatic class SomeOtherActor extends AbstractActor {\n  // Props(new DemoActor(42)) would not be safe\n  ActorRef demoActor = getContext().actorOf(DemoActor.props(42), \"demo\");\n  // ...\n}\nAnother good practice is to declare what messages an Actor can receive in the companion object of the Actor as close to the actor definition as possible (e.g. as static classes inside the Actor or using other suitable class), which makes easier to know what it can receive:\nScala copysourceobject MyActor {\n  case class Greeting(from: String)\n  case object Goodbye\n}\nclass MyActor extends Actor with ActorLogging {\n  import MyActor._\n  def receive = {\n    case Greeting(greeter) => log.info(s\"I was greeted by $greeter.\")\n    case Goodbye           => log.info(\"Someone said goodbye to me.\")\n  }\n} Java copysourcestatic class DemoMessagesActor extends AbstractLoggingActor {\n\n  public static class Greeting {\n    private final String from;\n\n    public Greeting(String from) {\n      this.from = from;\n    }\n\n    public String getGreeter() {\n      return from;\n    }\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Greeting.class,\n            g -> {\n              log().info(\"I was greeted by {}\", g.getGreeter());\n            })\n        .build();\n  }\n}","title":"Recommended Practices"},{"location":"/actors.html#creating-actors-with-props","text":"Actors are created by passing a PropsProps instance into the actorOf factory method which is available on ActorSystemActorSystem and ActorContextActorContext.\nScala copysourceimport org.apache.pekko.actor.ActorSystem\n\n// ActorSystem is a heavy object: create only one per application\nval system = ActorSystem(\"mySystem\")\nval myActor = system.actorOf(Props[MyActor](), \"myactor2\") Java copysourceimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.ActorSystem;\nUsing the ActorSystem will create top-level actors, supervised by the actor system’s provided guardian actor while using an actor’s context will create a child actor.\nScala copysourceclass FirstActor extends Actor {\n  val child = context.actorOf(Props[MyActor](), name = \"myChild\")\n  def receive = {\n    case x => sender() ! x\n  }\n} Java copysourcestatic class FirstActor extends AbstractActor {\n  final ActorRef child = getContext().actorOf(Props.create(MyActor.class), \"myChild\");\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().matchAny(x -> getSender().tell(x, getSelf())).build();\n  }\n}\nIt is recommended to create a hierarchy of children, grand-children and so on such that it fits the logical failure-handling structure of the application, see Actor Systems.\nThe call to actorOf returns an instance of ActorRefActorRef. This is a handle to the actor instance and the only way to interact with it. The ActorRef is immutable and has a one to one relationship with the Actor it represents. The ActorRef is also serializable and network-aware. This means that you can serialize it, send it over the wire and use it on a remote host, and it will still be representing the same Actor on the original node, across the network.\nThe name parameter is optional, but you should preferably name your actors, since that is used in log messages and for identifying actors. The name must not be empty or start with $, but it may contain URL encoded characters (eg., %20 for a blank space). If the given name is already in use by another child to the same parent an InvalidActorNameExceptionInvalidActorNameException is thrown.\nActors are automatically started asynchronously when created.\nValue classes as constructor arguments The recommended way to instantiate actor props uses reflection at runtime to determine the correct actor constructor to be invoked and due to technical limitations it is not supported when said constructor takes arguments that are value classes. In these cases you should either unpack the arguments or create the props by calling the constructor manually: copysourceclass Argument(val value: String) extends AnyVal\nclass ValueClassActor(arg: Argument) extends Actor {\n  def receive = { case _ => () }\n}\n\nobject ValueClassActor {\n  def props1(arg: Argument) = Props(classOf[ValueClassActor], arg) // fails at runtime\n  def props2(arg: Argument) = Props(classOf[ValueClassActor], arg.value) // ok\n  def props3(arg: Argument) = Props(new ValueClassActor(arg)) // ok\n}","title":"Creating Actors with Props"},{"location":"/actors.html#dependency-injection","text":"If your ActorActor has a constructor that takes parameters then those need to be part of the PropsProps as well, as described above. But there are cases when a factory method must be used, for example when the actual constructor arguments are determined by a dependency injection framework.\nScala copysourceimport org.apache.pekko.actor.IndirectActorProducer\n\nclass DependencyInjector(applicationContext: AnyRef, beanName: String) extends IndirectActorProducer {\n\n  override def actorClass = classOf[Actor]\n  override def produce() =\n    new Echo(beanName)\n\n  def this(beanName: String) = this(\"\", beanName)\n}\n\nval actorRef = system.actorOf(Props(classOf[DependencyInjector], applicationContext, \"hello\"), \"helloBean\") Java copysourceimport org.apache.pekko.actor.Actor;\nimport org.apache.pekko.actor.IndirectActorProducer;\nclass DependencyInjector implements IndirectActorProducer {\n  final Object applicationContext;\n  final String beanName;\n\n  public DependencyInjector(Object applicationContext, String beanName) {\n    this.applicationContext = applicationContext;\n    this.beanName = beanName;\n  }\n\n  @Override\n  public Class<? extends Actor> actorClass() {\n    return TheActor.class;\n  }\n\n  @Override\n  public TheActor produce() {\n    TheActor result;\n    result = new TheActor((String) applicationContext);\n    return result;\n  }\n}\n\n  final ActorRef myActor =\n      getContext()\n          .actorOf(\n              Props.create(DependencyInjector.class, applicationContext, \"TheActor\"), \"TheActor\");\nWarning You might be tempted at times to offer an IndirectActorProducerIndirectActorProducer which always returns the same instance, e.g. by using a lazy val. static field. This is not supported, as it goes against the meaning of an actor restart, which is described here: What Restarting Means. When using a dependency injection framework, actor beans MUST NOT have singleton scope. Actor API The ActorActor trait defines only one abstract method, the above mentioned receive, which implements the behavior of the actor. The AbstractActorAbstractActor class defines a method called createReceive, that is used to set the “initial behavior” of the actor. If the current actor behavior does not match a received message, unhandled is called, which by default publishes an actor.UnhandledMessage(message, sender, recipient)actor.UnhandledMessage(message, sender, recipient) on the actor system’s event stream (set configuration item pekko.actor.debug.unhandled to on to have them converted into actual Debug messages). In addition, it offers: self getSelf() reference to the ActorRefActorRef of the actor sender getSender() reference sender Actor of the last received message, typically used as described in Actor.Reply LambdaActor.Reply supervisorStrategy supervisorStrategy() user overridable definition the strategy to use for supervising child actors This strategy is typically declared inside the actor to have access to the actor’s internal state within the decider function: since failure is communicated as a message sent to the supervisor and processed like other messages (albeit outside the normal behavior), all values and variables within the actor are available, as is the sender reference (which will be the immediate child reporting the failure; if the original failure occurred within a distant descendant it is still reported one level up at a time). context getContext() exposes contextual information for the actor and the current message, such as: factory methods to create child actors (actorOf) system that the actor belongs to parent supervisor supervised children lifecycle monitoring hotswap behavior stack as described in Actor.HotSwap Become/Unbecome @@@ div { .group-scala } You can import the members in the context to avoid prefixing access with context. copysourceclass FirstActor extends Actor {\n  import context._\n  val myActor = actorOf(Props[MyActor](), name = \"myactor\")\n  def receive = {\n    case x => myActor ! x\n  }\n}\nThe remaining visible methods are user-overridable life-cycle hooks which are described in the following:\nScala copysourcedef preStart(): Unit = ()\n\ndef postStop(): Unit = ()\n\ndef preRestart(@unused reason: Throwable, @unused message: Option[Any]): Unit = {\n  context.children.foreach { child =>\n    context.unwatch(child)\n    context.stop(child)\n  }\n  postStop()\n}\n\ndef postRestart(@unused reason: Throwable): Unit = {\n  preStart()\n} Java copysourcepublic void preStart() {}\n\npublic void preRestart(Throwable reason, Optional<Object> message) {\n  for (ActorRef each : getContext().getChildren()) {\n    getContext().unwatch(each);\n    getContext().stop(each);\n  }\n  postStop();\n}\n\npublic void postRestart(Throwable reason) {\n  preStart();\n}\n\npublic void postStop() {}\nThe implementations shown above are the defaults provided by the Actor trait. AbstractActor class.","title":"Dependency Injection"},{"location":"/actors.html#actor-lifecycle","text":"A path in an actor system represents a “place” that might be occupied by a living actor. Initially (apart from system initialized actors) a path is empty. When actorOf() is called it assigns an incarnation of the actor described by the passed PropsProps to the given path. An actor incarnation is identified by the path and a UID.\nIt is worth noting about the difference between:\nrestart stop, followed by a re-creation of the actor\nas explained below.\nA restart only swaps the ActorActor instance defined by the PropsProps but the incarnation and hence the UID remains the same. As long as the incarnation is the same, you can keep using the same ActorRefActorRef. Restart is handled by the Supervision Strategy of actor’s parent actor, and there is more discussion about what restart means.\nThe lifecycle of an incarnation ends when the actor is stopped. At that point, the appropriate lifecycle events are called and watching actors are notified of the termination. After the incarnation is stopped, the path can be reused again by creating an actor with actorOf(). In this case, the name of the new incarnation will be the same as the previous one, but the UIDs will differ. An actor can be stopped by the actor itself, another actor or the ActorSystemActorSystem (see Stopping actors).\nNote It is important to note that Actors do not stop automatically when no longer referenced, every Actor that is created must also explicitly be destroyed. The only simplification is that stopping a parent Actor will also recursively stop all the child Actors that this parent has created.\nAn ActorRefActorRef always represents an incarnation (path and UID) not just a given path. Therefore, if an actor is stopped, and a new one with the same name is created then an ActorRef of the old incarnation will not point to the new one.\nActorSelectionActorSelection on the other hand points to the path (or multiple paths if wildcards are used) and is completely oblivious to which incarnation is currently occupying it. ActorSelection cannot be watched for this reason. It is possible to resolve the current incarnation’s ActorRef living under the path by sending an IdentifyIdentify message to the ActorSelection which will be replied to with an ActorIdentityActorIdentity containing the correct reference (see ActorSelection). This can also be done with the resolveOneresolveOne method of the ActorSelection, which returns a FutureCompletionStage of the matching ActorRef.","title":"Actor Lifecycle"},{"location":"/actors.html#lifecycle-monitoring-aka-deathwatch","text":"To be notified when another actor terminates (i.e., stops permanently, not a temporary failure and restart), an actor may register itself for reception of the TerminatedTerminated message dispatched by the other actor upon termination (see Stopping Actors). This service is provided by the DeathWatch component of the actor system.\nRegistering a monitor is easy:\nScala copysourceimport org.apache.pekko.actor.{ Actor, Props, Terminated }\n\nclass WatchActor extends Actor {\n  val child = context.actorOf(Props.empty, \"child\")\n  context.watch(child) // <-- this is the only call needed for registration\n  var lastSender = context.system.deadLetters\n\n  def receive = {\n    case \"kill\" =>\n      context.stop(child)\n      lastSender = sender()\n    case Terminated(`child`) =>\n      lastSender ! \"finished\"\n  }\n} Java copysourceimport org.apache.pekko.actor.Terminated;\nstatic class WatchActor extends AbstractActor {\n  private final ActorRef child = getContext().actorOf(Props.empty(), \"target\");\n  private ActorRef lastSender = system.deadLetters();\n\n  public WatchActor() {\n    getContext().watch(child); // <-- this is the only call needed for registration\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            \"kill\",\n            s -> {\n              getContext().stop(child);\n              lastSender = getSender();\n            })\n        .match(\n            Terminated.class,\n            t -> t.actor().equals(child),\n            t -> {\n              lastSender.tell(\"finished\", getSelf());\n            })\n        .build();\n  }\n}\nIt should be noted that the TerminatedTerminated message is generated independently of the order in which registration and termination occur. In particular, the watching actor will receive a Terminated message even if the watched actor has already been terminated at the time of registration.\nRegistering multiple times does not necessarily lead to multiple messages being generated, but there is no guarantee that only exactly one such message is received: if termination of the watched actor has generated and queued the message, and another registration is done before this message has been processed, then a second message will be queued because registering for monitoring of an already terminated actor leads to the immediate generation of the Terminated message.\nIt is also possible to deregister from watching another actor’s liveliness using context.unwatch(target)context.unwatch(target). This works even if the Terminated message has already been enqueued in the mailbox; after calling unwatch no Terminated message for that actor will be processed anymore.","title":"Lifecycle Monitoring aka DeathWatch"},{"location":"/actors.html#start-hook","text":"Right after starting the actor, its preStartpreStart method is invoked.\nScala copysourceoverride def preStart(): Unit = {\n  child = context.actorOf(Props[MyActor](), \"child\")\n} Java copysource@Override\npublic void preStart() {\n  target = getContext().actorOf(Props.create(MyActor.class, \"target\"));\n}\nThis method is called when the actor is first created. During restarts, it is called by the default implementation of postRestartpostRestart, which means that by overriding that method you can choose whether the initialization code in this method is called only exactly once for this actor or every restart. Initialization code which is part of the actor’s constructor will always be called when an instance of the actor class is created, which happens at every restart.","title":"Start Hook"},{"location":"/actors.html#restart-hooks","text":"All actors are supervised, i.e., linked to another actor with a fault handling strategy. Actors may be restarted in case an exception is thrown while processing a message (see supervision). This restart involves the hooks mentioned above:\nThe old actor is informed by calling preRestartpreRestart with the exception which caused the restart, and the message which triggered that exception; the latter may be None if the restart was not caused by processing a message, e.g. when a supervisor does not trap the exception and is restarted in turn by its supervisor, or if an actor is restarted due to a sibling’s failure. If the message is available, then that message’s sender is also accessible in the usual way (i.e. by calling sender getSender()). This method is the best place for cleaning up, preparing hand-over to the fresh actor instance, etc. By default, it stops all children and calls postStoppostStop. The initial factory from the actorOf call is used to produce the fresh instance. The new actor’s postRestart method is invoked with the exception which caused the restart. By default the preStart is called, just as in the normal start-up case.\nAn actor restart replaces only the actual actor object; the contents of the mailbox are unaffected by the restart, so the processing of messages will resume after the postRestart hook returns. The message that triggered the exception will not be received again. Any message sent to an actor while it is being restarted will be queued to its mailbox as usual.\nWarning Be aware that the ordering of failure notifications relative to user messages is not deterministic. In particular, a parent might restart its child before it has processed the last messages sent by the child before the failure. See Discussion: Message Ordering for details.","title":"Restart Hooks"},{"location":"/actors.html#stop-hook","text":"After stopping an actor, its postStoppostStop hook is called, which may be used e.g. for deregistering this actor from other services. This hook is guaranteed to run after message queuing has been disabled for this actor, i.e. messages sent to a stopped actor will be redirected to the deadLettersdeadLetters of the ActorSystemActorSystem.","title":"Stop Hook"},{"location":"/actors.html#identifying-actors-via-actor-selection","text":"As described in Actor References, Paths and Addresses, each actor has a unique logical path, which is obtained by following the chain of actors from child to parent until reaching the root of the actor system, and it has a physical path, which may differ if the supervision chain includes any remote supervisors. These paths are used by the system to look up actors, e.g. when a remote message is received and the recipient is searched, but they are also useful more directly: actors may look up other actors by specifying absolute or relative paths—logical or physical—and receive back an ActorSelectionActorSelection with the result:\nScala copysource// will look up this absolute path\ncontext.actorSelection(\"/user/serviceA/aggregator\")\n// will look up sibling beneath same supervisor\ncontext.actorSelection(\"../joe\") Java copysource// will look up this absolute path\ngetContext().actorSelection(\"/user/serviceA/actor\");\n// will look up sibling beneath same supervisor\ngetContext().actorSelection(\"../joe\");\nNote It is always preferable to communicate with other Actors using their ActorRef instead of relying upon ActorSelection. Exceptions are sending messages using the At-Least-Once Delivery facility initiating the first contact with a remote system In all other cases, ActorRefs can be provided during Actor creation or initialization, passing them from parent to child or introducing Actors by sending their ActorRefs to other Actors within messages.\nThe supplied path is parsed as a java.net.URI, which means that it is split on / into path elements. If the path starts with /, it is absolute and the look-up starts at the root guardian (which is the parent of \"/user\"); otherwise it starts at the current actor. If a path element equals .., the look-up will take a step “up” towards the supervisor of the currently traversed actor, otherwise it will step “down” to the named child. It should be noted that the .. in actor paths here always means the logical structure, i.e. the supervisor.\nThe path elements of an actor selection may contain wildcard patterns allowing for broadcasting of messages to that section:\nScala copysource// will look all children to serviceB with names starting with worker\ncontext.actorSelection(\"/user/serviceB/worker*\")\n// will look up all siblings beneath same supervisor\ncontext.actorSelection(\"../*\") Java copysource// will look all children to serviceB with names starting with worker\ngetContext().actorSelection(\"/user/serviceB/worker*\");\n// will look up all siblings beneath same supervisor\ngetContext().actorSelection(\"../*\");\nMessages can be sent via the ActorSelectionActorSelection and the path of the ActorSelection is looked up when delivering each message. If the selection does not match any actors the message will be dropped.\nTo acquire an ActorRefActorRef for an ActorSelection you need to send a message to the selection and use the sender getSender()) reference of the reply from the actor. There is a built-in IdentifyIdentify message that all Actors will understand and automatically reply to with an ActorIdentityActorIdentity message containing the ActorRefActorRef. This message is handled specially by the actors which are traversed in the sense that if a concrete name lookup fails (i.e. a non-wildcard path element does not correspond to a live actor) then a negative result is generated. Please note that this does not mean that delivery of that reply is guaranteed, it still is a normal message.\nScala copysourceimport org.apache.pekko.actor.{ Actor, ActorIdentity, Identify, Props, Terminated }\n\nclass Follower extends Actor {\n  val identifyId = 1\n  context.actorSelection(\"/user/another\") ! Identify(identifyId)\n\n  def receive = {\n    case ActorIdentity(`identifyId`, Some(ref)) =>\n      context.watch(ref)\n      context.become(active(ref))\n    case ActorIdentity(`identifyId`, None) => context.stop(self)\n\n  }\n\n  def active(another: ActorRef): Actor.Receive = {\n    case Terminated(`another`) => context.stop(self)\n  }\n} Java copysourceimport org.apache.pekko.actor.ActorIdentity;\nimport org.apache.pekko.actor.ActorSelection;\nimport org.apache.pekko.actor.Identify;\nstatic class Follower extends AbstractActor {\n  final Integer identifyId = 1;\n\n  public Follower() {\n    ActorSelection selection = getContext().actorSelection(\"/user/another\");\n    selection.tell(new Identify(identifyId), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            ActorIdentity.class,\n            id -> id.getActorRef().isPresent(),\n            id -> {\n              ActorRef ref = id.getActorRef().get();\n              getContext().watch(ref);\n              getContext().become(active(ref));\n            })\n        .match(\n            ActorIdentity.class,\n            id -> !id.getActorRef().isPresent(),\n            id -> {\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n\n  final AbstractActor.Receive active(final ActorRef another) {\n    return receiveBuilder()\n        .match(\n            Terminated.class, t -> t.actor().equals(another), t -> getContext().stop(getSelf()))\n        .build();\n  }\n}\nYou can also acquire an ActorRef for an ActorSelection with the resolveOneresolveOne method of the ActorSelection. It returns a FutureCompletionStage of the matching ActorRef if such an actor exists. It is completed with failure ActorNotFoundActorNotFound if no such actor exists or the identification didn’t complete within the supplied timeout.\nRemote actor addresses may also be looked up, if remoting is enabled:\nScala copysourcecontext.actorSelection(\"pekko://app@otherhost:1234/user/serviceB\") Java copysourcegetContext().actorSelection(\"pekko://app@otherhost:1234/user/serviceB\");\nAn example demonstrating actor look-up is given in Remoting Sample.","title":"Identifying Actors via Actor Selection"},{"location":"/actors.html#messages-and-immutability","text":"IMPORTANT Messages can be any kind of object but have to be immutable. Scala Pekko can’t enforce immutability (yet) so this has to be by convention. Primitives like String, Int, Boolean are always immutable. Apart from these the recommended approach is to use Scala case classes that are immutable (if you don’t explicitly expose the state) and works great with pattern matching at the receiver side.\nHere is an example: example of an immutable message:\nScala copysourcecase class User(name: String)\n\n// define the case class\ncase class Register(user: User)\nval user = User(\"Mike\")\n// create a new case class message\nval message = Register(user) Java copysourcepublic class ImmutableMessage {\n  private final int sequenceNumber;\n  private final List<String> values;\n\n  public ImmutableMessage(int sequenceNumber, List<String> values) {\n    this.sequenceNumber = sequenceNumber;\n    this.values = Collections.unmodifiableList(new ArrayList<String>(values));\n  }\n\n  public int getSequenceNumber() {\n    return sequenceNumber;\n  }\n\n  public List<String> getValues() {\n    return values;\n  }\n}","title":"Messages and immutability"},{"location":"/actors.html#send-messages","text":"Messages are sent to an Actor through one of the following methods.\n! tell means “fire-and-forget”, e.g. send a message asynchronously and return immediately. Also known as tell. ? ask sends a message asynchronously and returns a FutureCompletionStage representing a possible reply. Also known as ask.\nMessage ordering is guaranteed on a per-sender basis.\nNote There are performance implications of using ask since something needs to keep track of when it times out, there needs to be something that bridges a Promise into an ActorRefActorRef and it also needs to be reachable through remoting. So always prefer tell for performance, and only ask if you must.\nIn all these methods you have the option of passing along your own ActorRefActorRef. Make it a practice of doing so because it will allow the receiver actors to be able to respond to your message since the sender reference is sent along with the message.","title":"Send messages"},{"location":"/actors.html#tell-fire-forget","text":"This is the preferred way of sending messages. No blocking waiting for a message. This gives the best concurrency and scalability characteristics.\nScala copysourceactorRef ! message Java copysource// don’t forget to think about who is the sender (2nd argument)\ntarget.tell(message, getSelf());\nIf invoked from within an Actor, then the sending actor reference will be implicitly passed along with the message and available to the receiving Actor in its sender(): ActorRef member method. The target actor can use this to reply to the original sender, by using sender() ! replyMsg. If invoked from an instance that is not an Actor the sender will be deadLetters actor reference by default.\nThe sender reference is passed along with the message and available within the receiving actor via its getSender() method while processing this message. Inside of an actor it is usually getSelf() who shall be the sender, but there can be cases where replies shall be routed to some other actor—e.g. the parent—in which the second argument to tell would be a different one. Outside of an actor and if no reply is needed the second argument can be null; if a reply is needed outside of an actor you can use the ask-pattern described next.","title":"Tell: Fire-forget"},{"location":"/actors.html#ask-send-and-receive-future","text":"The ask pattern involves actors as well as futures, hence it is offered as a use pattern rather than a method on ActorRefActorRef:\nScala copysourceimport org.apache.pekko.pattern.{ ask, pipe }\nimport system.dispatcher // The ExecutionContext that will be used\nfinal case class Result(x: Int, s: String, d: Double)\ncase object Request\n\nimplicit val timeout: Timeout = 5.seconds // needed for `?` below\n\nval f: Future[Result] =\n  for {\n    x <- ask(actorA, Request).mapTo[Int] // call pattern directly\n    s <- actorB.ask(Request).mapTo[String] // call by implicit conversion\n    d <- (actorC ? Request).mapTo[Double] // call by symbolic name\n  } yield Result(x, s, d)\n\nf.pipeTo(actorD) // .. or ..\npipe(f) to actorD Java copysourceimport static org.apache.pekko.pattern.Patterns.ask;\nimport static org.apache.pekko.pattern.Patterns.pipe;\n\nimport java.util.concurrent.CompletableFuture;\nfinal Duration t = Duration.ofSeconds(5);\n\n// using 1000ms timeout\nCompletableFuture<Object> future1 =\n    ask(actorA, \"request\", Duration.ofMillis(1000)).toCompletableFuture();\n\n// using timeout from above\nCompletableFuture<Object> future2 = ask(actorB, \"another request\", t).toCompletableFuture();\n\nCompletableFuture<Result> transformed =\n    future1.thenCombine(future2, (x, s) -> new Result((String) x, (String) s));\n\npipe(transformed, system.dispatcher()).to(actorC);\nThis example demonstrates ask together with the pipeTopipeTo pattern on futures, because this is likely to be a common combination. Please note that all of the above is completely non-blocking and asynchronous: ask produces a FutureCompletionStage, three two of which are composed into a new future using the for-comprehension and then pipeTo installs an onComplete-handler on the Future to affect CompletableFuture.allOf and thenApply methods and then pipe installs a handler on the CompletionStage to effect the submission of the aggregated Result to another actor.\nUsing ask will send a message to the receiving Actor as with tell, and the receiving actor must reply with sender() ! reply getSender().tell(reply, getSelf()) in order to complete the returned FutureCompletionStage with a value. The ask operation involves creating an internal actor for handling this reply, which needs to have a timeout after which it is destroyed in order not to leak resources; see more below.\nWarning To complete the FutureCompletionStage with an exception you need to send an Status.FailureStatus.Failure message to the sender. This is not done automatically when an actor throws an exception while processing a message. Please note that Scala’s Try sub types scala.util.Failure and scala.util.Success are not treated especially, and would complete the ask FutureCompletionStage with the given value - only the actor.Status messages are treated specially by the ask pattern.\nScala copysourcetry {\n  val result = operation()\n  sender() ! result\n} catch {\n  case e: Exception =>\n    sender() ! pekko.actor.Status.Failure(e)\n    throw e\n} Java copysourcetry {\n  String result = operation();\n  getSender().tell(result, getSelf());\n} catch (Exception e) {\n  getSender().tell(new org.apache.pekko.actor.Status.Failure(e), getSelf());\n  throw e;\n}\nIf the actor does not complete the FutureCompletionStage, it will expire after the timeout period, completing it with an AskTimeoutException. The timeout is taken from one of the following locations in order of precedence: specified as parameter to the ask method; this will complete the CompletionStage with an AskTimeoutException.\nexplicitly given timeout as in: copysourceimport scala.concurrent.duration._\nimport org.apache.pekko.pattern.ask\nval future = myActor.ask(\"hello\")(5 seconds) implicit argument of type org.apache.pekko.util.Timeout, e.g. copysourceimport scala.concurrent.duration._\nimport org.apache.pekko\nimport pekko.util.Timeout\nimport pekko.pattern.ask\nimplicit val timeout: Timeout = 5.seconds\nval future = myActor ? \"hello\"\nThe onComplete method of the FuturethenRun method of the CompletionStage can be used to register a callback to get a notification when the FutureCompletionStage completes, giving you a way to avoid blocking.\nWarning When using future callbacks, such as onComplete, or mapsuch as thenRun, or thenApply inside actors you need to carefully avoid closing over the containing actor’s reference, i.e. do not call methods or access mutable state on the enclosing actor from within the callback. This would break the actor encapsulation and may introduce synchronization bugs and race conditions because the callback will be scheduled concurrently to the enclosing actor. Unfortunately, there is not yet a way to detect these illegal accesses at compile time. See also: Actors and shared mutable state","title":"Ask: Send-And-Receive-Future"},{"location":"/actors.html#forward-message","text":"You can forward a message from one actor to another. This means that the original sender address/reference is maintained even though the message is going through a ‘mediator’. This can be useful when writing actors that work as routers, load-balancers, replicators etc.\nScala copysourcetarget.forward(message) Java copysourcetarget.forward(result, getContext());","title":"Forward message"},{"location":"/actors.html#receive-messages","text":"An Actor has to implement the receive method to receive messages: define its initial receive behavior by implementing the createReceive method in the AbstractActor:\nScala copysourcetype Receive = PartialFunction[Any, Unit]\n\ndef receive: Actor.Receive Java copysource@Override\npublic Receive createReceive() {\n  return receiveBuilder().match(String.class, s -> System.out.println(s.toLowerCase())).build();\n}\nThis method returns a PartialFunction, e.g. a ‘match/case’ clause in which the message can be matched against the different case clauses using Scala pattern matching. Here is an example:\nThe return type is AbstractActor.Receive that defines which messages your Actor can handle, along with the implementation of how the messages should be processed. You can build such behavior with a builder named receiveBuilder. Here is an example:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.Actor\nimport pekko.actor.Props\nimport pekko.event.Logging\n\nclass MyActor extends Actor {\n  val log = Logging(context.system, this)\n\n  def receive = {\n    case \"test\" => log.info(\"received test\")\n    case _      => log.info(\"received unknown message\")\n  }\n} Java copysourceimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\n\npublic class MyActor extends AbstractActor {\n  private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            s -> {\n              log.info(\"Received String message: {}\", s);\n            })\n        .matchAny(o -> log.info(\"received unknown message\"))\n        .build();\n  }\n}\nIn case you want to provide many match cases but want to avoid creating a long call trail, you can split the creation of the builder into multiple statements as in the example: copysourceimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\nimport org.apache.pekko.japi.pf.ReceiveBuilder;\n\npublic class GraduallyBuiltActor extends AbstractActor {\n  private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  @Override\n  public Receive createReceive() {\n    ReceiveBuilder builder = ReceiveBuilder.create();\n\n    builder.match(\n        String.class,\n        s -> {\n          log.info(\"Received String message: {}\", s);\n        });\n\n    // do some other stuff in between\n\n    builder.matchAny(o -> log.info(\"received unknown message\"));\n\n    return builder.build();\n  }\n} Using small methods is a good practice, also in actors. It’s recommended to delegate the actual work of the message processing to methods instead of defining a huge ReceiveBuilder with lots of code in each lambda. A well-structured actor can look like this: copysourcestatic class WellStructuredActor extends AbstractActor {\n\n  public static class Msg1 {}\n\n  public static class Msg2 {}\n\n  public static class Msg3 {}\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(Msg1.class, this::receiveMsg1)\n        .match(Msg2.class, this::receiveMsg2)\n        .match(Msg3.class, this::receiveMsg3)\n        .build();\n  }\n\n  private void receiveMsg1(Msg1 msg) {\n    // actual work\n  }\n\n  private void receiveMsg2(Msg2 msg) {\n    // actual work\n  }\n\n  private void receiveMsg3(Msg3 msg) {\n    // actual work\n  }\n} That has benefits such as: easier to see what kind of messages the actor can handle readable stack traces in case of exceptions works better with performance profiling tools Java HotSpot has a better opportunity for making optimizations The Receive can be implemented in other ways than using the ReceiveBuilder since in the end, it is just a wrapper around a Scala PartialFunction. In Java, you can implement PartialFunction by extending AbstractPartialFunction. For example, one could implement an adapter to Vavr Pattern Matching DSL. See the Pekko Vavr sample project for more details. If the validation of the ReceiveBuilder match logic turns out to be a bottleneck for some of your actors you can consider implementing it at a lower level by extending UntypedAbstractActor instead of AbstractActor. The partial functions created by the ReceiveBuilder consist of multiple lambda expressions for every match statement, where each lambda is referencing the code to be run. This is something that the JVM can have problems optimizing and the resulting code might not be as performant as the untyped version. When extending UntypedAbstractActor each message is received as an untyped Object and you have to inspect and cast it to the actual message type in other ways, like this: copysourcestatic class OptimizedActor extends UntypedAbstractActor {\n\n  public static class Msg1 {}\n\n  public static class Msg2 {}\n\n  public static class Msg3 {}\n\n  @Override\n  public void onReceive(Object msg) throws Exception {\n    if (msg instanceof Msg1) receiveMsg1((Msg1) msg);\n    else if (msg instanceof Msg2) receiveMsg2((Msg2) msg);\n    else if (msg instanceof Msg3) receiveMsg3((Msg3) msg);\n    else unhandled(msg);\n  }\n\n  private void receiveMsg1(Msg1 msg) {\n    // actual work\n  }\n\n  private void receiveMsg2(Msg2 msg) {\n    // actual work\n  }\n\n  private void receiveMsg3(Msg3 msg) {\n    // actual work\n  }\n}","title":"Receive messages"},{"location":"/actors.html#reply-to-messages","text":"If you want to have a handle for replying to a message, you can use sender getSender(), which gives you an ActorRef. You can reply by sending to that ActorRef with sender() ! replyMsg. getSender().tell(replyMsg, getSelf()). You can also store the ActorRef for replying later, or passing it on to other actors. If there is no sender (a message was sent without an actor or future context) then the sender defaults to a ‘dead-letter’ actor ref.\nScala copysourcesender() ! x // replies will go to this actor Java copysourcegetSender().tell(s, getSelf());","title":"Reply to messages"},{"location":"/actors.html#receive-timeout","text":"The ActorContext.setReceiveTimeoutActorContext.setReceiveTimeout defines the inactivity timeout after which the sending of a ReceiveTimeoutReceiveTimeout message is triggered. When specified, the receive function should be able to handle an org.apache.pekko.actor.ReceiveTimeout message. 1 millisecond is the minimum supported timeout.\nPlease note that the receive timeout might fire and enqueue the ReceiveTimeout message right after another message was enqueued; hence it is not guaranteed that upon reception of the receive timeout there must have been an idle period beforehand as configured via this method.\nOnce set, the receive timeout stays in effect (i.e. continues firing repeatedly after inactivity periods).\nTo cancel the sending of receive timeout notifications, use cancelReceiveTimeout.\nScala copysourceimport org.apache.pekko.actor.ReceiveTimeout\nimport scala.concurrent.duration._\nclass MyActor extends Actor {\n  // To set an initial delay\n  context.setReceiveTimeout(30 milliseconds)\n  def receive = {\n    case \"Hello\" =>\n      // To set in a response to a message\n      context.setReceiveTimeout(100 milliseconds)\n    case ReceiveTimeout =>\n      // To turn it off\n      context.setReceiveTimeout(Duration.Undefined)\n      throw new RuntimeException(\"Receive timed out\")\n  }\n} Java copysourcestatic class ReceiveTimeoutActor extends AbstractActor {\n  public ReceiveTimeoutActor() {\n    // To set an initial delay\n    getContext().setReceiveTimeout(Duration.ofSeconds(10));\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            \"Hello\",\n            s -> {\n              // To set in a response to a message\n              getContext().setReceiveTimeout(Duration.ofSeconds(1));\n            })\n        .match(\n            ReceiveTimeout.class,\n            r -> {\n              // To turn it off\n              getContext().cancelReceiveTimeout();\n            })\n        .build();\n  }\n}\nMessages marked with NotInfluenceReceiveTimeoutNotInfluenceReceiveTimeout will not reset the timer. This can be useful when ReceiveTimeoutReceiveTimeout should be fired by external inactivity but not influenced by internal activity, e.g. scheduled tick messages.","title":"Receive timeout"},{"location":"/actors.html#timers-scheduled-messages","text":"Messages can be scheduled to be sent at a later point by using the Scheduler directly, but when scheduling periodic or single messages in an actor to itself it’s more convenient and safe to use the support for named timers. The lifecycle of scheduled messages can be difficult to manage when the actor is restarted and that is taken care of by the timers.\nScala copysourceimport scala.concurrent.duration._\n\nimport org.apache.pekko\nimport pekko.actor.Actor\nimport pekko.actor.Timers\n\nobject MyActor {\n  private case object TickKey\n  private case object FirstTick\n  private case object Tick\n}\n\nclass MyActor extends Actor with Timers {\n  import MyActor._\n  timers.startSingleTimer(TickKey, FirstTick, 500.millis)\n\n  def receive = {\n    case FirstTick =>\n      // do something useful here\n      timers.startTimerWithFixedDelay(TickKey, Tick, 1.second)\n    case Tick =>\n    // do something useful here\n  }\n} Java copysourceimport java.time.Duration;\nimport org.apache.pekko.actor.AbstractActorWithTimers;\n\nstatic class MyActor extends AbstractActorWithTimers {\n\n  private static Object TICK_KEY = \"TickKey\";\n\n  private static final class FirstTick {}\n\n  private static final class Tick {}\n\n  public MyActor() {\n    getTimers().startSingleTimer(TICK_KEY, new FirstTick(), Duration.ofMillis(500));\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            FirstTick.class,\n            message -> {\n              // do something useful here\n              getTimers().startTimerWithFixedDelay(TICK_KEY, new Tick(), Duration.ofSeconds(1));\n            })\n        .match(\n            Tick.class,\n            message -> {\n              // do something useful here\n            })\n        .build();\n  }\n}\nThe Scheduler documentation describes the difference between fixed-delay and fixed-rate scheduling. If you are uncertain of which one to use you should pick startTimerWithFixedDelaystartTimerWithFixedDelay.\nEach timer has a key and can be replaced or cancelled. It’s guaranteed that a message from the previous incarnation of the timer with the same key is not received, even though it might already be enqueued in the mailbox when it was cancelled or the new timer was started.\nThe timers are bound to the lifecycle of the actor that owns it and thus are cancelled automatically when it is restarted or stopped. Note that the TimerSchedulerTimerScheduler is not thread-safe, i.e. it must only be used within the actor that owns it.","title":"Timers, scheduled messages"},{"location":"/actors.html#stopping-actors","text":"Actors are stopped by invoking the stop method of a ActorRefFactoryActorRefFactory, i.e. ActorContextActorContext or ActorSystemActorSystem. Typically the context is used for stopping the actor itself or child actors and the system for stopping top-level actors. The actual termination of the actor is performed asynchronously, i.e. stop may return before the actor is stopped.\nScala copysourceclass MyActor extends Actor {\n\n  val child: ActorRef = ???\n\n  def receive = {\n    case \"interrupt-child\" =>\n      context.stop(child)\n\n    case \"done\" =>\n      context.stop(self)\n  }\n\n}\n Java copysourceimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.AbstractActor;\n\npublic class MyStoppingActor extends AbstractActor {\n\n  ActorRef child = null;\n\n  // ... creation of child ...\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"interrupt-child\", m -> getContext().stop(child))\n        .matchEquals(\"done\", m -> getContext().stop(getSelf()))\n        .build();\n  }\n}\nProcessing of the current message, if any, will continue before the actor is stopped, but additional messages in the mailbox will not be processed. By default, these messages are sent to the deadLettersdeadLetters of the ActorSystem, but that depends on the mailbox implementation.\nTermination of an actor proceeds in two steps: first the actor suspends its mailbox processing and sends a stop command to all its children, then it keeps processing the internal termination notifications from its children until the last one is gone, finally terminating itself (invoking postStoppostStop, dumping mailbox, publishing TerminatedTerminated on the DeathWatch, telling its supervisor). This procedure ensures that actor system sub-trees terminate in an orderly fashion, propagating the stop command to the leaves and collecting their confirmation back to the stopped supervisor. If one of the actors do not respond (i.e. processing a message for extended periods of time and therefore not receiving the stop command), this whole process will be stuck.\nUpon ActorSystem.terminate()ActorSystem.terminate(), the system guardian actors will be stopped, and the aforementioned process will ensure proper termination of the whole system. See Coordinated Shutdown.\nThe postStop() hook is invoked after an actor is fully stopped. This enables cleaning up of resources:\nScala copysourceoverride def postStop(): Unit = {\n  ()\n} Java copysource@Override\npublic void postStop() {\n  final String message = \"stopped\";\n  // don’t forget to think about who is the sender (2nd argument)\n  target.tell(message, getSelf());\n  final Object result = \"\";\n  target.forward(result, getContext());\n  target = null;\n}\nNote Since stopping an actor is asynchronous, you cannot immediately reuse the name of the child you just stopped; this will result in an InvalidActorNameExceptionInvalidActorNameException. Instead, watch() the terminating actor and create its replacement in response to the TerminatedTerminated message which will eventually arrive.","title":"Stopping actors"},{"location":"/actors.html#poisonpill","text":"You can also send an actor the PoisonPillPoisonPill message, which will stop the actor when the message is processed. PoisonPill is enqueued as ordinary messages and will be handled after messages that were already queued in the mailbox.\nScala copysourcewatch(victim)\nvictim ! PoisonPill Java copysourcevictim.tell(org.apache.pekko.actor.PoisonPill.getInstance(), ActorRef.noSender());","title":"PoisonPill"},{"location":"/actors.html#killing-an-actor","text":"You can also “kill” an actor by sending a KillKill message. Unlike PoisonPill this will cause the actor to throw a ActorKilledExceptionActorKilledException, triggering a failure. The actor will suspend operation and its supervisor will be asked how to handle the failure, which may mean resuming the actor, restarting it or terminating it completely. See What Supervision Means for more information.\nUse Kill like this:\nScala copysourcecontext.watch(victim) // watch the Actor to receive Terminated message once it dies\n\nvictim ! Kill\n\nexpectMsgPF(hint = \"expecting victim to terminate\") {\n  case Terminated(v) if v == victim => v // the Actor has indeed terminated\n} Java copysourcevictim.tell(org.apache.pekko.actor.Kill.getInstance(), ActorRef.noSender());\n\n// expecting the actor to indeed terminate:\nexpectTerminated(Duration.ofSeconds(3), victim);\nIn general, it is not recommended to overly rely on either PoisonPill or Kill in designing your actor interactions, as often a protocol-level message like PleaseCleanupAndStop which the actor knows how to handle is encouraged. The messages are there for being able to stop actors over which design you do not have control over.","title":"Killing an Actor"},{"location":"/actors.html#graceful-stop","text":"gracefulStopgracefulStop is useful if you need to wait for termination or compose ordered termination of several actors:\nScala copysourceimport org.apache.pekko.pattern.gracefulStop\nimport scala.concurrent.Await\n\ntry {\n  val stopped: Future[Boolean] = gracefulStop(actorRef, 5 seconds, Manager.Shutdown)\n  Await.result(stopped, 6 seconds)\n  // the actor has been stopped\n} catch {\n  // the actor wasn't stopped within 5 seconds\n  case e: pekko.pattern.AskTimeoutException =>\n} Java copysourceimport static org.apache.pekko.pattern.Patterns.gracefulStop;\nimport org.apache.pekko.pattern.AskTimeoutException;\nimport java.util.concurrent.CompletionStage;\n\ntry {\n  CompletionStage<Boolean> stopped =\n      gracefulStop(actorRef, Duration.ofSeconds(5), Manager.SHUTDOWN);\n  stopped.toCompletableFuture().get(6, TimeUnit.SECONDS);\n  // the actor has been stopped\n} catch (AskTimeoutException e) {\n  // the actor wasn't stopped within 5 seconds\n}\nWhen gracefulStop() returns successfully, the actor’s postStoppostStop hook will have been executed: there exists a happens-before edge between the end of postStop() and the return of gracefulStop().\nIn the above example, a custom Manager.Shutdown message is sent to the target actor to initiate the process of stopping the actor. You can use PoisonPill for this, but then you have limited possibilities to perform interactions with other actors before stopping the target actor. Simple cleanup tasks can be handled in postStop.\nWarning Keep in mind that an actor stopping and its name being deregistered are separate events that happen asynchronously from each other. Therefore it may be that you will find the name still in use after gracefulStop() returned. To guarantee proper deregistration, only reuse names from within a supervisor you control and only in response to a TerminatedTerminated message, i.e. not for top-level actors.","title":"Graceful Stop"},{"location":"/actors.html#become-unbecome","text":"","title":"Become/Unbecome"},{"location":"/actors.html#upgrade","text":"Pekko supports hotswapping the Actor’s message loop (e.g. its implementation) at runtime: invoke the context.becomecontext.become method from within the Actor. become takes a PartialFunction[Any, Unit] PartialFunction<Object, BoxedUnit> that implements the new message handler. The hotswapped code is kept in a Stack that can be pushed and popped.\nWarning Please note that the actor will revert to its original behavior when restarted by its Supervisor.\nTo hotswap the Actor behavior using become:\nScala copysourceclass HotSwapActor extends Actor {\n  import context._\n  def angry: Receive = {\n    case \"foo\" => sender() ! \"I am already angry?\"\n    case \"bar\" => become(happy)\n  }\n\n  def happy: Receive = {\n    case \"bar\" => sender() ! \"I am already happy :-)\"\n    case \"foo\" => become(angry)\n  }\n\n  def receive = {\n    case \"foo\" => become(angry)\n    case \"bar\" => become(happy)\n  }\n} Java copysourcestatic class HotSwapActor extends AbstractActor {\n  private AbstractActor.Receive angry;\n  private AbstractActor.Receive happy;\n\n  public HotSwapActor() {\n    angry =\n        receiveBuilder()\n            .matchEquals(\n                \"foo\",\n                s -> {\n                  getSender().tell(\"I am already angry?\", getSelf());\n                })\n            .matchEquals(\n                \"bar\",\n                s -> {\n                  getContext().become(happy);\n                })\n            .build();\n\n    happy =\n        receiveBuilder()\n            .matchEquals(\n                \"bar\",\n                s -> {\n                  getSender().tell(\"I am already happy :-)\", getSelf());\n                })\n            .matchEquals(\n                \"foo\",\n                s -> {\n                  getContext().become(angry);\n                })\n            .build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"foo\", s -> getContext().become(angry))\n        .matchEquals(\"bar\", s -> getContext().become(happy))\n        .build();\n  }\n}\nThis variant of the become method is useful for many different things, such as to implement a Finite State Machine (FSM). It will replace the current behavior (i.e. the top of the behavior stack), which means that you do not use unbecomeunbecome, instead always the next behavior is explicitly installed.\nThe other way of using become does not replace but add to the top of the behavior stack. In this case, care must be taken to ensure that the number of “pop” operations (i.e. unbecome) matches the number of “push” ones in the long run, otherwise this amounts to a memory leak (which is why this behavior is not the default).\nScala copysourcecase object Swap\nclass Swapper extends Actor {\n  import context._\n  val log = Logging(system, this)\n\n  def receive = {\n    case Swap =>\n      log.info(\"Hi\")\n      become({\n          case Swap =>\n            log.info(\"Ho\")\n            unbecome() // resets the latest 'become' (just for fun)\n        }, discardOld = false) // push on top instead of replace\n  }\n}\n\nobject SwapperApp extends App {\n  val system = ActorSystem(\"SwapperSystem\")\n  val swap = system.actorOf(Props[Swapper](), name = \"swapper\")\n  swap ! Swap // logs Hi\n  swap ! Swap // logs Ho\n  swap ! Swap // logs Hi\n  swap ! Swap // logs Ho\n  swap ! Swap // logs Hi\n  swap ! Swap // logs Ho\n} Java copysourcestatic class Swapper extends AbstractLoggingActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            Swap,\n            s -> {\n              log().info(\"Hi\");\n              getContext()\n                  .become(\n                      receiveBuilder()\n                          .matchEquals(\n                              Swap,\n                              x -> {\n                                log().info(\"Ho\");\n                                getContext()\n                                    .unbecome(); // resets the latest 'become' (just for fun)\n                              })\n                          .build(),\n                      false); // push on top instead of replace\n            })\n        .build();\n  }\n}\n\nstatic class SwapperApp {\n  public static void main(String[] args) {\n    ActorSystem system = ActorSystem.create(\"SwapperSystem\");\n    ActorRef swapper = system.actorOf(Props.create(Swapper.class), \"swapper\");\n    swapper.tell(Swap, ActorRef.noSender()); // logs Hi\n    swapper.tell(Swap, ActorRef.noSender()); // logs Ho\n    swapper.tell(Swap, ActorRef.noSender()); // logs Hi\n    swapper.tell(Swap, ActorRef.noSender()); // logs Ho\n    swapper.tell(Swap, ActorRef.noSender()); // logs Hi\n    swapper.tell(Swap, ActorRef.noSender()); // logs Ho\n    system.terminate();\n  }\n}","title":"Upgrade"},{"location":"/actors.html#encoding-scala-actors-nested-receives-without-accidentally-leaking-memory","text":"See this Unnested receive example.","title":"Encoding Scala Actors nested receives without accidentally leaking memory"},{"location":"/actors.html#stash","text":"The Stash trait AbstractActorWithStash class enables an actor to temporarily stash away messages that can not or should not be handled using the actor’s current behavior. Upon changing the actor’s message handler, i.e., right before invoking context.become or context.unbecome getContext().become() or getContext().unbecome(), all stashed messages can be “unstashed”, thereby prepending them to the actor’s mailbox. This way, the stashed messages can be processed in the same order as they have been received originally. An actor that extends AbstractActorWithStash will automatically get a deque-based mailbox.\nNote The trait Stash extends the marker trait RequiresMessageQueue[DequeBasedMessageQueueSemantics] which requests the system to automatically choose a deque based mailbox implementation for the actor. If you want more control over the mailbox, see the documentation on mailboxes: Mailboxes.\nNote The abstract class AbstractActorWithStash implements the marker interface RequiresMessageQueue<DequeBasedMessageQueueSemantics> which requests the system to automatically choose a deque based mailbox implementation for the actor. If you want more control over the mailbox, see the documentation on mailboxes: Mailboxes.\nHere is an example of the Stash trait AbstractActorWithStash class in action:\nScala copysourceimport org.apache.pekko.actor.Stash\nclass ActorWithProtocol extends Actor with Stash {\n  def receive = {\n    case \"open\" =>\n      unstashAll()\n      context.become({\n          case \"write\" => // do writing...\n          case \"close\" =>\n            unstashAll()\n            context.unbecome()\n          case msg => stash()\n        }, discardOld = false) // stack on top instead of replacing\n    case msg => stash()\n  }\n} Java copysourcestatic class ActorWithProtocol extends AbstractActorWithStash {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            \"open\",\n            s -> {\n              getContext()\n                  .become(\n                      receiveBuilder()\n                          .matchEquals(\n                              \"write\",\n                              ws -> {\n                                /* do writing */\n                              })\n                          .matchEquals(\n                              \"close\",\n                              cs -> {\n                                unstashAll();\n                                getContext().unbecome();\n                              })\n                          .matchAny(msg -> stash())\n                          .build(),\n                      false);\n            })\n        .matchAny(msg -> stash())\n        .build();\n  }\n}\nInvoking stash() adds the current message (the message that the actor received last) to the actor’s stash. It is typically invoked when handling the default case in the actor’s message handler to stash messages that aren’t handled by the other cases. It is illegal to stash the same message twice; to do so results in an IllegalStateException being thrown. The stash may also be bounded in which case invoking stash() may lead to a capacity violation, which results in a StashOverflowExceptionStashOverflowException. The capacity of the stash can be configured using the stash-capacity setting (an Int) of the mailbox’s configuration.\nInvoking unstashAll() enqueues messages from the stash to the actor’s mailbox until the capacity of the mailbox (if any) has been reached (note that messages from the stash are prepended to the mailbox). In case a bounded mailbox overflows, a MessageQueueAppendFailedException is thrown. The stash is guaranteed to be empty after calling unstashAll().\nThe stash is backed by a scala.collection.immutable.Vector. As a result, even a very large number of messages may be stashed without a major impact on performance.\nWarning Note that the Stash trait must be mixed into (a subclass of) the Actor trait before any trait/class that overrides the preRestart] callback. This means it’s not possible to write Actor with MyActor with Stash if MyActor overrides preRestart.\nNote that the stash is part of the ephemeral actor state, unlike the mailbox. Therefore, it should be managed like other parts of the actor’s state which have the same property.\nHowever, the Stash trait AbstractActorWithStash class implementation of preRestart will call unstashAll(). This means that before the actor restarts, it will transfer all stashed messages back to the actor’s mailbox.\nThe result of this is that when an actor is restarted, any stashed messages will be delivered to the new incarnation of the actor. This is usually the desired behavior.\nNote If you want to enforce that your actor can only work with an unbounded stash, then you should use the UnboundedStash trait AbstractActorWithUnboundedStash class instead.\nExtending Actors using PartialFunction chaining Sometimes, it can be useful to share common behavior among a few actors, or compose one actor’s behavior from multiple smaller functions. This is possible because an actor’s receivecreateReceive method returns an Actor.Receive, which is a type alias for PartialFunction[Any,Unit], and partial functions can be chained together using the PartialFunction#orElse method. You can chain as many functions as you need, however you should keep in mind that “first match” wins - which may be important when combining functions that both can handle the same type of message. For example, imagine you have a set of actors which are either Producers or Consumers, yet sometimes it makes sense to have an actor share both behaviors. This can be achieved without having to duplicate code by extracting the behaviors to traits and implementing the actor’s receive as a combination of these partial functions. copysource trait ProducerBehavior {\n  this: Actor =>\n\n  val producerBehavior: Receive = {\n    case GiveMeThings =>\n      sender() ! Give(\"thing\")\n  }\n}\n\ntrait ConsumerBehavior {\n  this: Actor with ActorLogging =>\n\n  val consumerBehavior: Receive = {\n    case ref: ActorRef =>\n      ref ! GiveMeThings\n\n    case Give(thing) =>\n      log.info(\"Got a thing! It's {}\", thing)\n  }\n}\n\nclass Producer extends Actor with ProducerBehavior {\n  def receive = producerBehavior\n}\n\nclass Consumer extends Actor with ActorLogging with ConsumerBehavior {\n  def receive = consumerBehavior\n}\n\nclass ProducerConsumer extends Actor with ActorLogging with ProducerBehavior with ConsumerBehavior {\n\n  def receive = producerBehavior.orElse[Any, Unit](consumerBehavior)\n}\n\n// protocol\ncase object GiveMeThings\nfinal case class Give(thing: Any)\n Instead of inheritance the same pattern can be applied via composition - compose the receive method using partial functions from delegates.","title":"Stash"},{"location":"/actors.html#initialization-patterns","text":"The rich lifecycle hooks of Actors provide a useful toolkit to implement various initialization patterns. During the lifetime of an ActorRefActorRef, an actor can potentially go through several restarts, where the old instance is replaced by a fresh one, invisibly to the outside observer who only sees the ActorRef.\nInitialization might be necessary every time an actor is instantiated, but sometimes one needs initialization to happen only at the birth of the first instance when the ActorRef is created. The following sections provide patterns for different initialization needs.","title":"Initialization patterns"},{"location":"/actors.html#initialization-via-constructor","text":"Using the constructor for initialization has various benefits. First of all, it makes it possible to use val fields to store any state that does not change during the life of the actor instance, making the implementation of the actor more robust. The constructor is invoked when an actor instance is created calling actorOf and also on restart, therefore the internals of the actor can always assume that proper initialization happened. This is also the drawback of this approach, as there are cases when one would like to avoid reinitializing internals on restart. For example, it is often useful to preserve child actors across restarts. The following section provides a pattern for this case.","title":"Initialization via constructor"},{"location":"/actors.html#initialization-via-prestart","text":"The method preStartpreStart of an actor is only called once directly during the initialization of the first instance, that is, at the creation of its ActorRef. In the case of restarts, preStart() is called from postRestartpostRestart, therefore if not overridden, preStart() is called on every restart. However, by overriding postRestart() one can disable this behavior, and ensure that there is only one call to preStart().\nOne useful usage of this pattern is to disable creation of new ActorRefs for children during restarts. This can be achieved by overriding preRestartpreRestart. Below is the default implementation of these lifecycle hooks:\nScala copysourceoverride def preStart(): Unit = {\n  // Initialize children here\n}\n\n// Overriding postRestart to disable the call to preStart()\n// after restarts\noverride def postRestart(reason: Throwable): Unit = ()\n\n// The default implementation of preRestart() stops all the children\n// of the actor. To opt-out from stopping the children, we\n// have to override preRestart()\noverride def preRestart(reason: Throwable, message: Option[Any]): Unit = {\n  // Keep the call to postStop(), but no stopping of children\n  postStop()\n} Java copysource@Override\npublic void preStart() {\n  // Initialize children here\n}\n\n// Overriding postRestart to disable the call to preStart()\n// after restarts\n@Override\npublic void postRestart(Throwable reason) {}\n\n// The default implementation of preRestart() stops all the children\n// of the actor. To opt-out from stopping the children, we\n// have to override preRestart()\n@Override\npublic void preRestart(Throwable reason, Optional<Object> message) throws Exception {\n  // Keep the call to postStop(), but no stopping of children\n  postStop();\n}\nPlease note, that the child actors are still restarted, but no new ActorRefActorRef is created. One can recursively apply the same principles for the children, ensuring that their preStart() method is called only at the creation of their refs.\nFor more information see What Restarting Means.","title":"Initialization via preStart"},{"location":"/actors.html#initialization-via-message-passing","text":"There are cases when it is impossible to pass all the information needed for actor initialization in the constructor, for example in the presence of circular dependencies. In this case, the actor should listen for an initialization message, and use become()become() or a finite state-machine state transition to encode the initialized and uninitialized states of the actor.\nScala copysourcevar initializeMe: Option[String] = None\n\noverride def receive = {\n  case \"init\" =>\n    initializeMe = Some(\"Up and running\")\n    context.become(initialized, discardOld = true)\n\n}\n\ndef initialized: Receive = {\n  case \"U OK?\" => initializeMe.foreach { sender() ! _ }\n} Java copysource@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n      .matchEquals(\n          \"init\",\n          m1 -> {\n            initializeMe = \"Up and running\";\n            getContext()\n                .become(\n                    receiveBuilder()\n                        .matchEquals(\n                            \"U OK?\",\n                            m2 -> {\n                              getSender().tell(initializeMe, getSelf());\n                            })\n                        .build());\n          })\n      .build();\n}\nIf the actor may receive messages before it has been initialized, a useful tool can be the Stash to save messages until the initialization finishes, and replaying them after the actor became initialized.\nWarning This pattern should be used with care, and applied only when none of the patterns above are applicable. One of the potential issues is that messages might be lost when sent to remote actors. Also, publishing an ActorRefActorRef in an uninitialized state might lead to the condition that it receives a user message before the initialization has been done.","title":"Initialization via message passing"},{"location":"/supervision-classic.html","text":"","title":"Classic Supervision"},{"location":"/supervision-classic.html#classic-supervision","text":"This chapter outlines the concept behind the supervision in Pekko Classic, for the corresponding overview of the new APIs see supervision","title":"Classic Supervision"},{"location":"/supervision-classic.html#what-supervision-means","text":"Supervision describes a dependency relationship between actors: the supervisor delegates tasks to subordinates and therefore must respond to their failures. When a subordinate detects a failure (i.e. throws an exception), it suspends itself and all its subordinates and sends a message to its supervisor, signaling failure. Depending on the nature of the work to be supervised and the nature of the failure, the supervisor has a choice of the following four options:\nResume the subordinate, keeping its accumulated internal state Restart the subordinate, clearing out its accumulated internal state Stop the subordinate permanently Escalate the failure, thereby failing itself\nIt is important to always view an actor as part of a supervision hierarchy, which explains the existence of the fourth choice (as a supervisor also is subordinate to another supervisor higher up) and has implications on the first three: resuming an actor resumes all its subordinates, restarting an actor entails restarting all its subordinates (but see below for more details), similarly terminating an actor will also terminate all its subordinates. It should be noted that the default behavior of the preRestart hook of the Actor class is to terminate all its children before restarting, but this hook can be overridden; the recursive restart applies to all children left after this hook has been executed.\nEach supervisor is configured with a function translating all possible failure causes (i.e. exceptions) into one of the four choices given above; notably, this function does not take the failed actor’s identity as an input. It is quite easy to come up with examples of structures where this might not seem flexible enough, e.g. wishing for different strategies to be applied to different subordinates. At this point, it is vital to understand that supervision is about forming a recursive fault handling structure. If you try to do too much at one level, it will become hard to reason about, hence the recommended way, in this case, is to add a level of supervision.\nPekko implements a specific form called “parental supervision”. Actors can only be created by other actors—where the top-level actor is provided by the library—and each created actor is supervised by its parent. This restriction makes the formation of actor supervision hierarchies implicit and encourages sound design decisions. It should be noted that this also guarantees that actors cannot be orphaned or attached to supervisors from the outside, which might otherwise catch them unawares. Besides, this yields a natural and clean shutdown procedure for (sub-trees of) actor applications.\nWarning Supervision-related communication happens by special system messages that have their mailboxes separate from user messages. This implies that supervision related events are not deterministically ordered relative to ordinary messages. In general, the user cannot influence the order of normal messages and failure notifications. For details and example see the Discussion: Message Ordering section.","title":"What Supervision Means"},{"location":"/supervision-classic.html#the-top-level-supervisors","text":"An actor system will during its creation start at least three actors, shown in the image above. For more information about the consequences for actor paths see Top-Level Scopes for Actor Paths.","title":"The Top-Level Supervisors"},{"location":"/supervision-classic.html#user-the-guardian-actor","text":"The actor which is probably most interacted with is the parent of all user-created actors, the guardian named \"/user\". Actors created using system.actorOf() are children of this actor. This means that when this guardian terminates, all normal actors in the system will be shutdown, too. It also means that this guardian’s supervisor strategy determines how the top-level normal actors are supervised. Since Pekko 2.1 it is possible to configure this using the setting pekko.actor.guardian-supervisor-strategy, which takes the fully-qualified class-name of a SupervisorStrategyConfigurator. When the guardian escalates a failure, the root guardian’s response will be to terminate the guardian, which in effect will shut down the whole actor system.","title":"/user: The Guardian Actor"},{"location":"/supervision-classic.html#system-the-system-guardian","text":"This special guardian has been introduced to achieve an orderly shut-down sequence where logging remains active while all normal actors terminate, even though logging itself is implemented using actors. This is realized by having the system guardian watch the user guardian and initiate its shut-down upon reception of the Terminated message. The top-level system actors are supervised using a strategy which will restart indefinitely upon all types of Exception except for ActorInitializationException and ActorKilledException, which will terminate the child in question. All other throwables are escalated, which will shut down the whole actor system.","title":"/system: The System Guardian"},{"location":"/supervision-classic.html#the-root-guardian","text":"The root guardian is the grand-parent of all so-called “top-level” actors and supervises all the special actors mentioned in Top-Level Scopes for Actor Paths using the SupervisorStrategy.stoppingStrategy, whose purpose is to terminate the child upon any type of Exception. All other throwables will be escalated … but to whom? Since every real actor has a supervisor, the supervisor of the root guardian cannot be a real actor. And because this means that it is “outside of the bubble”, it is called the “bubble-walker”. This is a synthetic ActorRef which in effect stops its child upon the first sign of trouble and sets the actor system’s isTerminated status to true as soon as the root guardian is fully terminated (all children recursively stopped).","title":"/: The Root Guardian"},{"location":"/supervision-classic.html#one-for-one-strategy-vs-all-for-one-strategy","text":"There are two classes of supervision strategies which come with Pekko: OneForOneStrategy and AllForOneStrategy. Both are configured with a mapping from exception type to supervision directive (see above) and limits on how often a child is allowed to fail before terminating it. The difference between them is that the former applies the obtained directive only to the failed child, whereas the latter applies it to all siblings as well. Normally, you should use the OneForOneStrategy, which also is the default if none is specified explicitly.\nThe AllForOneStrategy is applicable in cases where the ensemble of children has such tight dependencies among them, that a failure of one child affects the function of the others, i.e. they are inextricably linked. Since a restart does not clear out the mailbox, it often is best to terminate the children upon failure and re-create them explicitly from the supervisor (by watching the children’s lifecycle); otherwise, you have to make sure that it is no problem for any of the actors to receive a message which was queued before the restart but processed afterwards.\nNormally stopping a child (i.e. not in response to a failure) will not automatically terminate the other children in an all-for-one strategy; this can be done by watching their lifecycle: if the Terminated message is not handled by the supervisor, it will throw a DeathPactException which (depending on its supervisor) will restart it, and the default preRestart action will terminate all children. Of course, this can be handled explicitly as well.\nPlease note that creating one-off actors from an all-for-one supervisor entails that failures escalated by the temporary actor will affect all the permanent ones. If this is not desired, install an intermediate supervisor; this can very be done by declaring a router of size 1 for the worker, see Routing.","title":"One-For-One Strategy vs. All-For-One Strategy"},{"location":"/fault-tolerance.html","text":"","title":"Classic Fault Tolerance"},{"location":"/fault-tolerance.html#classic-fault-tolerance","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see fault tolerance.","title":"Classic Fault Tolerance"},{"location":"/fault-tolerance.html#dependency","text":"The concept of fault tolerance relates to actors, so in order to use these make sure to depend on actors:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/fault-tolerance.html#introduction","text":"As explained in Actor Systems each actor is the supervisor of its children, and as such each actor defines fault handling supervisor strategy. This strategy cannot be changed afterwards as it is an integral part of the actor system’s structure.","title":"Introduction"},{"location":"/fault-tolerance.html#fault-handling-in-practice","text":"First, let us look at a sample that illustrates one way to handle data store errors, which is a typical source of failure in real world applications. Of course it depends on the actual application what is possible to do when the data store is unavailable, but in this sample we use a best effort re-connect approach.\nRead the following source code. The inlined comments explain the different pieces of the fault handling and why they are added. It is also highly recommended to run this sample as it is easy to follow the log output to understand what is happening at runtime.","title":"Fault Handling in Practice"},{"location":"/fault-tolerance.html#creating-a-supervisor-strategy","text":"The following sections explain the fault handling mechanism and alternatives in more depth.\nFor the sake of demonstration let us consider the following strategy:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.OneForOneStrategy\nimport pekko.actor.SupervisorStrategy._\nimport scala.concurrent.duration._\n\noverride val supervisorStrategy =\n  OneForOneStrategy(maxNrOfRetries = 10, withinTimeRange = 1 minute) {\n    case _: ArithmeticException      => Resume\n    case _: NullPointerException     => Restart\n    case _: IllegalArgumentException => Stop\n    case _: Exception                => Escalate\n  } Java copysourceprivate static SupervisorStrategy strategy =\n    new OneForOneStrategy(\n        10,\n        Duration.ofMinutes(1),\n        DeciderBuilder.match(ArithmeticException.class, e -> SupervisorStrategy.resume())\n            .match(NullPointerException.class, e -> SupervisorStrategy.restart())\n            .match(IllegalArgumentException.class, e -> SupervisorStrategy.stop())\n            .matchAny(o -> SupervisorStrategy.escalate())\n            .build());\n\n@Override\npublic SupervisorStrategy supervisorStrategy() {\n  return strategy;\n}\nWe have chosen a few well-known exception types in order to demonstrate the application of the fault handling directives described in supervision. First off, it is a one-for-one strategy, meaning that each child is treated separately (an all-for-one strategy works very similarly, the only difference is that any decision is applied to all children of the supervisor, not only the failing one). In the above example, 10 and 1 minuteDuration.create(1, TimeUnit.MINUTES) are passed to the maxNrOfRetries and withinTimeRange parameters respectively, which means that the strategy restarts a child up to 10 restarts per minute. The child actor is stopped if the restart count exceeds maxNrOfRetries during the withinTimeRange duration.\nAlso, there are special values for these parameters. If you specify:\n-1 to maxNrOfRetries, and Duration.InfDuration.Inf() to withinTimeRange then the child is always restarted without any limit -1 to maxNrOfRetries, and a non-infinite Duration to withinTimeRange maxNrOfRetries is treated as 1 a non-negative number to maxNrOfRetries and Duration.InfDuration.Inf() to withinTimeRange withinTimeRange is treated as infinite duration (i.e.) no matter how long it takes, once the restart count exceeds maxNrOfRetries, the child actor is stopped\nThe match statement which forms the bulk of the body is of type Decider which is a PartialFunction[Throwable, Directive]. consists of PFBuilder returned by DeciderBuilder’s match method, where the builder is finished by the build method. This is the piece which maps child failure types to their corresponding directives.\nNote If the strategy is declared inside the supervising actor (as opposed to within a companion objecta separate class) its decider has access to all internal state of the actor in a thread-safe fashion, including obtaining a reference to the currently failed child (available as the sendergetSender() of the failure message).","title":"Creating a Supervisor Strategy"},{"location":"/fault-tolerance.html#default-supervisor-strategy","text":"Escalate is used if the defined strategy doesn’t cover the exception that was thrown.\nWhen the supervisor strategy is not defined for an actor the following exceptions are handled by default:\nActorInitializationException will stop the failing child actor ActorKilledException will stop the failing child actor DeathPactException will stop the failing child actor Exception will restart the failing child actor Other types of Throwable will be escalated to parent actor\nIf the exception escalate all the way up to the root guardian it will handle it in the same way as the default strategy defined above.\nYou can combine your own strategy with the default strategy: copysourceimport org.apache.pekko\nimport pekko.actor.OneForOneStrategy\nimport pekko.actor.SupervisorStrategy._\nimport scala.concurrent.duration._\n\noverride val supervisorStrategy =\n  OneForOneStrategy(maxNrOfRetries = 10, withinTimeRange = 1 minute) {\n    case _: ArithmeticException => Resume\n    case t =>\n      super.supervisorStrategy.decider.applyOrElse(t, (_: Any) => Escalate)\n  }","title":"Default Supervisor Strategy"},{"location":"/fault-tolerance.html#stopping-supervisor-strategy","text":"Closer to the Erlang way is the strategy to stop children when they fail and then take corrective action in the supervisor when DeathWatch signals the loss of the child. This strategy is also provided pre-packaged as SupervisorStrategy.stoppingStrategy with an accompanying StoppingSupervisorStrategy configurator to be used when you want the \"/user\" guardian to apply it.","title":"Stopping Supervisor Strategy"},{"location":"/fault-tolerance.html#logging-of-actor-failures","text":"By default the SupervisorStrategy logs failures unless they are escalated. Escalated failures are supposed to be handled, and potentially logged, at a level higher in the hierarchy.\nLog levels can be controlled by providing a Decider and using the appropriate decision methods accepting a LogLevel on SupervisorStrategy.\nYou can mute the default logging of a SupervisorStrategy by setting loggingEnabled to false when instantiating it. Customized logging can be done inside the Decider. Note that the reference to the currently failed child is available as the sender when the SupervisorStrategy is declared inside the supervising actor.\nYou may also customize the logging in your own SupervisorStrategy implementation by overriding the logFailure method.","title":"Logging of Actor Failures"},{"location":"/fault-tolerance.html#supervision-of-top-level-actors","text":"Toplevel actors means those which are created using system.actorOf(), and they are children of the User Guardian. There are no special rules applied in this case, the guardian applies the configured strategy.","title":"Supervision of Top-Level Actors"},{"location":"/fault-tolerance.html#test-application","text":"The following section shows the effects of the different directives in practice, where a test setup is needed. First off, we need a suitable supervisor:\nScala copysourceimport org.apache.pekko.actor.Actor\n\nclass Supervisor extends Actor {\n  import org.apache.pekko\n  import pekko.actor.OneForOneStrategy\n  import pekko.actor.SupervisorStrategy._\n  import scala.concurrent.duration._\n\n  override val supervisorStrategy =\n    OneForOneStrategy(maxNrOfRetries = 10, withinTimeRange = 1 minute) {\n      case _: ArithmeticException      => Resume\n      case _: NullPointerException     => Restart\n      case _: IllegalArgumentException => Stop\n      case _: Exception                => Escalate\n    }\n\n  def receive = {\n    case p: Props => sender() ! context.actorOf(p)\n  }\n} Java copysourceimport org.apache.pekko.japi.pf.DeciderBuilder;\nimport org.apache.pekko.actor.SupervisorStrategy;\n\nstatic class Supervisor extends AbstractActor {\n\n  private static SupervisorStrategy strategy =\n      new OneForOneStrategy(\n          10,\n          Duration.ofMinutes(1),\n          DeciderBuilder.match(ArithmeticException.class, e -> SupervisorStrategy.resume())\n              .match(NullPointerException.class, e -> SupervisorStrategy.restart())\n              .match(IllegalArgumentException.class, e -> SupervisorStrategy.stop())\n              .matchAny(o -> SupervisorStrategy.escalate())\n              .build());\n\n  @Override\n  public SupervisorStrategy supervisorStrategy() {\n    return strategy;\n  }\n\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Props.class,\n            props -> {\n              getSender().tell(getContext().actorOf(props), getSelf());\n            })\n        .build();\n  }\n}\nThis supervisor will be used to create a child, with which we can experiment:\nScala copysourceimport org.apache.pekko.actor.Actor\n\nclass Child extends Actor {\n  var state = 0\n  def receive = {\n    case ex: Exception => throw ex\n    case x: Int        => state = x\n    case \"get\"         => sender() ! state\n  }\n} Java copysourcestatic class Child extends AbstractActor {\n  int state = 0;\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Exception.class,\n            exception -> {\n              throw exception;\n            })\n        .match(Integer.class, i -> state = i)\n        .matchEquals(\"get\", s -> getSender().tell(state, getSelf()))\n        .build();\n  }\n}\nThe test is easier by using the utilities described in Testing Actor SystemsTestKit, where TestProbe provides an actor ref useful for receiving and inspecting replies.\nScala copysourceimport com.typesafe.config.{ Config, ConfigFactory }\nimport org.scalatest.BeforeAndAfterAll\nimport org.scalatest.matchers.should.Matchers\nimport org.scalatest.wordspec.AnyWordSpecLike\nimport org.apache.pekko.testkit.{ EventFilter, ImplicitSender, TestKit }\n\nclass FaultHandlingDocSpec(_system: ActorSystem)\n    extends TestKit(_system)\n    with ImplicitSender\n    with AnyWordSpecLike\n    with Matchers\n    with BeforeAndAfterAll {\n\n  def this() =\n    this(\n      ActorSystem(\n        \"FaultHandlingDocSpec\",\n        ConfigFactory.parseString(\"\"\"\n      pekko {\n        loggers = [\"org.apache.pekko.testkit.TestEventListener\"]\n        loglevel = \"WARNING\"\n      }\n      \"\"\")))\n\n  override def afterAll(): Unit = {\n    TestKit.shutdownActorSystem(system)\n  }\n\n  \"A supervisor\" must {\n    \"apply the chosen strategy for its child\" in {\n      // code here\n    }\n  }\n} Java copysourceimport org.apache.pekko.testkit.TestProbe;\nimport org.apache.pekko.testkit.ErrorFilter;\nimport org.apache.pekko.testkit.EventFilter;\nimport org.apache.pekko.testkit.TestEvent;\nimport static java.util.concurrent.TimeUnit.SECONDS;\nimport static org.apache.pekko.japi.Util.immutableSeq;\nimport static org.junit.Assert.assertEquals;\n\nimport scala.concurrent.Await;\n\npublic class FaultHandlingTest extends AbstractJavaTest {\n  static ActorSystem system;\n  scala.concurrent.duration.Duration timeout =\n      scala.concurrent.duration.Duration.create(5, SECONDS);\n\n  @BeforeClass\n  public static void start() {\n    system = ActorSystem.create(\"FaultHandlingTest\", config);\n  }\n\n  @AfterClass\n  public static void cleanup() {\n    TestKit.shutdownActorSystem(system);\n    system = null;\n  }\n\n  @Test\n  public void mustEmploySupervisorStrategy() throws Exception {\n    // code here\n  }\n}\nLet us create actors:\nScala copysourceval supervisor = system.actorOf(Props[Supervisor](), \"supervisor\")\n\nsupervisor ! Props[Child]()\nval child = expectMsgType[ActorRef] // retrieve answer from TestKit’s testActor Java copysourceProps superprops = Props.create(Supervisor.class);\nActorRef supervisor = system.actorOf(superprops, \"supervisor\");\nActorRef child =\n    (ActorRef) Await.result(ask(supervisor, Props.create(Child.class), 5000), timeout);\nThe first test shall demonstrate the Resume directive, so we try it out by setting some non-initial state in the actor and have it fail:\nScala copysourcechild ! 42 // set state to 42\nchild ! \"get\"\nexpectMsg(42)\n\nchild ! new ArithmeticException // crash it\nchild ! \"get\"\nexpectMsg(42) Java copysourcechild.tell(42, ActorRef.noSender());\nassertEquals(42, Await.result(ask(child, \"get\", 5000), timeout));\nchild.tell(new ArithmeticException(), ActorRef.noSender());\nassertEquals(42, Await.result(ask(child, \"get\", 5000), timeout));\nAs you can see the value 42 survives the fault handling directive. Now, if we change the failure to a more serious NullPointerException, that will no longer be the case:\nScala copysourcechild ! new NullPointerException // crash it harder\nchild ! \"get\"\nexpectMsg(0) Java copysourcechild.tell(new NullPointerException(), ActorRef.noSender());\nassertEquals(0, Await.result(ask(child, \"get\", 5000), timeout));\nAnd finally in case of the fatal IllegalArgumentException the child will be terminated by the supervisor:\nScala copysourcewatch(child) // have testActor watch “child”\nchild ! new IllegalArgumentException // break it\nexpectMsgPF() { case Terminated(`child`) => () } Java copysourcefinal TestProbe probe = new TestProbe(system);\nprobe.watch(child);\nchild.tell(new IllegalArgumentException(), ActorRef.noSender());\nprobe.expectMsgClass(Terminated.class);\nUp to now the supervisor was completely unaffected by the child’s failure, because the directives set did handle it. In case of an Exception, this is not true anymore and the supervisor escalates the failure.\nScala copysourcesupervisor ! Props[Child]() // create new child\nval child2 = expectMsgType[ActorRef]\nwatch(child2)\nchild2 ! \"get\" // verify it is alive\nexpectMsg(0)\n\nchild2 ! new Exception(\"CRASH\") // escalate failure\nexpectMsgPF() {\n  case t @ Terminated(`child2`) if t.existenceConfirmed => ()\n} Java copysourcechild = (ActorRef) Await.result(ask(supervisor, Props.create(Child.class), 5000), timeout);\nprobe.watch(child);\nassertEquals(0, Await.result(ask(child, \"get\", 5000), timeout));\nchild.tell(new Exception(), ActorRef.noSender());\nprobe.expectMsgClass(Terminated.class);\nThe supervisor itself is supervised by the top-level actor provided by the ActorSystem, which has the default policy to restart in case of all Exception cases (with the notable exceptions of ActorInitializationException and ActorKilledException). Since the default directive in case of a restart is to kill all children, we expected our poor child not to survive this failure.\nIn case this is not desired (which depends on the use case), we need to use a different supervisor which overrides this behavior.\nScala copysourceclass Supervisor2 extends Actor {\n  import org.apache.pekko\n  import pekko.actor.OneForOneStrategy\n  import pekko.actor.SupervisorStrategy._\n  import scala.concurrent.duration._\n\n  override val supervisorStrategy =\n    OneForOneStrategy(maxNrOfRetries = 10, withinTimeRange = 1 minute) {\n      case _: ArithmeticException      => Resume\n      case _: NullPointerException     => Restart\n      case _: IllegalArgumentException => Stop\n      case _: Exception                => Escalate\n    }\n\n  def receive = {\n    case p: Props => sender() ! context.actorOf(p)\n  }\n  // override default to kill all children during restart\n  override def preRestart(cause: Throwable, msg: Option[Any]): Unit = {}\n} Java copysourcestatic class Supervisor2 extends AbstractActor {\n\n  private static SupervisorStrategy strategy =\n      new OneForOneStrategy(\n          10,\n          Duration.ofMinutes(1),\n          DeciderBuilder.match(ArithmeticException.class, e -> SupervisorStrategy.resume())\n              .match(NullPointerException.class, e -> SupervisorStrategy.restart())\n              .match(IllegalArgumentException.class, e -> SupervisorStrategy.stop())\n              .matchAny(o -> SupervisorStrategy.escalate())\n              .build());\n\n  @Override\n  public SupervisorStrategy supervisorStrategy() {\n    return strategy;\n  }\n\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Props.class,\n            props -> {\n              getSender().tell(getContext().actorOf(props), getSelf());\n            })\n        .build();\n  }\n\n  @Override\n  public void preRestart(Throwable cause, Optional<Object> msg) {\n    // do not kill all children, which is the default here\n  }\n}\nWith this parent, the child survives the escalated restart, as demonstrated in the last test:\nScala copysourceval supervisor2 = system.actorOf(Props[Supervisor2](), \"supervisor2\")\n\nsupervisor2 ! Props[Child]()\nval child3 = expectMsgType[ActorRef]\n\nchild3 ! 23\nchild3 ! \"get\"\nexpectMsg(23)\n\nchild3 ! new Exception(\"CRASH\")\nchild3 ! \"get\"\nexpectMsg(0) Java copysourcesuperprops = Props.create(Supervisor2.class);\nsupervisor = system.actorOf(superprops);\nchild = (ActorRef) Await.result(ask(supervisor, Props.create(Child.class), 5000), timeout);\nchild.tell(23, ActorRef.noSender());\nassertEquals(23, Await.result(ask(child, \"get\", 5000), timeout));\nchild.tell(new Exception(), ActorRef.noSender());\nassertEquals(0, Await.result(ask(child, \"get\", 5000), timeout));","title":"Test Application"},{"location":"/fault-tolerance.html#delayed-restarts-for-classic-actors","text":"The supervision strategy to restart a classic actor only provides immediate restart. In some cases that will only trigger the same failure right away and giving things a bit of time before restarting is required to actually resolve the failure.\nThe org.apache.pekko.pattern.BackoffSupervisor implements the so-called exponential backoff supervision strategy, starting a child actor again when it fails, each time with a growing time delay between restarts.\nThis pattern is useful when the started actor fails [1] because some external resource is not available, and we need to give it some time to start-up again. One of the prime examples when this is useful is when a PersistentActor fails (by stopping) with a persistence failure - which indicates that the database may be down or overloaded, in such situations it makes most sense to give it a little bit of time to recover before the persistent actor is started.\n[1] A failure can be indicated in two different ways; by an actor stopping or crashing.","title":"Delayed restarts for classic actors"},{"location":"/fault-tolerance.html#supervision-strategies","text":"There are two basic supervision strategies available for backoff:\n‘On failure’: The supervisor will terminate and then start the supervised actor if it crashes. If the supervised actor stops normally (e.g. through context.stop), the supervisor will be terminated and no further attempt to start the supervised actor will be done. ‘On stop’: The supervisor will terminate and then start the supervised actor if it terminates in any way (consider this for PersistentActor since they stop on persistence failures instead of crashing)\nTo note that this supervision strategy does not restart the actor but rather stops and starts it. Be aware of it if you use Stash trait’s AbstractActorWithStash in combination with the backoff supervision strategy. The preRestart hook will not be executed if the supervised actor fails or stops and you will miss the opportunity to unstash the messages.","title":"Supervision strategies"},{"location":"/fault-tolerance.html#sharding","text":"If the ‘on stop’ strategy is used for sharded actors a final termination message should be configured and used to terminate the actor on passivation. Otherwise the supervisor will just stop and start the actor again.\nThe termination message is configured with:\ncopysourceval supervisor = BackoffSupervisor.props(\n  BackoffOpts\n    .onStop(childProps, childName = \"myEcho\", minBackoff = 3.seconds, maxBackoff = 30.seconds, randomFactor = 0.2)\n    .withFinalStopMessage(_ == StopMessage))\nAnd must be used for passivation:\ncopysourcecontext.parent ! Passivate(StopMessage)","title":"Sharding"},{"location":"/fault-tolerance.html#simple-backoff","text":"The following snippet shows how to create a backoff supervisor which will start the given echo actor after it has stopped because of a failure, in increasing intervals of 3, 6, 12, 24 and finally 30 seconds:\nScala copysourceval childProps = Props(classOf[EchoActor])\n\nval supervisor = BackoffSupervisor.props(\n  BackoffOpts.onStop(\n    childProps,\n    childName = \"myEcho\",\n    minBackoff = 3.seconds,\n    maxBackoff = 30.seconds,\n    randomFactor = 0.2 // adds 20% \"noise\" to vary the intervals slightly\n  ))\n\nsystem.actorOf(supervisor, name = \"echoSupervisor\") Java copysourcefinal Props childProps = Props.create(EchoActor.class);\n\nfinal Props supervisorProps =\n    BackoffSupervisor.props(\n        BackoffOpts.onStop(\n            childProps,\n            \"myEcho\",\n            Duration.ofSeconds(3),\n            Duration.ofSeconds(30),\n            0.2)); // adds 20% \"noise\" to vary the intervals slightly\n\nsystem.actorOf(supervisorProps, \"echoSupervisor\");\nUsing a randomFactor to add a little bit of additional variance to the backoff intervals is highly recommended, in order to avoid multiple actors re-start at the exact same point in time, for example because they were stopped due to a shared resource such as a database going down and re-starting after the same configured interval. By adding additional randomness to the re-start intervals the actors will start in slightly different points in time, thus avoiding large spikes of traffic hitting the recovering shared database or other resource that they all need to contact.\nThe org.apache.pekko.pattern.BackoffSupervisor actor can also be configured to stop and start the actor after a delay when the actor crashes and the supervision strategy decides that it should restart.\nThe following snippet shows how to create a backoff supervisor which will start the given echo actor after it has crashed because of some exception, in increasing intervals of 3, 6, 12, 24 and finally 30 seconds:\nScala copysourceval childProps = Props(classOf[EchoActor])\n\nval supervisor = BackoffSupervisor.props(\n  BackoffOpts.onFailure(\n    childProps,\n    childName = \"myEcho\",\n    minBackoff = 3.seconds,\n    maxBackoff = 30.seconds,\n    randomFactor = 0.2 // adds 20% \"noise\" to vary the intervals slightly\n  ))\n\nsystem.actorOf(supervisor, name = \"echoSupervisor\") Java copysourcefinal Props childProps = Props.create(EchoActor.class);\n\nfinal Props supervisorProps =\n    BackoffSupervisor.props(\n        BackoffOpts.onFailure(\n            childProps,\n            \"myEcho\",\n            Duration.ofSeconds(3),\n            Duration.ofSeconds(30),\n            0.2)); // adds 20% \"noise\" to vary the intervals slightly\n\nsystem.actorOf(supervisorProps, \"echoSupervisor\");","title":"Simple backoff"},{"location":"/fault-tolerance.html#customization","text":"The org.apache.pekko.pattern.BackoffOnFailureOptions and org.apache.pekko.pattern.BackoffOnRestartOptions can be used to customize the behavior of the back-off supervisor actor. Options are: * withAutoReset: The backoff is reset if no failure/stop occurs within the duration. This is the default behaviour with minBackoff as default value * withManualReset: The child must send BackoffSupervisor.Reset to its backoff supervisor (parent) * withSupervisionStrategy: Sets a custom OneForOneStrategy (as each backoff supervisor only has one child). The default strategy uses the pekko.actor.SupervisorStrategy.defaultDecider which stops and starts the child on exceptions. * withMaxNrOfRetries: Sets the maximum number of retries until the supervisor will give up (-1 is default which means no limit of retries). Note: This is set on the supervision strategy, so setting a different strategy resets the maxNrOfRetries. * withReplyWhileStopped: By default all messages received while the child is stopped are forwarded to dead letters. With this set, the supervisor will reply to the sender instead.\nOnly available on BackoffOnStopOptions: * withDefaultStoppingStrategy: Sets a OneForOneStrategy with the stopping decider that stops the child on all exceptions. * withFinalStopMessage: Allows to define a predicate to decide on finally stopping the child (and supervisor). Used for passivate sharded actors - see above.\nSome examples:\ncopysourceval supervisor = BackoffSupervisor.props(\n  BackoffOpts\n    .onStop(\n      childProps,\n      childName = \"myEcho\",\n      minBackoff = 3.seconds,\n      maxBackoff = 30.seconds,\n      randomFactor = 0.2 // adds 20% \"noise\" to vary the intervals slightly\n    )\n    .withManualReset // the child must send BackoffSupervisor.Reset to its parent\n    .withDefaultStoppingStrategy // Stop at any Exception thrown\n)\nThe above code sets up a back-off supervisor that requires the child actor to send a org.apache.pekko.pattern.BackoffSupervisor.Reset message to its parent when a message is successfully processed, resetting the back-off. It also uses a default stopping strategy, any exception will cause the child to stop.\ncopysourceval supervisor = BackoffSupervisor.props(\n  BackoffOpts\n    .onFailure(\n      childProps,\n      childName = \"myEcho\",\n      minBackoff = 3.seconds,\n      maxBackoff = 30.seconds,\n      randomFactor = 0.2 // adds 20% \"noise\" to vary the intervals slightly\n    )\n    .withAutoReset(10.seconds) // reset if the child does not throw any errors within 10 seconds\n    .withSupervisorStrategy(OneForOneStrategy() {\n      case _: MyException => SupervisorStrategy.Restart\n      case _              => SupervisorStrategy.Escalate\n    }))\nThe above code sets up a back-off supervisor that stops and starts the child after back-off if MyException is thrown, any other exception will be escalated. The back-off is automatically reset if the child does not throw any errors within 10 seconds.","title":"Customization"},{"location":"/dispatchers.html","text":"","title":"Classic Dispatchers"},{"location":"/dispatchers.html#classic-dispatchers","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see Dispatchers.","title":"Classic Dispatchers"},{"location":"/dispatchers.html#dependency","text":"Dispatchers are part of core Pekko, which means that they are part of the pekko-actor dependency:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/dispatchers.html#looking-up-a-dispatcher","text":"Dispatchers implement the ExecutionContextExecutor interface and can thus be used to run FutureCompletableFuture invocations etc.\nScala copysource// for use with Futures, Scheduler, etc.\nimplicit val executionContext = system.dispatchers.lookup(\"my-dispatcher\") Java copysource// this is scala.concurrent.ExecutionContextExecutor, which implements\n// both scala.concurrent.ExecutionContext (for use with Futures, Scheduler, etc.)\n// and java.util.concurrent.Executor (for use with CompletableFuture etc.)\nfinal ExecutionContextExecutor ex = system.dispatchers().lookup(\"my-dispatcher\");","title":"Looking up a Dispatcher"},{"location":"/dispatchers.html#setting-the-dispatcher-for-an-actor","text":"So in case you want to give your ActorActor a different dispatcher than the default, you need to do two things, of which the first is to configure the dispatcher:\ncopysourcemy-dispatcher {\n  # Dispatcher is the name of the event-based dispatcher\n  type = Dispatcher\n  # What kind of ExecutionService to use\n  executor = \"fork-join-executor\"\n  # Configuration for the fork join pool\n  fork-join-executor {\n    # Min number of threads to cap factor-based parallelism number to\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads to cap factor-based parallelism number to\n    parallelism-max = 10\n  }\n  # Throughput defines the maximum number of messages to be\n  # processed per actor before the thread jumps to the next actor.\n  # Set to 1 for as fair as possible.\n  throughput = 100\n}\nNote Note that the parallelism-max does not set the upper bound on the total number of threads allocated by the ForkJoinPool. It is a setting specifically talking about the number of hot threads the pool keep running in order to reduce the latency of handling a new incoming task. You can read more about parallelism in the JDK’s ForkJoinPool documentation.\nAnother example that uses the “thread-pool-executor”:\ncopysourceblocking-io-dispatcher {\n  type = Dispatcher\n  executor = \"thread-pool-executor\"\n  thread-pool-executor {\n    fixed-pool-size = 32\n  }\n  throughput = 1\n}\nNote The thread pool executor dispatcher is implemented using a java.util.concurrent.ThreadPoolExecutor. You can read more about it in the JDK’s ThreadPoolExecutor documentation.\nFor more options, see Dispatchers and the default-dispatcher section of the configuration.\nThen you create the actor as usual and define the dispatcher in the deployment configuration.\nScala copysourceimport org.apache.pekko.actor.Props\nval myActor = context.actorOf(Props[MyActor](), \"myactor\") Java copysourceActorRef myActor = system.actorOf(Props.create(MyActor.class), \"myactor\");\ncopysourcepekko.actor.deployment {\n  /myactor {\n    dispatcher = my-dispatcher\n  }\n}\nAn alternative to the deployment configuration is to define the dispatcher in code. If you define the dispatcher in the deployment configuration then this value will be used instead of programmatically provided parameter.\nScala copysourceimport org.apache.pekko.actor.Props\nval myActor =\n  context.actorOf(Props[MyActor]().withDispatcher(\"my-dispatcher\"), \"myactor1\") Java copysourceActorRef myActor =\n    system.actorOf(Props.create(MyActor.class).withDispatcher(\"my-dispatcher\"), \"myactor3\");\nNote The dispatcher you specify in withDispatcherwithDispatcher and the dispatcher property in the deployment configuration is in fact a path into your configuration. So in this example it’s a top-level section, but you could for instance put it as a sub-section, where you’d use periods to denote sub-sections, like this: \"foo.bar.my-dispatcher\"","title":"Setting the dispatcher for an Actor"},{"location":"/mailboxes.html","text":"","title":"Classic Mailboxes"},{"location":"/mailboxes.html#classic-mailboxes","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see mailboxes.","title":"Classic Mailboxes"},{"location":"/mailboxes.html#dependency","text":"To use Mailboxes, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/mailboxes.html#introduction","text":"A Pekko Mailbox holds the messages that are destined for an ActorActor. Normally each Actor has its own mailbox, but with for example a BalancingPoolBalancingPool all routees will share a single mailbox instance.\nFor more details on advanced mailbox config and custom mailbox implementations, see Mailboxes.","title":"Introduction"},{"location":"/mailboxes.html#mailbox-selection","text":"","title":"Mailbox Selection"},{"location":"/mailboxes.html#default-mailbox","text":"The default mailbox is used when the mailbox is not specified. This is an unbounded mailbox, backed by a java.util.concurrent.ConcurrentLinkedQueue.\nSingleConsumerOnlyUnboundedMailboxSingleConsumerOnlyUnboundedMailbox is an even more efficient mailbox, and it can be used as the default mailbox, but it cannot be used with a BalancingDispatcher.\nConfiguration of SingleConsumerOnlyUnboundedMailbox as default mailbox:\npekko.actor.default-mailbox {\n  mailbox-type = \"org.apache.pekko.dispatch.SingleConsumerOnlyUnboundedMailbox\"\n}","title":"Default Mailbox"},{"location":"/mailboxes.html#requiring-a-message-queue-type-for-an-actor","text":"It is possible to require a certain type of message queue for a certain type of actor by having that actor extendimplement the parameterized traitinterface RequiresMessageQueueRequiresMessageQueue. Here is an example:\nScala copysourceimport org.apache.pekko\nimport pekko.dispatch.RequiresMessageQueue\nimport pekko.dispatch.BoundedMessageQueueSemantics\n\nclass MyBoundedActor extends MyActor with RequiresMessageQueue[BoundedMessageQueueSemantics] Java copysourceimport org.apache.pekko.dispatch.BoundedMessageQueueSemantics;\nimport org.apache.pekko.dispatch.RequiresMessageQueue;\n\npublic class MyBoundedActor extends MyActor\n    implements RequiresMessageQueue<BoundedMessageQueueSemantics> {}\nThe type parameter to the RequiresMessageQueue traitinterface needs to be mapped to a mailbox in configuration like this:\ncopysourcebounded-mailbox {\n  mailbox-type = \"org.apache.pekko.dispatch.NonBlockingBoundedMailbox\"\n  mailbox-capacity = 1000 \n}\n\npekko.actor.mailbox.requirements {\n  \"org.apache.pekko.dispatch.BoundedMessageQueueSemantics\" = bounded-mailbox\n}\nNow every time you create an actor of type MyBoundedActor it will try to get a bounded mailbox. If the actor has a different mailbox configured in deployment, either directly or via a dispatcher with a specified mailbox type, then that will override this mapping.\nNote The type of the queue in the mailbox created for an actor will be checked against the required type in the traitinterface and if the queue doesn’t implement the required type then actor creation will fail.","title":"Requiring a Message Queue Type for an Actor"},{"location":"/mailboxes.html#requiring-a-message-queue-type-for-a-dispatcher","text":"A dispatcher may also have a requirement for the mailbox type used by the actors running on it. An example is the BalancingDispatcher which requires a message queue that is thread-safe for multiple concurrent consumers. Such a requirement is formulated within the dispatcher configuration section:\nmy-dispatcher {\n  mailbox-requirement = org.example.MyInterface\n}\nThe given requirement names a class or interface which will then be ensured to be a supertype of the message queue’s implementation. In case of a conflict—e.g. if the actor requires a mailbox type which does not satisfy this requirement—then actor creation will fail.","title":"Requiring a Message Queue Type for a Dispatcher"},{"location":"/mailboxes.html#how-the-mailbox-type-is-selected","text":"When an actor is created, the ActorRefProviderActorRefProvider first determines the dispatcher which will execute it. Then the mailbox is determined as follows:\nIf the actor’s deployment configuration section contains a mailbox key, this refers to a configuration section describing the mailbox type. If the actor’s PropsProps contains a mailbox selection then that names a configuration section describing the mailbox type to be used. This needs to be an absolute config path, for example myapp.special-mailbox, and is not nested inside the pekko namespace. If the dispatcher’s configuration section contains a mailbox-type key the same section will be used to configure the mailbox type. If the actor requires a mailbox type as described above then the mapping for that requirement will be used to determine the mailbox type to be used; if that fails then the dispatcher’s requirement—if any—will be tried instead. If the dispatcher requires a mailbox type as described above then the mapping for that requirement will be used to determine the mailbox type to be used. The default mailbox pekko.actor.default-mailbox will be used.","title":"How the Mailbox Type is Selected"},{"location":"/mailboxes.html#mailbox-configuration-examples","text":"","title":"Mailbox configuration examples"},{"location":"/mailboxes.html#prioritymailbox","text":"How to create a PriorityMailbox:\nScala copysourceimport org.apache.pekko\nimport pekko.dispatch.PriorityGenerator\nimport pekko.dispatch.UnboundedStablePriorityMailbox\nimport com.typesafe.config.Config\n\n// We inherit, in this case, from UnboundedStablePriorityMailbox\n// and seed it with the priority generator\nclass MyPrioMailbox(settings: ActorSystem.Settings, config: Config)\n    extends UnboundedStablePriorityMailbox(\n      // Create a new PriorityGenerator, lower prio means more important\n      PriorityGenerator {\n        // highpriority messages should be treated first if possible\n        case \"highpriority\" => 0\n\n        // lowpriority messages should be treated last if possible\n        case \"lowpriority\" => 2\n\n        // PoisonPill when no other left\n        case PoisonPill => 3\n\n        // We default to 1, which is in between high and low\n        case otherwise => 1\n      }) Java copysourcestatic class MyPrioMailbox extends UnboundedStablePriorityMailbox {\n  // needed for reflective instantiation\n  public MyPrioMailbox(ActorSystem.Settings settings, Config config) {\n    // Create a new PriorityGenerator, lower prio means more important\n    super(\n        new PriorityGenerator() {\n          @Override\n          public int gen(Object message) {\n            if (message.equals(\"highpriority\"))\n              return 0; // 'highpriority messages should be treated first if possible\n            else if (message.equals(\"lowpriority\"))\n              return 2; // 'lowpriority messages should be treated last if possible\n            else if (message.equals(PoisonPill.getInstance()))\n              return 3; // PoisonPill when no other left\n            else return 1; // By default they go between high and low prio\n          }\n        });\n  }\n}\nAnd then add it to the configuration:\ncopysourceprio-dispatcher {\n  mailbox-type = \"docs.dispatcher.DispatcherDocSpec$MyPrioMailbox\"\n  //Other dispatcher configuration goes here\n}\nAnd then an example on how you would use it:\nScala copysource // We create a new Actor that just prints out what it processes\nclass Logger extends Actor {\n  val log: LoggingAdapter = Logging(context.system, this)\n\n  self ! \"lowpriority\"\n  self ! \"lowpriority\"\n  self ! \"highpriority\"\n  self ! \"pigdog\"\n  self ! \"pigdog2\"\n  self ! \"pigdog3\"\n  self ! \"highpriority\"\n\n  self ! PoisonPill\n\n  def receive = {\n    case x => log.info(x.toString)\n  }\n}\nval a = system.actorOf(Props(classOf[Logger], this).withDispatcher(\"prio-dispatcher\"))\n\n/*\n * Logs:\n * highpriority\n * highpriority\n * pigdog\n * pigdog2\n * pigdog3\n * lowpriority\n * lowpriority\n */ Java copysource class Demo extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  {\n    for (Object msg :\n        new Object[] {\n          \"lowpriority\",\n          \"lowpriority\",\n          \"highpriority\",\n          \"pigdog\",\n          \"pigdog2\",\n          \"pigdog3\",\n          \"highpriority\",\n          PoisonPill.getInstance()\n        }) {\n      getSelf().tell(msg, getSelf());\n    }\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchAny(\n            message -> {\n              log.info(message.toString());\n            })\n        .build();\n  }\n}\n\n// We create a new Actor that just prints out what it processes\nActorRef myActor =\n    system.actorOf(Props.create(Demo.class, this).withDispatcher(\"prio-dispatcher\"));\n\n/*\nLogs:\n  'highpriority\n  'highpriority\n  'pigdog\n  'pigdog2\n  'pigdog3\n  'lowpriority\n  'lowpriority\n*/\nIt is also possible to configure a mailbox type directly like this (this is a top-level configuration entry):\nScala copysourceprio-mailbox {\n  mailbox-type = \"docs.dispatcher.DispatcherDocSpec$MyPrioMailbox\"\n  //Other mailbox configuration goes here\n}\n\npekko.actor.deployment {\n  /priomailboxactor {\n    mailbox = prio-mailbox\n  }\n} Java copysourceprio-mailbox {\n  mailbox-type = \"docs.dispatcher.DispatcherDocSpec$MyPrioMailbox\"\n  //Other mailbox configuration goes here\n}\n\npekko.actor.deployment {\n  /priomailboxactor {\n    mailbox = prio-mailbox\n  }\n}\nAnd then use it either from deployment like this:\nScala copysourceimport org.apache.pekko.actor.Props\nval myActor = context.actorOf(Props[MyActor](), \"priomailboxactor\") Java copysourceActorRef myActor = system.actorOf(Props.create(MyActor.class), \"priomailboxactor\");\nOr code like this:\nScala copysourceimport org.apache.pekko.actor.Props\nval myActor = context.actorOf(Props[MyActor]().withMailbox(\"prio-mailbox\")) Java copysourceActorRef myActor = system.actorOf(Props.create(MyActor.class).withMailbox(\"prio-mailbox\"));","title":"PriorityMailbox"},{"location":"/mailboxes.html#controlawaremailbox","text":"A ControlAwareMailbox can be very useful if an actor needs to be able to receive control messages immediately no matter how many other messages are already in its mailbox.\nIt can be configured like this:\ncopysourcecontrol-aware-dispatcher {\n  mailbox-type = \"org.apache.pekko.dispatch.UnboundedControlAwareMailbox\"\n  //Other dispatcher configuration goes here\n}\nControl messages need to extend the ControlMessageControlMessage traitinterface:\nScala copysourceimport org.apache.pekko.dispatch.ControlMessage\n\ncase object MyControlMessage extends ControlMessage Java copysourcestatic class MyControlMessage implements ControlMessage {}\nAnd then an example on how you would use it:\nScala copysource // We create a new Actor that just prints out what it processes\nclass Logger extends Actor {\n  val log: LoggingAdapter = Logging(context.system, this)\n\n  self ! \"foo\"\n  self ! \"bar\"\n  self ! MyControlMessage\n  self ! PoisonPill\n\n  def receive = {\n    case x => log.info(x.toString)\n  }\n}\nval a = system.actorOf(Props(classOf[Logger], this).withDispatcher(\"control-aware-dispatcher\"))\n\n/*\n * Logs:\n * MyControlMessage\n * foo\n * bar\n */ Java copysource class Demo extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  {\n    for (Object msg :\n        new Object[] {\"foo\", \"bar\", new MyControlMessage(), PoisonPill.getInstance()}) {\n      getSelf().tell(msg, getSelf());\n    }\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchAny(\n            message -> {\n              log.info(message.toString());\n            })\n        .build();\n  }\n}\n\n// We create a new Actor that just prints out what it processes\nActorRef myActor =\n    system.actorOf(Props.create(Demo.class, this).withDispatcher(\"control-aware-dispatcher\"));\n\n/*\nLogs:\n  'MyControlMessage\n  'foo\n  'bar\n*/","title":"ControlAwareMailbox"},{"location":"/mailboxes.html#special-semantics-of-system-actorof","text":"In order to make system.actorOfsystem.actorOf both synchronous and non-blocking while keeping the return type ActorRefActorRef (and the semantics that the returned ref is fully functional), special handling takes place for this case. Behind the scenes, a hollow kind of actor reference is constructed, which is sent to the system’s guardian actor who actually creates the actor and its context and puts those inside the reference. Until that has happened, messages sent to the ActorRefActorRef will be queued locally, and only upon swapping the real filling in will they be transferred into the real mailbox. Thus,\nScala val props: Props = ...\n// this actor uses MyCustomMailbox, which is assumed to be a singleton\nsystem.actorOf(props.withDispatcher(\"myCustomMailbox\")) ! \"bang\"\nassert(MyCustomMailbox.instance.getLastEnqueuedMessage == \"bang\")\n Java final Props props = ...\n// this actor uses MyCustomMailbox, which is assumed to be a singleton\nsystem.actorOf(props.withDispatcher(\"myCustomMailbox\").tell(\"bang\", sender);\nassert(MyCustomMailbox.getInstance().getLastEnqueued().equals(\"bang\"));\nwill probably fail; you will have to allow for some time to pass and retry the check à la TestKit.awaitCondTestKit.awaitCond.","title":"Special Semantics of system.actorOf"},{"location":"/routing.html","text":"","title":"Classic Routing"},{"location":"/routing.html#classic-routing","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the documentation of the new API of this feature and for new projects see routers.","title":"Classic Routing"},{"location":"/routing.html#dependency","text":"To use Routing, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/routing.html#introduction","text":"Messages can be sent via a router to efficiently route them to destination actors, known as its routees. A RouterRouter can be used inside or outside of an actor, and you can manage the routees yourselves or use a self contained router actor with configuration capabilities.\nDifferent routing strategies can be used, according to your application’s needs. Pekko comes with several useful routing strategies right out of the box. But, as you will see in this chapter, it is also possible to create your own.","title":"Introduction"},{"location":"/routing.html#a-simple-router","text":"The following example illustrates how to use a Router and manage the routees from within an actor.\nScala copysourceimport org.apache.pekko.routing.{ ActorRefRoutee, RoundRobinRoutingLogic, Router }\n\nclass Master extends Actor {\n  var router = {\n    val routees = Vector.fill(5) {\n      val r = context.actorOf(Props[Worker]())\n      context.watch(r)\n      ActorRefRoutee(r)\n    }\n    Router(RoundRobinRoutingLogic(), routees)\n  }\n\n  def receive = {\n    case w: Work =>\n      router.route(w, sender())\n    case Terminated(a) =>\n      router = router.removeRoutee(a)\n      val r = context.actorOf(Props[Worker]())\n      context.watch(r)\n      router = router.addRoutee(r)\n  }\n} Java copysourcestatic final class Work implements Serializable {\n  private static final long serialVersionUID = 1L;\n  public final String payload;\n\n  public Work(String payload) {\n    this.payload = payload;\n  }\n}\n\nstatic class Master extends AbstractActor {\n\n  Router router;\n\n  {\n    List<Routee> routees = new ArrayList<Routee>();\n    for (int i = 0; i < 5; i++) {\n      ActorRef r = getContext().actorOf(Props.create(Worker.class));\n      getContext().watch(r);\n      routees.add(new ActorRefRoutee(r));\n    }\n    router = new Router(new RoundRobinRoutingLogic(), routees);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Work.class,\n            message -> {\n              router.route(message, getSender());\n            })\n        .match(\n            Terminated.class,\n            message -> {\n              router = router.removeRoutee(message.actor());\n              ActorRef r = getContext().actorOf(Props.create(Worker.class));\n              getContext().watch(r);\n              router = router.addRoutee(new ActorRefRoutee(r));\n            })\n        .build();\n  }\n}\nWe create a Router and specify that it should use RoundRobinRoutingLogicRoundRobinRoutingLogic when routing the messages to the routees.\nThe routing logic shipped with Pekko are:\nRoundRobinRoutingLogicRoundRobinRoutingLogic RandomRoutingLogicRandomRoutingLogic SmallestMailboxRoutingLogicSmallestMailboxRoutingLogic BroadcastRoutingLogicBroadcastRoutingLogic ScatterGatherFirstCompletedRoutingLogicScatterGatherFirstCompletedRoutingLogic TailChoppingRoutingLogicTailChoppingRoutingLogic ConsistentHashingRoutingLogicConsistentHashingRoutingLogic\nWe create the routees as ordinary child actors wrapped in ActorRefRouteeActorRefRoutee. We watch the routees to be able to replace them if they are terminated.\nSending messages via the router is done with the routeroute method, as is done for the Work messages in the example above.\nThe Router is immutable and the RoutingLogicRoutingLogic is thread safe; meaning that they can also be used outside of actors.\nNote In general, any message sent to a router will be sent onwards to its routees, but there is one exception. The special Broadcast Messages will send to all of a router’s routees. However, do not use Broadcast Messages when you use BalancingPool for routees as described in Specially Handled Messages.","title":"A Simple Router"},{"location":"/routing.html#a-router-actor","text":"A router can also be created as a self contained actor that manages the routees itself and loads routing logic and other settings from configuration.\nThis type of router actor comes in two distinct flavors:\nPool - The router creates routees as child actors and removes them from the router if they terminate. Group - The routee actors are created externally to the router and the router sends messages to the specified path using actor selection, without watching for termination.\nThe settings for a router actor can be defined in configuration or programmatically. In order to make an actor to make use of an externally configurable router the FromConfig props wrapper must be used to denote that the actor accepts routing settings from configuration. This is in contrast with Remote Deployment where such marker props is not necessary. If the props of an actor is NOT wrapped in FromConfig it will ignore the router section of the deployment configuration.\nYou send messages to the routees via the router actor in the same way as for ordinary actors, i.e. via its ActorRefActorRef. The router actor forwards messages onto its routees without changing the original sender. When a routee replies to a routed message, the reply will be sent to the original sender, not to the router actor.\nNote In general, any message sent to a router will be sent onwards to its routees, but there are a few exceptions. These are documented in the Specially Handled Messages section below.","title":"A Router Actor"},{"location":"/routing.html#pool","text":"The following code and configuration snippets show how to create a round-robin router that forwards messages to five Worker routees. The routees will be created as the router’s children.\ncopysourcepekko.actor.deployment {\n  /parent/router1 {\n    router = round-robin-pool\n    nr-of-instances = 5\n  }\n}\nScala copysourceval router1: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router1\") Java copysourceActorRef router1 =\n    getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router1\");\nHere is the same example, but with the router configuration provided programmatically instead of from configuration.\nScala copysourceval router2: ActorRef =\n  context.actorOf(RoundRobinPool(5).props(Props[Worker]()), \"router2\") Java copysourceActorRef router2 =\n    getContext().actorOf(new RoundRobinPool(5).props(Props.create(Worker.class)), \"router2\");","title":"Pool"},{"location":"/routing.html#remote-deployed-routees","text":"In addition to being able to create local actors as routees, you can instruct the router to deploy its created children on a set of remote hosts. Routees will be deployed in round-robin fashion. In order to deploy routees remotely, wrap the router configuration in a RemoteRouterConfigRemoteRouterConfig, attaching the remote addresses of the nodes to deploy to. Remote deployment requires the pekko-remote module to be included in the classpath.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.{ Address, AddressFromURIString }\nimport pekko.remote.routing.RemoteRouterConfig\nval addresses =\n  Seq(Address(\"pekko\", \"remotesys\", \"otherhost\", 1234), AddressFromURIString(\"pekko://othersys@anotherhost:1234\"))\nval routerRemote = system.actorOf(RemoteRouterConfig(RoundRobinPool(5), addresses).props(Props[Echo]())) Java copysourceAddress[] addresses = {\n  new Address(\"pekko\", \"remotesys\", \"otherhost\", 1234),\n  AddressFromURIString.parse(\"pekko://othersys@anotherhost:1234\")\n};\nActorRef routerRemote =\n    system.actorOf(\n        new RemoteRouterConfig(new RoundRobinPool(5), addresses)\n            .props(Props.create(Echo.class)));","title":"Remote Deployed Routees"},{"location":"/routing.html#senders","text":"By default, when a routee sends a message, it will implicitly set itself as the sender .\nScala copysourcesender() ! x // replies will go to this actor Java copysourcegetSender().tell(\"reply\", getSelf());\nHowever, it is often useful for routees to set the router as a sender. For example, you might want to set the router as the sender if you want to hide the details of the routees behind the router. The following code snippet shows how to set the parent router as sender.\nScala copysourcesender().tell(\"reply\", context.parent) // replies will go back to parent\nsender().!(\"reply\")(context.parent) // alternative syntax Java copysourcegetSender().tell(\"reply\", getContext().getParent());","title":"Senders"},{"location":"/routing.html#supervision","text":"Routees that are created by a pool router will be created as the router’s children. The router is therefore also the children’s supervisor.\nThe supervision strategy of the router actor can be configured with the supervisorStrategy property of the Pool. If no configuration is provided, routers default to a strategy of “always escalate”. This means that errors are passed up to the router’s supervisor for handling. The router’s supervisor will decide what to do about any errors.\nNote the router’s supervisor will treat the error as an error with the router itself. Therefore a directive to stop or restart will cause the router itself to stop or restart. The router, in turn, will cause its children to stop and restart.\nIt should be mentioned that the router’s restart behavior has been overridden so that a restart, while still re-creating the children, will still preserve the same number of actors in the pool.\nThis means that if you have not specified supervisorStrategy of the router or its parent a failure in a routee will escalate to the parent of the router, which will by default restart the router, which will restart all routees (it uses Escalate and does not stop routees during restart). The reason is to make the default behave such that adding withRouterwithRouter to a child’s definition does not change the supervision strategy applied to the child. This might be an inefficiency that you can avoid by specifying the strategy when defining the router.\nSetting the strategy is done like this:\nScala copysourceval escalator = OneForOneStrategy() {\n  case e => testActor ! e; SupervisorStrategy.Escalate\n}\nval router =\n  system.actorOf(RoundRobinPool(1, supervisorStrategy = escalator).props(routeeProps = Props[TestActor]())) Java copysourcefinal SupervisorStrategy strategy =\n    new OneForOneStrategy(\n        5,\n        Duration.ofMinutes(1),\n        Collections.<Class<? extends Throwable>>singletonList(Exception.class));\nfinal ActorRef router =\n    system.actorOf(\n        new RoundRobinPool(5).withSupervisorStrategy(strategy).props(Props.create(Echo.class)));\nNote If the child of a pool router terminates, the pool router will not automatically spawn a new child. In the event that all children of a pool router have terminated the router will terminate itself unless it is a dynamic router, e.g. using a resizer.","title":"Supervision"},{"location":"/routing.html#group","text":"Sometimes, rather than having the router actor create its routees, it is desirable to create routees separately and provide them to the router for its use. You can do this by passing in paths of the routees to the router’s configuration. Messages will be sent with ActorSelectionActorSelection to these paths, wildcards can be and will result in the same semantics as explicitly using ActorSelection.\nThe example below shows how to create a router by providing it with the path strings of three routee actors.\ncopysourcepekko.actor.deployment {\n  /parent/router3 {\n    router = round-robin-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n  }\n}\nScala copysourceval router3: ActorRef =\n  context.actorOf(FromConfig.props(), \"router3\") Java copysourceActorRef router3 = getContext().actorOf(FromConfig.getInstance().props(), \"router3\");\nHere is the same example, but with the router configuration provided programmatically instead of from configuration.\nScala copysourceval paths = List(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\")\nval router4: ActorRef =\n  context.actorOf(RoundRobinGroup(paths).props(), \"router4\") Java copysourceList<String> paths = Arrays.asList(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\");\nActorRef router4 = getContext().actorOf(new RoundRobinGroup(paths).props(), \"router4\");\nThe routee actors are created externally from the router:\nScala copysourcesystem.actorOf(Props[Workers](), \"workers\") Java copysourcesystem.actorOf(Props.create(Workers.class), \"workers\");\nScala copysourceclass Workers extends Actor {\n  context.actorOf(Props[Worker](), name = \"w1\")\n  context.actorOf(Props[Worker](), name = \"w2\")\n  context.actorOf(Props[Worker](), name = \"w3\")\n  // ... Java copysourcestatic class Workers extends AbstractActor {\n  @Override\n  public void preStart() {\n    getContext().actorOf(Props.create(Worker.class), \"w1\");\n    getContext().actorOf(Props.create(Worker.class), \"w2\");\n    getContext().actorOf(Props.create(Worker.class), \"w3\");\n  }\n  // ...\nThe paths may contain protocol and address information for actors running on remote hosts. Remoting requires the pekko-remote module to be included in the classpath.\ncopysourcepekko.actor.deployment {\n  /parent/remoteGroup {\n    router = round-robin-group\n    routees.paths = [\n      \"pekko://app@10.0.0.1:2552/user/workers/w1\",\n      \"pekko://app@10.0.0.2:2552/user/workers/w1\",\n      \"pekko://app@10.0.0.3:2552/user/workers/w1\"]\n  }\n}","title":"Group"},{"location":"/routing.html#router-usage","text":"In this section we will describe how to create the different types of router actors.\nThe router actors in this section are created from within a top level actor named parent. Note that deployment paths in the configuration starts with /parent/ followed by the name of the router actor.\nScala copysourcesystem.actorOf(Props[Parent](), \"parent\") Java copysourcesystem.actorOf(Props.create(Parent.class), \"parent\");","title":"Router usage"},{"location":"/routing.html#roundrobinpool-and-roundrobingroup","text":"Routes in a round-robin fashion to its routees.\nRoundRobinPool defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router1 {\n    router = round-robin-pool\n    nr-of-instances = 5\n  }\n}\nScala copysourceval router1: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router1\") Java copysourceActorRef router1 =\n    getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router1\");\nRoundRobinPool defined in code:\nScala copysourceval router2: ActorRef =\n  context.actorOf(RoundRobinPool(5).props(Props[Worker]()), \"router2\") Java copysourceActorRef router2 =\n    getContext().actorOf(new RoundRobinPool(5).props(Props.create(Worker.class)), \"router2\");\nRoundRobinGroup defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router3 {\n    router = round-robin-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n  }\n}\nScala copysourceval router3: ActorRef =\n  context.actorOf(FromConfig.props(), \"router3\") Java copysourceActorRef router3 = getContext().actorOf(FromConfig.getInstance().props(), \"router3\");\nRoundRobinGroup defined in code:\nScala copysourceval paths = List(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\")\nval router4: ActorRef =\n  context.actorOf(RoundRobinGroup(paths).props(), \"router4\") Java copysourceList<String> paths = Arrays.asList(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\");\nActorRef router4 = getContext().actorOf(new RoundRobinGroup(paths).props(), \"router4\");","title":"RoundRobinPool and RoundRobinGroup"},{"location":"/routing.html#randompool-and-randomgroup","text":"This router type selects one of its routees randomly for each message.\nRandomPool defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router5 {\n    router = random-pool\n    nr-of-instances = 5\n  }\n}\nScala copysourceval router5: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router5\") Java copysourceActorRef router5 =\n    getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router5\");\nRandomPool defined in code:\nScala copysourceval router6: ActorRef =\n  context.actorOf(RandomPool(5).props(Props[Worker]()), \"router6\") Java copysourceActorRef router6 =\n    getContext().actorOf(new RandomPool(5).props(Props.create(Worker.class)), \"router6\");\nRandomGroup defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router7 {\n    router = random-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n  }\n}\nScala copysourceval router7: ActorRef =\n  context.actorOf(FromConfig.props(), \"router7\") Java copysourceActorRef router7 = getContext().actorOf(FromConfig.getInstance().props(), \"router7\");\nRandomGroup defined in code:\nScala copysourceval paths = List(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\")\nval router8: ActorRef =\n  context.actorOf(RandomGroup(paths).props(), \"router8\") Java copysourceList<String> paths = Arrays.asList(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\");\nActorRef router8 = getContext().actorOf(new RandomGroup(paths).props(), \"router8\");","title":"RandomPool and RandomGroup"},{"location":"/routing.html#balancingpool","text":"A Router that will try to redistribute work from busy routees to idle routees. All routees share the same mailbox.\nNote The BalancingPool has the property that its routees do not have truly distinct identity: they have different names, but talking to them will not end up at the right actor in most cases. Therefore you cannot use it for workflows that require state to be kept within the routee, you would in this case have to include the whole state in the messages. With a SmallestMailboxPool you can have a vertically scaling service that can interact in a stateful fashion with other services in the back-end before replying to the original client. The other advantage is that it does not place a restriction on the message queue implementation as BalancingPool does.\nNote Do not use Broadcast Messages when you use BalancingPool for routers, as described in Specially Handled Messages.\nBalancingPool defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router9 {\n    router = balancing-pool\n    nr-of-instances = 5\n  }\n}\nScala copysourceval router9: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router9\") Java copysourceActorRef router9 =\n    getContext().actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router9\");\nBalancingPool defined in code:\nScala copysourceval router10: ActorRef =\n  context.actorOf(BalancingPool(5).props(Props[Worker]()), \"router10\") Java copysourceActorRef router10 =\n    getContext().actorOf(new BalancingPool(5).props(Props.create(Worker.class)), \"router10\");\nAddition configuration for the balancing dispatcher, which is used by the pool, can be configured in the pool-dispatcher section of the router deployment configuration.\ncopysourcepekko.actor.deployment {\n  /parent/router9b {\n    router = balancing-pool\n    nr-of-instances = 5\n    pool-dispatcher {\n      attempt-teamwork = off\n    }\n  }\n}\nThe BalancingPoolBalancingPool automatically uses a special BalancingDispatcher for its routees - disregarding any dispatcher that is set on the routee Props object. This is needed in order to implement the balancing semantics via sharing the same mailbox by all the routees.\nWhile it is not possible to change the dispatcher used by the routees, it is possible to fine tune the used executor. By default the fork-join-dispatcher is used and can be configured as explained in Dispatchers. In situations where the routees are expected to perform blocking operations it may be useful to replace it with a thread-pool-executor hinting the number of allocated threads explicitly:\ncopysourcepekko.actor.deployment {\n  /parent/router10b {\n    router = balancing-pool\n    nr-of-instances = 5\n    pool-dispatcher {\n      executor = \"thread-pool-executor\"\n\n      # allocate exactly 5 threads for this pool\n      thread-pool-executor {\n        core-pool-size-min = 5\n        core-pool-size-max = 5\n      }\n    }\n  }\n}\nIt is also possible to change the mailbox used by the balancing dispatcher for scenarios where the default unbounded mailbox is not well suited. An example of such a scenario could arise whether there exists the need to manage priority for each message. You can then implement a priority mailbox and configure your dispatcher:\ncopysourcepekko.actor.deployment {\n  /parent/router10c {\n    router = balancing-pool\n    nr-of-instances = 5\n    pool-dispatcher {\n      mailbox = myapp.myprioritymailbox\n    }\n  }\n}\nNote Bear in mind that BalancingDispatcher requires a message queue that must be thread-safe for multiple concurrent consumers. So it is mandatory for the message queue backing a custom mailbox for this kind of dispatcher to implement org.apache.pekko.dispatch.MultipleConsumerSemantics. See details on how to implement your custom mailbox in Mailboxes.\nThere is no Group variant of the BalancingPool.","title":"BalancingPool"},{"location":"/routing.html#smallestmailboxpool","text":"A Router that tries to send to the non-suspended child routee with fewest messages in mailbox. The selection is done in this order:\npick any idle routee (not processing message) with empty mailbox pick any routee with empty mailbox pick routee with fewest pending messages in mailbox pick any remote routee, remote actors are consider lowest priority, since their mailbox size is unknown\nSmallestMailboxPool defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router11 {\n    router = smallest-mailbox-pool\n    nr-of-instances = 5\n  }\n}\nScala copysourceval router11: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router11\") Java copysourceActorRef router11 =\n    getContext()\n        .actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router11\");\nSmallestMailboxPool defined in code:\nScala copysourceval router12: ActorRef =\n  context.actorOf(SmallestMailboxPool(5).props(Props[Worker]()), \"router12\") Java copysourceActorRef router12 =\n    getContext()\n        .actorOf(new SmallestMailboxPool(5).props(Props.create(Worker.class)), \"router12\");\nThere is no Group variant of the SmallestMailboxPool because the size of the mailbox and the internal dispatching state of the actor is not practically available from the paths of the routees.","title":"SmallestMailboxPool"},{"location":"/routing.html#broadcastpool-and-broadcastgroup","text":"A broadcast router forwards the message it receives to all its routees.\nBroadcastPool defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router13 {\n    router = broadcast-pool\n    nr-of-instances = 5\n  }\n}\nScala copysourceval router13: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router13\") Java copysourceActorRef router13 =\n    getContext()\n        .actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router13\");\nBroadcastPool defined in code:\nScala copysourceval router14: ActorRef =\n  context.actorOf(BroadcastPool(5).props(Props[Worker]()), \"router14\") Java copysourceActorRef router14 =\n    getContext().actorOf(new BroadcastPool(5).props(Props.create(Worker.class)), \"router14\");\nBroadcastGroup defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router15 {\n    router = broadcast-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n  }\n}\nScala copysourceval router15: ActorRef =\n  context.actorOf(FromConfig.props(), \"router15\") Java copysourceActorRef router15 = getContext().actorOf(FromConfig.getInstance().props(), \"router15\");\nBroadcastGroup defined in code:\nScala copysourceval paths = List(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\")\nval router16: ActorRef =\n  context.actorOf(BroadcastGroup(paths).props(), \"router16\") Java copysourceList<String> paths = Arrays.asList(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\");\nActorRef router16 = getContext().actorOf(new BroadcastGroup(paths).props(), \"router16\");\nNote Broadcast routers always broadcast every message to their routees. If you do not want to broadcast every message, then you can use a non-broadcasting router and use Broadcast Messages as needed.","title":"BroadcastPool and BroadcastGroup"},{"location":"/routing.html#scattergatherfirstcompletedpool-and-scattergatherfirstcompletedgroup","text":"The ScatterGatherFirstCompletedRouter will send the message on to all its routees. It then waits for first reply it gets back. This result will be sent back to original sender. Other replies are discarded.\nIt is expecting at least one reply within a configured duration, otherwise it will reply with AskTimeoutExceptionAskTimeoutException in a Status.FailureStatus.Failure.\nScatterGatherFirstCompletedPool defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router17 {\n    router = scatter-gather-pool\n    nr-of-instances = 5\n    within = 10 seconds\n  }\n}\nScala copysourceval router17: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router17\") Java copysourceActorRef router17 =\n    getContext()\n        .actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router17\");\nScatterGatherFirstCompletedPool defined in code:\nScala copysourceval router18: ActorRef =\n  context.actorOf(ScatterGatherFirstCompletedPool(5, within = 10.seconds).props(Props[Worker]()), \"router18\") Java copysourceDuration within = Duration.ofSeconds(10);\nActorRef router18 =\n    getContext()\n        .actorOf(\n            new ScatterGatherFirstCompletedPool(5, within).props(Props.create(Worker.class)),\n            \"router18\");\nScatterGatherFirstCompletedGroup defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router19 {\n    router = scatter-gather-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n    within = 10 seconds\n  }\n}\nScala copysourceval router19: ActorRef =\n  context.actorOf(FromConfig.props(), \"router19\") Java copysourceActorRef router19 = getContext().actorOf(FromConfig.getInstance().props(), \"router19\");\nScatterGatherFirstCompletedGroup defined in code:\nScala copysourceval paths = List(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\")\nval router20: ActorRef =\n  context.actorOf(ScatterGatherFirstCompletedGroup(paths, within = 10.seconds).props(), \"router20\") Java copysourceList<String> paths = Arrays.asList(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\");\nDuration within2 = Duration.ofSeconds(10);\nActorRef router20 =\n    getContext()\n        .actorOf(new ScatterGatherFirstCompletedGroup(paths, within2).props(), \"router20\");","title":"ScatterGatherFirstCompletedPool and ScatterGatherFirstCompletedGroup"},{"location":"/routing.html#tailchoppingpool-and-tailchoppinggroup","text":"The TailChoppingRouter will first send the message to one, randomly picked, routee and then after a small delay to a second routee (picked randomly from the remaining routees) and so on. It waits for first reply it gets back and forwards it back to original sender. Other replies are discarded.\nThe goal of this router is to decrease latency by performing redundant queries to multiple routees, assuming that one of the other actors may still be faster to respond than the initial one.\nThis optimisation was described nicely in a blog post by Peter Bailis: Doing redundant work to speed up distributed queries.\nTailChoppingPool defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router21 {\n    router = tail-chopping-pool\n    nr-of-instances = 5\n    within = 10 seconds\n    tail-chopping-router.interval = 20 milliseconds\n  }\n}\nScala copysourceval router21: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router21\") Java copysourceActorRef router21 =\n    getContext()\n        .actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router21\");\nTailChoppingPool defined in code:\nScala copysourceval router22: ActorRef =\n  context.actorOf(TailChoppingPool(5, within = 10.seconds, interval = 20.millis).props(Props[Worker]()), \"router22\") Java copysourceDuration within3 = Duration.ofSeconds(10);\nDuration interval = Duration.ofMillis(20);\nActorRef router22 =\n    getContext()\n        .actorOf(\n            new TailChoppingPool(5, within3, interval).props(Props.create(Worker.class)),\n            \"router22\");\nTailChoppingGroup defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router23 {\n    router = tail-chopping-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n    within = 10 seconds\n    tail-chopping-router.interval = 20 milliseconds\n  }\n}\nScala copysourceval router23: ActorRef =\n  context.actorOf(FromConfig.props(), \"router23\") Java copysourceActorRef router23 = getContext().actorOf(FromConfig.getInstance().props(), \"router23\");\nTailChoppingGroup defined in code:\nScala copysourceval paths = List(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\")\nval router24: ActorRef =\n  context.actorOf(TailChoppingGroup(paths, within = 10.seconds, interval = 20.millis).props(), \"router24\") Java copysourceList<String> paths = Arrays.asList(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\");\nDuration within4 = Duration.ofSeconds(10);\nDuration interval2 = Duration.ofMillis(20);\nActorRef router24 =\n    getContext().actorOf(new TailChoppingGroup(paths, within4, interval2).props(), \"router24\");","title":"TailChoppingPool and TailChoppingGroup"},{"location":"/routing.html#consistenthashingpool-and-consistenthashinggroup","text":"The ConsistentHashingPool uses consistent hashing to select a routee based on the sent message. This article gives good insight into how consistent hashing is implemented.\nThere is 3 ways to define what data to use for the consistent hash key.\nYou can define hashMappingwithHashMapper of the router to map incoming messages to their consistent hash key. This makes the decision transparent for the sender. The messages may implement ConsistentHashingRouter.ConsistentHashableConsistentHashingRouter.ConsistentHashable. The key is part of the message and it’s convenient to define it together with the message definition. The messages can be wrapped in a ConsistentHashingRouter.ConsistentHashableEnvelopeConsistentHashingRouter.ConsistentHashableEnvelope to define what data to use for the consistent hash key. The sender knows the key to use.\nThese ways to define the consistent hash key can be use together and at the same time for one router. The hashMappingwithHashMapper is tried first.\nCode example:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.Actor\nimport pekko.routing.ConsistentHashingRouter.ConsistentHashable\n\nclass Cache extends Actor {\n  var cache = Map.empty[String, String]\n\n  def receive = {\n    case Entry(key, value) => cache += (key -> value)\n    case Get(key)          => sender() ! cache.get(key)\n    case Evict(key)        => cache -= key\n  }\n}\n\nfinal case class Evict(key: String)\n\nfinal case class Get(key: String) extends ConsistentHashable {\n  override def consistentHashKey: Any = key\n}\n\nfinal case class Entry(key: String, value: String) Java copysourcestatic class Cache extends AbstractActor {\n  Map<String, String> cache = new HashMap<String, String>();\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Entry.class,\n            entry -> {\n              cache.put(entry.key, entry.value);\n            })\n        .match(\n            Get.class,\n            get -> {\n              Object value = cache.get(get.key);\n              getSender().tell(value == null ? NOT_FOUND : value, getSelf());\n            })\n        .match(\n            Evict.class,\n            evict -> {\n              cache.remove(evict.key);\n            })\n        .build();\n  }\n}\n\nstatic final class Evict implements Serializable {\n  private static final long serialVersionUID = 1L;\n  public final String key;\n\n  public Evict(String key) {\n    this.key = key;\n  }\n}\n\nstatic final class Get implements Serializable, ConsistentHashable {\n  private static final long serialVersionUID = 1L;\n  public final String key;\n\n  public Get(String key) {\n    this.key = key;\n  }\n\n  public Object consistentHashKey() {\n    return key;\n  }\n}\n\nstatic final class Entry implements Serializable {\n  private static final long serialVersionUID = 1L;\n  public final String key;\n  public final String value;\n\n  public Entry(String key, String value) {\n    this.key = key;\n    this.value = value;\n  }\n}\n\nstatic final String NOT_FOUND = \"NOT_FOUND\";\nScala copysourceimport org.apache.pekko\nimport pekko.actor.Props\nimport pekko.routing.ConsistentHashingPool\nimport pekko.routing.ConsistentHashingRouter.ConsistentHashMapping\nimport pekko.routing.ConsistentHashingRouter.ConsistentHashableEnvelope\n\ndef hashMapping: ConsistentHashMapping = {\n  case Evict(key) => key\n}\n\nval cache: ActorRef =\n  context.actorOf(ConsistentHashingPool(10, hashMapping = hashMapping).props(Props[Cache]()), name = \"cache\")\n\ncache ! ConsistentHashableEnvelope(message = Entry(\"hello\", \"HELLO\"), hashKey = \"hello\")\ncache ! ConsistentHashableEnvelope(message = Entry(\"hi\", \"HI\"), hashKey = \"hi\")\n\ncache ! Get(\"hello\")\nexpectMsg(Some(\"HELLO\"))\n\ncache ! Get(\"hi\")\nexpectMsg(Some(\"HI\"))\n\ncache ! Evict(\"hi\")\ncache ! Get(\"hi\")\nexpectMsg(None)\n Java copysource final ConsistentHashMapper hashMapper =\n    new ConsistentHashMapper() {\n      @Override\n      public Object hashKey(Object message) {\n        if (message instanceof Evict) {\n          return ((Evict) message).key;\n        } else {\n          return null;\n        }\n      }\n    };\n\nActorRef cache =\n    system.actorOf(\n        new ConsistentHashingPool(10)\n            .withHashMapper(hashMapper)\n            .props(Props.create(Cache.class)),\n        \"cache\");\n\ncache.tell(new ConsistentHashableEnvelope(new Entry(\"hello\", \"HELLO\"), \"hello\"), getRef());\ncache.tell(new ConsistentHashableEnvelope(new Entry(\"hi\", \"HI\"), \"hi\"), getRef());\n\ncache.tell(new Get(\"hello\"), getRef());\nexpectMsgEquals(\"HELLO\");\n\ncache.tell(new Get(\"hi\"), getRef());\nexpectMsgEquals(\"HI\");\n\ncache.tell(new Evict(\"hi\"), getRef());\ncache.tell(new Get(\"hi\"), getRef());\nexpectMsgEquals(NOT_FOUND);\nIn the above example you see that the Get message implements ConsistentHashable itself, while the Entry message is wrapped in a ConsistentHashableEnvelope. The Evict message is handled by the hashMapping partial function.\nConsistentHashingPool defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router25 {\n    router = consistent-hashing-pool\n    nr-of-instances = 5\n    virtual-nodes-factor = 10\n  }\n}\nScala copysourceval router25: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router25\") Java copysourceActorRef router25 =\n    getContext()\n        .actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router25\");\nConsistentHashingPool defined in code:\nScala copysourceval router26: ActorRef =\n  context.actorOf(ConsistentHashingPool(5).props(Props[Worker]()), \"router26\") Java copysourceActorRef router26 =\n    getContext()\n        .actorOf(new ConsistentHashingPool(5).props(Props.create(Worker.class)), \"router26\");\nConsistentHashingGroup defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router27 {\n    router = consistent-hashing-group\n    routees.paths = [\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\"]\n    virtual-nodes-factor = 10\n  }\n}\nScala copysourceval router27: ActorRef =\n  context.actorOf(FromConfig.props(), \"router27\") Java copysourceActorRef router27 = getContext().actorOf(FromConfig.getInstance().props(), \"router27\");\nConsistentHashingGroup defined in code:\nScala copysourceval paths = List(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\")\nval router28: ActorRef =\n  context.actorOf(ConsistentHashingGroup(paths).props(), \"router28\") Java copysourceList<String> paths = Arrays.asList(\"/user/workers/w1\", \"/user/workers/w2\", \"/user/workers/w3\");\nActorRef router28 = getContext().actorOf(new ConsistentHashingGroup(paths).props(), \"router28\");\nvirtual-nodes-factor is the number of virtual nodes per routee that is used in the consistent hash node ring to make the distribution more uniform.","title":"ConsistentHashingPool and ConsistentHashingGroup"},{"location":"/routing.html#specially-handled-messages","text":"Most messages sent to router actors will be forwarded according to the routers’ routing logic. However there are a few types of messages that have special behavior.\nNote that these special messages, except for the BroadcastBroadcast message, are only handled by self contained router actors and not by the RouterRouter component described in A Simple Router.","title":"Specially Handled Messages"},{"location":"/routing.html#broadcast-messages","text":"A Broadcast message can be used to send a message to all of a router’s routees. When a router receives a Broadcast message, it will broadcast that message’s payload to all routees, no matter how that router would normally route its messages.\nThe example below shows how you would use a Broadcast message to send a very important message to every routee of a router.\nScala copysourceimport org.apache.pekko.routing.Broadcast\nrouter ! Broadcast(\"Watch out for Davy Jones' locker\") Java copysourcerouter.tell(new Broadcast(\"Watch out for Davy Jones' locker\"), getTestActor());\nIn this example the router receives the Broadcast message, extracts its payload (\"Watch out for Davy Jones' locker\"), and then sends the payload on to all of the router’s routees. It is up to each routee actor to handle the received payload message.\nNote Do not use Broadcast Messages when you use BalancingPool for routers. Routees on BalancingPool shares the same mailbox instance, thus some routees can possibly get the broadcast message multiple times, while other routees get no broadcast message.","title":"Broadcast Messages"},{"location":"/routing.html#poisonpill-messages","text":"A PoisonPillPoisonPill message has special handling for all actors, including for routers. When any actor receives a PoisonPill message, that actor will be stopped. See the PoisonPill documentation for details.\nScala copysourceimport org.apache.pekko.actor.PoisonPill\nrouter ! PoisonPill Java copysourcerouter.tell(PoisonPill.getInstance(), getTestActor());\nFor a router, which normally passes on messages to routees, it is important to realise that PoisonPill messages are processed by the router only. PoisonPill messages sent to a router will not be sent on to routees.\nHowever, a PoisonPill message sent to a router may still affect its routees, because it will stop the router and when the router stops it also stops its children. Stopping children is normal actor behavior. The router will stop routees that it has created as children. Each child will process its current message and then stop. This may lead to some messages being unprocessed. See the documentation on Stopping actors for more information.\nIf you wish to stop a router and its routees, but you would like the routees to first process all the messages currently in their mailboxes, then you should not send a PoisonPill message to the router. Instead you should wrap a PoisonPill message inside a Broadcast message so that each routee will receive the PoisonPill message. Note that this will stop all routees, even if the routees aren’t children of the router, i.e. even routees programmatically provided to the router.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.PoisonPill\nimport pekko.routing.Broadcast\nrouter ! Broadcast(PoisonPill) Java copysourcerouter.tell(new Broadcast(PoisonPill.getInstance()), getTestActor());\nWith the code shown above, each routee will receive a PoisonPill message. Each routee will continue to process its messages as normal, eventually processing the PoisonPill. This will cause the routee to stop. After all routees have stopped the router will itself be stopped automatically unless it is a dynamic router, e.g. using a resizer.","title":"PoisonPill Messages"},{"location":"/routing.html#kill-messages","text":"KillKill messages are another type of message that has special handling. See Killing an Actor for general information about how actors handle Kill messages.\nWhen a Kill message is sent to a router the router processes the message internally, and does not send it on to its routees. The router will throw an ActorKilledExceptionActorKilledException and fail. It will then be either resumed, restarted or terminated, depending how it is supervised.\nRoutees that are children of the router will also be suspended, and will be affected by the supervision directive that is applied to the router. Routees that are not the routers children, i.e. those that were created externally to the router, will not be affected.\nScala copysourceimport org.apache.pekko.actor.Kill\nrouter ! Kill Java copysourcerouter.tell(Kill.getInstance(), getTestActor());\nAs with the PoisonPillPoisonPill message, there is a distinction between killing a router, which indirectly kills its children (who happen to be routees), and killing routees directly (some of whom may not be children.) To kill routees directly the router should be sent a Kill message wrapped in a BroadcastBroadcast message.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.Kill\nimport pekko.routing.Broadcast\nrouter ! Broadcast(Kill) Java copysourcerouter.tell(new Broadcast(Kill.getInstance()), getTestActor());","title":"Kill Messages"},{"location":"/routing.html#management-messages","text":"Sending GetRouteesGetRoutees to a router actor will make it send back its currently used routees in a RouteesRoutees message. Sending AddRouteeAddRoutee to a router actor will add that routee to its collection of routees. Sending RemoveRouteeRemoveRoutee to a router actor will remove that routee to its collection of routees. Sending AdjustPoolSizeAdjustPoolSize to a pool router actor will add or remove that number of routees to its collection of routees.\nThese management messages may be handled after other messages, so if you send AddRoutee immediately followed by an ordinary message you are not guaranteed that the routees have been changed when the ordinary message is routed. If you need to know when the change has been applied you can send AddRoutee followed by GetRoutees and when you receive the Routees reply you know that the preceding change has been applied.","title":"Management Messages"},{"location":"/routing.html#dynamically-resizable-pool","text":"MostAll pools can be used with a fixed number of routees or with a resize strategy to adjust the number of routees dynamically.\nThere are two types of resizers: the default ResizerResizer and the OptimalSizeExploringResizerOptimalSizeExploringResizer.","title":"Dynamically Resizable Pool"},{"location":"/routing.html#default-resizer","text":"The default resizer ramps up and down pool size based on pressure, measured by the percentage of busy routees in the pool. It ramps up pool size if the pressure is higher than a certain threshold and backs off if the pressure is lower than certain threshold. Both thresholds are configurable.\nPool with default resizer defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router29 {\n    router = round-robin-pool\n    resizer {\n      lower-bound = 2\n      upper-bound = 15\n      messages-per-resize = 100\n    }\n  }\n}\nScala copysourceval router29: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router29\") Java copysourceActorRef router29 =\n    getContext()\n        .actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router29\");\nSeveral more configuration options are available and described in pekko.actor.deployment.default.resizer section of the reference configuration.\nPool with resizer defined in code:\nScala copysourceval resizer = DefaultResizer(lowerBound = 2, upperBound = 15)\nval router30: ActorRef =\n  context.actorOf(RoundRobinPool(5, Some(resizer)).props(Props[Worker]()), \"router30\") Java copysourceDefaultResizer resizer = new DefaultResizer(2, 15);\nActorRef router30 =\n    getContext()\n        .actorOf(\n            new RoundRobinPool(5).withResizer(resizer).props(Props.create(Worker.class)),\n            \"router30\");\nIt is also worth pointing out that if you define the router in the configuration file then this value will be used instead of any programmatically sent parameters.","title":"Default Resizer"},{"location":"/routing.html#optimal-size-exploring-resizer","text":"The OptimalSizeExploringResizerOptimalSizeExploringResizer resizes the pool to an optimal size that provides the most message throughput.\nThis resizer works best when you expect the pool size to performance function to be a convex function. For example, when you have a CPU bound tasks, the optimal size is bound to the number of CPU cores. When your task is IO bound, the optimal size is bound to optimal number of concurrent connections to that IO service - e.g. a 4 node elastic search cluster may handle 4-8 concurrent requests at optimal speed.\nIt achieves this by keeping track of message throughput at each pool size and performing the following three resizing operations (one at a time) periodically:\nDownsize if it hasn’t seen all routees ever fully utilized for a period of time. Explore to a random nearby pool size to try and collect throughput metrics. Optimize to a nearby pool size with a better (than any other nearby sizes) throughput metrics.\nWhen the pool is fully-utilized (i.e. all routees are busy), it randomly choose between exploring and optimizing. When the pool has not been fully-utilized for a period of time, it will downsize the pool to the last seen max utilization multiplied by a configurable ratio.\nBy constantly exploring and optimizing, the resizer will eventually walk to the optimal size and remain nearby. When the optimal size changes it will start walking towards the new one.\nIt keeps a performance log so it’s stateful as well as having a larger memory footprint than the default Resizer. The memory usage is O(n) where n is the number of sizes you allow, i.e. upperBound - lowerBound.\nPool with OptimalSizeExploringResizer defined in configuration:\ncopysourcepekko.actor.deployment {\n  /parent/router31 {\n    router = round-robin-pool\n    optimal-size-exploring-resizer {\n      enabled = on\n      action-interval = 5s\n      downsize-after-underutilized-for = 72h\n    }\n  }\n}\nScala copysourceval router31: ActorRef =\n  context.actorOf(FromConfig.props(Props[Worker]()), \"router31\") Java copysourceActorRef router31 =\n    getContext()\n        .actorOf(FromConfig.getInstance().props(Props.create(Worker.class)), \"router31\");\nSeveral more configuration options are available and described in pekko.actor.deployment.default.optimal-size-exploring-resizer section of the reference configuration.\nNote Resizing is triggered by sending messages to the actor pool, but it is not completed synchronously; instead a message is sent to the “head” RouterActor to perform the size change. Thus you cannot rely on resizing to instantaneously create new workers when all others are busy, because the message just sent will be queued to the mailbox of a busy actor. To remedy this, configure the pool to use a balancing dispatcher, see Configuring Dispatchers for more information.","title":"Optimal Size Exploring Resizer"},{"location":"/routing.html#how-routing-is-designed-within-pekko","text":"On the surface routers look like normal actors, but they are actually implemented differently. Routers are designed to be extremely efficient at receiving messages and passing them quickly on to routees.\nA normal actor can be used for routing messages, but an actor’s single-threaded processing can become a bottleneck. Routers can achieve much higher throughput with an optimization to the usual message-processing pipeline that allows concurrent routing. This is achieved by embedding routers’ routing logic directly in their ActorRefActorRef rather than in the router actor. Messages sent to a router’s ActorRef can be immediately routed to the routee, bypassing the single-threaded router actor entirely.\nThe cost to this is that the internals of routing code are more complicated than if routers were implemented with normal actors. Fortunately all of this complexity is invisible to consumers of the routing API. However, it is something to be aware of when implementing your own routers.","title":"How Routing is Designed within Pekko"},{"location":"/routing.html#custom-router","text":"You can create your own router should you not find any of the ones provided by Pekko sufficient for your needs. In order to roll your own router you have to fulfill certain criteria which are explained in this section.\nBefore creating your own router you should consider whether a normal actor with router-like behavior might do the job just as well as a full-blown router. As explained above, the primary benefit of routers over normal actors is their higher performance. But they are somewhat more complicated to write than normal actors. Therefore if lower maximum throughput is acceptable in your application you may wish to stick with traditional actors. This section, however, assumes that you wish to get maximum performance and so demonstrates how you can create your own router.\nThe router created in this example is replicating each message to a few destinations.\nStart with the routing logic:\nScala copysourceimport scala.collection.immutable\nimport java.util.concurrent.ThreadLocalRandom\nimport org.apache.pekko\nimport pekko.routing.RoundRobinRoutingLogic\nimport pekko.routing.RoutingLogic\nimport pekko.routing.Routee\nimport pekko.routing.SeveralRoutees\n\nclass RedundancyRoutingLogic(nbrCopies: Int) extends RoutingLogic {\n  val roundRobin = RoundRobinRoutingLogic()\n  def select(message: Any, routees: immutable.IndexedSeq[Routee]): Routee = {\n    val targets = (1 to nbrCopies).map(_ => roundRobin.select(message, routees))\n    SeveralRoutees(targets)\n  }\n} Java copysourcestatic class RedundancyRoutingLogic implements RoutingLogic {\n  private final int nbrCopies;\n\n  public RedundancyRoutingLogic(int nbrCopies) {\n    this.nbrCopies = nbrCopies;\n  }\n\n  RoundRobinRoutingLogic roundRobin = new RoundRobinRoutingLogic();\n\n  @Override\n  public Routee select(Object message, IndexedSeq<Routee> routees) {\n    List<Routee> targets = new ArrayList<Routee>();\n    for (int i = 0; i < nbrCopies; i++) {\n      targets.add(roundRobin.select(message, routees));\n    }\n    return new SeveralRoutees(targets);\n  }\n}\nselect will be called for each message and in this example pick a few destinations by round-robin, by reusing the existing RoundRobinRoutingLogicRoundRobinRoutingLogic and wrap the result in a SeveralRouteesSeveralRoutees instance. SeveralRoutees will send the message to all of the supplied routes.\nThe implementation of the routing logic must be thread safe, since it might be used outside of actors.\nA unit test of the routing logic:\nScala copysourcefinal case class TestRoutee(n: Int) extends Routee {\n  override def send(message: Any, sender: ActorRef): Unit = ()\n}\n\n  val logic = new RedundancyRoutingLogic(nbrCopies = 3)\n\n  val routees = for (n <- 1 to 7) yield TestRoutee(n)\n\n  val r1 = logic.select(\"msg\", routees)\n  r1.asInstanceOf[SeveralRoutees].routees should be(Vector(TestRoutee(1), TestRoutee(2), TestRoutee(3)))\n\n  val r2 = logic.select(\"msg\", routees)\n  r2.asInstanceOf[SeveralRoutees].routees should be(Vector(TestRoutee(4), TestRoutee(5), TestRoutee(6)))\n\n  val r3 = logic.select(\"msg\", routees)\n  r3.asInstanceOf[SeveralRoutees].routees should be(Vector(TestRoutee(7), TestRoutee(1), TestRoutee(2))) Java copysourcestatic final class TestRoutee implements Routee {\n  public final int n;\n\n  public TestRoutee(int n) {\n    this.n = n;\n  }\n\n  @Override\n  public void send(Object message, ActorRef sender) {}\n\n  @Override\n  public int hashCode() {\n    return n;\n  }\n\n  @Override\n  public boolean equals(Object obj) {\n    return (obj instanceof TestRoutee) && n == ((TestRoutee) obj).n;\n  }\n}\n\n  RedundancyRoutingLogic logic = new RedundancyRoutingLogic(3);\n\n  List<Routee> routeeList = new ArrayList<Routee>();\n  for (int n = 1; n <= 7; n++) {\n    routeeList.add(new TestRoutee(n));\n  }\n  IndexedSeq<Routee> routees = immutableIndexedSeq(routeeList);\n\n  SeveralRoutees r1 = (SeveralRoutees) logic.select(\"msg\", routees);\n  assertEquals(r1.getRoutees().get(0), routeeList.get(0));\n  assertEquals(r1.getRoutees().get(1), routeeList.get(1));\n  assertEquals(r1.getRoutees().get(2), routeeList.get(2));\n\n  SeveralRoutees r2 = (SeveralRoutees) logic.select(\"msg\", routees);\n  assertEquals(r2.getRoutees().get(0), routeeList.get(3));\n  assertEquals(r2.getRoutees().get(1), routeeList.get(4));\n  assertEquals(r2.getRoutees().get(2), routeeList.get(5));\n\n  SeveralRoutees r3 = (SeveralRoutees) logic.select(\"msg\", routees);\n  assertEquals(r3.getRoutees().get(0), routeeList.get(6));\n  assertEquals(r3.getRoutees().get(1), routeeList.get(0));\n  assertEquals(r3.getRoutees().get(2), routeeList.get(1));\nYou could stop here and use the RedundancyRoutingLogic with a org.apache.pekko.routing.Router as described in A Simple Router.\nLet us continue and make this into a self contained, configurable, router actor.\nCreate a class that extends PoolPool, GroupGroup or CustomRouterConfigCustomRouterConfig. That class is a factory for the routing logic and holds the configuration for the router. Here we make it a Group.\nScala copysourceimport org.apache.pekko\nimport pekko.dispatch.Dispatchers\nimport pekko.routing.Group\nimport pekko.routing.Router\nimport pekko.japi.Util.immutableSeq\nimport com.typesafe.config.Config\n\nfinal case class RedundancyGroup(routeePaths: immutable.Iterable[String], nbrCopies: Int) extends Group {\n\n  def this(config: Config) =\n    this(routeePaths = immutableSeq(config.getStringList(\"routees.paths\")), nbrCopies = config.getInt(\"nbr-copies\"))\n\n  override def paths(system: ActorSystem): immutable.Iterable[String] = routeePaths\n\n  override def createRouter(system: ActorSystem): Router =\n    new Router(new RedundancyRoutingLogic(nbrCopies))\n\n  override val routerDispatcher: String = Dispatchers.DefaultDispatcherId\n} Java copysourceimport java.util.List;\n\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.dispatch.Dispatchers;\nimport org.apache.pekko.routing.Router;\n\nimport com.typesafe.config.Config;\n\nimport org.apache.pekko.routing.GroupBase;\nimport static jdocs.routing.CustomRouterDocTest.RedundancyRoutingLogic;\n\npublic class RedundancyGroup extends GroupBase {\n  private final List<String> paths;\n  private final int nbrCopies;\n\n  public RedundancyGroup(List<String> paths, int nbrCopies) {\n    this.paths = paths;\n    this.nbrCopies = nbrCopies;\n  }\n\n  public RedundancyGroup(Config config) {\n    this(config.getStringList(\"routees.paths\"), config.getInt(\"nbr-copies\"));\n  }\n\n  @Override\n  public java.lang.Iterable<String> getPaths(ActorSystem system) {\n    return paths;\n  }\n\n  @Override\n  public Router createRouter(ActorSystem system) {\n    return new Router(new RedundancyRoutingLogic(nbrCopies));\n  }\n\n  @Override\n  public String routerDispatcher() {\n    return Dispatchers.DefaultDispatcherId();\n  }\n}\nThis can be used exactly as the router actors provided by Pekko.\nScala copysourcefor (n <- 1 to 10) system.actorOf(Props[Storage](), \"s\" + n)\n\nval paths = for (n <- 1 to 10) yield \"/user/s\" + n\nval redundancy1: ActorRef =\n  system.actorOf(RedundancyGroup(paths, nbrCopies = 3).props(), name = \"redundancy1\")\nredundancy1 ! \"important\" Java copysourcefor (int n = 1; n <= 10; n++) {\n  system.actorOf(Props.create(Storage.class), \"s\" + n);\n}\n\nList<String> paths = new ArrayList<String>();\nfor (int n = 1; n <= 10; n++) {\n  paths.add(\"/user/s\" + n);\n}\n\nActorRef redundancy1 = system.actorOf(new RedundancyGroup(paths, 3).props(), \"redundancy1\");\nredundancy1.tell(\"important\", getTestActor());\nNote that we added a constructor in RedundancyGroup that takes a Config parameter. That makes it possible to define it in configuration.\nScala copysourcepekko.actor.deployment {\n  /redundancy2 {\n    router = \"jdocs.routing.RedundancyGroup\"\n    routees.paths = [\"/user/s1\", \"/user/s2\", \"/user/s3\"]\n    nbr-copies = 5\n  }\n} Java copysourcepekko.actor.deployment {\n  /redundancy2 {\n    router = \"jdocs.routing.RedundancyGroup\"\n    routees.paths = [\"/user/s1\", \"/user/s2\", \"/user/s3\"]\n    nbr-copies = 5\n  }\n}\nNote the fully qualified class name in the router property. The router class must extend org.apache.pekko.routing.RouterConfig (Pool, Group or CustomRouterConfig) and have constructor with one com.typesafe.config.Config parameter. The deployment section of the configuration is passed to the constructor.\nScala copysourceval redundancy2: ActorRef = system.actorOf(FromConfig.props(), name = \"redundancy2\")\nredundancy2 ! \"very important\" Java copysourceActorRef redundancy2 = system.actorOf(FromConfig.getInstance().props(), \"redundancy2\");\nredundancy2.tell(\"very important\", getTestActor());","title":"Custom Router"},{"location":"/routing.html#configuring-dispatchers","text":"The dispatcher for created children of the pool will be taken from PropsProps as described in Dispatchers.\nTo make it easy to define the dispatcher of the routees of the pool you can define the dispatcher inline in the deployment section of the config.\ncopysourcepekko.actor.deployment {\n  /poolWithDispatcher {\n    router = random-pool\n    nr-of-instances = 5\n    pool-dispatcher {\n      fork-join-executor.parallelism-min = 5\n      fork-join-executor.parallelism-max = 5\n    }\n  }\n}\nThat is the only thing you need to do enable a dedicated dispatcher for a pool.\nNote If you use a group of actors and route to their paths, then they will still use the same dispatcher that was configured for them in their Props, it is not possible to change an actors dispatcher after it has been created.\nThe “head” router cannot always run on the same dispatcher, because it does not process the same type of messages, hence this special actor does not use the dispatcher configured in PropsProps, but takes the routerDispatcher from the RouterConfigRouterConfig instead, which defaults to the actor system’s default dispatcher. All standard routers allow setting this property in their constructor or factory method, custom routers have to implement the method in a suitable way.\nScala copysourceval router: ActorRef = system.actorOf(\n  // “head” router actor will run on \"router-dispatcher\" dispatcher\n  // Worker routees will run on \"pool-dispatcher\" dispatcher\n  RandomPool(5, routerDispatcher = \"router-dispatcher\").props(Props[Worker]()),\n  name = \"poolWithDispatcher\") Java copysourceProps props =\n    // “head” router actor will run on \"router-dispatcher\" dispatcher\n    // Worker routees will run on \"pool-dispatcher\" dispatcher\n    new RandomPool(5).withDispatcher(\"router-dispatcher\").props(Props.create(Worker.class));\nActorRef router = system.actorOf(props, \"poolWithDispatcher\");\nNote It is not allowed to configure the routerDispatcher to be a BalancingDispatcherConfiguratorBalancingDispatcherConfigurator since the messages meant for the special router actor cannot be processed by any other actor.","title":"Configuring Dispatchers"},{"location":"/fsm.html","text":"","title":"Classic FSM"},{"location":"/fsm.html#classic-fsm","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the documentation of the new API of this feature and for new projects see fsm.","title":"Classic FSM"},{"location":"/fsm.html#dependency","text":"To use Finite State Machine actors, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/fsm.html#overview","text":"The FSM (Finite State Machine) is available as a mixin for the an abstract base class that implements an Pekko Actor and is best described in the Erlang design principles\nA FSM can be described as a set of relations of the form:\nState(S) x Event(E) -> Actions (A), State(S’)\nThese relations are interpreted as meaning:\nIf we are in state S and the event E occurs, we should perform the actions A and make a transition to the state S’.","title":"Overview"},{"location":"/fsm.html#a-simple-example","text":"To demonstrate most of the features of the FSM traitAbstractFSM class, consider an actor which shall receive and queue messages while they arrive in a burst and send them on after the burst ended or a flush request is received.\nFirst, consider all of the below to use these import statements:\nScala copysourceimport pekko.actor.{ ActorRef, FSM }\nimport scala.concurrent.duration._ Java copysourceimport org.apache.pekko.actor.AbstractFSM;\nimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.japi.pf.UnitMatch;\nimport java.util.Arrays;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.time.Duration;\nThe contract of our “Buncher” actor is that it accepts or produces the following messages:\nScala copysource// received events\nfinal case class SetTarget(ref: ActorRef)\nfinal case class Queue(obj: Any)\ncase object Flush\n\n// sent events\nfinal case class Batch(obj: immutable.Seq[Any]) Java copysourcestatic final class SetTarget {\n  private final ActorRef ref;\n\n  public SetTarget(ActorRef ref) {\n    this.ref = ref;\n  }\n\n  public ActorRef getRef() {\n    return ref;\n  }\n\n  @Override\n  public String toString() {\n    return \"SetTarget{\" + \"ref=\" + ref + '}';\n  }\n}\n\nstatic final class Queue {\n  private final Object obj;\n\n  public Queue(Object obj) {\n    this.obj = obj;\n  }\n\n  public Object getObj() {\n    return obj;\n  }\n\n  @Override\n  public String toString() {\n    return \"Queue{\" + \"obj=\" + obj + '}';\n  }\n}\n\nstatic final class Batch {\n  private final List<Object> list;\n\n  public Batch(List<Object> list) {\n    this.list = list;\n  }\n\n  public List<Object> getList() {\n    return list;\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n    if (o == null || getClass() != o.getClass()) return false;\n\n    Batch batch = (Batch) o;\n\n    return list.equals(batch.list);\n  }\n\n  @Override\n  public int hashCode() {\n    return list.hashCode();\n  }\n\n  @Override\n  public String toString() {\n    final StringBuilder builder = new StringBuilder();\n    builder.append(\"Batch{list=\");\n    list.stream()\n        .forEachOrdered(\n            e -> {\n              builder.append(e);\n              builder.append(\",\");\n            });\n    int len = builder.length();\n    builder.replace(len, len, \"}\");\n    return builder.toString();\n  }\n}\n\nstatic enum Flush {\n  Flush\n}\nSetTarget is needed for starting it up, setting the destination for the Batches to be passed on; Queue will add to the internal queue while Flush will mark the end of a burst.\nScala copysource// states\nsealed trait State\ncase object Idle extends State\ncase object Active extends State\n\nsealed trait Data\ncase object Uninitialized extends Data\nfinal case class Todo(target: ActorRef, queue: immutable.Seq[Any]) extends Data Java copysource// states\nenum State {\n  Idle,\n  Active\n}\n\n// state data\ninterface Data {}\n\nenum Uninitialized implements Data {\n  Uninitialized\n}\n\nfinal class Todo implements Data {\n  private final ActorRef target;\n  private final List<Object> queue;\n\n  public Todo(ActorRef target, List<Object> queue) {\n    this.target = target;\n    this.queue = queue;\n  }\n\n  public ActorRef getTarget() {\n    return target;\n  }\n\n  public List<Object> getQueue() {\n    return queue;\n  }\n\n  @Override\n  public String toString() {\n    return \"Todo{\" + \"target=\" + target + \", queue=\" + queue + '}';\n  }\n\n  public Todo addElement(Object element) {\n    List<Object> nQueue = new LinkedList<>(queue);\n    nQueue.add(element);\n    return new Todo(this.target, nQueue);\n  }\n\n  public Todo copy(List<Object> queue) {\n    return new Todo(this.target, queue);\n  }\n\n  public Todo copy(ActorRef target) {\n    return new Todo(target, this.queue);\n  }\n}\nThe actor can be in two states: no message queued (aka Idle) or some message queued (aka Active). It will stay in the Active state as long as messages keep arriving and no flush is requested. The internal state data of the actor is made up of the target actor reference to send the batches to and the actual queue of messages.\nNow let’s take a look at the skeleton for our FSM actor:\nScala copysourceclass Buncher extends FSM[State, Data] {\n\n  startWith(Idle, Uninitialized)\n\n  when(Idle) {\n    case Event(SetTarget(ref), Uninitialized) =>\n      stay().using(Todo(ref, Vector.empty))\n  }\n\n  onTransition {\n    case Active -> Idle =>\n      stateData match {\n        case Todo(ref, queue) => ref ! Batch(queue)\n        case _                => // nothing to do\n      }\n  }\n\n  when(Active, stateTimeout = 1 second) {\n    case Event(Flush | StateTimeout, t: Todo) =>\n      goto(Idle).using(t.copy(queue = Vector.empty))\n  }\n\n  whenUnhandled {\n    // common code for both states\n    case Event(Queue(obj), t @ Todo(_, v)) =>\n      goto(Active).using(t.copy(queue = v :+ obj))\n\n    case Event(e, s) =>\n      log.warning(\"received unhandled request {} in state {}/{}\", e, stateName, s)\n      stay()\n  }\n\n  initialize()\n} Java copysourcepublic class Buncher extends AbstractFSM<State, Data> {\n  {\n    startWith(Idle, Uninitialized);\n\n    when(\n        Idle,\n        matchEvent(\n            SetTarget.class,\n            Uninitialized.class,\n            (setTarget, uninitialized) ->\n                stay().using(new Todo(setTarget.getRef(), new LinkedList<>()))));\n\n    onTransition(\n        matchState(\n                Active,\n                Idle,\n                () -> {\n                  // reuse this matcher\n                  final UnitMatch<Data> m =\n                      UnitMatch.create(\n                          matchData(\n                              Todo.class,\n                              todo ->\n                                  todo.getTarget().tell(new Batch(todo.getQueue()), getSelf())));\n                  m.match(stateData());\n                })\n            .state(\n                Idle,\n                Active,\n                () -> {\n                  /* Do something here */\n                }));\n\n    when(\n        Active,\n        Duration.ofSeconds(1L),\n        matchEvent(\n            Arrays.asList(Flush.class, StateTimeout()),\n            Todo.class,\n            (event, todo) -> goTo(Idle).using(todo.copy(new LinkedList<>()))));\n\n    whenUnhandled(\n        matchEvent(\n                Queue.class,\n                Todo.class,\n                (queue, todo) -> goTo(Active).using(todo.addElement(queue.getObj())))\n            .anyEvent(\n                (event, state) -> {\n                  log()\n                      .warning(\n                          \"received unhandled request {} in state {}/{}\",\n                          event,\n                          stateName(),\n                          state);\n                  return stay();\n                }));\n\n    initialize();\n  }\n}\nThe basic strategy is to declare the actor, mixing in the FSM traitby inheriting the AbstractFSM class and specifying the possible states and data values as type parameters. Within the body of the actor a DSL is used for declaring the state machine:\nstartWith defines the initial state and initial data then there is one when(<state>) { ... } declaration per state to be handled (could potentially be multiple ones, the passed PartialFunction will be concatenated using orElse) finally starting it up using initialize, which performs the transition into the initial state and sets up timers (if required).\nIn this case, we start out in the Idle state with Uninitialized data, where only the SetTarget() message is handled; stay prepares to end this event’s processing for not leaving the current state, while the using modifier makes the FSM replace the internal state (which is Uninitialized at this point) with a fresh Todo() object containing the target actor reference. The Active state has a state timeout declared, which means that if no message is received for 1 second, a FSM.StateTimeout message will be generated. This has the same effect as receiving the Flush command in this case, namely to transition back into the Idle state and resetting the internal queue to the empty vector. But how do messages get queued? Since this shall work identically in both states, we make use of the fact that any event which is not handled by the when() block is passed to the whenUnhandled() block:\nScala copysourcewhenUnhandled {\n  // common code for both states\n  case Event(Queue(obj), t @ Todo(_, v)) =>\n    goto(Active).using(t.copy(queue = v :+ obj))\n\n  case Event(e, s) =>\n    log.warning(\"received unhandled request {} in state {}/{}\", e, stateName, s)\n    stay()\n} Java copysourcewhenUnhandled(\n    matchEvent(\n            Queue.class,\n            Todo.class,\n            (queue, todo) -> goTo(Active).using(todo.addElement(queue.getObj())))\n        .anyEvent(\n            (event, state) -> {\n              log()\n                  .warning(\n                      \"received unhandled request {} in state {}/{}\",\n                      event,\n                      stateName(),\n                      state);\n              return stay();\n            }));\nThe first case handled here is adding Queue() requests to the internal queue and going to the Active state (this does the obvious thing of staying in the Active state if already there), but only if the FSM data are not Uninitialized when the Queue() event is received. Otherwise—and in all other non-handled cases—the second case just logs a warning and does not change the internal state.\nThe only missing piece is where the Batches are actually sent to the target, for which we use the onTransition mechanism: you can declare multiple such blocks and all of them will be tried for matching behavior in case a state transition occurs (i.e. only when the state actually changes).\nScala copysourceonTransition {\n  case Active -> Idle =>\n    stateData match {\n      case Todo(ref, queue) => ref ! Batch(queue)\n      case _                => // nothing to do\n    }\n} Java copysourceonTransition(\n    matchState(\n            Active,\n            Idle,\n            () -> {\n              // reuse this matcher\n              final UnitMatch<Data> m =\n                  UnitMatch.create(\n                      matchData(\n                          Todo.class,\n                          todo ->\n                              todo.getTarget().tell(new Batch(todo.getQueue()), getSelf())));\n              m.match(stateData());\n            })\n        .state(\n            Idle,\n            Active,\n            () -> {\n              /* Do something here */\n            }));\nThe transition callback is a partial functionbuilder constructed by matchState, followed by zero or multiple state, which takes as input a pair of states—the current and the next state. The FSM trait includes a convenience extractor for these in form of an arrow operator, which conveniently reminds you of the direction of the state change which is being matched. During the state change, the old state data is available via stateDatastateData() as shown, and the new state data would be available as nextStateDatanextStateData().\nNote Same-state transitions can be implemented (when currently in state S) using goto(S) or stay(). The difference between those being that goto(S) will emit an event S->S event that can be handled by onTransition, whereas stay() will not.\nTo verify that this buncher actually works, it is quite easy to write a test using the Testing Actor Systems which is conveniently bundled with ScalaTest traits into PekkoSpecTestKit, here using JUnit as an example:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.Props\nimport scala.collection.immutable\n\nobject FSMDocSpec {\n  // messages and data types\n}\n\nclass FSMDocSpec extends MyFavoriteTestFrameWorkPlusPekkoTestKit {\n  import FSMDocSpec._\n\n  import pekko.actor.{ ActorRef, FSM }\n  import scala.concurrent.duration._\n  class Buncher extends FSM[State, Data] {\n\n    startWith(Idle, Uninitialized)\n\n    when(Idle) {\n      case Event(SetTarget(ref), Uninitialized) =>\n        stay().using(Todo(ref, Vector.empty))\n    }\n\n    onTransition {\n      case Active -> Idle =>\n        stateData match {\n          case Todo(ref, queue) => ref ! Batch(queue)\n          case _                => // nothing to do\n        }\n    }\n\n    when(Active, stateTimeout = 1 second) {\n      case Event(Flush | StateTimeout, t: Todo) =>\n        goto(Idle).using(t.copy(queue = Vector.empty))\n    }\n\n    whenUnhandled {\n      // common code for both states\n      case Event(Queue(obj), t @ Todo(_, v)) =>\n        goto(Active).using(t.copy(queue = v :+ obj))\n\n      case Event(e, s) =>\n        log.warning(\"received unhandled request {} in state {}/{}\", e, stateName, s)\n        stay()\n    }\n\n    initialize()\n  }\n  object DemoCode {\n    trait StateType\n    case object SomeState extends StateType\n    case object Processing extends StateType\n    case object Error extends StateType\n    case object Idle extends StateType\n    case object Active extends StateType\n\n    class Dummy extends FSM[StateType, Int] {\n      class X\n      val newData = 42\n      object WillDo\n      object Tick\n\n      when(SomeState) {\n        case Event(msg, _) =>\n          goto(Processing).using(newData).forMax(5 seconds).replying(WillDo)\n      }\n\n      onTransition {\n        case Idle -> Active => startTimerWithFixedDelay(\"timeout\", Tick, 1 second)\n        case Active -> _    => cancelTimer(\"timeout\")\n        case x -> Idle      => log.info(\"entering Idle from \" + x)\n      }\n\n      onTransition(handler _)\n\n      def handler(from: StateType, to: StateType): Unit = {\n        // handle it here ...\n      }\n\n      when(Error) {\n        case Event(\"stop\", _) =>\n          // do cleanup ...\n          stop()\n      }\n\n      when(SomeState)(transform {\n        case Event(bytes: ByteString, read) => stay().using(read + bytes.length)\n      }.using {\n        case s @ FSM.State(state, read, timeout, stopReason, replies) if read > 1000 =>\n          goto(Processing)\n      })\n\n      val processingTrigger: PartialFunction[State, State] = {\n        case s @ FSM.State(state, read, timeout, stopReason, replies) if read > 1000 =>\n          goto(Processing)\n      }\n\n      when(SomeState)(transform {\n        case Event(bytes: ByteString, read) => stay().using(read + bytes.length)\n      }.using(processingTrigger))\n\n      onTermination {\n        case StopEvent(FSM.Normal, state, data)         => // ...\n        case StopEvent(FSM.Shutdown, state, data)       => // ...\n        case StopEvent(FSM.Failure(cause), state, data) => // ...\n      }\n\n      whenUnhandled {\n        case Event(x: X, data) =>\n          log.info(\"Received unhandled event: \" + x)\n          stay()\n        case Event(msg, _) =>\n          log.warning(\"Received unknown event: \" + msg)\n          goto(Error)\n      }\n\n    }\n\n    import org.apache.pekko.actor.LoggingFSM\n    class MyFSM extends LoggingFSM[StateType, Data] {\n      override def logDepth = 12\n      onTermination {\n        case StopEvent(FSM.Failure(_), state, data) =>\n          val lastEvents = getLog.mkString(\"\\n\\t\")\n          log.warning(\n            \"Failure in state \" + state + \" with data \" + data + \"\\n\" +\n            \"Events leading up to this point:\\n\\t\" + lastEvents)\n      }\n      // ...\n    }\n\n  }\n\n  \"simple finite state machine\" must {\n\n    \"demonstrate NullFunction\" in {\n      class A extends FSM[Int, Null] {\n        val SomeState = 0\n        when(SomeState)(FSM.NullFunction)\n      }\n    }\n\n    \"batch correctly\" in {\n      val buncher = system.actorOf(Props(classOf[Buncher], this))\n      buncher ! SetTarget(testActor)\n      buncher ! Queue(42)\n      buncher ! Queue(43)\n      expectMsg(Batch(immutable.Seq(42, 43)))\n      buncher ! Queue(44)\n      buncher ! Flush\n      buncher ! Queue(45)\n      expectMsg(Batch(immutable.Seq(44)))\n      expectMsg(Batch(immutable.Seq(45)))\n    }\n\n    \"not batch if uninitialized\" in {\n      val buncher = system.actorOf(Props(classOf[Buncher], this))\n      buncher ! Queue(42)\n      expectNoMessage()\n    }\n  }\n} Java copysourcepublic class BuncherTest extends AbstractJavaTest {\n\n  static ActorSystem system;\n\n  @BeforeClass\n  public static void setup() {\n    system = ActorSystem.create(\"BuncherTest\");\n  }\n\n  @AfterClass\n  public static void tearDown() {\n    TestKit.shutdownActorSystem(system);\n    system = null;\n  }\n\n  @Test\n  public void testBuncherActorBatchesCorrectly() {\n    new TestKit(system) {\n      {\n        final ActorRef buncher = system.actorOf(Props.create(Buncher.class));\n        final ActorRef probe = getRef();\n\n        buncher.tell(new SetTarget(probe), probe);\n        buncher.tell(new Queue(42), probe);\n        buncher.tell(new Queue(43), probe);\n        LinkedList<Object> list1 = new LinkedList<>();\n        list1.add(42);\n        list1.add(43);\n        expectMsgEquals(new Batch(list1));\n        buncher.tell(new Queue(44), probe);\n        buncher.tell(Flush, probe);\n        buncher.tell(new Queue(45), probe);\n        LinkedList<Object> list2 = new LinkedList<>();\n        list2.add(44);\n        expectMsgEquals(new Batch(list2));\n        LinkedList<Object> list3 = new LinkedList<>();\n        list3.add(45);\n        expectMsgEquals(new Batch(list3));\n        system.stop(buncher);\n      }\n    };\n  }\n\n  @Test\n  public void testBuncherActorDoesntBatchUninitialized() {\n    new TestKit(system) {\n      {\n        final ActorRef buncher = system.actorOf(Props.create(Buncher.class));\n        final ActorRef probe = getRef();\n\n        buncher.tell(new Queue(42), probe);\n        expectNoMessage();\n        system.stop(buncher);\n      }\n    };\n  }\n}","title":"A Simple Example"},{"location":"/fsm.html#reference","text":"","title":"Reference"},{"location":"/fsm.html#the-","text":"The FSM trait inherits directly from Actor, when you extend FSM you must be aware that an actor is actually created: The AbstractFSM abstract class is the base class used to implement an FSM. It implements Actor since an Actor is created to drive the FSM.\nScala copysourceclass Buncher extends FSM[State, Data] {\n\n  startWith(Idle, Uninitialized)\n\n  when(Idle) {\n    case Event(SetTarget(ref), Uninitialized) =>\n      stay().using(Todo(ref, Vector.empty))\n  }\n\n  onTransition {\n    case Active -> Idle =>\n      stateData match {\n        case Todo(ref, queue) => ref ! Batch(queue)\n        case _                => // nothing to do\n      }\n  }\n\n  when(Active, stateTimeout = 1 second) {\n    case Event(Flush | StateTimeout, t: Todo) =>\n      goto(Idle).using(t.copy(queue = Vector.empty))\n  }\n\n  whenUnhandled {\n    // common code for both states\n    case Event(Queue(obj), t @ Todo(_, v)) =>\n      goto(Active).using(t.copy(queue = v :+ obj))\n\n    case Event(e, s) =>\n      log.warning(\"received unhandled request {} in state {}/{}\", e, stateName, s)\n      stay()\n  }\n\n  initialize()\n} Java copysourcepublic class Buncher extends AbstractFSM<State, Data> {\n  {\n    startWith(Idle, Uninitialized);\n\n    when(\n        Idle,\n        matchEvent(\n            SetTarget.class,\n            Uninitialized.class,\n            (setTarget, uninitialized) ->\n                stay().using(new Todo(setTarget.getRef(), new LinkedList<>()))));\n\n    onTransition(\n        matchState(\n                Active,\n                Idle,\n                () -> {\n                  // reuse this matcher\n                  final UnitMatch<Data> m =\n                      UnitMatch.create(\n                          matchData(\n                              Todo.class,\n                              todo ->\n                                  todo.getTarget().tell(new Batch(todo.getQueue()), getSelf())));\n                  m.match(stateData());\n                })\n            .state(\n                Idle,\n                Active,\n                () -> {\n                  /* Do something here */\n                }));\n\n    when(\n        Active,\n        Duration.ofSeconds(1L),\n        matchEvent(\n            Arrays.asList(Flush.class, StateTimeout()),\n            Todo.class,\n            (event, todo) -> goTo(Idle).using(todo.copy(new LinkedList<>()))));\n\n    whenUnhandled(\n        matchEvent(\n                Queue.class,\n                Todo.class,\n                (queue, todo) -> goTo(Active).using(todo.addElement(queue.getObj())))\n            .anyEvent(\n                (event, state) -> {\n                  log()\n                      .warning(\n                          \"received unhandled request {} in state {}/{}\",\n                          event,\n                          stateName(),\n                          state);\n                  return stay();\n                }));\n\n    initialize();\n  }\n}\nNote The FSM traitAbstractFSM class defines a receive method which handles internal messages and passes everything else through to the FSM logic (according to the current state). When overriding the receive method, keep in mind that e.g. state timeout handling depends on actually passing the messages through the FSM logic.\nThe FSM traitAbstractFSM class takes two type parameters:\nthe supertype of all state names, usually a sealed trait with case objects extending itan enum the type of the state data which are tracked by the FSMAbstractFSM module itself.\nNote The state data together with the state name describe the internal state of the state machine; if you stick to this scheme and do not add mutable fields to the FSM class you have the advantage of making all changes of the internal state explicit in a few well-known places.","title":"The FSM Trait and ObjectAbstractFSM Class"},{"location":"/fsm.html#defining-states","text":"A state is defined by one or more invocations of the method\nwhen(<name>[, stateTimeout = <timeout>])(stateFunction)\nThe given name must be an object which is type-compatible with the first type parameter given to the FSM traitAbstractFSM class. This object is used as a hash key, so you must ensure that it properly implements equals and hashCode; in particular it must not be mutable. The easiest fit for these requirements are case objects.\nIf the stateTimeout parameter is given, then all transitions into this state, including staying, receive this timeout by default. Initiating the transition with an explicit timeout may be used to override this default, see Initiating Transitions for more information. The state timeout of any state may be changed during action processing with setStateTimeout(state, duration). This enables runtime configuration e.g. via external message.\nThe stateFunction argument is a PartialFunction[Event, State], which is conveniently given using the partial function literalstate function builder syntax as demonstrated below:\nScala copysourcewhen(Idle) {\n  case Event(SetTarget(ref), Uninitialized) =>\n    stay().using(Todo(ref, Vector.empty))\n}\n\nwhen(Active, stateTimeout = 1 second) {\n  case Event(Flush | StateTimeout, t: Todo) =>\n    goto(Idle).using(t.copy(queue = Vector.empty))\n} Java copysourcewhen(\n    Idle,\n    matchEvent(\n        SetTarget.class,\n        Uninitialized.class,\n        (setTarget, uninitialized) ->\n            stay().using(new Todo(setTarget.getRef(), new LinkedList<>()))));\nThe Event(msg: Any, data: D) case class is parameterized with the data type held by the FSM for convenient pattern matching.\nWarning It is required that you define handlers for each of the possible FSM states, otherwise there will be failures when trying to switch to undeclared states.\nIt is recommended practice to declare the states as objects extending a sealed traitan enum and then verify that there is a when clause for each of the states. If you want to leave the handling of a state “unhandled” (more below), it still needs to be declared like this:\nScala copysourcewhen(SomeState)(FSM.NullFunction) Java copysourcewhen(SomeState, AbstractFSM.NullFunction());","title":"Defining States"},{"location":"/fsm.html#defining-the-initial-state","text":"Each FSM needs a starting point, which is declared using\nstartWith(state, data[, timeout])\nThe optionally given timeout argument overrides any specification given for the desired initial state. If you want to cancel a default timeout, use NoneDuration.Inf.","title":"Defining the Initial State"},{"location":"/fsm.html#unhandled-events","text":"If a state doesn’t handle a received event a warning is logged. If you want to do something else in this case you can specify that with whenUnhandled(stateFunction):\nScala copysourcewhenUnhandled {\n  case Event(x: X, data) =>\n    log.info(\"Received unhandled event: \" + x)\n    stay()\n  case Event(msg, _) =>\n    log.warning(\"Received unknown event: \" + msg)\n    goto(Error)\n} Java copysource  whenUnhandled(\n      matchEvent(\n              X.class,\n              (x, data) -> {\n                log().info(\"Received unhandled event: \" + x);\n                return stay();\n              })\n          .anyEvent(\n              (event, data) -> {\n                log().warning(\"Received unknown event: \" + event);\n                return goTo(Error);\n              }));\n}\nWithin this handler the state of the FSM may be queried using the stateName method.\nIMPORTANT: This handler is not stacked, meaning that each invocation of whenUnhandled replaces the previously installed handler.","title":"Unhandled Events"},{"location":"/fsm.html#initiating-transitions","text":"The result of any stateFunction must be a definition of the next state unless terminating the FSM, which is described in Termination from Inside. The state definition can either be the current state, as described by the stay directive, or it is a different state as given by goto(state). The resulting object allows further qualification by way of the modifiers described in the following:\nforMax(duration) This modifier sets a state timeout on the next state. This means that a timer is started which upon expiry sends a StateTimeout message to the FSM. This timer is canceled upon reception of any other message in the meantime; you can rely on the fact that the StateTimeout message will not be processed after an intervening message. This modifier can also be used to override any default timeout which is specified for the target state. If you want to cancel the default timeout, use Duration.Inf. using(data) This modifier replaces the old state data with the new data given. If you follow the advice above, this is the only place where internal state data are ever modified. replying(msg) This modifier sends a reply to the currently processed message and otherwise does not modify the state transition.\nAll modifiers can be chained to achieve a nice and concise description:\nScala copysourcewhen(SomeState) {\n  case Event(msg, _) =>\n    goto(Processing).using(newData).forMax(5 seconds).replying(WillDo)\n} Java copysourcewhen(\n    SomeState,\n    matchAnyEvent(\n        (msg, data) -> {\n          return goTo(Processing)\n              .using(newData)\n              .forMax(Duration.ofSeconds(5))\n              .replying(WillDo);\n        }));\nThe parentheses are not actually needed in all cases, but they visually distinguish between modifiers and their arguments and therefore make the code even more pleasant to read.\nNote Please note that the return statement may not be used in when blocks or similar; this is a Scala restriction. Either refactor your code using if () ... else ... or move it into a method definition.","title":"Initiating Transitions"},{"location":"/fsm.html#monitoring-transitions","text":"Transitions occur “between states” conceptually, which means after any actions you have put into the event handling block; this is obvious since the next state is only defined by the value returned by the event handling logic. You do not need to worry about the exact order with respect to setting the internal state variable, as everything within the FSM actor is running single-threaded anyway.","title":"Monitoring Transitions"},{"location":"/fsm.html#internal-monitoring","text":"Up to this point, the FSM DSL has been centered on states and events. The dual view is to describe it as a series of transitions. This is enabled by the method\nonTransition(handler)\nwhich associates actions with a transition instead of with a state and event. The handler is a partial function which takes a pair of states as input; no resulting state is needed as it is not possible to modify the transition in progress.\nScala copysourceonTransition {\n  case Idle -> Active => startTimerWithFixedDelay(\"timeout\", Tick, 1 second)\n  case Active -> _    => cancelTimer(\"timeout\")\n  case x -> Idle      => log.info(\"entering Idle from \" + x)\n} Java copysourceonTransition(\n    matchState(\n            Idle,\n            Active,\n            () -> startTimerWithFixedDelay(\"timeout\", Tick, Duration.ofSeconds(1L)))\n        .state(Active, null, () -> cancelTimer(\"timeout\"))\n        .state(null, Idle, (f, t) -> log().info(\"entering Idle from \" + f)));\nThe convenience extractor -> enables decomposition of the pair of states with a clear visual reminder of the transition’s direction. As usual in pattern matches, an underscore may be used for irrelevant parts; alternatively you could bind the unconstrained state to a variable, e.g. for logging as shown in the last case.\nIt is also possible to pass a function object accepting two states to onTransition, in case your transition handling logic is implemented as a method:\nScala copysourceonTransition(handler _)\n\ndef handler(from: StateType, to: StateType): Unit = {\n  // handle it here ...\n} Java copysourcepublic void handler(StateType from, StateType to) {\n  // handle transition here\n}\n\n  onTransition(this::handler);\nThe handlers registered with this method are stacked, so you can intersperse onTransition blocks with when blocks as suits your design. It should be noted, however, that all handlers will be invoked for each transition, not only the first matching one. This is designed specifically so you can put all transition handling for a certain aspect into one place without having to worry about earlier declarations shadowing later ones; the actions are still executed in declaration order, though.\nNote This kind of internal monitoring may be used to structure your FSM according to transitions, so that for example the cancellation of a timer upon leaving a certain state cannot be forgot when adding new target states.","title":"Internal Monitoring"},{"location":"/fsm.html#external-monitoring","text":"External actors may be registered to be notified of state transitions by sending a message SubscribeTransitionCallBack(actorRef). The named actor will be sent a CurrentState(self, stateName) message immediately and will receive Transition(actorRef, oldState, newState) messages whenever a state change is triggered.\nPlease note that a state change includes the action of performing an goto(S), while already being state S. In that case the monitoring actor will be notified with an Transition(ref,S,S) message. This may be useful if your FSM should react on all (also same-state) transitions. In case you’d rather not emit events for same-state transitions use stay() instead of goto(S).\nExternal monitors may be unregistered by sending UnsubscribeTransitionCallBack(actorRef) to the FSM actor.\nStopping a listener without unregistering will not remove the listener from the subscription list; use UnsubscribeTransitionCallback before stopping the listener.\nTransforming State The partial functions supplied as argument to the when() blocks can be transformed using Scala’s full supplement of functional programming tools. In order to retain type inference, there is a helper function which may be used in case some common handling logic shall be applied to different clauses: copysourcewhen(SomeState)(transform {\n  case Event(bytes: ByteString, read) => stay().using(read + bytes.length)\n}.using {\n  case s @ FSM.State(state, read, timeout, stopReason, replies) if read > 1000 =>\n    goto(Processing)\n}) It goes without saying that the arguments to this method may also be stored, to be used several times, e.g. when applying the same transformation to several when() blocks: copysourceval processingTrigger: PartialFunction[State, State] = {\n  case s @ FSM.State(state, read, timeout, stopReason, replies) if read > 1000 =>\n    goto(Processing)\n}\n\nwhen(SomeState)(transform {\n  case Event(bytes: ByteString, read) => stay().using(read + bytes.length)\n}.using(processingTrigger))","title":"External Monitoring"},{"location":"/fsm.html#timers","text":"Besides state timeouts, FSM manages timers identified by String names. You may set a timer using\nstartSingleTimer(name, msg, interval)\nstartTimerWithFixedDelay(name, msg, interval)\nwhere msg is the message object which will be sent after the duration interval has elapsed.\nAny existing timer with the same name will automatically be canceled before adding the new timer.\nThe Scheduler documentation describes the difference between fixed-delay and fixed-rate scheduling. If you are uncertain of which one to use you should pick startTimerWithFixedDelay.\nTimers may be canceled using\ncancelTimer(name)\nwhich is guaranteed to work immediately, meaning that the scheduled message will not be processed after this call even if the timer already fired and queued it. The status of any timer may be inquired with\nisTimerActive(name)\nThese named timers complement state timeouts because they are not affected by intervening reception of other messages.","title":"Timers"},{"location":"/fsm.html#termination-from-inside","text":"The FSM is stopped by specifying the result state as\nstop([reason[, data]])\nThe reason must be one of Normal (which is the default), Shutdown or Failure(reason), and the second argument may be given to change the state data which is available during termination handling.\nNote It should be noted that stop does not abort the actions and stop the FSM immediately. The stop action must be returned from the event handler in the same way as a state transition (but note that the return statement may not be used within a when block).\nScala copysourcewhen(Error) {\n  case Event(\"stop\", _) =>\n    // do cleanup ...\n    stop()\n} Java copysourcewhen(\n    Error,\n    matchEventEquals(\n        \"stop\",\n        (event, data) -> {\n          // do cleanup ...\n          return stop();\n        }));\nYou can use onTermination(handler) to specify custom code that is executed when the FSM is stopped. The handler is a partial function which takes a StopEvent(reason, stateName, stateData) as argument:\nScala copysourceonTermination {\n  case StopEvent(FSM.Normal, state, data)         => // ...\n  case StopEvent(FSM.Shutdown, state, data)       => // ...\n  case StopEvent(FSM.Failure(cause), state, data) => // ...\n} Java copysourceonTermination(\n    matchStop(\n            Normal(),\n            (state, data) -> {\n              /* Do something here */\n            })\n        .stop(\n            Shutdown(),\n            (state, data) -> {\n              /* Do something here */\n            })\n        .stop(\n            Failure.class,\n            (reason, state, data) -> {\n              /* Do something here */\n            }));\nAs for the whenUnhandled case, this handler is not stacked, so each invocation of onTermination replaces the previously installed handler.","title":"Termination from Inside"},{"location":"/fsm.html#termination-from-outside","text":"When an ActorRef associated to a FSM is stopped using the stop() method, its postStop hook will be executed. The default implementation by the FSM traitAbstractFSM class is to execute the onTermination handler if that is prepared to handle a StopEvent(Shutdown, ...).\nWarning In case you override postStop and want to have your onTermination handler called, do not forget to call super.postStop.","title":"Termination from Outside"},{"location":"/fsm.html#testing-and-debugging-finite-state-machines","text":"During development and for trouble shooting FSMs need care just as any other actor. There are specialized tools available as described in TestFSMRef and in the following.","title":"Testing and Debugging Finite State Machines"},{"location":"/fsm.html#event-tracing","text":"The setting pekko.actor.debug.fsm in configuration enables logging of an event trace by LoggingFSM instances:\nScala copysourceimport org.apache.pekko.actor.LoggingFSM\nclass MyFSM extends LoggingFSM[StateType, Data] {\n  override def logDepth = 12\n  onTermination {\n    case StopEvent(FSM.Failure(_), state, data) =>\n      val lastEvents = getLog.mkString(\"\\n\\t\")\n      log.warning(\n        \"Failure in state \" + state + \" with data \" + data + \"\\n\" +\n        \"Events leading up to this point:\\n\\t\" + lastEvents)\n  }\n  // ...\n} Java copysourcestatic class MyFSM extends AbstractLoggingFSM<StateType, Data> {\n  @Override\n  public int logDepth() {\n    return 12;\n  }\n\n  {\n    onTermination(\n        matchStop(\n            Failure.class,\n            (reason, state, data) -> {\n              String lastEvents = getLog().mkString(\"\\n\\t\");\n              log()\n                  .warning(\n                      \"Failure in state \"\n                          + state\n                          + \" with data \"\n                          + data\n                          + \"\\n\"\n                          + \"Events leading up to this point:\\n\\t\"\n                          + lastEvents);\n            }));\n    // ...\n  }\n}\nThis FSM will log at DEBUG level:\nall processed events, including StateTimeout and scheduled timer messages every setting and cancellation of named timers all state transitions\nLife cycle changes and special messages can be logged as described for Actors.","title":"Event Tracing"},{"location":"/fsm.html#rolling-event-log","text":"The LoggingFSM traitAbstractLoggingFSM class adds one more feature to the FSM: a rolling event log which may be used during debugging (for tracing how the FSM entered a certain failure state) or for other creative uses:\nScala copysourceimport org.apache.pekko.actor.LoggingFSM\nclass MyFSM extends LoggingFSM[StateType, Data] {\n  override def logDepth = 12\n  onTermination {\n    case StopEvent(FSM.Failure(_), state, data) =>\n      val lastEvents = getLog.mkString(\"\\n\\t\")\n      log.warning(\n        \"Failure in state \" + state + \" with data \" + data + \"\\n\" +\n        \"Events leading up to this point:\\n\\t\" + lastEvents)\n  }\n  // ...\n} Java copysourcestatic class MyFSM extends AbstractLoggingFSM<StateType, Data> {\n  @Override\n  public int logDepth() {\n    return 12;\n  }\n\n  {\n    onTermination(\n        matchStop(\n            Failure.class,\n            (reason, state, data) -> {\n              String lastEvents = getLog().mkString(\"\\n\\t\");\n              log()\n                  .warning(\n                      \"Failure in state \"\n                          + state\n                          + \" with data \"\n                          + data\n                          + \"\\n\"\n                          + \"Events leading up to this point:\\n\\t\"\n                          + lastEvents);\n            }));\n    // ...\n  }\n}\nThe logDepth defaults to zero, which turns off the event log.\nWarning The log buffer is allocated during actor creation, which is why the configuration is done using a virtual method call. If you want to override with a val, make sure that its initialization happens before the initializer of LoggingFSM runs, and do not change the value returned by logDepth after the buffer has been allocated.\nThe contents of the event log are available using method getLog, which returns an IndexedSeq[LogEntry] where the oldest entry is at index zero.","title":"Rolling Event Log"},{"location":"/persistence.html","text":"","title":"Classic Persistence"},{"location":"/persistence.html#classic-persistence","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see Event Sourcing.","title":"Classic Persistence"},{"location":"/persistence.html#module-info","text":"To use Pekko Persistence, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-persistence\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-persistence-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-persistence-testkit_${versions.ScalaBinary}\"\n}\nYou also have to select journal plugin and optionally snapshot store plugin, see Persistence Plugins.\nProject Info: Pekko Persistence (classic) Artifact org.apache.pekko pekko-persistence 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.persistence License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/persistence.html#introduction","text":"See introduction in Persistence\nPekko Persistence also provides point-to-point communication with at-least-once message delivery semantics.","title":"Introduction"},{"location":"/persistence.html#architecture","text":"PersistentActorAbstractPersistentActor: Is a persistent, stateful actor. It is able to persist events to a journal and can react to them in a thread-safe manner. It can be used to implement both command as well as event sourced actors. When a persistent actor is started or restarted, journaled messages are replayed to that actor so that it can recover its state from these messages. AtLeastOnceDeliveryAbstractPersistentActorWithAtLeastOnceDelivery: To send messages with at-least-once delivery semantics to destinations, also in case of sender and receiver JVM crashes. AsyncWriteJournalAsyncWriteJournal: A journal stores the sequence of messages sent to a persistent actor. An application can control which messages are journaled and which are received by the persistent actor without being journaled. Journal maintains highestSequenceNr that is increased on each message. The storage backend of a journal is pluggable. Replicated journals are available as Community plugins. Snapshot store: A snapshot store persists snapshots of a persistent actor’s state. Snapshots are used for optimizing recovery times. The storage backend of a snapshot store is pluggable. The persistence extension comes with a “local” snapshot storage plugin, which writes to the local filesystem. Replicated snapshot stores are available as Community plugins Event Sourcing. Based on the building blocks described above, Pekko persistence provides abstractions for the development of event sourced applications (see section Event Sourcing).","title":"Architecture"},{"location":"/persistence.html#example","text":"Pekko persistence supports Event Sourcing with the PersistentActor traitAbstractPersistentActor abstract class. An actor that extends this traitclass uses the persistpersist method to persist and handle events. The behavior of a PersistentActoran AbstractPersistentActor is defined by implementing receiveRecovercreateReceiveRecover and receiveCommandcreateReceive. This is demonstrated in the following example.\nScala copysourceimport org.apache.pekko\nimport pekko.actor._\nimport pekko.persistence._\n\ncase class Cmd(data: String)\ncase class Evt(data: String)\n\ncase class ExampleState(events: List[String] = Nil) {\n  def updated(evt: Evt): ExampleState = copy(evt.data :: events)\n  def size: Int = events.length\n  override def toString: String = events.reverse.toString\n}\n\nclass ExamplePersistentActor extends PersistentActor {\n  override def persistenceId = \"sample-id-1\"\n\n  var state = ExampleState()\n\n  def updateState(event: Evt): Unit =\n    state = state.updated(event)\n\n  def numEvents =\n    state.size\n\n  val receiveRecover: Receive = {\n    case evt: Evt                                 => updateState(evt)\n    case SnapshotOffer(_, snapshot: ExampleState) => state = snapshot\n  }\n\n  val snapShotInterval = 1000\n  val receiveCommand: Receive = {\n    case Cmd(data) =>\n      persist(Evt(s\"${data}-${numEvents}\")) { event =>\n        updateState(event)\n        context.system.eventStream.publish(event)\n        if (lastSequenceNr % snapShotInterval == 0 && lastSequenceNr != 0)\n          saveSnapshot(state)\n      }\n    case \"print\" => println(state)\n  }\n\n} Java copysource import org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.actor.Props;\nimport org.apache.pekko.persistence.AbstractPersistentActor;\nimport org.apache.pekko.persistence.SnapshotOffer;\n\nimport java.io.Serializable;\nimport java.util.ArrayList;\n\nclass Cmd implements Serializable {\n  private static final long serialVersionUID = 1L;\n  private final String data;\n\n  public Cmd(String data) {\n    this.data = data;\n  }\n\n  public String getData() {\n    return data;\n  }\n}\n\nclass Evt implements Serializable {\n  private static final long serialVersionUID = 1L;\n  private final String data;\n\n  public Evt(String data) {\n    this.data = data;\n  }\n\n  public String getData() {\n    return data;\n  }\n}\n\nclass ExampleState implements Serializable {\n  private static final long serialVersionUID = 1L;\n  private final ArrayList<String> events;\n\n  public ExampleState() {\n    this(new ArrayList<>());\n  }\n\n  public ExampleState(ArrayList<String> events) {\n    this.events = events;\n  }\n\n  public ExampleState copy() {\n    return new ExampleState(new ArrayList<>(events));\n  }\n\n  public void update(Evt evt) {\n    events.add(evt.getData());\n  }\n\n  public int size() {\n    return events.size();\n  }\n\n  @Override\n  public String toString() {\n    return events.toString();\n  }\n}\n\nclass ExamplePersistentActor extends AbstractPersistentActor {\n\n  private ExampleState state = new ExampleState();\n  private int snapShotInterval = 1000;\n\n  public int getNumEvents() {\n    return state.size();\n  }\n\n  @Override\n  public String persistenceId() {\n    return \"sample-id-1\";\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder()\n        .match(Evt.class, state::update)\n        .match(SnapshotOffer.class, ss -> state = (ExampleState) ss.snapshot())\n        .build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Cmd.class,\n            c -> {\n              final String data = c.getData();\n              final Evt evt = new Evt(data + \"-\" + getNumEvents());\n              persist(\n                  evt,\n                  (Evt e) -> {\n                    state.update(e);\n                    getContext().getSystem().getEventStream().publish(e);\n                    if (lastSequenceNr() % snapShotInterval == 0 && lastSequenceNr() != 0)\n                      // IMPORTANT: create a copy of snapshot because ExampleState is mutable\n                      saveSnapshot(state.copy());\n                  });\n            })\n        .matchEquals(\"print\", s -> System.out.println(state))\n        .build();\n  }\n}\nThe example defines two data types, Cmd and Evt to represent commands and events, respectively. The state of the ExamplePersistentActor is a list of persisted event data contained in ExampleState.\nThe persistent actor’s receiveRecovercreateReceiveRecover method defines how state is updated during recovery by handling Evt and SnapshotOffer messages. The persistent actor’s receiveCommandcreateReceive method is a command handler. In this example, a command is handled by generating an event which is then persisted and handled. Events are persisted by calling persist with an event (or a sequence of events) as first argument and an event handler as second argument.\nThe persist method persists events asynchronously and the event handler is executed for successfully persisted events. Successfully persisted events are internally sent back to the persistent actor as individual messages that trigger event handler executions. An event handler may close over persistent actor state and mutate it. The sender of a persisted event is the sender of the corresponding command. This allows event handlers to reply to the sender of a command (not shown).\nThe main responsibility of an event handler is changing persistent actor state using event data and notifying others about successful state changes by publishing events.\nWhen persisting events with persist it is guaranteed that the persistent actor will not receive further commands between the persist call and the execution(s) of the associated event handler. This also holds for multiple persist calls in context of a single command. Incoming messages are stashed until the persist is completed.\nIf persistence of an event fails, onPersistFailure will be invoked (logging the error by default), and the actor will unconditionally be stopped. If persistence of an event is rejected before it is stored, e.g. due to serialization error, onPersistRejected will be invoked (logging a warning by default) and the actor continues with the next message.\nNote It’s also possible to switch between different command handlers during normal processing and recovery with context.become()getContext().become() and context.unbecome()getContext().unbecome(). To get the actor into the same state after recovery you need to take special care to perform the same state transitions with become and unbecome in the receiveRecovercreateReceiveRecover method as you would have done in the command handler. Note that when using become from receiveRecovercreateReceiveRecover it will still only use the receiveRecovercreateReceiveRecover behavior when replaying the events. When replay is completed it will use the new behavior.","title":"Example"},{"location":"/persistence.html#identifiers","text":"A persistent actor must have an identifier that doesn’t change across different actor incarnations. The identifier must be defined with the persistenceId method.\nScala copysourceoverride def persistenceId = \"my-stable-persistence-id\" Java copysource@Override\npublic String persistenceId() {\n  return \"my-stable-persistence-id\";\n}\nNote persistenceId must be unique to a given entity in the journal (database table/keyspace). When replaying messages persisted to the journal, you query messages with a persistenceId. So, if two different entities share the same persistenceId, message-replaying behavior is corrupted.","title":"Identifiers"},{"location":"/persistence.html#recovery","text":"By default, a persistent actor is automatically recovered on start and on restart by replaying journaled messages. New messages sent to a persistent actor during recovery do not interfere with replayed messages. They are stashed and received by a persistent actor after recovery phase completes.\nThe number of concurrent recoveries that can be in progress at the same time is limited to not overload the system and the backend data store. When exceeding the limit the actors will wait until other recoveries have been completed. This is configured by:\npekko.persistence.max-concurrent-recoveries = 50\nNote Accessing the sender()sender with getSender() for replayed messages will always result in a deadLetters reference, as the original sender is presumed to be long gone. If you indeed have to notify an actor during recovery in the future, store its ActorPathActorPath explicitly in your persisted events.","title":"Recovery"},{"location":"/persistence.html#recovery-customization","text":"Applications may also customise how recovery is performed by returning a customised RecoveryRecovery object in the recovery method of a PersistentActorAbstractPersistentActor,\nTo skip loading snapshots and replay all events you can use SnapshotSelectionCriteria.NoneSnapshotSelectionCriteria.none(). This can be useful if snapshot serialization format has changed in an incompatible way. It should typically not be used when events have been deleted.\nScala copysourceoverride def recovery =\n  Recovery(fromSnapshot = SnapshotSelectionCriteria.None) Java copysource@Override\npublic Recovery recovery() {\n  return Recovery.create(SnapshotSelectionCriteria.none());\n}\nAnother possible recovery customization, which can be useful for debugging, is setting an upper bound on the replay, causing the actor to be replayed only up to a certain point “in the past” (instead of being replayed to its most up to date state). Note that after that it is a bad idea to persist new events because a later recovery will probably be confused by the new events that follow the events that were previously skipped.\nScala copysourceoverride def recovery = Recovery(toSequenceNr = 457L) Java copysource@Override\npublic Recovery recovery() {\n  return Recovery.create(457L);\n}\nRecovery can be disabled by returning Recovery.none() in the recovery method of a PersistentActor:\nScala copysourceoverride def recovery = Recovery.none Java copysource@Override\npublic Recovery recovery() {\n  return Recovery.none();\n}","title":"Recovery customization"},{"location":"/persistence.html#recovery-status","text":"A persistent actor can query its own recovery status via the methods\nScala copysourcedef recoveryRunning: Boolean\ndef recoveryFinished: Boolean Java copysourcepublic boolean recoveryRunning();\n\npublic boolean recoveryFinished();\nSometimes, there is a need for performing additional initialization when the recovery has completed before processing any other message sent to the persistent actor. The persistent actor will receive a special RecoveryCompletedRecoveryCompleted message right after recovery and before any other received messages.\nScala copysource override def receiveRecover: Receive = {\n  case RecoveryCompleted =>\n  // perform init after recovery, before any other messages\n  // ...\n  case evt => // ...\n}\n\noverride def receiveCommand: Receive = {\n  case msg => // ...\n} Java copysourceclass MyPersistentActor5 extends AbstractPersistentActor {\n\n  @Override\n  public String persistenceId() {\n    return \"my-stable-persistence-id\";\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder()\n        .match(\n            RecoveryCompleted.class,\n            r -> {\n              // perform init after recovery, before any other messages\n              // ...\n            })\n        .match(String.class, this::handleEvent)\n        .build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(String.class, s -> s.equals(\"cmd\"), s -> persist(\"evt\", this::handleEvent))\n        .build();\n  }\n\n  private void handleEvent(String event) {\n    // update state\n    // ...\n  }\n}\nThe actor will always receive a RecoveryCompleted message, even if there are no events in the journal and the snapshot store is empty, or if it’s a new persistent actor with a previously unused persistenceId.\nIf there is a problem with recovering the state of the actor from the journal, onRecoveryFailure is called (logging the error by default) and the actor will be stopped.","title":"Recovery status"},{"location":"/persistence.html#internal-stash","text":"The persistent actor has a private stash for internally caching incoming messages during recovery or the persistpersist\\persistAllpersistAll method persisting events. You can still use/inherit from the StashStash interface. The internal stash cooperates with the normal stash by hooking into unstashAllunstashAll making sure messages are unstashed properly to the internal stash to maintain ordering guarantees.\nYou should be careful to not send more messages to a persistent actor than it can keep up with, otherwise the number of stashed messages will grow without bounds. It can be wise to protect against OutOfMemoryError by defining a maximum stash capacity in the mailbox configuration:\npekko.actor.default-mailbox.stash-capacity=10000\nNote that the stash capacity is per actor. If you have many persistent actors, e.g. when using cluster sharding, you may need to define a small stash capacity to ensure that the total number of stashed messages in the system doesn’t consume too much memory. Additionally, the persistent actor defines three strategies to handle failure when the internal stash capacity is exceeded. The default overflow strategy is the ThrowOverflowExceptionStrategyThrowOverflowExceptionStrategy, which discards the current received message and throws a StashOverflowExceptionStashOverflowException, causing actor restart if the default supervision strategy is used. You can override the internalStashOverflowStrategyinternalStashOverflowStrategy method to return DiscardToDeadLetterStrategyDiscardToDeadLetterStrategy or ReplyToStrategyReplyToStrategy for any “individual” persistent actor, or define the “default” for all persistent actors by providing FQCN, which must be a subclass of StashOverflowStrategyConfiguratorStashOverflowStrategyConfigurator, in the persistence configuration:\npekko.persistence.internal-stash-overflow-strategy=\n  \"org.apache.pekko.persistence.ThrowExceptionConfigurator\"\nThe DiscardToDeadLetterStrategy strategy also has a pre-packaged companion configurator DiscardConfiguratorDiscardConfigurator.\nYou can also query the default strategy via the Pekko persistence extension singleton:\nScala Persistence(context.system).defaultInternalStashOverflowStrategy\n Java Persistence.get(getContext().getSystem()).defaultInternalStashOverflowStrategy();\nNote The bounded mailbox should be avoided in the persistent actor, by which the messages come from storage backends may be discarded. You can use bounded stash instead of it.","title":"Internal stash"},{"location":"/persistence.html#relaxed-local-consistency-requirements-and-high-throughput-use-cases","text":"If faced with relaxed local consistency requirements and high throughput demands sometimes PersistentActorAbstractPersistentActor and its persistpersist may not be enough in terms of consuming incoming Commands at a high rate, because it has to wait until all Events related to a given Command are processed in order to start processing the next Command. While this abstraction is very useful for most cases, sometimes you may be faced with relaxed requirements about consistency – for example you may want to process commands as fast as you can, assuming that the Event will eventually be persisted and handled properly in the background, retroactively reacting to persistence failures if needed.\nThe persistAsyncpersistAsync method provides a tool for implementing high-throughput persistent actors. It will not stash incoming Commands while the Journal is still working on persisting and/or user code is executing event callbacks.\nIn the below example, the event callbacks may be called “at any time”, even after the next Command has been processed. The ordering between events is still guaranteed (“evt-b-1” will be sent after “evt-a-2”, which will be sent after “evt-a-1” etc.).\nScala copysourceclass MyPersistentActor extends PersistentActor {\n\n  override def persistenceId = \"my-stable-persistence-id\"\n\n  override def receiveRecover: Receive = {\n    case _ => // handle recovery here\n  }\n\n  override def receiveCommand: Receive = {\n    case c: String => {\n      sender() ! c\n      persistAsync(s\"evt-$c-1\") { e =>\n        sender() ! e\n      }\n      persistAsync(s\"evt-$c-2\") { e =>\n        sender() ! e\n      }\n    }\n  }\n}\n\n// usage\npersistentActor ! \"a\"\npersistentActor ! \"b\"\n\n// possible order of received messages:\n// a\n// b\n// evt-a-1\n// evt-a-2\n// evt-b-1\n// evt-b-2\n Java copysourceclass MyPersistentActor extends AbstractPersistentActor {\n\n  @Override\n  public String persistenceId() {\n    return \"my-stable-persistence-id\";\n  }\n\n  private void handleCommand(String c) {\n    getSender().tell(c, getSelf());\n\n    persistAsync(\n        String.format(\"evt-%s-1\", c),\n        e -> {\n          getSender().tell(e, getSelf());\n        });\n    persistAsync(\n        String.format(\"evt-%s-2\", c),\n        e -> {\n          getSender().tell(e, getSelf());\n        });\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().match(String.class, this::handleCommand).build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(String.class, this::handleCommand).build();\n  }\n}\nNote In order to implement the pattern known as “command sourcing” call persistAsync(cmd)(...)persistAsync right away on all incoming messages and handle them in the callback.\nWarning The callback will not be invoked if the actor is restarted (or stopped) in between the call to persistAsync and the journal has confirmed the write.","title":"Relaxed local consistency requirements and high throughput use-cases"},{"location":"/persistence.html#deferring-actions-until-preceding-persist-handlers-have-executed","text":"Sometimes, when working with persistAsyncpersistAsync or persistpersist you may find that it would be nice to define some actions in terms of ‘‘happens-after the previous persistAsync/persist handlers have been invoked’’. PersistentActorAbstractPersistentActor provides utility methods called deferdefer and deferAsyncdeferAsync, which work similarly to persist and persistAsync respectively yet do not persist the passed in event. It is recommended to use them for read operations, and actions which do not have corresponding events in your domain model.\nUsing those methods is very similar to the persist family of methods, yet they do not persist the passed in event. It will be kept in memory and used when invoking the handler.\nScala copysourceclass MyPersistentActor extends PersistentActor {\n\n  override def persistenceId = \"my-stable-persistence-id\"\n\n  override def receiveRecover: Receive = {\n    case _ => // handle recovery here\n  }\n\n  override def receiveCommand: Receive = {\n    case c: String => {\n      sender() ! c\n      persistAsync(s\"evt-$c-1\") { e =>\n        sender() ! e\n      }\n      persistAsync(s\"evt-$c-2\") { e =>\n        sender() ! e\n      }\n      deferAsync(s\"evt-$c-3\") { e =>\n        sender() ! e\n      }\n    }\n  }\n} Java copysourceclass MyPersistentActor extends AbstractPersistentActor {\n\n  @Override\n  public String persistenceId() {\n    return \"my-stable-persistence-id\";\n  }\n\n  private void handleCommand(String c) {\n    persistAsync(\n        String.format(\"evt-%s-1\", c),\n        e -> {\n          getSender().tell(e, getSelf());\n        });\n    persistAsync(\n        String.format(\"evt-%s-2\", c),\n        e -> {\n          getSender().tell(e, getSelf());\n        });\n\n    deferAsync(\n        String.format(\"evt-%s-3\", c),\n        e -> {\n          getSender().tell(e, getSelf());\n        });\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().match(String.class, this::handleCommand).build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(String.class, this::handleCommand).build();\n  }\n}\nNotice that the sender() is safe to access in the handler callback, and will be pointing to the original sender of the command for which this defer or deferAsync handler was called.\nThe calling side will get the responses in this (guaranteed) order:\nScala copysourcepersistentActor ! \"a\"\npersistentActor ! \"b\"\n\n// order of received messages:\n// a\n// b\n// evt-a-1\n// evt-a-2\n// evt-a-3\n// evt-b-1\n// evt-b-2\n// evt-b-3\n Java copysourcefinal ActorRef persistentActor = system.actorOf(Props.create(MyPersistentActor.class));\npersistentActor.tell(\"a\", sender);\npersistentActor.tell(\"b\", sender);\n\n// order of received messages:\n// a\n// b\n// evt-a-1\n// evt-a-2\n// evt-a-3\n// evt-b-1\n// evt-b-2\n// evt-b-3\nYou can also call defer or deferAsync with persist.\nScala copysourceclass MyPersistentActor extends PersistentActor {\n\n  override def persistenceId = \"my-stable-persistence-id\"\n\n  override def receiveRecover: Receive = {\n    case _ => // handle recovery here\n  }\n\n  override def receiveCommand: Receive = {\n    case c: String => {\n      sender() ! c\n      persist(s\"evt-$c-1\") { e =>\n        sender() ! e\n      }\n      persist(s\"evt-$c-2\") { e =>\n        sender() ! e\n      }\n      defer(s\"evt-$c-3\") { e =>\n        sender() ! e\n      }\n    }\n  }\n} Java copysourceclass MyPersistentActor extends AbstractPersistentActor {\n\n  @Override\n  public String persistenceId() {\n    return \"my-stable-persistence-id\";\n  }\n\n  private void handleCommand(String c) {\n    persist(\n        String.format(\"evt-%s-1\", c),\n        e -> {\n          sender().tell(e, self());\n        });\n    persist(\n        String.format(\"evt-%s-2\", c),\n        e -> {\n          sender().tell(e, self());\n        });\n\n    defer(\n        String.format(\"evt-%s-3\", c),\n        e -> {\n          sender().tell(e, self());\n        });\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().match(String.class, this::handleCommand).build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(String.class, this::handleCommand).build();\n  }\n}\nWarning The callback will not be invoked if the actor is restarted (or stopped) in between the call to defer or deferAsync and the journal has processed and confirmed all preceding writes.","title":"Deferring actions until preceding persist handlers have executed"},{"location":"/persistence.html#nested-persist-calls","text":"It is possible to call persistpersist and persistAsyncpersistAsync inside their respective callback blocks and they will properly retain both the thread safety (including the right value of sender()getSender()) as well as stashing guarantees.\nIn general it is encouraged to create command handlers which do not need to resort to nested event persisting, however there are situations where it may be useful. It is important to understand the ordering of callback execution in those situations, as well as their implication on the stashing behavior (that persist() enforces). In the following example two persist calls are issued, and each of them issues another persist inside its callback:\nScala copysourceoverride def receiveCommand: Receive = {\n  case c: String =>\n    sender() ! c\n\n    persist(s\"$c-1-outer\") { outer1 =>\n      sender() ! outer1\n      persist(s\"$c-1-inner\") { inner1 =>\n        sender() ! inner1\n      }\n    }\n\n    persist(s\"$c-2-outer\") { outer2 =>\n      sender() ! outer2\n      persist(s\"$c-2-inner\") { inner2 =>\n        sender() ! inner2\n      }\n    }\n} Java copysource@Override\npublic Receive createReceiveRecover() {\n  final Procedure<String> replyToSender = event -> getSender().tell(event, getSelf());\n\n  return receiveBuilder()\n      .match(\n          String.class,\n          msg -> {\n            persist(\n                String.format(\"%s-outer-1\", msg),\n                event -> {\n                  getSender().tell(event, getSelf());\n                  persist(String.format(\"%s-inner-1\", event), replyToSender);\n                });\n\n            persist(\n                String.format(\"%s-outer-2\", msg),\n                event -> {\n                  getSender().tell(event, getSelf());\n                  persist(String.format(\"%s-inner-2\", event), replyToSender);\n                });\n          })\n      .build();\n}\nWhen sending two commands to this PersistentActorAbstractPersistentActor, the persist handlers will be executed in the following order:\nScala copysourcepersistentActor ! \"a\"\npersistentActor ! \"b\"\n\n// order of received messages:\n// a\n// a-outer-1\n// a-outer-2\n// a-inner-1\n// a-inner-2\n// and only then process \"b\"\n// b\n// b-outer-1\n// b-outer-2\n// b-inner-1\n// b-inner-2\n Java copysourcepersistentActor.tell(\"a\", ActorRef.noSender());\npersistentActor.tell(\"b\", ActorRef.noSender());\n\n// order of received messages:\n// a\n// a-outer-1\n// a-outer-2\n// a-inner-1\n// a-inner-2\n// and only then process \"b\"\n// b\n// b-outer-1\n// b-outer-2\n// b-inner-1\n// b-inner-2\nFirst the “outer layer” of persist calls is issued and their callbacks are applied. After these have successfully completed, the inner callbacks will be invoked (once the events they are persisting have been confirmed to be persisted by the journal). Only after all these handlers have been successfully invoked will the next command be delivered to the persistent Actor. In other words, the stashing of incoming commands that is guaranteed by initially calling persist() on the outer layer is extended until all nested persist callbacks have been handled.\nIt is also possible to nest persistAsync calls, using the same pattern:\nScala copysourceoverride def receiveCommand: Receive = {\n  case c: String =>\n    sender() ! c\n    persistAsync(c + \"-outer-1\") { outer =>\n      sender() ! outer\n      persistAsync(c + \"-inner-1\") { inner =>\n        sender() ! inner\n      }\n    }\n    persistAsync(c + \"-outer-2\") { outer =>\n      sender() ! outer\n      persistAsync(c + \"-inner-2\") { inner =>\n        sender() ! inner\n      }\n    }\n} Java copysource@Override\npublic Receive createReceive() {\n  final Procedure<String> replyToSender = event -> getSender().tell(event, getSelf());\n\n  return receiveBuilder()\n      .match(\n          String.class,\n          msg -> {\n            persistAsync(\n                String.format(\"%s-outer-1\", msg),\n                event -> {\n                  getSender().tell(event, getSelf());\n                  persistAsync(String.format(\"%s-inner-1\", event), replyToSender);\n                });\n\n            persistAsync(\n                String.format(\"%s-outer-2\", msg),\n                event -> {\n                  getSender().tell(event, getSelf());\n                  persistAsync(String.format(\"%s-inner-1\", event), replyToSender);\n                });\n          })\n      .build();\n}\nIn this case no stashing is happening, yet events are still persisted and callbacks are executed in the expected order:\nScala copysourcepersistentActor ! \"a\"\npersistentActor ! \"b\"\n\n// order of received messages:\n// a\n// b\n// a-outer-1\n// a-outer-2\n// b-outer-1\n// b-outer-2\n// a-inner-1\n// a-inner-2\n// b-inner-1\n// b-inner-2\n\n// which can be seen as the following causal relationship:\n// a -> a-outer-1 -> a-outer-2 -> a-inner-1 -> a-inner-2\n// b -> b-outer-1 -> b-outer-2 -> b-inner-1 -> b-inner-2\n Java copysourcepersistentActor.tell(\"a\", getSelf());\npersistentActor.tell(\"b\", getSelf());\n\n// order of received messages:\n// a\n// b\n// a-outer-1\n// a-outer-2\n// b-outer-1\n// b-outer-2\n// a-inner-1\n// a-inner-2\n// b-inner-1\n// b-inner-2\n\n// which can be seen as the following causal relationship:\n// a -> a-outer-1 -> a-outer-2 -> a-inner-1 -> a-inner-2\n// b -> b-outer-1 -> b-outer-2 -> b-inner-1 -> b-inner-2\nWhile it is possible to nest mixed persist and persistAsync with keeping their respective semantics it is not a recommended practice, as it may lead to overly complex nesting.\nWarning While it is possible to nest persist calls within one another, it is not legal call persist from any other Thread than the Actors message processing Thread. For example, it is not legal to call persist from Futures! Doing so will break the guarantees that the persist methods aim to provide. Always call persist and persistAsync from within the Actor’s receive block (or methods synchronously invoked from there).","title":"Nested persist calls"},{"location":"/persistence.html#failures","text":"If persistence of an event fails, onPersistFailure will be invoked (logging the error by default), and the actor will unconditionally be stopped.\nThe reason that it cannot resume when persist fails is that it is unknown if the event was actually persisted or not, and therefore it is in an inconsistent state. Restarting on persistent failures will most likely fail anyway since the journal is probably unavailable. It is better to stop the actor and after a back-off timeout start it again. The BackoffSupervisorBackoffSupervisor actor is provided to support such restarts.\nScala copysourceval childProps = Props[MyPersistentActor]()\nval props = BackoffSupervisor.props(BackoffOpts\n  .onStop(childProps, childName = \"myActor\", minBackoff = 3.seconds, maxBackoff = 30.seconds, randomFactor = 0.2))\ncontext.actorOf(props, name = \"mySupervisor\") Java copysource@Override\npublic void preStart() throws Exception {\n  final Props childProps = Props.create(MyPersistentActor1.class);\n  final Props props =\n      BackoffSupervisor.props(\n          BackoffOpts.onStop(\n              childProps, \"myActor\", Duration.ofSeconds(3), Duration.ofSeconds(30), 0.2));\n  getContext().actorOf(props, \"mySupervisor\");\n  super.preStart();\n}\nSee Backoff Supervision strategies for more details about actor supervision.\nIf persistence of an event is rejected before it is stored, e.g. due to serialization error, onPersistRejected will be invoked (logging a warning by default), and the actor continues with next message.\nIf there is a problem with recovering the state of the actor from the journal when the actor is started, onRecoveryFailure is called (logging the error by default), and the actor will be stopped. Note that failure to load snapshot is also treated like this, but you can disable loading of snapshots if you for example know that serialization format has changed in an incompatible way, see Recovery customization.","title":"Failures"},{"location":"/persistence.html#atomic-writes","text":"Each event is stored atomically, but it is also possible to store several events atomically by using the persistAllpersistAll or persistAllAsyncpersistAllAsync method. That means that all events passed to that method are stored or none of them are stored if there is an error.\nThe recovery of a persistent actor will therefore never be done partially with only a subset of events persisted by persistAll.\nSome journals may not support atomic writes of several events and they will then reject the persistAll command, i.e. onPersistRejected is called with an exception (typically UnsupportedOperationException).","title":"Atomic writes"},{"location":"/persistence.html#batch-writes","text":"In order to optimize throughput when using persistAsyncpersistAsync, a persistent actor internally batches events to be stored under high load before writing them to the journal (as a single batch). The batch size is dynamically determined by how many events are emitted during the time of a journal round-trip: after sending a batch to the journal no further batch can be sent before confirmation has been received that the previous batch has been written. Batch writes are never timer-based which keeps latencies at a minimum.","title":"Batch writes"},{"location":"/persistence.html#message-deletion","text":"It is possible to delete all messages (journaled by a single persistent actor) up to a specified sequence number; Persistent actors may call the deleteMessagesdeleteMessages method to this end.\nDeleting messages in Event Sourcing based applications is typically either not used at all, or used in conjunction with snapshotting, i.e. after a snapshot has been successfully stored, a deleteMessages(toSequenceNr) up until the sequence number of the data held by that snapshot can be issued to safely delete the previous events while still having access to the accumulated state during replays - by loading the snapshot.\nWarning If you are using Persistence Query, query results may be missing deleted messages in a journal, depending on how deletions are implemented in the journal plugin. Unless you use a plugin which still shows deleted messages in persistence query results, you have to design your application so that it is not affected by missing messages.\nThe result of the deleteMessages request is signaled to the persistent actor with a DeleteMessagesSuccessDeleteMessagesSuccess message if the delete was successful or a DeleteMessagesFailureDeleteMessagesFailure message if it failed.\nMessage deletion doesn’t affect the highest sequence number of the journal, even if all messages were deleted from it after deleteMessages invocation.","title":"Message deletion"},{"location":"/persistence.html#persistence-status-handling","text":"Persisting, deleting, and replaying messages can either succeed or fail.\nMethod Success persist / persistAsync persist handler invoked onPersistRejected No automatic actions. recovery RecoveryCompleted deleteMessages DeleteMessagesSuccess\nThe most important operations (persist and recovery) have failure handlers modelled as explicit callbacks which the user can override in the PersistentActor. The default implementations of these handlers emit a log message (error for persist/recovery failures, and warning for others), logging the failure cause and information about which message caused the failure.\nFor critical failures, such as recovery or persisting events failing, the persistent actor will be stopped after the failure handler is invoked. This is because if the underlying journal implementation is signalling persistence failures it is most likely either failing completely or overloaded and restarting right-away and trying to persist the event again will most likely not help the journal recover – as it would likely cause a Thundering herd problem, as many persistent actors would restart and try to persist their events again. Instead, using a BackoffSupervisorBackoffSupervisor (as described in Failures) which implements an exponential-backoff strategy which allows for more breathing room for the journal to recover between restarts of the persistent actor.\nNote Journal implementations may choose to implement a retry mechanism, e.g. such that only after a write fails N number of times a persistence failure is signalled back to the user. In other words, once a journal returns a failure, it is considered fatal by Pekko Persistence, and the persistent actor which caused the failure will be stopped. Check the documentation of the journal implementation you are using for details if/how it is using this technique.","title":"Persistence status handling"},{"location":"/persistence.html#safely-shutting-down-persistent-actors","text":"Special care should be given when shutting down persistent actors from the outside. With normal Actors it is often acceptable to use the special PoisonPill message to signal to an Actor that it should stop itself once it receives this message – in fact this message is handled automatically by Pekko, leaving the target actor no way to refuse stopping itself when given a poison pill.\nThis can be dangerous when used with PersistentActor due to the fact that incoming commands are stashed while the persistent actor is awaiting confirmation from the Journal that events have been written when persist()persist() was used. Since the incoming commands will be drained from the Actor’s mailbox and put into its internal stash while awaiting the confirmation (thus, before calling the persist handlers) the Actor may receive and (auto)handle the PoisonPill before it processes the other messages which have been put into its stash, causing a pre-mature shutdown of the Actor.\nWarning Consider using explicit shut-down messages instead of PoisonPill when working with persistent actors.\nThe example below highlights how messages arrive in the Actor’s mailbox and how they interact with its internal stashing mechanism when persist() is used. Notice the early stop behavior that occurs when PoisonPill is used:\nScala copysource/** Explicit shutdown message */\ncase object Shutdown\n\nclass SafePersistentActor extends PersistentActor {\n  override def persistenceId = \"safe-actor\"\n\n  override def receiveCommand: Receive = {\n    case c: String =>\n      println(c)\n      persist(s\"handle-$c\") { println(_) }\n    case Shutdown =>\n      context.stop(self)\n  }\n\n  override def receiveRecover: Receive = {\n    case _ => // handle recovery here\n  }\n} Java copysourcefinal class Shutdown {}\n\nclass MyPersistentActor extends AbstractPersistentActor {\n  @Override\n  public String persistenceId() {\n    return \"some-persistence-id\";\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Shutdown.class,\n            shutdown -> {\n              getContext().stop(getSelf());\n            })\n        .match(\n            String.class,\n            msg -> {\n              System.out.println(msg);\n              persist(\"handle-\" + msg, e -> System.out.println(e));\n            })\n        .build();\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().matchAny(any -> {}).build();\n  }\n}\nScala copysource// UN-SAFE, due to PersistentActor's command stashing:\npersistentActor ! \"a\"\npersistentActor ! \"b\"\npersistentActor ! PoisonPill\n// order of received messages:\n// a\n//   # b arrives at mailbox, stashing;        internal-stash = [b]\n// PoisonPill is an AutoReceivedMessage, is handled automatically\n// !! stop !!\n// Actor is stopped without handling `b` nor the `a` handler! Java copysource// UN-SAFE, due to PersistentActor's command stashing:\npersistentActor.tell(\"a\", ActorRef.noSender());\npersistentActor.tell(\"b\", ActorRef.noSender());\npersistentActor.tell(PoisonPill.getInstance(), ActorRef.noSender());\n// order of received messages:\n// a\n//   # b arrives at mailbox, stashing;        internal-stash = [b]\n//   # PoisonPill arrives at mailbox, stashing; internal-stash = [b, Shutdown]\n// PoisonPill is an AutoReceivedMessage, is handled automatically\n// !! stop !!\n// Actor is stopped without handling `b` nor the `a` handler!\nScala copysource// SAFE:\npersistentActor ! \"a\"\npersistentActor ! \"b\"\npersistentActor ! Shutdown\n// order of received messages:\n// a\n//   # b arrives at mailbox, stashing;        internal-stash = [b]\n//   # Shutdown arrives at mailbox, stashing; internal-stash = [b, Shutdown]\n// handle-a\n//   # unstashing;                            internal-stash = [Shutdown]\n// b\n// handle-b\n//   # unstashing;                            internal-stash = []\n// Shutdown\n// -- stop -- Java copysource// SAFE:\npersistentActor.tell(\"a\", ActorRef.noSender());\npersistentActor.tell(\"b\", ActorRef.noSender());\npersistentActor.tell(new Shutdown(), ActorRef.noSender());\n// order of received messages:\n// a\n//   # b arrives at mailbox, stashing;        internal-stash = [b]\n//   # Shutdown arrives at mailbox, stashing; internal-stash = [b, Shutdown]\n// handle-a\n//   # unstashing;                            internal-stash = [Shutdown]\n// b\n// handle-b\n//   # unstashing;                            internal-stash = []\n// Shutdown\n// -- stop --","title":"Safely shutting down persistent actors"},{"location":"/persistence.html#replay-filter","text":"See Replay filter in the documentation of the new API.","title":"Replay Filter"},{"location":"/persistence.html#snapshots","text":"As you model your domain using actors, you may notice that some actors may be prone to accumulating extremely long event logs and experiencing long recovery times. Sometimes, the right approach may be to split out into a set of shorter lived actors. However, when this is not an option, you can use snapshots to reduce recovery times drastically.\nPersistent actors can save snapshots of internal state by calling the saveSnapshotsaveSnapshot method. If saving of a snapshot succeeds, the persistent actor receives a SaveSnapshotSuccessSaveSnapshotSuccess message, otherwise a SaveSnapshotFailureSaveSnapshotFailure message\nScala copysourcevar state: Any = _\n\nval snapShotInterval = 1000\noverride def receiveCommand: Receive = {\n  case SaveSnapshotSuccess(metadata)         => // ...\n  case SaveSnapshotFailure(metadata, reason) => // ...\n  case cmd: String =>\n    persist(s\"evt-$cmd\") { e =>\n      updateState(e)\n      if (lastSequenceNr % snapShotInterval == 0 && lastSequenceNr != 0)\n        saveSnapshot(state)\n    }\n} Java copysourceprivate Object state;\nprivate int snapShotInterval = 1000;\n\n@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n      .match(\n          SaveSnapshotSuccess.class,\n          ss -> {\n            SnapshotMetadata metadata = ss.metadata();\n            // ...\n          })\n      .match(\n          SaveSnapshotFailure.class,\n          sf -> {\n            SnapshotMetadata metadata = sf.metadata();\n            // ...\n          })\n      .match(\n          String.class,\n          cmd -> {\n            persist(\n                \"evt-\" + cmd,\n                e -> {\n                  updateState(e);\n                  if (lastSequenceNr() % snapShotInterval == 0 && lastSequenceNr() != 0)\n                    saveSnapshot(state);\n                });\n          })\n      .build();\n}\nwhere metadata is of type SnapshotMetadataSnapshotMetadata and contains:\npersistenceId sequenceNr timestamp\nDuring recovery, the persistent actor is offered the latest saved snapshot via a SnapshotOfferSnapshotOffer message from which it can initialize internal state.\nScala copysourcevar state: Any = _\n\noverride def receiveRecover: Receive = {\n  case SnapshotOffer(metadata, offeredSnapshot) => state = offeredSnapshot\n  case RecoveryCompleted                        =>\n  case event                                    => // ...\n} Java copysourceprivate Object state;\n\n@Override\npublic Receive createReceiveRecover() {\n  return receiveBuilder()\n      .match(\n          SnapshotOffer.class,\n          s -> {\n            state = s.snapshot();\n            // ...\n          })\n      .match(\n          String.class,\n          s -> {\n            /* ...*/\n          })\n      .build();\n}\nThe replayed messages that follow the SnapshotOffer message, if any, are younger than the offered snapshot. They finally recover the persistent actor to its current (i.e. latest) state.\nIn general, a persistent actor is only offered a snapshot if that persistent actor has previously saved one or more snapshots and at least one of these snapshots matches the SnapshotSelectionCriteriaSnapshotSelectionCriteria that can be specified for recovery.\nScala copysourceoverride def recovery =\n  Recovery(\n    fromSnapshot = SnapshotSelectionCriteria(maxSequenceNr = 457L, maxTimestamp = System.currentTimeMillis)) Java copysource@Override\npublic Recovery recovery() {\n  return Recovery.create(\n      SnapshotSelectionCriteria.create(457L, System.currentTimeMillis()));\n}\nIf not specified, they default to SnapshotSelectionCriteria.LatestSnapshotSelectionCriteria.latest() which selects the latest (= youngest) snapshot. To disable snapshot-based recovery, applications should use SnapshotSelectionCriteria.NoneSnapshotSelectionCriteria.none(). A recovery where no saved snapshot matches the specified SnapshotSelectionCriteria will replay all journaled messages.\nNote In order to use snapshots, a default snapshot-store (pekko.persistence.snapshot-store.plugin) must be configured, or the PersistentActorpersistent actor can pick a snapshot store explicitly by overriding def snapshotPluginId: StringString snapshotPluginId(). Because some use cases may not benefit from or need snapshots, it is perfectly valid not to not configure a snapshot store. However, Pekko will log a warning message when this situation is detected and then continue to operate until an actor tries to store a snapshot, at which point the operation will fail (by replying with an SaveSnapshotFailureSaveSnapshotFailure for example). Note that the “persistence mode” of Cluster Sharding makes use of snapshots. If you use that mode, you’ll need to define a snapshot store plugin.","title":"Snapshots"},{"location":"/persistence.html#snapshot-deletion","text":"A persistent actor can delete individual snapshots by calling the deleteSnapshotdeleteSnapshot method with the sequence number of when the snapshot was taken.\nTo bulk-delete a range of snapshots matching scala/org/apache/pekko/persistence.SnapshotSelectionCriteria], persistent actors should use the deleteSnapshotsdeleteSnapshots method. Depending on the journal used this might be inefficient. It is best practice to do specific deletes with deleteSnapshot or to include a minSequenceNr as well as a maxSequenceNr for the SnapshotSelectionCriteria.","title":"Snapshot deletion"},{"location":"/persistence.html#snapshot-status-handling","text":"Saving or deleting snapshots can either succeed or fail – this information is reported back to the persistent actor via status messages as illustrated in the following table.\nMethod Success Failure message saveSnapshot(Any) SaveSnapshotSuccess SaveSnapshotFailure deleteSnapshot(Long) DeleteSnapshotSuccess DeleteSnapshotFailure deleteSnapshots(SnapshotSelectionCriteria) DeleteSnapshotsSuccess DeleteSnapshotsFailure\nIf failure messages are left unhandled by the actor, a default warning log message will be logged for each incoming failure message. No default action is performed on the success messages, however you’re free to handle them e.g. in order to delete an in memory representation of the snapshot, or in the case of failure to attempt save the snapshot again.","title":"Snapshot status handling"},{"location":"/persistence.html#optional-snapshots","text":"By default, the persistent actor will unconditionally be stopped if the snapshot can’t be loaded in the recovery. It is possible to make snapshot loading optional. This can be useful when it is alright to ignore snapshot in case of for example deserialization errors. When snapshot loading fails it will instead recover by replaying all events.\nEnable this feature by setting snapshot-is-optional = true in the snapshot store configuration.\nWarning Don’t set snapshot-is-optional = true if events have been deleted because that would result in wrong recovered state if snapshot load fails.","title":"Optional snapshots"},{"location":"/persistence.html#scaling-out","text":"See Scaling out in the documentation of the new API.","title":"Scaling out"},{"location":"/persistence.html#at-least-once-delivery","text":"To send messages with at-least-once delivery semantics to destinations you can mix-in AtLeastOnceDelivery trait to your PersistentActorextend the AbstractPersistentActorWithAtLeastOnceDelivery class instead of AbstractPersistentActor on the sending side. It takes care of re-sending messages when they have not been confirmed within a configurable timeout.\nThe state of the sending actor, including which messages have been sent that have not been confirmed by the recipient must be persistent so that it can survive a crash of the sending actor or JVM. The AtLeastOnceDelivery traitAbstractPersistentActorWithAtLeastOnceDelivery class does not persist anything by itself. It is your responsibility to persist the intent that a message is sent and that a confirmation has been received.\nNote At-least-once delivery implies that original message sending order is not always preserved, and the destination may receive duplicate messages. Semantics do not match those of a normal ActorRefActorRef send operation: it is not at-most-once delivery message order for the same sender–receiver pair is not preserved due to possible resends after a crash and restart of the destination messages are still delivered to the new actor incarnation These semantics are similar to what an ActorPathActorPath represents (see Actor Lifecycle), therefore you need to supply a path and not a reference when delivering messages. The messages are sent to the path with an actor selection.\nUse the deliverdeliver method to send a message to a destination. Call the confirmDeliveryconfirmDelivery method when the destination has replied with a confirmation message.","title":"At-Least-Once Delivery"},{"location":"/persistence.html#relationship-between-deliver-and-confirmdelivery","text":"To send messages to the destination path, use the deliver method after you have persisted the intent to send the message.\nThe destination actor must send back a confirmation message. When the sending actor receives this confirmation message you should persist the fact that the message was delivered successfully and then call the confirmDelivery method.\nIf the persistent actor is not currently recovering, the deliver method will send the message to the destination actor. When recovering, messages will be buffered until they have been confirmed using confirmDelivery. Once recovery has completed, if there are outstanding messages that have not been confirmed (during the message replay), the persistent actor will resend these before sending any other messages.\nDeliver requires a deliveryIdToMessage function to pass the provided deliveryId into the message so that the correlation between deliver and confirmDelivery is possible. The deliveryId must do the round trip. Upon receipt of the message, the destination actor will send the samedeliveryId wrapped in a confirmation message back to the sender. The sender will then use it to call confirmDelivery method to complete the delivery routine.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.{ Actor, ActorSelection }\nimport pekko.persistence.AtLeastOnceDelivery\n\ncase class Msg(deliveryId: Long, s: String)\ncase class Confirm(deliveryId: Long)\n\nsealed trait Evt\ncase class MsgSent(s: String) extends Evt\ncase class MsgConfirmed(deliveryId: Long) extends Evt\n\nclass MyPersistentActor(destination: ActorSelection) extends PersistentActor with AtLeastOnceDelivery {\n\n  override def persistenceId: String = \"persistence-id\"\n\n  override def receiveCommand: Receive = {\n    case s: String           => persist(MsgSent(s))(updateState)\n    case Confirm(deliveryId) => persist(MsgConfirmed(deliveryId))(updateState)\n  }\n\n  override def receiveRecover: Receive = {\n    case evt: Evt => updateState(evt)\n  }\n\n  def updateState(evt: Evt): Unit = evt match {\n    case MsgSent(s) =>\n      deliver(destination)(deliveryId => Msg(deliveryId, s))\n\n    case MsgConfirmed(deliveryId) => confirmDelivery(deliveryId)\n  }\n}\n\nclass MyDestination extends Actor {\n  def receive = {\n    case Msg(deliveryId, s) =>\n      // ...\n      sender() ! Confirm(deliveryId)\n  }\n} Java copysource class Msg implements Serializable {\n  private static final long serialVersionUID = 1L;\n  public final long deliveryId;\n  public final String s;\n\n  public Msg(long deliveryId, String s) {\n    this.deliveryId = deliveryId;\n    this.s = s;\n  }\n}\n\nclass Confirm implements Serializable {\n  private static final long serialVersionUID = 1L;\n  public final long deliveryId;\n\n  public Confirm(long deliveryId) {\n    this.deliveryId = deliveryId;\n  }\n}\n\nclass MsgSent implements Serializable {\n  private static final long serialVersionUID = 1L;\n  public final String s;\n\n  public MsgSent(String s) {\n    this.s = s;\n  }\n}\n\nclass MsgConfirmed implements Serializable {\n  private static final long serialVersionUID = 1L;\n  public final long deliveryId;\n\n  public MsgConfirmed(long deliveryId) {\n    this.deliveryId = deliveryId;\n  }\n}\n\nclass MyPersistentActor extends AbstractPersistentActorWithAtLeastOnceDelivery {\n  private final ActorSelection destination;\n\n  public MyPersistentActor(ActorSelection destination) {\n    this.destination = destination;\n  }\n\n  @Override\n  public String persistenceId() {\n    return \"persistence-id\";\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            s -> {\n              persist(new MsgSent(s), evt -> updateState(evt));\n            })\n        .match(\n            Confirm.class,\n            confirm -> {\n              persist(new MsgConfirmed(confirm.deliveryId), evt -> updateState(evt));\n            })\n        .build();\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().match(Object.class, evt -> updateState(evt)).build();\n  }\n\n  void updateState(Object event) {\n    if (event instanceof MsgSent) {\n      final MsgSent evt = (MsgSent) event;\n      deliver(destination, deliveryId -> new Msg(deliveryId, evt.s));\n    } else if (event instanceof MsgConfirmed) {\n      final MsgConfirmed evt = (MsgConfirmed) event;\n      confirmDelivery(evt.deliveryId);\n    }\n  }\n}\n\nclass MyDestination extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Msg.class,\n            msg -> {\n              // ...\n              getSender().tell(new Confirm(msg.deliveryId), getSelf());\n            })\n        .build();\n  }\n}\nThe deliveryId generated by the persistence module is a strictly monotonically increasing sequence number without gaps. The same sequence is used for all destinations of the actor, i.e. when sending to multiple destinations the destinations will see gaps in the sequence. It is not possible to use custom deliveryId. However, you can send a custom correlation identifier in the message to the destination. You must then retain a mapping between the internal deliveryId (passed into the deliveryIdToMessage function) and your custom correlation id (passed into the message). You can do this by storing such mapping in a Map(correlationId -> deliveryId) from which you can retrieve the deliveryId to be passed into the confirmDelivery method once the receiver of your message has replied with your custom correlation id.\nThe AtLeastOnceDelivery traitAbstractPersistentActorWithAtLeastOnceDelivery class has a state consisting of unconfirmed messages and a sequence number. It does not store this state itself. You must persist events corresponding to the deliverdeliver and confirmDeliveryconfirmDelivery invocations from your PersistentActor so that the state can be restored by calling the same methods during the recovery phase of the PersistentActor. Sometimes these events can be derived from other business level events, and sometimes you must create separate events. During recovery, calls to deliver will not send out messages, those will be sent later if no matching confirmDelivery will have been performed.\nSupport for snapshots is provided by getDeliverySnapshotgetDeliverySnapshot and setDeliverySnapshotsetDeliverySnapshot. The AtLeastOnceDelivery.AtLeastOnceDeliverySnapshotAtLeastOnceDelivery.AtLeastOnceDeliverySnapshot contains the full delivery state, including unconfirmed messages. If you need a custom snapshot for other parts of the actor state you must also include the AtLeastOnceDeliverySnapshot. It is serialized using protobuf with the ordinary Pekko serialization mechanism. It is easiest to include the bytes of the AtLeastOnceDeliverySnapshot as a blob in your custom snapshot.\nThe interval between redelivery attempts is defined by the redeliverIntervalredeliverInterval method. The default value can be configured with the pekko.persistence.at-least-once-delivery.redeliver-interval configuration key. The method can be overridden by implementation classes to return non-default values.\nThe maximum number of messages that will be sent at each redelivery burst is defined by the redeliverBurstLimitredeliverBurstLimit method (burst frequency is half of the redelivery interval). If there’s a lot of unconfirmed messages (e.g. if the destination is not available for a long time), this helps to prevent an overwhelming amount of messages to be sent at once. The default value can be configured with the pekko.persistence.at-least-once-delivery.redelivery-burst-limit configuration key. The method can be overridden by implementation classes to return non-default values.\nAfter a number of delivery attempts a AtLeastOnceDelivery.UnconfirmedWarningAtLeastOnceDelivery.UnconfirmedWarning message will be sent to self. The re-sending will still continue, but you can choose to call confirmDelivery to cancel the re-sending. The number of delivery attempts before emitting the warning is defined by the warnAfterNumberOfUnconfirmedAttemptswarnAfterNumberOfUnconfirmedAttempts method. The default value can be configured with the pekko.persistence.at-least-once-delivery.warn-after-number-of-unconfirmed-attempts configuration key. The method can be overridden by implementation classes to return non-default values.\nThe AtLeastOnceDelivery traitAbstractPersistentActorWithAtLeastOnceDelivery class holds messages in memory until their successful delivery has been confirmed. The maximum number of unconfirmed messages that the actor is allowed to hold in memory is defined by the maxUnconfirmedMessagesmaxUnconfirmedMessages method. If this limit is exceed the deliver method will not accept more messages and it will throw AtLeastOnceDelivery.MaxUnconfirmedMessagesExceededExceptionAtLeastOnceDelivery.MaxUnconfirmedMessagesExceededException. The default value can be configured with the pekko.persistence.at-least-once-delivery.max-unconfirmed-messages configuration key. The method can be overridden by implementation classes to return non-default values.","title":"Relationship between deliver and confirmDelivery"},{"location":"/persistence.html#event-adapters","text":"In long running projects using Event Sourcing sometimes the need arises to detach the data model from the domain model completely.\nEvent Adapters help in situations where:\nVersion Migrations – existing events stored in Version 1 should be “upcasted” to a new Version 2 representation, and the process of doing so involves actual code, not just changes on the serialization layer. For these scenarios the toJournaltoJournal function is usually an identity function, however the fromJournalfromJournal is implemented as v1.Event=>v2.Event, performing the necessary mapping inside the fromJournal method. This technique is sometimes referred to as “upcasting” in other CQRS libraries. Separating Domain and Data models – thanks to EventAdapters it is possible to completely separate the domain model from the model used to persist data in the Journals. For example one may want to use case classes in the domain model, however persist their protocol-buffer (or any other binary serialization format) counter-parts to the Journal. A simple toJournal:MyModel=>MyDataModel and fromJournal:MyDataModel=>MyModel adapter can be used to implement this feature. Journal Specialized Data Types – exposing data types understood by the underlying Journal, for example for data stores which understand JSON it is possible to write an EventAdapter toJournal:Any=>JSON such that the Journal can directly store the json instead of serializing the object to its binary representation.\nImplementing an EventAdapter is rather straightforward:\nScala copysourceclass MyEventAdapter(system: ExtendedActorSystem) extends EventAdapter {\n  override def manifest(event: Any): String =\n    \"\" // when no manifest needed, return \"\"\n\n  override def toJournal(event: Any): Any =\n    event // identity\n\n  override def fromJournal(event: Any, manifest: String): EventSeq =\n    EventSeq.single(event) // identity\n} Java copysourceclass MyEventAdapter implements EventAdapter {\n  @Override\n  public String manifest(Object event) {\n    return \"\"; // if no manifest needed, return \"\"\n  }\n\n  @Override\n  public Object toJournal(Object event) {\n    return event; // identity\n  }\n\n  @Override\n  public EventSeq fromJournal(Object event, String manifest) {\n    return EventSeq.single(event); // identity\n  }\n}\nThen in order for it to be used on events coming to and from the journal you must bind it using the below configuration syntax:\ncopysourcepekko.persistence.journal {\n  inmem {\n    event-adapters {\n      tagging        = \"docs.persistence.MyTaggingEventAdapter\"\n      user-upcasting = \"docs.persistence.UserUpcastingEventAdapter\"\n      item-upcasting = \"docs.persistence.ItemUpcastingEventAdapter\"\n    }\n\n    event-adapter-bindings {\n      \"docs.persistence.Item\"        = tagging\n      \"docs.persistence.TaggedEvent\" = tagging\n      \"docs.persistence.v1.Event\"    = [user-upcasting, item-upcasting]\n    }\n  }\n}\nIt is possible to bind multiple adapters to one class for recovery, in which case the fromJournal methods of all bound adapters will be applied to a given matching event (in order of definition in the configuration). Since each adapter may return from 0 to n adapted events (called as EventSeq), each adapter can investigate the event and if it should indeed adapt it return the adapted event(s) for it. Other adapters which do not have anything to contribute during this adaptation simply return EventSeq.emptyEventSeq.empty. The adapted events are then delivered in-order to the PersistentActor during replay.\nNote For more advanced schema evolution techniques refer to the Persistence - Schema Evolution documentation.","title":"Event Adapters"},{"location":"/persistence.html#custom-serialization","text":"Serialization of snapshots and payloads of Persistent messages is configurable with Pekko’s Serialization infrastructure. For example, if an application wants to serialize\npayloads of type MyPayload with a custom MyPayloadSerializer and snapshots of type MySnapshot with a custom MySnapshotSerializer\nit must add\ncopysourcepekko.actor {\n  serializers {\n    my-payload = \"docs.persistence.MyPayloadSerializer\"\n    my-snapshot = \"docs.persistence.MySnapshotSerializer\"\n  }\n  serialization-bindings {\n    \"docs.persistence.MyPayload\" = my-payload\n    \"docs.persistence.MySnapshot\" = my-snapshot\n  }\n}\nto the application configuration. If not specified, an exception will be throw when trying to persist events or snapshots.\nFor more advanced schema evolution techniques refer to the Persistence - Schema Evolution documentation.","title":"Custom serialization"},{"location":"/persistence.html#testing-with-leveldb-journal","text":"The LevelDB journal is deprecated and will be removed from a future Pekko version, it is not advised to build new applications with it. For testing the built in “inmem” journal or the actual journal that will be used in production of the application is recommended. See Persistence Plugins for some journal implementation choices.\nWhen running tests with LevelDB default settings in sbt, make sure to set fork := true in your sbt project. Otherwise, you’ll see an UnsatisfiedLinkError. Alternatively, you can switch to a LevelDB Java port by setting\ncopysourcepekko.persistence.journal.leveldb.native = off\nor\ncopysourcepekko.persistence.journal.leveldb-shared.store.native = off\nin your Pekko configuration. Also note that for the LevelDB Java port, you will need the following dependencies:\nsbt libraryDependencies += \"org.iq80.leveldb\" % \"leveldb\" % \"0.9\" Maven <dependencies>\n  <dependency>\n    <groupId>org.iq80.leveldb</groupId>\n    <artifactId>leveldb</artifactId>\n    <version>0.9</version>\n  </dependency>\n</dependencies> Gradle dependencies {\n  implementation \"org.iq80.leveldb:leveldb:0.9\"\n}\nWarning It is not possible to test persistence provided classes (i.e. PersistentActor and AtLeastOnceDelivery) using TestActorRefTestActorRef due to its synchronous nature. These traits need to be able to perform asynchronous tasks in the background in order to handle internal persistence related events. When testing Persistence based projects always rely on asynchronous messaging using the TestKit.","title":"Testing with LevelDB journal"},{"location":"/persistence.html#configuration","text":"There are several configuration properties for the persistence module, please refer to the reference configuration.\nThe journal and snapshot store plugins have specific configuration, see reference documentation of the chosen plugin.","title":"Configuration"},{"location":"/persistence.html#multiple-persistence-plugin-configurations","text":"By default, a persistent actor will use the “default” journal and snapshot store plugins configured in the following sections of the reference.conf configuration resource:\ncopysource# Absolute path to the default journal plugin configuration entry.\npekko.persistence.journal.plugin = \"pekko.persistence.journal.inmem\"\n# Absolute path to the default snapshot store plugin configuration entry.\npekko.persistence.snapshot-store.plugin = \"pekko.persistence.snapshot-store.local\"\nNote that in this case the actor overrides only the persistenceIdpersistenceId method:\nScala copysourcetrait ActorWithDefaultPlugins extends PersistentActor {\n  override def persistenceId = \"123\"\n}\n Java copysourceabstract class AbstractPersistentActorWithDefaultPlugins extends AbstractPersistentActor {\n  @Override\n  public String persistenceId() {\n    return \"123\";\n  }\n}\nWhen the persistent actor overrides the journalPluginIdjournalPluginId and snapshotPluginIdsnapshotPluginId methods, the actor will be serviced by these specific persistence plugins instead of the defaults:\nScala copysourcetrait ActorWithOverridePlugins extends PersistentActor {\n  override def persistenceId = \"123\"\n\n  // Absolute path to the journal plugin configuration entry in the `reference.conf`.\n  override def journalPluginId = \"pekko.persistence.chronicle.journal\"\n\n  // Absolute path to the snapshot store plugin configuration entry in the `reference.conf`.\n  override def snapshotPluginId = \"pekko.persistence.chronicle.snapshot-store\"\n}\n Java copysourceabstract class AbstractPersistentActorWithOverridePlugins extends AbstractPersistentActor {\n  @Override\n  public String persistenceId() {\n    return \"123\";\n  }\n\n  // Absolute path to the journal plugin configuration entry in the `reference.conf`\n  @Override\n  public String journalPluginId() {\n    return \"pekko.persistence.chronicle.journal\";\n  }\n\n  // Absolute path to the snapshot store plugin configuration entry in the `reference.conf`\n  @Override\n  public String snapshotPluginId() {\n    return \"pekko.persistence.chronicle.snapshot-store\";\n  }\n}\nNote that journalPluginId and snapshotPluginId must refer to properly configured reference.conf plugin entries with a standard class property as well as settings which are specific for those plugins, i.e.:\ncopysource# Configuration entry for the custom journal plugin, see `journalPluginId`.\npekko.persistence.chronicle.journal {\n  # Standard persistence extension property: provider FQCN.\n  class = \"org.apache.pekko.persistence.chronicle.ChronicleSyncJournal\"\n  # Custom setting specific for the journal `ChronicleSyncJournal`.\n  folder = $${user.dir}/store/journal\n}\n# Configuration entry for the custom snapshot store plugin, see `snapshotPluginId`.\npekko.persistence.chronicle.snapshot-store {\n  # Standard persistence extension property: provider FQCN.\n  class = \"org.apache.pekko.persistence.chronicle.ChronicleSnapshotStore\"\n  # Custom setting specific for the snapshot store `ChronicleSnapshotStore`.\n  folder = $${user.dir}/store/snapshot\n}","title":"Multiple persistence plugin configurations"},{"location":"/persistence.html#give-persistence-plugin-configurations-at-runtime","text":"By default, a persistent actor will use the configuration loaded at ActorSystemActorSystem creation time to create journal and snapshot store plugins.\nWhen the persistent actor overrides the journalPluginConfigjournalPluginConfig and snapshotPluginConfigsnapshotPluginConfig methods, the actor will use the declared Config objects with a fallback on the default configuration. It allows a dynamic configuration of the journal and the snapshot store at runtime:\nScala copysourcetrait ActorWithRuntimePluginConfig extends PersistentActor with RuntimePluginConfig {\n  // Variable that is retrieved at runtime, from an external service for instance.\n  val runtimeDistinction = \"foo\"\n\n  override def persistenceId = \"123\"\n\n  // Absolute path to the journal plugin configuration entry, not defined in the `reference.conf`.\n  override def journalPluginId = s\"journal-plugin-$runtimeDistinction\"\n\n  // Absolute path to the snapshot store plugin configuration entry, not defined in the `reference.conf`.\n  override def snapshotPluginId = s\"snapshot-store-plugin-$runtimeDistinction\"\n\n  // Configuration which contains the journal plugin id defined above\n  override def journalPluginConfig =\n    ConfigFactory\n      .empty()\n      .withValue(\n        s\"journal-plugin-$runtimeDistinction\",\n        context.system.settings.config\n          .getValue(\"journal-plugin\") // or a very different configuration coming from an external service.\n      )\n\n  // Configuration which contains the snapshot store plugin id defined above\n  override def snapshotPluginConfig =\n    ConfigFactory\n      .empty()\n      .withValue(\n        s\"snapshot-plugin-$runtimeDistinction\",\n        context.system.settings.config\n          .getValue(\"snapshot-store-plugin\") // or a very different configuration coming from an external service.\n      )\n\n}\n Java copysourceabstract class AbstractPersistentActorWithRuntimePluginConfig extends AbstractPersistentActor\n    implements RuntimePluginConfig {\n  // Variable that is retrieved at runtime, from an external service for instance.\n  String runtimeDistinction = \"foo\";\n\n  @Override\n  public String persistenceId() {\n    return \"123\";\n  }\n\n  // Absolute path to the journal plugin configuration entry in the `reference.conf`\n  @Override\n  public String journalPluginId() {\n    return \"journal-plugin-\" + runtimeDistinction;\n  }\n\n  // Absolute path to the snapshot store plugin configuration entry in the `reference.conf`\n  @Override\n  public String snapshotPluginId() {\n    return \"snapshot-store-plugin-\" + runtimeDistinction;\n  }\n\n  // Configuration which contains the journal plugin id defined above\n  @Override\n  public Config journalPluginConfig() {\n    return ConfigFactory.empty()\n        .withValue(\n            \"journal-plugin-\" + runtimeDistinction,\n            getContext()\n                .getSystem()\n                .settings()\n                .config()\n                .getValue(\n                    \"journal-plugin\") // or a very different configuration coming from an external\n            // service.\n            );\n  }\n\n  // Configuration which contains the snapshot store plugin id defined above\n  @Override\n  public Config snapshotPluginConfig() {\n    return ConfigFactory.empty()\n        .withValue(\n            \"snapshot-plugin-\" + runtimeDistinction,\n            getContext()\n                .getSystem()\n                .settings()\n                .config()\n                .getValue(\n                    \"snapshot-store-plugin\") // or a very different configuration coming from an\n            // external service.\n            );\n  }\n}","title":"Give persistence plugin configurations at runtime"},{"location":"/persistence.html#see-also","text":"Persistent FSM Persistence plugins Building a new storage backend","title":"See also"},{"location":"/persistence-fsm.html","text":"","title":"Classic Persistent FSM"},{"location":"/persistence-fsm.html#classic-persistent-fsm","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.","title":"Classic Persistent FSM"},{"location":"/persistence-fsm.html#dependency","text":"Persistent FSMs are part of Pekko persistence, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-persistence\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-persistence_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-persistence_${versions.ScalaBinary}\"\n}\nWarning Persistent FSM is no longer actively developed and will be replaced by Pekko Persistence Typed. It is not advised to build new applications with Persistent FSM. Existing users of Persistent FSM should migrate.\nPersistentFSMAbstractPersistentFSM handles the incoming messages in an FSM like fashion. Its internal state is persisted as a sequence of changes, later referred to as domain events. Relationship between incoming messages, FSM’s states and transitions, persistence of domain events is defined by a DSL.","title":"Dependency"},{"location":"/persistence-fsm.html#a-simple-example","text":"To demonstrate the features of the PersistentFSM traitAbstractPersistentFSM, consider an actor which represents a Web store customer. The contract of our “WebStoreCustomerFSMActor” is that it accepts the following commands:\nScala copysourcesealed trait Command\ncase class AddItem(item: Item) extends Command\ncase object Buy extends Command\ncase object Leave extends Command\ncase object GetCurrentCart extends Command Java copysourcepublic static final class AddItem implements Command {\n  private final Item item;\n\n  public AddItem(Item item) {\n    this.item = item;\n  }\n\n  public Item getItem() {\n    return item;\n  }\n}\n\npublic enum Buy implements Command {\n  INSTANCE\n}\n\npublic enum Leave implements Command {\n  INSTANCE\n}\n\npublic enum GetCurrentCart implements Command {\n  INSTANCE\n}\nAddItem sent when the customer adds an item to a shopping cart Buy - when the customer finishes the purchase Leave - when the customer leaves the store without purchasing anything GetCurrentCart allows to query the current state of customer’s shopping cart\nThe customer can be in one of the following states:\nScala copysourcesealed trait UserState extends FSMState\ncase object LookingAround extends UserState {\n  override def identifier: String = \"Looking Around\"\n}\ncase object Shopping extends UserState {\n  override def identifier: String = \"Shopping\"\n}\ncase object Inactive extends UserState {\n  override def identifier: String = \"Inactive\"\n}\ncase object Paid extends UserState {\n  override def identifier: String = \"Paid\"\n} Java copysourceenum UserState implements PersistentFSM.FSMState {\n  LOOKING_AROUND(\"Looking Around\"),\n  SHOPPING(\"Shopping\"),\n  INACTIVE(\"Inactive\"),\n  PAID(\"Paid\");\n\n  private final String stateIdentifier;\n\n  UserState(String stateIdentifier) {\n    this.stateIdentifier = stateIdentifier;\n  }\n\n  @Override\n  public String identifier() {\n    return stateIdentifier;\n  }\n}\nLookingAround customer is browsing the site, but hasn’t added anything to the shopping cart Shopping customer has recently added items to the shopping cart Inactive customer has items in the shopping cart, but hasn’t added anything recently Paid customer has purchased the items\nNote PersistentFSMAbstractPersistentFSM states must inherit from traitimplement interface PersistentFSM.FSMState and implement the def identifier: StringString identifier() method. This is required in order to simplify the serialization of FSM states. String identifiers should be unique!\nCustomer’s actions are “recorded” as a sequence of “domain events” which are persisted. Those events are replayed on an actor’s start in order to restore the latest customer’s state:\nScala copysourcesealed trait DomainEvent\ncase class ItemAdded(item: Item) extends DomainEvent\ncase object OrderExecuted extends DomainEvent\ncase object OrderDiscarded extends DomainEvent\ncase object CustomerInactive extends DomainEvent Java copysourcepublic static final class ItemAdded implements DomainEvent {\n  private final Item item;\n\n  public ItemAdded(Item item) {\n    this.item = item;\n  }\n\n  public Item getItem() {\n    return item;\n  }\n}\n\npublic enum OrderExecuted implements DomainEvent {\n  INSTANCE\n}\n\npublic enum OrderDiscarded implements DomainEvent {\n  INSTANCE\n}\nCustomer state data represents the items in a customer’s shopping cart:\nScala copysourcecase class Item(id: String, name: String, price: Float)\n\nsealed trait ShoppingCart {\n  def addItem(item: Item): ShoppingCart\n  def empty(): ShoppingCart\n}\ncase object EmptyShoppingCart extends ShoppingCart {\n  def addItem(item: Item) = NonEmptyShoppingCart(item :: Nil)\n  def empty() = this\n}\ncase class NonEmptyShoppingCart(items: Seq[Item]) extends ShoppingCart {\n  def addItem(item: Item) = NonEmptyShoppingCart(items :+ item)\n  def empty() = EmptyShoppingCart\n} Java copysourcepublic static class ShoppingCart {\n  private final List<Item> items = new ArrayList<>();\n\n  public ShoppingCart(Item initialItem) {\n    items.add(initialItem);\n  }\n\n  public ShoppingCart() {}\n\n  public List<Item> getItems() {\n    return Collections.unmodifiableList(items);\n  }\n\n  public ShoppingCart addItem(Item item) {\n    items.add(item);\n    return this;\n  }\n\n  public void empty() {\n    items.clear();\n  }\n}\n\npublic static class Item implements Serializable {\n  private final String id;\n  private final String name;\n  private final float price;\n\n  Item(String id, String name, float price) {\n    this.id = id;\n    this.name = name;\n    this.price = price;\n  }\n\n  public String getId() {\n    return id;\n  }\n\n  public float getPrice() {\n    return price;\n  }\n\n  public String getName() {\n    return name;\n  }\n\n  @Override\n  public String toString() {\n    return String.format(\"Item{id=%s, name=%s, price=%s}\", id, price, name);\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (this == o) return true;\n    if (o == null || getClass() != o.getClass()) return false;\n\n    Item item = (Item) o;\n\n    return item.price == price && id.equals(item.id) && name.equals(item.name);\n  }\n}\nHere is how everything is wired together:\nScala copysourcestartWith(LookingAround, EmptyShoppingCart)\n\nwhen(LookingAround) {\n  case Event(AddItem(item), _) =>\n    goto(Shopping).applying(ItemAdded(item)).forMax(1 seconds)\n  case Event(GetCurrentCart, data) =>\n    stay().replying(data)\n}\n\nwhen(Shopping) {\n  case Event(AddItem(item), _) =>\n    stay().applying(ItemAdded(item)).forMax(1 seconds)\n  case Event(Buy, _) =>\n    goto(Paid).applying(OrderExecuted).andThen {\n      case NonEmptyShoppingCart(items) =>\n        reportActor ! PurchaseWasMade(items)\n        saveStateSnapshot()\n      case EmptyShoppingCart => saveStateSnapshot()\n    }\n  case Event(Leave, _) =>\n    stop().applying(OrderDiscarded).andThen {\n      case _ =>\n        reportActor ! ShoppingCardDiscarded\n        saveStateSnapshot()\n    }\n  case Event(GetCurrentCart, data) =>\n    stay().replying(data)\n  case Event(StateTimeout, _) =>\n    goto(Inactive).forMax(2 seconds)\n}\n\nwhen(Inactive) {\n  case Event(AddItem(item), _) =>\n    goto(Shopping).applying(ItemAdded(item)).forMax(1 seconds)\n  case Event(StateTimeout, _) =>\n    stop().applying(OrderDiscarded).andThen {\n      case _ => reportActor ! ShoppingCardDiscarded\n    }\n}\n\nwhen(Paid) {\n  case Event(Leave, _) => stop()\n  case Event(GetCurrentCart, data) =>\n    stay().replying(data)\n} Java copysourcestartWith(UserState.LOOKING_AROUND, new ShoppingCart());\n\nwhen(\n    UserState.LOOKING_AROUND,\n    matchEvent(\n            AddItem.class,\n            (event, data) ->\n                goTo(UserState.SHOPPING)\n                    .applying(new ItemAdded(event.getItem()))\n                    .forMax(Duration.ofSeconds(1)))\n        .event(GetCurrentCart.class, (event, data) -> stay().replying(data)));\n\nwhen(\n    UserState.SHOPPING,\n    matchEvent(\n            AddItem.class,\n            (event, data) ->\n                stay().applying(new ItemAdded(event.getItem())).forMax(Duration.ofSeconds(1)))\n        .event(\n            Buy.class,\n            (event, data) ->\n                goTo(UserState.PAID)\n                    .applying(OrderExecuted.INSTANCE)\n                    .andThen(\n                        exec(\n                            cart -> {\n                              reportActor.tell(new PurchaseWasMade(cart.getItems()), self());\n                              saveStateSnapshot();\n                            })))\n        .event(\n            Leave.class,\n            (event, data) ->\n                stop()\n                    .applying(OrderDiscarded.INSTANCE)\n                    .andThen(\n                        exec(\n                            cart -> {\n                              reportActor.tell(ShoppingCardDiscarded.INSTANCE, self());\n                              saveStateSnapshot();\n                            })))\n        .event(GetCurrentCart.class, (event, data) -> stay().replying(data))\n        .event(\n            StateTimeout$.class,\n            (event, data) -> goTo(UserState.INACTIVE).forMax(Duration.ofSeconds(2))));\n\nwhen(\n    UserState.INACTIVE,\n    matchEvent(\n            AddItem.class,\n            (event, data) ->\n                goTo(UserState.SHOPPING)\n                    .applying(new ItemAdded(event.getItem()))\n                    .forMax(Duration.ofSeconds(1)))\n        .event(GetCurrentCart.class, (event, data) -> stay().replying(data))\n        .event(\n            StateTimeout$.class,\n            (event, data) ->\n                stop()\n                    .applying(OrderDiscarded.INSTANCE)\n                    .andThen(\n                        exec(\n                            cart ->\n                                reportActor.tell(ShoppingCardDiscarded.INSTANCE, self())))));\n\nwhen(\n    UserState.PAID,\n    matchEvent(Leave.class, (event, data) -> stop())\n        .event(GetCurrentCart.class, (event, data) -> stay().replying(data)));\nNote State data can only be modified directly on initialization. Later it’s modified only as a result of applying domain events. Override the applyEvent method to define how state data is affected by domain events, see the example below\nScala copysourceoverride def applyEvent(event: DomainEvent, cartBeforeEvent: ShoppingCart): ShoppingCart = {\n  event match {\n    case ItemAdded(item)  => cartBeforeEvent.addItem(item)\n    case OrderExecuted    => cartBeforeEvent\n    case OrderDiscarded   => cartBeforeEvent.empty()\n    case CustomerInactive => cartBeforeEvent\n  }\n} Java copysource@Override\npublic ShoppingCart applyEvent(DomainEvent event, ShoppingCart currentData) {\n  if (event instanceof ItemAdded) {\n    currentData.addItem(((ItemAdded) event).getItem());\n    return currentData;\n  } else if (event instanceof OrderExecuted) {\n    return currentData;\n  } else if (event instanceof OrderDiscarded) {\n    currentData.empty();\n    return currentData;\n  }\n  throw new RuntimeException(\"Unhandled\");\n}\nandThen can be used to define actions which will be executed following event’s persistence - convenient for “side effects” like sending a message or logging. Notice that actions defined in andThen block are not executed on recovery:\nScala copysourcegoto(Paid).applying(OrderExecuted).andThen {\n  case NonEmptyShoppingCart(items) =>\n    reportActor ! PurchaseWasMade(items)\n} Java copysource(event, data) ->\n    goTo(UserState.PAID)\n        .applying(OrderExecuted.INSTANCE)\n        .andThen(\n            exec(\n                cart -> {\n                  reportActor.tell(new PurchaseWasMade(cart.getItems()), self());\n                })))\nA snapshot of state data can be persisted by calling the saveStateSnapshot() method:\nScala copysourcestop().applying(OrderDiscarded).andThen {\n  case _ =>\n    reportActor ! ShoppingCardDiscarded\n    saveStateSnapshot()\n} Java copysource(event, data) ->\n    stop()\n        .applying(OrderDiscarded.INSTANCE)\n        .andThen(\n            exec(\n                cart -> {\n                  reportActor.tell(ShoppingCardDiscarded.INSTANCE, self());\n                  saveStateSnapshot();\n                })))\nOn recovery state data is initialized according to the latest available snapshot, then the remaining domain events are replayed, triggering the applyEvent method.","title":"A Simple Example"},{"location":"/persistence-fsm.html#migration-to-eventsourcedbehavior","text":"Persistent FSMs can be represented using Persistence Typed. The data stored by Persistence FSM can be read by an EventSourcedBehaviorEventSourcedBehavior using a snapshot adapter and an event adapter. The adapters are required as Persistent FSM doesn’t store snapshots and user data directly, it wraps them in internal types that include state transition information.\nBefore reading the migration guide it is advised to understand Persistence Typed.","title":"Migration to EventSourcedBehavior"},{"location":"/persistence-fsm.html#migration-steps","text":"Modify or create new commands to include replyTo ActorRefActorRef Typically persisted events will remain the same Create an EventSourcedBehavior that mimics the old PersistentFSM Replace any state timeouts with Behaviors.withTimers either hard coded or stored in the state Add an EventAdapterEventAdapter to convert state transition events added by PersistentFSM into private events or filter them If snapshots are used add a SnapshotAdapterSnapshotAdapter to convert PersistentFSM snapshots into the EventSourcedBehaviors State\nThe following is the shopping cart example above converted to an EventSourcedBehavior.\nThe new commands, note the replyTo field for getting the current cart.\nScala copysourcesealed trait Command\ncase class AddItem(item: Item) extends Command\ncase object Buy extends Command\ncase object Leave extends Command\ncase class GetCurrentCart(replyTo: ActorRef[ShoppingCart]) extends Command\nprivate case object Timeout extends Command Java copysourceinterface Command {}\n\npublic static class AddItem implements Command {\n  public final Item item;\n\n  public AddItem(Item item) {\n    this.item = item;\n  }\n}\n\npublic static class GetCurrentCart implements Command {\n  public final ActorRef<ShoppingCart> replyTo;\n\n  public GetCurrentCart(ActorRef<ShoppingCart> replyTo) {\n    this.replyTo = replyTo;\n  }\n}\n\npublic enum Buy implements Command {\n  INSTANCE\n}\n\npublic enum Leave implements Command {\n  INSTANCE\n}\n\nprivate enum Timeout implements Command {\n  INSTANCE\n}\nThe states of the FSM are represented using the EventSourcedBehavior’s state parameter along with the event and command handlers. Here are the states:\nScala copysourcesealed trait State\ncase class LookingAround(cart: ShoppingCart) extends State\ncase class Shopping(cart: ShoppingCart) extends State\ncase class Inactive(cart: ShoppingCart) extends State\ncase class Paid(cart: ShoppingCart) extends State Java copysourceabstract static class State {\n  public final ShoppingCart cart;\n\n  protected State(ShoppingCart cart) {\n    this.cart = cart;\n  }\n}\n\npublic static class LookingAround extends State {\n  public LookingAround(ShoppingCart cart) {\n    super(cart);\n  }\n}\n\npublic static class Shopping extends State {\n  public Shopping(ShoppingCart cart) {\n    super(cart);\n  }\n}\n\npublic static class Inactive extends State {\n  public Inactive(ShoppingCart cart) {\n    super(cart);\n  }\n}\n\npublic static class Paid extends State {\n  public Paid(ShoppingCart cart) {\n    super(cart);\n  }\n}\nThe command handler has a separate section for each of the PersistentFSM’s states:\nScala copysourcedef commandHandler(timers: TimerScheduler[Command])(state: State, command: Command): Effect[DomainEvent, State] =\n  state match {\n    case LookingAround(cart) =>\n      command match {\n        case AddItem(item) =>\n          Effect.persist(ItemAdded(item)).thenRun(_ => timers.startSingleTimer(StateTimeout, Timeout, 1.second))\n        case GetCurrentCart(replyTo) =>\n          replyTo ! cart\n          Effect.none\n        case _ =>\n          Effect.none\n      }\n    case Shopping(cart) =>\n      command match {\n        case AddItem(item) =>\n          Effect.persist(ItemAdded(item)).thenRun(_ => timers.startSingleTimer(StateTimeout, Timeout, 1.second))\n        case Buy =>\n          Effect.persist(OrderExecuted).thenRun(_ => timers.cancel(StateTimeout))\n        case Leave =>\n          Effect.persist(OrderDiscarded).thenStop()\n        case GetCurrentCart(replyTo) =>\n          replyTo ! cart\n          Effect.none\n        case Timeout =>\n          Effect.persist(CustomerInactive)\n      }\n    case Inactive(_) =>\n      command match {\n        case AddItem(item) =>\n          Effect.persist(ItemAdded(item)).thenRun(_ => timers.startSingleTimer(StateTimeout, Timeout, 1.second))\n        case Timeout =>\n          Effect.persist(OrderDiscarded)\n        case _ =>\n          Effect.none\n      }\n    case Paid(cart) =>\n      command match {\n        case Leave =>\n          Effect.stop()\n        case GetCurrentCart(replyTo) =>\n          replyTo ! cart\n          Effect.none\n        case _ =>\n          Effect.none\n      }\n  } Java copysource  CommandHandlerBuilder<Command, DomainEvent, State> builder = newCommandHandlerBuilder();\n\n  builder.forStateType(LookingAround.class).onCommand(AddItem.class, this::addItem);\n\n  builder\n      .forStateType(Shopping.class)\n      .onCommand(AddItem.class, this::addItem)\n      .onCommand(Buy.class, this::buy)\n      .onCommand(Leave.class, this::discardShoppingCart)\n      .onCommand(Timeout.class, this::timeoutShopping);\n\n  builder\n      .forStateType(Inactive.class)\n      .onCommand(AddItem.class, this::addItem)\n      .onCommand(Timeout.class, () -> Effect().persist(OrderDiscarded.INSTANCE).thenStop());\n\n  builder.forStateType(Paid.class).onCommand(Leave.class, () -> Effect().stop());\n\n  builder.forAnyState().onCommand(GetCurrentCart.class, this::getCurrentCart);\n  return builder.build();\n}\nNote that there is no explicit support for state timeout as with PersistentFSM but the same behavior can be achieved using Behaviors.withTimers. If the timer is the same for all events then it can be hard coded, otherwise the old PersistentFSM timeout can be taken from the StateChangeEvent in the event adapter and is also available when constructing a SnapshotAdapter. This can be added to an internal event and then stored in the State. Care must also be taken to restart timers on recovery in the signal handler:\nScala copysource.receiveSignal {\n  case (state, RecoveryCompleted) =>\n    state match {\n      case _: Shopping | _: Inactive =>\n        timers.startSingleTimer(StateTimeout, Timeout, 1.second)\n      case _ =>\n    }\n} Java copysource@Override\npublic SignalHandler<State> signalHandler() {\n  return newSignalHandlerBuilder()\n      .onSignal(\n          RecoveryCompleted.class,\n          (state, signal) -> {\n            if (state instanceof Shopping || state instanceof Inactive) {\n              timers.startSingleTimer(TIMEOUT_KEY, Timeout.INSTANCE, Duration.ofSeconds(1));\n            }\n          })\n      .build();\n}\nThen the event handler:\nScala copysourcedef eventHandler(state: State, event: DomainEvent): State = {\n  state match {\n    case la @ LookingAround(cart) =>\n      event match {\n        case ItemAdded(item) => Shopping(cart.addItem(item))\n        case _               => la\n      }\n    case Shopping(cart) =>\n      event match {\n        case ItemAdded(item)  => Shopping(cart.addItem(item))\n        case OrderExecuted    => Paid(cart)\n        case OrderDiscarded   => state // will be stopped\n        case CustomerInactive => Inactive(cart)\n      }\n    case i @ Inactive(cart) =>\n      event match {\n        case ItemAdded(item) => Shopping(cart.addItem(item))\n        case OrderDiscarded  => i // will be stopped\n        case _               => i\n      }\n    case Paid(_) => state // no events after paid\n  }\n} Java copysource@Override\npublic EventHandler<State, DomainEvent> eventHandler() {\n  EventHandlerBuilder<State, DomainEvent> eventHandlerBuilder = newEventHandlerBuilder();\n\n  eventHandlerBuilder\n      .forStateType(LookingAround.class)\n      .onEvent(ItemAdded.class, item -> new Shopping(new ShoppingCart(item.getItem())));\n\n  eventHandlerBuilder\n      .forStateType(Shopping.class)\n      .onEvent(\n          ItemAdded.class, (state, item) -> new Shopping(state.cart.addItem(item.getItem())))\n      .onEvent(OrderExecuted.class, (state, item) -> new Paid(state.cart))\n      .onEvent(OrderDiscarded.class, (state, item) -> state) // will be stopped\n      .onEvent(CustomerInactive.class, (state, event) -> new Inactive(state.cart));\n\n  eventHandlerBuilder\n      .forStateType(Inactive.class)\n      .onEvent(\n          ItemAdded.class, (state, item) -> new Shopping(state.cart.addItem(item.getItem())))\n      .onEvent(OrderDiscarded.class, (state, item) -> state); // will be stopped\n\n  return eventHandlerBuilder.build();\n}\nThe last step is the adapters that will allow the new EventSourcedBehaviorEventSourcedBehavior to read the old data:\nScala copysourceclass PersistentFsmEventAdapter extends EventAdapter[DomainEvent, Any] {\n  override def toJournal(e: DomainEvent): Any = e\n  override def manifest(event: DomainEvent): String = \"\"\n  override def fromJournal(journalEvent: Any, manifest: String): EventSeq[DomainEvent] = {\n    journalEvent match {\n      case _: StateChangeEvent =>\n        // In this example the state transitions can be inferred from the events\n        // Alternatively the StateChangeEvent can be converted to a private event if either the StateChangeEvent.stateIdentifier\n        // or StateChangeEvent.timeout is required\n        // Many use cases have the same timeout so it can be hard coded, otherwise it cane be stored in the state\n        EventSeq.empty\n      case other =>\n        // If using a new domain event model the conversion would happen here\n        EventSeq.single(other.asInstanceOf[DomainEvent])\n    }\n\n  }\n} Java copysourcepublic static class PersistentFSMEventAdapter extends EventAdapter<DomainEvent, Object> {\n\n  @Override\n  public Object toJournal(DomainEvent domainEvent) {\n    // leave events as is, can't roll back to PersistentFSM\n    return domainEvent;\n  }\n\n  @Override\n  public String manifest(DomainEvent event) {\n    return \"\";\n  }\n\n  @Override\n  public EventSeq<DomainEvent> fromJournal(Object event, String manifest) {\n    if (event instanceof org.apache.pekko.persistence.fsm.PersistentFSM.StateChangeEvent) {\n      // In this example the state transitions can be inferred from the events\n      // Alternatively the StateChangeEvent can be converted to a private event if either the\n      // StateChangeEvent.stateIdentifier\n      // or StateChangeEvent.timeout is required\n      // Many use cases have the same timeout so it can be hard coded, otherwise it cane be stored\n      // in the state\n      return EventSeq.empty();\n    } else {\n      // If using a new domain event model the conversion would happen here\n      return EventSeq.single((DomainEvent) event);\n    }\n  }\nThe snapshot adapter needs to adapt an internal type of PersistentFSM so a helper function is provided to build the SnapshotAdapterSnapshotAdapter:\nScala copysourceval persistentFSMSnapshotAdapter: SnapshotAdapter[State] = PersistentFSMMigration.snapshotAdapter[State] {\n  case (stateIdentifier, data, _) =>\n    val cart = data.asInstanceOf[ShoppingCart]\n    stateIdentifier match {\n      case \"Looking Around\" => LookingAround(cart)\n      case \"Shopping\"       => Shopping(cart)\n      case \"Inactive\"       => Inactive(cart)\n      case \"Paid\"           => Paid(cart)\n      case id               => throw new IllegalStateException(s\"Unexpected state identifier $id\")\n    }\n} Java copysource@Override\npublic SnapshotAdapter<State> snapshotAdapter() {\n  return PersistentFSMMigration.snapshotAdapter(\n      (stateIdentifier, snapshot, timeout) -> {\n        ShoppingCart cart = (ShoppingCart) snapshot;\n        switch (stateIdentifier) {\n          case \"Looking Around\":\n            return new LookingAround(cart);\n          case \"Shopping\":\n            return new Shopping(cart);\n          case \"Inactive\":\n            return new Inactive(cart);\n          case \"Paid\":\n            return new Paid(cart);\n          default:\n            throw new IllegalStateException(\"Unexpected state identifier \" + stateIdentifier);\n        }\n      });\n}\nThat concludes all the steps to allow an EventSourcedBehaviorEventSourcedBehavior to read a PersistentFSM’s data. Once the new code has been running you can not roll back as the PersistentFSM will not be able to read data written by Persistence Typed.\nNote There is one case where a full shutdown and startup is required.","title":"Migration steps"},{"location":"/testing.html","text":"","title":"Testing Classic Actors"},{"location":"/testing.html#testing-classic-actors","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the new API see testing.","title":"Testing Classic Actors"},{"location":"/testing.html#module-info","text":"To use Pekko Testkit, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-testkit\" % PekkoVersion % Test Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  testImplementation \"org.apache.pekko:pekko-testkit_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Actor Testkit (classic) Artifact org.apache.pekko pekko-testkit 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.actor.testkit License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/testing.html#introduction","text":"As with any piece of software, automated tests are a very important part of the development cycle. The actor model presents a different view on how units of code are delimited and how they interact, which influences how to perform tests.\nPekko comes with a dedicated module pekko-testkit for supporting tests.","title":"Introduction"},{"location":"/testing.html#asynchronous-testing-testkit","text":"Testkit allows you to test your actors in a controlled but realistic environment. The definition of the environment depends very much on the problem at hand and the level at which you intend to test, ranging from simple checks to full system tests.\nThe minimal setup consists of the test procedure, which provides the desired stimuli, the actor under test, and an actor receiving replies. Bigger systems replace the actor under test with a network of actors, apply stimuli at varying injection points and arrange results to be sent from different emission points, but the basic principle stays the same in that a single procedure drives the test.\nThe TestKit class contains a collection of tools which makes this common task easy.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.ActorSystem\nimport pekko.testkit.{ ImplicitSender, TestActors, TestKit }\nimport org.scalatest.BeforeAndAfterAll\nimport org.scalatest.matchers.should.Matchers\nimport org.scalatest.wordspec.AnyWordSpecLike\n\nclass MySpec()\n    extends TestKit(ActorSystem(\"MySpec\"))\n    with ImplicitSender\n    with AnyWordSpecLike\n    with Matchers\n    with BeforeAndAfterAll {\n\n  override def afterAll(): Unit = {\n    TestKit.shutdownActorSystem(system)\n  }\n\n  \"An Echo actor\" must {\n\n    \"send back messages unchanged\" in {\n      val echo = system.actorOf(TestActors.echoActorProps)\n      echo ! \"hello world\"\n      expectMsg(\"hello world\")\n    }\n\n  }\n} Java copysourceimport jdocs.AbstractJavaTest;\nimport org.apache.pekko.testkit.javadsl.TestKit;\nimport org.junit.AfterClass;\nimport org.junit.Assert;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.actor.Props;\nimport org.apache.pekko.actor.AbstractActor;\n\nimport java.time.Duration;\n\npublic class TestKitSampleTest extends AbstractJavaTest {\n\n  public static class SomeActor extends AbstractActor {\n    ActorRef target = null;\n\n    @Override\n    public Receive createReceive() {\n      return receiveBuilder()\n          .matchEquals(\n              \"hello\",\n              message -> {\n                getSender().tell(\"world\", getSelf());\n                if (target != null) target.forward(message, getContext());\n              })\n          .match(\n              ActorRef.class,\n              actorRef -> {\n                target = actorRef;\n                getSender().tell(\"done\", getSelf());\n              })\n          .build();\n    }\n  }\n\n  static ActorSystem system;\n\n  @BeforeClass\n  public static void setup() {\n    system = ActorSystem.create();\n  }\n\n  @AfterClass\n  public static void teardown() {\n    TestKit.shutdownActorSystem(system);\n    system = null;\n  }\n\n  @Test\n  public void testIt() {\n    /*\n     * Wrap the whole test procedure within a testkit constructor\n     * if you want to receive actor replies or use Within(), etc.\n     */\n    new TestKit(system) {\n      {\n        final Props props = Props.create(SomeActor.class);\n        final ActorRef subject = system.actorOf(props);\n\n        // can also use JavaTestKit “from the outside”\n        final TestKit probe = new TestKit(system);\n        // “inject” the probe by passing it to the test subject\n        // like a real resource would be passed in production\n        subject.tell(probe.getRef(), getRef());\n        // await the correct response\n        expectMsg(Duration.ofSeconds(1), \"done\");\n\n        // the run() method needs to finish within 3 seconds\n        within(\n            Duration.ofSeconds(3),\n            () -> {\n              subject.tell(\"hello\", getRef());\n\n              // This is a demo: would normally use expectMsgEquals().\n              // Wait time is bounded by 3-second deadline above.\n              awaitCond(probe::msgAvailable);\n\n              // response must have been enqueued to us before probe\n              expectMsg(Duration.ZERO, \"world\");\n              // check that the probe we injected earlier got the msg\n              probe.expectMsg(Duration.ZERO, \"hello\");\n              Assert.assertEquals(getRef(), probe.getLastSender());\n\n              // Will wait for the rest of the 3 seconds\n              expectNoMessage();\n              return null;\n            });\n      }\n    };\n  }\n}\nThe TestKit contains an actor named testActor which is the entry point for messages to be examined with the various expectMsg... assertions detailed below. When mixing in the trait ImplicitSender this test actor is implicitly used as sender reference when dispatching messages from the test procedure. The test actor’s reference is obtained using the getRef() method as demonstrated above. The testActor may also be passed to other actors, as usual, usually subscribing it as notification listener. There is a whole set of examination methods, e.g. receiving all consecutive messages matching certain criteria, receiving a whole sequence of fixed messages or classes, receiving nothing for some time, etc.\nThe ActorSystem passed to the constructor of TestKit is accessible via the system membergetSystem() method.\nNote Remember to shut down the actor system after the test is finished (also in case of failure) so that all actors—including the test actor—are stopped.","title":"Asynchronous Testing: TestKit"},{"location":"/testing.html#built-in-assertions","text":"The above-mentioned expectMsgexpectMsgEquals is not the only method for formulating assertions concerning received messages, the full set is this:\nScala copysourceval hello: String = expectMsg(\"hello\")\nval any: String = expectMsgAnyOf(\"hello\", \"world\")\nval all: immutable.Seq[String] = expectMsgAllOf(\"hello\", \"world\")\nval i: Int = expectMsgType[Int]\nexpectNoMessage(200.millis)\nval two: immutable.Seq[AnyRef] = receiveN(2) Java copysourcefinal String hello = expectMsgEquals(\"hello\");\nfinal String any = expectMsgAnyOf(\"hello\", \"world\");\nfinal List<String> all = expectMsgAllOf(\"hello\", \"world\");\nfinal int i = expectMsgClass(Integer.class);\nfinal Number j = expectMsgAnyClassOf(Integer.class, Long.class);\nexpectNoMessage();\nfinal List<Object> two = receiveN(2);\nIn these examples, the maximum durations you will find mentioned below are left out, in which case they use the default value from the configuration item pekko.test.single-expect-default which itself defaults to 3 seconds (or they obey the innermost enclosing Within as detailed below). The full signatures are:\nexpectMsg[T](d: Duration, msg: T): Tpublic <T> T expectMsgEquals(Duration max, T msg) The given message object must be received within the specified time; the object will be returned. expectMsgPF[T](d: Duration)(pf: PartialFunction[Any, T]): Tpublic <T> T expectMsgPF(Duration max, String hint, Function<Object, T> f) Within the given time, a message must be received and the given partial function must be defined for that message; the result from applying the partial function to the received message is returned. The duration may be left unspecified (empty parentheses are required in this case) to use the deadline from the innermost enclosing within block instead. expectMsgClass[T](d: Duration, c: Class[T]): Tpublic <T> T expectMsgClass(Duration max, Class<T> c) An object which is an instance of the given Class must be received within the allotted time frame; the object will be returned. Note that this does a conformance check; if you need the class to be equal, have a look at expectMsgAllClassOf with a single given class argumentyou need to verify that afterwards.\nexpectMsgType[T: Manifest](d: Duration) An object which is an instance of the given type (after erasure) must be received within the allotted time frame; the object will be returned. This method is approximately equivalent to expectMsgClass(implicitly[ClassTag[T]].runtimeClass).\nexpectMsgAnyOf[T](d: Duration, obj: T*): Tpublic Object expectMsgAnyOf(Duration max, Object... msg) An object must be received within the given time, and it must be equal ( compared with ==equals()) to at least one of the passed reference objects; the received object will be returned. expectMsgAnyClassOf[T](d: Duration, obj: Class[_ <: T]*): Tpublic <T> T expectMsgAnyClassOf(Duration max, Class<? extends T>... c) An object must be received within the given time, and it must be an instance of at least one of the supplied Class objects; the received object will be returned. Note that this does a conformance check, if you need the class to be equal you need to verify that afterwards. expectMsgAllOf[T](d: Duration, obj: T*): Seq[T]public List<Object> expectMsgAllOf(Duration max, Object... msg) Several objects matching the size of the supplied object array must be received within the given time, and for each of the given objects there must exist at least one among the received ones which equals (compared with ==equals()) it. The full sequence of received objects is returned in the order received.\nexpectMsgAllClassOf[T](d: Duration, c: Class[_ <: T]*): Seq[T] Several objects matching the size of the supplied Class array must be received within the given time, and for each of the given classes there must exist at least one among the received objects whose class equals (compared with ==) it (this is not a conformance check). The full sequence of received objects is returned. expectMsgAllConformingOf[T](d: Duration, c: Class[_ <: T]*): Seq[T] Several objects matching the size of the supplied Class array must be received within the given time, and for each of the given classes there must exist at least one among the received objects which is an instance of this class. The full sequence of received objects is returned.\nexpectNoMessage(d: Duration)public void expectNoMessage(Duration max) No message must be received within the given time. This also fails if a message has been received before calling this method which has not been removed from the queue using one of the other methods. receiveN(n: Int, d: Duration): Seq[AnyRef]List<Object> receiveN(int n, Duration max) n messages must be received within the given time; the received messages are returned.\nfishForMessage(max: Duration, hint: String)(pf: PartialFunction[Any, Boolean]): Any Keep receiving messages as long as the time is not used up and the partial function matches and returns false. Returns the message received for which it returned true or throws an exception, which will include the provided hint for easier debugging.\nIn addition to message reception assertions there are also methods which help with message flows:\nreceiveOne(d: Duration): AnyRef Tries to receive one message for at most the given time interval and returns null in case of failure. If the given Duration is zero, the call is non-blocking (polling mode).\nreceiveWhile[T](max: Duration, idle: Duration, messages: Int)(pf: PartialFunction[Any, T]): Seq[T]public <T> List<T> receiveWhile(Duration max, Duration idle, Int messages, Function<Object, T> f) Collect messages as long as they are matching the given partial function the given time interval is not used up the next message is received within the idle timeout the number of messages has not yet reached the maximum All collected messages are returned. The maximum duration defaults to the time remaining in the innermost enclosing within block and the idle duration defaults to infinity (thereby disabling the idle-timeout feature). The number of expected messages defaults to Int.MaxValue, which effectively disables this limit. awaitCond(p: => Boolean, max: Duration, interval: Duration)public void awaitCond(Duration max, Duration interval, Supplier<Boolean> p) Poll the given condition every interval until it returns true or the max duration is used up. The interval defaults to 100 ms and the maximum defaults to the time remaining in the innermost enclosing within block. awaitAssert(a: => Any, max: Duration, interval: Duration)public void awaitAssert(Duration max, Duration interval, Supplier<Object> a) Poll the given assert function every interval until it does not throw an exception or the max duration is used up. If the timeout expires the last exception is thrown. The interval defaults to 100 ms and the maximum defaults to the time remaining in the innermost enclosing within block. The interval defaults to 100 ms and the maximum defaults to the time remaining in the innermost enclosing within block. Return an arbitrary value that would be returned from awaitAssert if successful, if not interested in such value you can return null. ignoreMsg(pf: PartialFunction[AnyRef, Boolean])public void ignoreMsg(Function<Object, Boolean> f) ignoreMsgpublic void ignoreMsg() There are also cases where not all messages sent to the test kit are actually relevant to the test, but removing them would mean altering the actors under test. For this purpose it is possible to ignore certain messages. The internal testActor contains a partial function for ignoring messages: it will only enqueue messages which do not match the function or for which the function returns false. This function can be set and reset using the methods given above; each invocation replaces the previous function, they are not composed. This feature is useful e.g. when testing a logging system, where you want to ignore regular messages and are only interested in your specific ones.","title":"Built-In Assertions"},{"location":"/testing.html#expecting-log-messages","text":"Since an integration test does not allow observing the internal processing of the participating actors, verifying expected exceptions cannot be done directly. Instead, use the logging system for this purpose: replacing the normal event handler with the TestEventListener and using an EventFilter allows assertions on log messages, including those which are generated by exceptions:\nScala copysourceimport org.apache.pekko.testkit.EventFilter\nimport com.typesafe.config.ConfigFactory\n\nimplicit val system: ActorSystem = ActorSystem(\n  \"testsystem\",\n  ConfigFactory.parseString(\"\"\"\n  pekko.loggers = [\"org.apache.pekko.testkit.TestEventListener\"]\n  \"\"\"))\ntry {\n  val actor = system.actorOf(Props.empty)\n  EventFilter[ActorKilledException](occurrences = 1).intercept {\n    actor ! Kill\n  }\n} finally {\n  shutdown(system)\n} Java copysourcenew TestKit(system) {\n  {\n    assertEquals(\"TestKitDocTest\", system.name());\n    final ActorRef victim = system.actorOf(Props.empty(), \"victim\");\n\n    final int result =\n        new EventFilter(ActorKilledException.class, system)\n            .from(\"akka://TestKitDocTest/user/victim\")\n            .occurrences(1)\n            .intercept(\n                () -> {\n                  victim.tell(Kill.getInstance(), ActorRef.noSender());\n                  return 42;\n                });\n    assertEquals(42, result);\n  }\n};\nIf the number of occurrences is specific—as demonstrated above—then intercept will block until that number of matching messages have been received or the timeout configured in pekko.test.filter-leeway is used up (time starts counting after the passed-in block of code returns). In case of a timeout the test fails.\nNote Be sure to exchange the default logger with the TestEventListener in your application.conf to enable this function: pekko.loggers = [org.apache.pekko.testkit.TestEventListener]","title":"Expecting Log Messages"},{"location":"/testing.html#overriding-behavior","text":"Sometimes, you want to ‘hook into’ your actor to be able to test some internals. Usually, it is better to test an actors’ external interface, but for example if you want to test timing-sensitive behavior this can come in handy. Say for instance you want to test an actor that schedules a task:\nScala copysourcecase class TriggerScheduling(foo: String)\n\nobject SchedKey\ncase class ScheduledMessage(foo: String)\n\nclass TestTimerActor extends Actor with Timers {\n  override def receive = {\n    case TriggerScheduling(foo) => triggerScheduling(ScheduledMessage(foo))\n  }\n\n  def triggerScheduling(msg: ScheduledMessage) =\n    timers.startSingleTimer(SchedKey, msg, 500.millis)\n} Java copysourcestatic class TestTimerActor extends AbstractActorWithTimers {\n  private static Object SCHED_KEY = \"SchedKey\";\n\n  static final class TriggerScheduling {}\n\n  static final class ScheduledMessage {}\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(TriggerScheduling.class, msg -> triggerScheduling()).build();\n  }\n\n  void triggerScheduling() {\n    getTimers().startSingleTimer(SCHED_KEY, new ScheduledMessage(), Duration.ofMillis(500));\n  }\n}\nYou can override the method that does the scheduling in your test:\nScala copysourceimport org.apache.pekko\nimport pekko.testkit.TestProbe\nimport pekko.actor.Props\n\nval probe = TestProbe()\nval actor = system.actorOf(Props(new TestTimerActor() {\n  override def triggerScheduling(msg: ScheduledMessage) =\n    probe.ref ! msg\n}))\n\nactor ! TriggerScheduling(\"abc\")\nprobe.expectMsg(ScheduledMessage(\"abc\")) Java copysourcenew TestKit(system) {\n  {\n    final TestKit probe = new TestKit(system);\n    final ActorRef target =\n        system.actorOf(\n            Props.create(\n                TestTimerActor.class,\n                () ->\n                    new TestTimerActor() {\n                      @Override\n                      void triggerScheduling() {\n                        probe.getRef().tell(new ScheduledMessage(), getSelf());\n                      }\n                    }));\n    target.tell(new TestTimerActor.TriggerScheduling(), ActorRef.noSender());\n    probe.expectMsgClass(TestTimerActor.ScheduledMessage.class);\n  }\n};","title":"Overriding behavior"},{"location":"/testing.html#timing-assertions","text":"Another important part of functional testing concerns timing: certain events must not happen immediately (like a timer), others need to happen before a deadline. Therefore, all examination methods accept an upper time limit within the positive or negative result must be obtained. Lower time limits need to be checked external to the examination, which is facilitated by a new construct for managing time constraints:\nScala copysourceimport org.apache.pekko.actor.Props\nimport scala.concurrent.duration._\n\nval worker = system.actorOf(Props[Worker]())\nwithin(200 millis) {\n  worker ! \"some work\"\n  expectMsg(\"some result\")\n  expectNoMessage() // will block for the rest of the 200ms\n  Thread.sleep(300) // will NOT make this block fail\n} Java copysourcenew TestKit(system) {\n  {\n    getRef().tell(42, ActorRef.noSender());\n    within(\n        Duration.ZERO,\n        Duration.ofSeconds(1),\n        () -> {\n          assertEquals((Integer) 42, expectMsgClass(Integer.class));\n          return null;\n        });\n  }\n};\nThe block given toin within must complete after a duration which is between min and max, where the former defaults to zero. The deadline calculated by adding the max parameter to the block’s start time is implicitly available within the block to all examination methods, if you do not specify it, it is inherited from the innermost enclosing within block.\nIt should be noted that if the last message-receiving assertion of the block is expectNoMessage or receiveWhile, the final check of the within is skipped to avoid false positives due to wake-up latencies. This means that while individual contained assertions still use the maximum time bound, the overall block may take arbitrarily longer in this case.\nNote All times are measured using System.nanoTime, meaning that they describe wall time, not CPU time or system time.\nRay Roestenburg has written a great article on using the TestKit: https://web.archive.org/web/20180114133958/http://roestenburg.agilesquad.com/2011/02/unit-testing-akka-actors-with-testkit_12.html. His full example is also available here.","title":"Timing Assertions"},{"location":"/testing.html#accounting-for-slow-test-systems","text":"The tight timeouts you use during testing on your lightning-fast notebook will invariably lead to spurious test failures on the heavily loaded Jenkins server (or similar). To account for this situation, all maximum durations are internally scaled by a factor taken from the Configuration, pekko.test.timefactor, which defaults to 1.\nYou can scale other durations with the same factor by using the implicit conversion in pekko.testkit package object to add dilated function to Durationdilated method in TestKit.\nScala copysourceimport scala.concurrent.duration._\nimport org.apache.pekko.testkit._\n10.milliseconds.dilated Java copysourcenew TestKit(system) {\n  {\n    final Duration original = Duration.ofSeconds(1);\n    final Duration stretched = dilated(original);\n    assertTrue(\"dilated\", stretched.compareTo(original) >= 0);\n  }\n};\nResolving Conflicts with Implicit ActorRef If you want the sender of messages inside your TestKit-based tests to be the testActor mix in ImplicitSender into your test. copysourceclass MySpec()\n    extends TestKit(ActorSystem(\"MySpec\"))\n    with ImplicitSender\n    with AnyWordSpecLike\n    with Matchers\n    with BeforeAndAfterAll {","title":"Accounting for Slow Test Systems"},{"location":"/testing.html#using-multiple-probe-actors","text":"When the actors under test are supposed to send various messages to different destinations, it may be difficult distinguishing the message streams arriving at the testActor when using the TestKit as a mixinshown until now. Another approach is to use it for the creation of simple probe actors to be inserted in the message flows. To make this more powerful and convenient, there is a concrete implementation called TestProbe. The functionality is best explained using a small example:\nScala copysourceimport scala.concurrent.duration._\nimport org.apache.pekko.actor._\nimport org.apache.pekko.testkit.TestProbe\n copysourceclass MyDoubleEcho extends Actor {\n  var dest1: ActorRef = _\n  var dest2: ActorRef = _\n  def receive = {\n    case (d1: ActorRef, d2: ActorRef) =>\n      dest1 = d1\n      dest2 = d2\n    case x =>\n      dest1 ! x\n      dest2 ! x\n  }\n}\n copysourceval probe1 = TestProbe()\nval probe2 = TestProbe()\nval actor = system.actorOf(Props[MyDoubleEcho]())\nactor ! ((probe1.ref, probe2.ref))\nactor ! \"hello\"\nprobe1.expectMsg(500 millis, \"hello\")\nprobe2.expectMsg(500 millis, \"hello\") Java copysourcenew TestKit(system) {\n  {\n    // simple actor which only forwards messages\n    class Forwarder extends AbstractActor {\n      final ActorRef target;\n\n      @SuppressWarnings(\"unused\")\n      public Forwarder(ActorRef target) {\n        this.target = target;\n      }\n\n      @Override\n      public Receive createReceive() {\n        return receiveBuilder()\n            .matchAny(message -> target.forward(message, getContext()))\n            .build();\n      }\n    }\n\n    // create a test probe\n    final TestKit probe = new TestKit(system);\n\n    // create a forwarder, injecting the probe’s testActor\n    final Props props = Props.create(Forwarder.class, this, probe.getRef());\n    final ActorRef forwarder = system.actorOf(props, \"forwarder\");\n\n    // verify correct forwarding\n    forwarder.tell(42, getRef());\n    probe.expectMsgEquals(42);\n    assertEquals(getRef(), probe.getLastSender());\n  }\n};\nHere the system under test is simulated by MyDoubleEcho, which is supposed to mirror its input to two outputs. Attaching two test probes enables verification of the (simplistic) behaviorThis simple test verifies an equally simple Forwarder actor by injecting a probe as the forwarder’s target. Another example would be two actors A and B which collaborate by A sending messages to B. To verify this message flow, a TestProbe could be inserted as a target of A, using the forwarding capabilities or auto-pilot described below to include a real B in the test setup.\nIf you have many test probes, you can name them to get meaningful actor names in test logs and assertions:\nScala copysourceval worker = TestProbe(\"worker\")\nval aggregator = TestProbe(\"aggregator\")\n\nworker.ref.path.name should startWith(\"worker\")\naggregator.ref.path.name should startWith(\"aggregator\") Java copysourcenew TestKit(system) {\n  {\n    final TestProbe worker = new TestProbe(system, \"worker\");\n    final TestProbe aggregator = new TestProbe(system, \"aggregator\");\n\n    assertTrue(worker.ref().path().name().startsWith(\"worker\"));\n    assertTrue(aggregator.ref().path().name().startsWith(\"aggregator\"));\n  }\n};\nProbes may also be equipped with custom assertions to make your test code even more concise and clear:\nScala copysourcefinal case class Update(id: Int, value: String)\n\nval probe = new TestProbe(system) {\n  def expectUpdate(x: Int) = {\n    expectMsgPF() {\n      case Update(id, _) if id == x => ()\n    }\n    sender() ! \"ACK\"\n  }\n} Java copysourcenew TestKit(system) {\n  {\n    class MyProbe extends TestKit {\n      public MyProbe() {\n        super(system);\n      }\n\n      public void assertHello() {\n        expectMsgEquals(\"hello\");\n      }\n    }\n\n    final MyProbe probe = new MyProbe();\n    probe.getRef().tell(\"hello\", ActorRef.noSender());\n    probe.assertHello();\n  }\n};\nYou have complete flexibility here in mixing and matching the TestKit facilities with your checks and choosing an intuitive name for it. In real life your code will probably be a bit more complicated than the example given above; just use the power!\nWarning Any message sent from a TestProbe to another actor which runs on the CallingThreadDispatcher runs the risk of dead-lock if that other actor might also send to this probe. The implementation of TestProbe.watch and TestProbe.unwatch will also send a message to the actor being watched, which means that it is dangerous to try watching e.g. TestActorRef from a TestProbe.","title":"Using Multiple Probe Actors"},{"location":"/testing.html#watching-other-actors-from-probes","text":"A TestProbeTestKit can register itself for DeathWatch of any other actor:\nScala copysourceval probe = TestProbe()\nprobe.watch(target)\ntarget ! PoisonPill\nprobe.expectTerminated(target) Java copysourcenew TestKit(system) {\n  {\n    final TestKit probe = new TestKit(system);\n    probe.watch(target);\n    target.tell(PoisonPill.getInstance(), ActorRef.noSender());\n    final Terminated msg = probe.expectMsgClass(Terminated.class);\n    assertEquals(msg.getActor(), target);\n  }\n};","title":"Watching Other Actors from Probes"},{"location":"/testing.html#replying-to-messages-received-by-probes","text":"The probes keep track of the communications channel for replies, if possible, so they can also replyThe probe stores the sender of the last dequeued message (i.e. after its expectMsg* reception), which may be retrieved using the getLastSender() method. This information can also implicitly be used for having the probe reply to the last received message:\nScala copysourceval probe = TestProbe()\nval future = probe.ref ? \"hello\"\nprobe.expectMsg(0 millis, \"hello\") // TestActor runs on CallingThreadDispatcher\nprobe.reply(\"world\")\nassert(future.isCompleted && future.value.contains(Success(\"world\"))) Java copysourcenew TestKit(system) {\n  {\n    final TestKit probe = new TestKit(system);\n    probe.getRef().tell(\"hello\", getRef());\n    probe.expectMsgEquals(\"hello\");\n    probe.reply(\"world\");\n    expectMsgEquals(\"world\");\n    assertEquals(probe.getRef(), getLastSender());\n  }\n};","title":"Replying to Messages Received by Probes"},{"location":"/testing.html#forwarding-messages-received-by-probes","text":"Given a destination actor dest which in the nominal actor network would receive a message from actor source. If you arrange for the message to be sent to a TestProbe probe instead, you can make assertions concerning volume and timing of the message flow while still keeping the network functioningThe probe can also forward a received message (i.e. after its expectMsg* reception), retaining the original sender:\nScala copysourceclass Source(target: ActorRef) extends Actor {\n  def receive = {\n    case \"start\" => target ! \"work\"\n  }\n}\n\nclass Destination extends Actor {\n  def receive = {\n    case x => // Do something..\n  }\n}\n copysourceval probe = TestProbe()\nval source = system.actorOf(Props(classOf[Source], probe.ref))\nval dest = system.actorOf(Props[Destination]())\nsource ! \"start\"\nprobe.expectMsg(\"work\")\nprobe.forward(dest) Java copysourcenew TestKit(system) {\n  {\n    final TestKit probe = new TestKit(system);\n    probe.getRef().tell(\"hello\", getRef());\n    probe.expectMsgEquals(\"hello\");\n    probe.forward(getRef());\n    expectMsgEquals(\"hello\");\n    assertEquals(getRef(), getLastSender());\n  }\n};\nThe dest actor will receive the same message invocation as if no test probe had intervened.","title":"Forwarding Messages Received by Probes"},{"location":"/testing.html#auto-pilot","text":"Receiving messages in a queue for later inspection is nice, but to keep a test running and verify traces later you can also install an AutoPilot in the participating test probes (actually in any TestKit) which is invoked before enqueueing to the inspection queue. This code can be used to forward messages, e.g. in a chain A --> Probe --> B, as long as a certain protocol is obeyed.\nScala copysourceval probe = TestProbe()\nprobe.setAutoPilot(new TestActor.AutoPilot {\n  def run(sender: ActorRef, msg: Any): TestActor.AutoPilot =\n    msg match {\n      case \"stop\" => TestActor.NoAutoPilot\n      case x      => testActor.tell(x, sender); TestActor.KeepRunning\n    }\n}) Java copysourcenew TestKit(system) {\n  {\n    final TestKit probe = new TestKit(system);\n    // install auto-pilot\n    probe.setAutoPilot(\n        new TestActor.AutoPilot() {\n          public AutoPilot run(ActorRef sender, Object msg) {\n            sender.tell(msg, ActorRef.noSender());\n            return noAutoPilot();\n          }\n        });\n    // first one is replied to directly ...\n    probe.getRef().tell(\"hello\", getRef());\n    expectMsgEquals(\"hello\");\n    // ... but then the auto-pilot switched itself off\n    probe.getRef().tell(\"world\", getRef());\n    expectNoMessage();\n  }\n};\nThe run method must return the auto-pilot for the next message, which can be KeepRunning to retain the current one or NoAutoPilot to switch it offwrapped in an Option; setting it to None terminates the auto-pilot.","title":"Auto-Pilot"},{"location":"/testing.html#caution-about-timing-assertions","text":"The behavior of within blocks when using test probes might be perceived as counter-intuitive: you need to remember that the nicely scoped deadline as described above is local to each probe. Hence, probes do not react to each other’s deadlines or the deadline set in an enclosing TestKit instance:\nScala copysourceval probe = TestProbe()\nwithin(1 second) {\n  probe.expectMsg(\"hello\")\n} Java copysourcenew TestKit(system) {\n  {\n    final TestKit probe = new TestKit(system);\n    within(Duration.ofSeconds(1), () -> probe.expectMsgEquals(\"hello\"));\n  }\n};\nHere, the expectMsgexpectMsgEquals call will use the default timeout.","title":"Caution about Timing Assertions"},{"location":"/testing.html#testing-parent-child-relationships","text":"The parent of an actor is always the actor that created it. At times this leads to a coupling between the two that may not be straightforward to test. There are several approaches to improve the testability of a child actor that needs to refer to its parent:\nwhen creating a child, pass an explicit reference to its parent create the child with a TestProbe as parent create a fabricated parent when testing\nConversely, a parent’s binding to its child can be lessened as follows:\nwhen creating a parent, tell the parent how to create its child\nFor example, the structure of the code you want to test may follow this pattern:\nScala copysourceclass Parent extends Actor {\n  val child = context.actorOf(Props[Child](), \"child\")\n  var ponged = false\n\n  def receive = {\n    case \"pingit\" => child ! \"ping\"\n    case \"pong\"   => ponged = true\n  }\n}\n\nclass Child extends Actor {\n  def receive = {\n    case \"ping\" => context.parent ! \"pong\"\n  }\n} Java copysourcestatic class Parent extends AbstractActor {\n  final ActorRef child = getContext().actorOf(Props.create(Child.class), \"child\");\n  boolean ponged = false;\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"pingit\", message -> child.tell(\"ping\", getSelf()))\n        .matchEquals(\"pong\", message -> ponged = true)\n        .build();\n  }\n}\n\nstatic class Child extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            \"ping\",\n            message -> {\n              getContext().getParent().tell(\"pong\", getSelf());\n            })\n        .build();\n  }\n}","title":"Testing parent-child relationships"},{"location":"/testing.html#introduce-a-child-to-its-parent","text":"The first option is to avoid the use of the context.parent function and create a child with a custom parent by passing an explicit reference to its parent instead.\nScala copysourceclass DependentChild(parent: ActorRef) extends Actor {\n  def receive = {\n    case \"ping\" => parent ! \"pong\"\n  }\n} Java copysourceclass DependentChild extends AbstractActor {\n  private final ActorRef parent;\n\n  public DependentChild(ActorRef parent) {\n    this.parent = parent;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"ping\", message -> parent.tell(\"pong\", getSelf()))\n        .build();\n  }\n}","title":"Introduce a child to its parent"},{"location":"/testing.html#create-the-child-using-","text":"The TestProbeTestKit class can create actors that will run with the test probe as a parent. This will cause any messages the child actor sends to context.parentgetContext().getParent() to end up in the test probe.\nScala copysource\"A TestProbe serving as parent\" should {\n  \"test its child responses\" in {\n    val parent = TestProbe()\n    val child = parent.childActorOf(Props(new Child))\n    parent.send(child, \"ping\")\n    parent.expectMsg(\"pong\")\n  }\n} Java copysourceTestKit parent = new TestKit(system);\nActorRef child = parent.childActorOf(Props.create(Child.class));\n\nparent.send(child, \"ping\");\nparent.expectMsgEquals(\"pong\");","title":"Create the child using TestProbeTestKit"},{"location":"/testing.html#using-a-fabricated-parent","text":"If you prefer to avoid modifying the parent or child constructor you can create a fabricated parent in your test. This, however, does not enable you to test the parent actor in isolation.\nScala copysource\"A fabricated parent\" should {\n  \"test its child responses\" in {\n    val proxy = TestProbe()\n    val parent = system.actorOf(Props(new Actor {\n      val child = context.actorOf(Props(new Child), \"child\")\n      def receive = {\n        case x if sender() == child => proxy.ref.forward(x)\n        case x                      => child.forward(x)\n      }\n    }))\n\n    proxy.send(parent, \"ping\")\n    proxy.expectMsg(\"pong\")\n  }\n} Java copysourceclass FabricatedParentCreator implements Creator<Actor> {\n  private final TestProbe proxy;\n\n  public FabricatedParentCreator(TestProbe proxy) {\n    this.proxy = proxy;\n  }\n\n  @Override\n  public Actor create() throws Exception {\n    return new AbstractActor() {\n      final ActorRef child = getContext().actorOf(Props.create(Child.class), \"child\");\n\n      @Override\n      public Receive createReceive() {\n        return receiveBuilder()\n            .matchAny(\n                message -> {\n                  if (getSender().equals(child)) {\n                    proxy.ref().forward(message, getContext());\n                  } else {\n                    child.forward(message, getContext());\n                  }\n                })\n            .build();\n      }\n    };\n  }\n} copysourceTestProbe proxy = new TestProbe(system);\nActorRef parent = system.actorOf(Props.create(Actor.class, new FabricatedParentCreator(proxy)));\n\nproxy.send(parent, \"ping\");\nproxy.expectMsg(\"pong\");","title":"Using a fabricated parent"},{"location":"/testing.html#externalize-child-making-from-the-parent","text":"Alternatively, you can tell the parent how to create its child. There are two ways to do this: by giving it a Props object or by giving it a function which takes care of creating the child actor:\nScala copysourceclass DependentParent(childProps: Props, probe: ActorRef) extends Actor {\n  val child = context.actorOf(childProps, \"child\")\n\n  def receive = {\n    case \"pingit\" => child ! \"ping\"\n    case \"pong\"   => probe ! \"ponged\"\n  }\n}\n\nclass GenericDependentParent(childMaker: ActorRefFactory => ActorRef) extends Actor {\n  val child = childMaker(context)\n  var ponged = false\n\n  def receive = {\n    case \"pingit\" => child ! \"ping\"\n    case \"pong\"   => ponged = true\n  }\n} Java copysourceclass DependentParent extends AbstractActor {\n  final ActorRef child;\n  final ActorRef probe;\n\n  public DependentParent(Props childProps, ActorRef probe) {\n    child = getContext().actorOf(childProps, \"child\");\n    this.probe = probe;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"pingit\", message -> child.tell(\"ping\", getSelf()))\n        .matchEquals(\"pong\", message -> probe.tell(\"ponged\", getSelf()))\n        .build();\n  }\n} copysourceclass GenericDependentParent extends AbstractActor {\n  final ActorRef child;\n  boolean ponged = false;\n\n  public GenericDependentParent(Function<ActorRefFactory, ActorRef> childMaker) throws Exception {\n    child = childMaker.apply(getContext());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"pingit\", message -> child.tell(\"ping\", getSelf()))\n        .matchEquals(\"pong\", message -> ponged = true)\n        .build();\n  }\n}\nCreating the PropsActor is straightforward and the function may look like this in your test code:\nScala copysourceval maker = (_: ActorRefFactory) => probe.ref\nval parent = system.actorOf(Props(new GenericDependentParent(maker))) Java copysourceFunction<ActorRefFactory, ActorRef> maker = param -> probe.ref();\nActorRef parent = system.actorOf(Props.create(GenericDependentParent.class, maker));\nAnd like this in your application code:\nScala copysourceval maker = (f: ActorRefFactory) => f.actorOf(Props(new Child))\nval parent = system.actorOf(Props(new GenericDependentParent(maker))) Java copysourceFunction<ActorRefFactory, ActorRef> maker = f -> f.actorOf(Props.create(Child.class));\nActorRef parent = system.actorOf(Props.create(GenericDependentParent.class, maker));\nWhich of these methods is the best depends on what is most important to test. The most generic option is to create the parent actor by passing it a function that is responsible for the Actor creation, but theusing TestProbe or having a fabricated parent is often sufficient.","title":"Externalize child making from the parent"},{"location":"/testing.html#callingthreaddispatcher","text":"The CallingThreadDispatcher runs invocations on the current thread only. This dispatcher does not create any new threads.\nIt is possible to use the CallingThreadDispatcher in unit testing, as described above, but originally it was conceived to allow uninterrupted stack traces to be generated in case of an error. As this special dispatcher runs everything which would normally be queued directly on the current thread, the full history of a message’s processing chain is recorded on the call stack, so long as all intervening actors run on this dispatcher.","title":"CallingThreadDispatcher"},{"location":"/testing.html#how-to-use-it","text":"Just set the dispatcher as you normally would:\nScala copysourceimport org.apache.pekko.testkit.CallingThreadDispatcher\nval ref = system.actorOf(Props[MyActor]().withDispatcher(CallingThreadDispatcher.Id)) Java copysourcesystem.actorOf(Props.create(MyActor.class).withDispatcher(CallingThreadDispatcher.Id()));","title":"How to use it"},{"location":"/testing.html#how-it-works","text":"When receiving an invocation, the CallingThreadDispatcher checks whether the receiving actor is already active on the current thread. The simplest example of this situation is an actor which sends a message to itself. In this case, processing cannot continue immediately as that would violate the actor model, so the invocation is queued and will be processed when the active invocation on that actor finishes its processing; thus, it will be processed on the calling thread, but after the actor finishes its previous work. In the other case, the invocation is processed immediately on the current thread. Futures scheduled via this dispatcher are also executed immediately.\nThis scheme makes the CallingThreadDispatcher work like a general purpose dispatcher for any actors which never block on external events.\nIn the presence of multiple threads, it may happen that two invocations of an actor running on this dispatcher happen on two different threads at the same time. In this case, both will be processed directly on their respective threads, where both compete for the actor’s lock and the loser has to wait. Thus, the actor model is left intact, but the price is the loss of concurrency due to limited scheduling. In a sense, this is equivalent to traditional mutex style concurrency.\nThe other remaining difficulty is correct handling of suspend and resume: when an actor is suspended, subsequent invocations will be queued in thread-local queues (the same ones used for queuing in the normal case). The call to resume, however, is done by one specific thread, and all other threads in the system will probably not be executing this specific actor, which leads to the problem that the thread-local queues cannot be emptied by their native threads. Hence, the thread calling resume will collect all currently queued invocations from all threads into its queue and process them.","title":"How it works"},{"location":"/testing.html#limitations","text":"Warning In case the CallingThreadDispatcher is used for top-level actors, without going through TestActorRef, then there is a time window during which the actor is awaiting construction by the user guardian actor. Sending messages to the actor during this time will result in them being enqueued and then executed on the guardian’s thread instead of the caller’s thread. To avoid this, use TestActorRef.\nIf an actor’s behavior blocks on something which would normally be affected by the calling actor after having sent the message, this will dead-lock when using this dispatcher. This is a common scenario in actor tests based on CountDownLatch for synchronization:\nval latch = new CountDownLatch(1)\nactor ! startWorkAfter(latch)   // actor will call latch.await() before proceeding\ndoSomeSetupStuff()\nlatch.countDown()\nThe example would hang indefinitely within the message processing initiated on the second line and never reach the fourth line, which would unblock it on a normal dispatcher.\nThus, keep in mind that the CallingThreadDispatcher is not a general-purpose replacement for the normal dispatchers. If you are looking for a tool to help you debug dead-locks, the CallingThreadDispatcher may help with some error scenarios, but keep in mind that it may give false negatives as well as false positives.","title":"Limitations"},{"location":"/testing.html#thread-interruptions","text":"If the CallingThreadDispatcher sees that the current thread has its isInterrupted() flag set when message processing returns, it will throw an InterruptedException after finishing all its processing (i.e. all messages which need processing as described above are processed before this happens). As tell cannot throw exceptions due to its contract, this exception will then be caught and logged, and the thread’s interrupted status will be set again.\nIf during message processing an InterruptedException is thrown then it will be caught inside the CallingThreadDispatcher’s message handling loop, the thread’s interrupted flag will be set and processing continues normally.\nNote In summary, if the current thread is interrupted while doing work under the CallingThreadDispatcher, then that will result in the isInterrupted flag to be true when the message send returns and no InterruptedException will be thrown.","title":"Thread Interruptions"},{"location":"/testing.html#benefits","text":"To summarize, these are the features that CallingThreadDispatcher has to offer:\nDeterministic execution of single-threaded tests while retaining nearly full actor semantics Full message processing history leading up to the point of failure in exception stack traces Exclusion of certain classes of dead-lock scenarios","title":"Benefits"},{"location":"/testing.html#tracing-actor-invocations","text":"The testing facilities described up to this point were aiming at formulating assertions about a system’s behavior. If a test fails, it is usually your job to find the cause, fix it and verify the test again. This process is supported by debuggers as well as logging, where the Pekko toolkit offers the following options:\nLogging of exceptions thrown within Actor instances It is always on; in contrast to the other logging mechanisms, this logs at ERROR level.\nLogging of message invocations on certain actors This is enabled by a setting in the Configuration — namely pekko.actor.debug.receive — which enables the loggable statement to be applied to an actor’s receive function: copysourceimport org.apache.pekko.event.LoggingReceive\ndef receive = LoggingReceive {\n  case msg => // Do something ...\n}\ndef otherState: Receive = LoggingReceive.withLabel(\"other\") {\n  case msg => // Do something else ...\n} If the aforementioned setting is not given in the Configuration, this method will pass through the given Receive function unmodified, meaning that there is no runtime cost unless enabled. The logging feature is coupled to this specific local mark-up because enabling it uniformly on all actors is not usually what you need, and it would lead to endless loops if it were applied to event bus logger listeners.\nLogging of special messages Actors handle certain special messages automatically, e.g. Kill, PoisonPill, etc. Tracing of these message invocations is enabled by the setting pekko.actor.debug.autoreceive, which enables this on all actors. Logging of the actor lifecycle Actor creation, start, restart, monitor start, monitor stop and stop may be traced by enabling the setting pekko.actor.debug.lifecycle; this, too, is enabled uniformly on all actors.\nLogging of these messages is at DEBUG level. To summarize, you can enable full logging of actor activities using this configuration fragment:\npekko {\n  loglevel = \"DEBUG\"\n  actor {\n    debug {\n      receive = on\n      autoreceive = on\n      lifecycle = on\n    }\n  }\n}\nDifferent Testing Frameworks Pekko’s test suite is written using ScalaTest, which also shines through in documentation examples. However, the TestKit and its facilities do not depend on that framework, so you can essentially use whichever suits your development style best. This section contains a collection of known gotchas with some other frameworks, which is by no means exhaustive and does not imply an endorsement or special support. When you need it to be a trait If for some reason it is a problem to inherit from TestKit due to it being a concrete class instead of a trait, there’s TestKitBase: copysourceimport org.apache.pekko.testkit.TestKitBase\n\nclass MyTest extends TestKitBase {\n  implicit lazy val system: ActorSystem = ActorSystem()\n\n  val probe = TestProbe()\n  probe.send(testActor, \"hello\")\n  try expectMsg(\"hello\")\n  catch { case NonFatal(e) => system.terminate(); throw e }\n\n  shutdown(system)\n} The implicit lazy val system must be declared exactly like that (you can of course pass arguments to the actor system factory as needed) because trait TestKitBase needs the system during its construction. Warning: use of the trait is discouraged because of potential issues with binary backwards compatibility in the future, use at own risk. Specs2 Some Specs2 users have contributed examples of how to work around some clashes which may arise: Mixing TestKit into org.specs2.mutable.Specification results in a name clash involving the end method (which is a private variable in TestKit and an abstract method in Specification); if mixing in TestKit first, the code may compile but might then fail at runtime. The workaround—which is beneficial also for the third point—is to apply the TestKit together with org.specs2.specification.Scope. The Specification traits provide a Duration DSL which uses partly the same method names as scala.concurrent.duration.Duration, resulting in ambiguous implicits if scala.concurrent.duration._ is imported. There are two workarounds: either use the Specification variant of Duration and supply an implicit conversion to the Pekko Duration. This conversion is not supplied with the Pekko distribution because that would mean that our JAR files would depend on Specs2, which is not justified by this little feature. or mix org.specs2.time.NoTimeConversions into the Specification. Specifications are by default executed concurrently, which requires some care when writing the tests or the sequential keyword.","title":"Tracing Actor Invocations"},{"location":"/testing.html#configuration","text":"There are several configuration properties for the TestKit module, please refer to the reference configuration.\nExample Ray Roestenburg’s example code from his blog, which unfortunately is only available on web archive, adapted to work with Pekko 2.x. copysourceimport scala.util.Random\n\nimport org.scalatest.BeforeAndAfterAll\nimport org.scalatest.wordspec.AnyWordSpecLike\nimport org.scalatest.matchers.should.Matchers\n\nimport com.typesafe.config.ConfigFactory\n\nimport org.apache.pekko\nimport pekko.actor.Actor\nimport pekko.actor.ActorRef\nimport pekko.actor.ActorSystem\nimport pekko.actor.Props\nimport pekko.testkit.{ DefaultTimeout, ImplicitSender, TestActors, TestKit }\nimport scala.concurrent.duration._\nimport scala.collection.immutable\n\n/**\n * a Test to show some TestKit examples\n */\nclass TestKitUsageSpec\n    extends TestKit(ActorSystem(\"TestKitUsageSpec\", ConfigFactory.parseString(TestKitUsageSpec.config)))\n    with DefaultTimeout\n    with ImplicitSender\n    with AnyWordSpecLike\n    with Matchers\n    with BeforeAndAfterAll {\n  import TestKitUsageSpec._\n\n  val echoRef = system.actorOf(TestActors.echoActorProps)\n  val forwardRef = system.actorOf(Props(classOf[ForwardingActor], testActor))\n  val filterRef = system.actorOf(Props(classOf[FilteringActor], testActor))\n  val randomHead = Random.nextInt(6)\n  val randomTail = Random.nextInt(10)\n  val headList = immutable.Seq().padTo(randomHead, \"0\")\n  val tailList = immutable.Seq().padTo(randomTail, \"1\")\n  val seqRef =\n    system.actorOf(Props(classOf[SequencingActor], testActor, headList, tailList))\n\n  override def afterAll(): Unit = {\n    shutdown()\n  }\n\n  \"An EchoActor\" should {\n    \"Respond with the same message it receives\" in {\n      within(500 millis) {\n        echoRef ! \"test\"\n        expectMsg(\"test\")\n      }\n    }\n  }\n  \"A ForwardingActor\" should {\n    \"Forward a message it receives\" in {\n      within(500 millis) {\n        forwardRef ! \"test\"\n        expectMsg(\"test\")\n      }\n    }\n  }\n  \"A FilteringActor\" should {\n    \"Filter all messages, except expected messagetypes it receives\" in {\n      var messages = Seq[String]()\n      within(500 millis) {\n        filterRef ! \"test\"\n        expectMsg(\"test\")\n        filterRef ! 1\n        expectNoMessage()\n        filterRef ! \"some\"\n        filterRef ! \"more\"\n        filterRef ! 1\n        filterRef ! \"text\"\n        filterRef ! 1\n\n        receiveWhile(500 millis) {\n          case msg: String => messages = msg +: messages\n        }\n      }\n      messages.length should be(3)\n      messages.reverse should be(Seq(\"some\", \"more\", \"text\"))\n    }\n  }\n  \"A SequencingActor\" should {\n    \"receive an interesting message at some point \" in {\n      within(500 millis) {\n        ignoreMsg {\n          case msg: String => msg != \"something\"\n        }\n        seqRef ! \"something\"\n        expectMsg(\"something\")\n        ignoreMsg {\n          case msg: String => msg == \"1\"\n        }\n        expectNoMessage()\n        ignoreNoMsg()\n      }\n    }\n  }\n}\n\nobject TestKitUsageSpec {\n  // Define your test specific configuration here\n  val config = \"\"\"\n    pekko {\n      loglevel = \"WARNING\"\n    }\n    \"\"\"\n\n  /**\n   * An Actor that forwards every message to a next Actor\n   */\n  class ForwardingActor(next: ActorRef) extends Actor {\n    def receive = {\n      case msg => next ! msg\n    }\n  }\n\n  /**\n   * An Actor that only forwards certain messages to a next Actor\n   */\n  class FilteringActor(next: ActorRef) extends Actor {\n    def receive = {\n      case msg: String => next ! msg\n      case _           => None\n    }\n  }\n\n  /**\n   * An actor that sends a sequence of messages with a random head list, an\n   * interesting value and a random tail list. The idea is that you would\n   * like to test that the interesting value is received and that you cant\n   * be bothered with the rest\n   */\n  class SequencingActor(next: ActorRef, head: immutable.Seq[String], tail: immutable.Seq[String]) extends Actor {\n    def receive = {\n      case msg => {\n        head.foreach { next ! _ }\n        next ! msg\n        tail.foreach { next ! _ }\n      }\n    }\n  }\n}","title":"Configuration"},{"location":"/testing.html#synchronous-testing-testactorref","text":"Testing the business logic inside Actor classes can be divided into two parts: first, each atomic operation must work in isolation, then sequences of incoming events must be processed correctly, even in the presence of some possible variability in the ordering of events. The former is the primary use case for single-threaded unit testing, while the latter can only be verified in integration tests.\nNormally, the ActorRef shields the underlying Actor instance from the outside, the only communications channel is the actor’s mailbox. This restriction impedes unit testing, which led to the inception of the TestActorRef. This special type of reference is designed specifically for test purposes and allows access to the actor in two ways: either by obtaining a reference to the underlying actor instance or by invoking or querying the actor’s behavior (receive). Each one warrants its section below.\nNote It is highly recommended to stick to traditional behavioral testing (using messaging to ask the Actor to reply with the state you want to run assertions against), instead of using TestActorRef whenever possible.\nWarning Due to the synchronous nature of TestActorRef, it will not work with some support traits that Pekko provides as they require asynchronous behaviors to function properly. Examples of traits that do not mix well with test actor refs are PersistentActor and AtLeastOnceDelivery provided by Pekko Persistence.","title":"Synchronous Testing: TestActorRef"},{"location":"/testing.html#obtaining-a-reference-to-an-actor","text":"Having access to the actual Actor object allows the application of all traditional unit testing techniques on the contained methods. Obtaining a reference is done like this:\nScala copysourceimport org.apache.pekko.testkit.TestActorRef\n\nval actorRef = TestActorRef[MyActor]\nval actor = actorRef.underlyingActor Java copysourcestatic class MyActor extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            \"say42\",\n            message -> {\n              getSender().tell(42, getSelf());\n            })\n        .match(\n            Exception.class,\n            (Exception ex) -> {\n              throw ex;\n            })\n        .build();\n  }\n\n  public boolean testMe() {\n    return true;\n  }\n}\n\n@Test\npublic void demonstrateTestActorRef() {\n  final Props props = Props.create(MyActor.class);\n  final TestActorRef<MyActor> ref = TestActorRef.create(system, props, \"testA\");\n  final MyActor actor = ref.underlyingActor();\n  assertTrue(actor.testMe());\n}\nSince TestActorRef is generic in the actor type it returns the underlying actor with its proper static type. From this point on you may bring any unit testing tool to bear on your actor as usual.\nTesting Finite State Machines If your actor under test is an FSM, you may use the special TestFSMRef which offers all features of a normal TestActorRef and besides allows access to the internal state: copysourceimport org.apache.pekko.testkit.TestFSMRef\nimport scala.concurrent.duration._\n\nval fsm = TestFSMRef(new TestFsmActor)\n\nval mustBeTypedProperly: TestActorRef[TestFsmActor] = fsm\n\nassert(fsm.stateName == 1)\nassert(fsm.stateData == \"\")\nfsm ! \"go\" // being a TestActorRef, this runs also on the CallingThreadDispatcher\nassert(fsm.stateName == 2)\nassert(fsm.stateData == \"go\")\n\nfsm.setState(stateName = 1)\nassert(fsm.stateName == 1)\n\nassert(fsm.isTimerActive(\"test\") == false)\nfsm.startTimerWithFixedDelay(\"test\", 12, 10 millis)\nassert(fsm.isTimerActive(\"test\") == true)\nfsm.cancelTimer(\"test\")\nassert(fsm.isTimerActive(\"test\") == false) Due to a limitation in Scala’s type inference, there is only the factory method shown above, so you will probably write code like TestFSMRef(new MyFSM) instead of the hypothetical ActorRef-inspired TestFSMRef[MyFSM]. All methods shown above directly access the FSM state without any synchronization; this is perfectly alright if the CallingThreadDispatcher is used and no other threads are involved, but it may lead to surprises if you were to exercise timer events, because those are executed on the Scheduler thread.","title":"Obtaining a Reference to an Actor"},{"location":"/testing.html#testing-the-actors-behavior","text":"When the dispatcher invokes the processing behavior of an actor on a message, it calls apply on the current behavior registered for the actor. This starts with the return value of the declared receive method, but it may also be changed using become and unbecome in response to external messages. All of this contributes to the overall actor behavior and it does not lend itself to easy testing on the Actor itself. Therefore the TestActorRef offers a different mode of operation to complement the Actor testing: it supports all operations also valid on normal ActorRef. Messages sent to the actor are processed synchronously on the current thread and answers may be sent back as usual. This trick is made possible by the CallingThreadDispatcher described below (see CallingThreadDispatcher); this dispatcher is set implicitly for any actor instantiated into a TestActorRef.\nScala copysourceimport org.apache.pekko\nimport pekko.testkit.TestActorRef\nimport pekko.pattern.ask\n\nval actorRef = TestActorRef(new MyActor)\n// hypothetical message stimulating a '42' answer\nval future: Future[Any] = actorRef ? Say42\nfuture.futureValue should be(42) Java copysourcefinal Props props = Props.create(MyActor.class);\nfinal TestActorRef<MyActor> ref = TestActorRef.create(system, props, \"testB\");\nfinal CompletableFuture<Object> future =\n    Patterns.ask(ref, \"say42\", Duration.ofMillis(3000)).toCompletableFuture();\nassertTrue(future.isDone());\nassertEquals(42, future.get());\nAs the TestActorRef is a subclass of LocalActorRef with a few special extras, also aspects like supervision and restarting work properly, but beware that execution is only strictly synchronous as long as all actors involved use the CallingThreadDispatcher. As soon as you add elements which include more sophisticated scheduling you leave the realm of unit testing as you then need to think about asynchronicity again (in most cases the problem will be to wait until the desired effect had a chance to happen).\nOne more special aspect which is overridden for single-threaded tests is the receiveTimeout, as including that would entail asynchronous queuing of ReceiveTimeout messages, violating the synchronous contract.\nNote To summarize: TestActorRef overwrites two fields: it sets the dispatcher to CallingThreadDispatcher.global and it sets the receiveTimeout to None.","title":"Testing the Actor’s Behavior"},{"location":"/testing.html#the-way-in-between-expecting-exceptions","text":"If you want to test the actor behavior, including hot swapping, but without involving a dispatcher and without having the TestActorRef swallow any thrown exceptions, then there is another mode available for you: use the receive method on TestActorRef, which will be forwarded to the underlying actor:\nScala copysourceimport org.apache.pekko.testkit.TestActorRef\n\nval actorRef = TestActorRef(new Actor {\n  def receive = {\n    case \"hello\" => throw new IllegalArgumentException(\"boom\")\n  }\n})\nintercept[IllegalArgumentException] { actorRef.receive(\"hello\") } Java copysourcefinal Props props = Props.create(MyActor.class);\nfinal TestActorRef<MyActor> ref = TestActorRef.create(system, props, \"myActor\");\ntry {\n  ref.receive(new Exception(\"expected\"));\n  Assert.fail(\"expected an exception to be thrown\");\n} catch (Exception e) {\n  assertEquals(\"expected\", e.getMessage());\n}","title":"The Way In-Between: Expecting Exceptions"},{"location":"/testing.html#use-cases","text":"You may mix and match both modi operandi of TestActorRef as suits your test needs:\none common use case is setting up the actor into a specific internal state before sending the test message another is to verify correct internal state transitions after having sent the test message\nFeel free to experiment with the possibilities, and if you find useful patterns, don’t hesitate to let the Pekko forums know about them! Who knows, common operations might even be worked into nice DSLs.","title":"Use Cases"},{"location":"/index-cluster.html","text":"","title":"Classic Clustering"},{"location":"/index-cluster.html#classic-clustering","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the new API see Cluster.\nClassic Cluster Usage Module info When and where to use Pekko Cluster Cluster API Extension Cluster Membership API Leaving Downing Subscribe to Cluster Events Node Roles How To Startup when Cluster Size Reached How To Startup when Member is Up How To Cleanup when Member is Removed Higher level Cluster tools Failure Detector How to Test Management Configuration Classic Cluster Aware Routers Dependency Router with Group of Routees Router with Pool of Remote Deployed Routees Classic Cluster Singleton Module info Introduction An Example Configuration Supervision Lease Classic Distributed Publish Subscribe in Cluster Module info Introduction Publish Send DistributedPubSub Extension Delivery Guarantee Classic Cluster Client Module info Introduction An Example ClusterClientReceptionist Extension Events Configuration Failure handling When the cluster cannot be reached at all Migration to Apache Pekko gRPC Classic Cluster Sharding Module info Introduction Basic example How it works Sharding State Store Mode Proxy Only Mode Passivation Remembering Entities Supervision Graceful Shutdown Removal of Internal Cluster Sharding Data Inspecting cluster sharding state Lease Configuration Classic Cluster Metrics Extension Module info Introduction Metrics Collector Metrics Events Hyperic Sigar Provisioning Adaptive Load Balancing Subscribe to Metrics Events Custom Metrics Collector Configuration Classic Distributed Data Dependency Introduction Using the Replicator Replicated data types Durable Storage Limitations Learn More about CRDTs Configuration Classic Multi-DC Cluster Membership Cluster Singleton Cluster Sharding Classic Serialization Dependency Serializing ActorRefs","title":"Classic Clustering"},{"location":"/cluster-usage.html","text":"","title":"Classic Cluster Usage"},{"location":"/cluster-usage.html#classic-cluster-usage","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see Cluster. For specific documentation topics see:\nCluster Specification Cluster Membership Service When and where to use Pekko Cluster Higher level Cluster tools Rolling Updates Operating, Managing, Observability\nNote You have to enable serialization to send messages between ActorSystems in the Cluster. Serialization with Jackson is a good choice in many cases, and our recommendation if you don’t have other preferences or constraints.","title":"Classic Cluster Usage"},{"location":"/cluster-usage.html#module-info","text":"To use Pekko Cluster add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster (classic) Artifact org.apache.pekko pekko-cluster 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/cluster-usage.html#when-and-where-to-use-pekko-cluster","text":"See Choosing Pekko Cluster in the documentation of the new APIs.","title":"When and where to use Pekko Cluster"},{"location":"/cluster-usage.html#cluster-api-extension","text":"The following configuration enables the ClusterCluster extension to be used. It joins the cluster and an actor subscribes to cluster membership events and logs them.\nAn actor that uses the cluster extension may look like this:\nScala copysource/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * license agreements; and to You under the Apache License, version 2.0:\n *\n *   https://www.apache.org/licenses/LICENSE-2.0\n *\n * This file is part of the Apache Pekko project, derived from Akka.\n */\n\n/*\n * Copyright (C) 2018-2022 Lightbend Inc. <https://www.lightbend.com>\n */\n\npackage scala.docs.cluster\n\nimport org.apache.pekko\nimport pekko.cluster.Cluster\nimport pekko.cluster.ClusterEvent._\nimport pekko.actor.ActorLogging\nimport pekko.actor.Actor\n\nclass SimpleClusterListener extends Actor with ActorLogging {\n\n  val cluster = Cluster(context.system)\n\n  // subscribe to cluster changes, re-subscribe when restart\n  override def preStart(): Unit = {\n    // #subscribe\n    cluster.subscribe(self, initialStateMode = InitialStateAsEvents, classOf[MemberEvent], classOf[UnreachableMember])\n    // #subscribe\n  }\n  override def postStop(): Unit = cluster.unsubscribe(self)\n\n  def receive = {\n    case MemberUp(member) =>\n      log.info(\"Member is Up: {}\", member.address)\n    case UnreachableMember(member) =>\n      log.info(\"Member detected as unreachable: {}\", member)\n    case MemberRemoved(member, previousStatus) =>\n      log.info(\"Member is Removed: {} after {}\", member.address, previousStatus)\n    case _: MemberEvent => // ignore\n  }\n} Java copysource/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * license agreements; and to You under the Apache License, version 2.0:\n *\n *   https://www.apache.org/licenses/LICENSE-2.0\n *\n * This file is part of the Apache Pekko project, derived from Akka.\n */\n\n/*\n * Copyright (C) 2018-2022 Lightbend Inc. <https://www.lightbend.com>\n */\n\npackage jdocs.cluster;\n\nimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.cluster.Cluster;\nimport org.apache.pekko.cluster.ClusterEvent;\nimport org.apache.pekko.cluster.ClusterEvent.MemberEvent;\nimport org.apache.pekko.cluster.ClusterEvent.MemberUp;\nimport org.apache.pekko.cluster.ClusterEvent.MemberRemoved;\nimport org.apache.pekko.cluster.ClusterEvent.UnreachableMember;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\n\npublic class SimpleClusterListener extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n  Cluster cluster = Cluster.get(getContext().getSystem());\n\n  // subscribe to cluster changes\n  @Override\n  public void preStart() {\n    // #subscribe\n    cluster.subscribe(\n        getSelf(), ClusterEvent.initialStateAsEvents(), MemberEvent.class, UnreachableMember.class);\n    // #subscribe\n  }\n\n  // re-subscribe when restart\n  @Override\n  public void postStop() {\n    cluster.unsubscribe(getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            MemberUp.class,\n            mUp -> {\n              log.info(\"Member is Up: {}\", mUp.member());\n            })\n        .match(\n            UnreachableMember.class,\n            mUnreachable -> {\n              log.info(\"Member detected as unreachable: {}\", mUnreachable.member());\n            })\n        .match(\n            MemberRemoved.class,\n            mRemoved -> {\n              log.info(\"Member is Removed: {}\", mRemoved.member());\n            })\n        .match(\n            MemberEvent.class,\n            message -> {\n              // ignore\n            })\n        .build();\n  }\n}\nAnd the minimum configuration required is to set a host/port for remoting and the pekko.actor.provider = \"cluster\".\ncopysourcepekko {\n  actor {\n    provider = \"cluster\"\n  }\n  remote.artery {\n    canonical {\n      hostname = \"127.0.0.1\"\n      port = 2551\n    }\n  }\n\n  cluster {\n    seed-nodes = [\n      \"akka://ClusterSystem@127.0.0.1:2551\",\n      \"akka://ClusterSystem@127.0.0.1:2552\"]\n    \n    downing-provider-class = \"org.apache.pekko.cluster.sbr.SplitBrainResolverProvider\"\n  }\n}\nThe actor registers itself as subscriber of certain cluster events. It receives events corresponding to the current state of the cluster when the subscription starts and then it receives events for changes that happen in the cluster.","title":"Cluster API Extension"},{"location":"/cluster-usage.html#cluster-membership-api","text":"This section shows the basic usage of the membership API. For the in-depth descriptions on joining, joining to seed nodes, downing and leaving of any node in the cluster please see the full Cluster Membership API documentation.","title":"Cluster Membership API"},{"location":"/cluster-usage.html#joining-to-seed-nodes","text":"The seed nodes are initial contact points for joining a cluster, which can be done in different ways:\nautomatically with Cluster Bootstrap with configuration of seed-nodes programatically\nAfter the joining process the seed nodes are not special and they participate in the cluster in exactly the same way as other nodes.","title":"Joining to Seed Nodes"},{"location":"/cluster-usage.html#joining-programmatically-to-seed-nodes","text":"You may also join programmatically, which is attractive when dynamically discovering other nodes at startup by using some external tool or API.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.Address\nimport pekko.cluster.Cluster\n\nval cluster = Cluster(system)\nval list: List[Address] = ??? // your method to dynamically get seed nodes\ncluster.joinSeedNodes(list) Java copysourceimport org.apache.pekko.actor.Address;\nimport org.apache.pekko.cluster.Cluster;\n\nfinal Cluster cluster = Cluster.get(system);\nList<Address> list =\n    new LinkedList<>(); // replace this with your method to dynamically get seed nodes\ncluster.joinSeedNodes(list);\nFor more information see tuning joins\nIt’s also possible to specifically join a single node as illustrated in below example, but joinSeedNodesjoinSeedNodes should be preferred since it has redundancy and retry mechanisms built-in.\nScala copysourceval cluster = Cluster(context.system)\n  cluster.join(cluster.selfAddress) Java copysourceCluster cluster = Cluster.get(getContext().getSystem());\n  cluster.join(cluster.selfAddress());","title":"Joining programmatically to seed nodes"},{"location":"/cluster-usage.html#leaving","text":"See Leaving in the documentation of the new APIs.","title":"Leaving"},{"location":"/cluster-usage.html#downing","text":"See Downing in the documentation of the new APIs.","title":"Downing"},{"location":"/cluster-usage.html#subscribe-to-cluster-events","text":"You can subscribe to change notifications of the cluster membership by using Cluster(system).subscribeCluster.get(system).subscribe.\nScala copysourcecluster.subscribe(self, classOf[MemberEvent], classOf[UnreachableMember]) Java copysourcecluster.subscribe(getSelf(), MemberEvent.class, UnreachableMember.class);\nA snapshot of the full state, CurrentClusterStateCurrentClusterState, is sent to the subscriber as the first message, followed by events for incremental updates.\nNote that you may receive an empty CurrentClusterState, containing no members, followed by MemberUpMemberUp events from other nodes which already joined, if you start the subscription before the initial join procedure has completed. This may for example happen when you start the subscription immediately after cluster.join()cluster.join() like below. This is expected behavior. When the node has been accepted in the cluster you will receive MemberUp for that node, and other nodes.\nScala copysourceval cluster = Cluster(context.system)\n  cluster.join(cluster.selfAddress)\ncluster.subscribe(self, classOf[MemberEvent], classOf[UnreachableMember]) Java copysourceCluster cluster = Cluster.get(getContext().getSystem());\n  cluster.join(cluster.selfAddress());\ncluster.subscribe(getSelf(), MemberEvent.class, UnreachableMember.class);\nTo avoid receiving an empty CurrentClusterState at the beginning, you can use it like shown in the following example, to defer subscription until the MemberUp event for the own node is received:\nScala copysourceval cluster = Cluster(context.system)\n  cluster.join(cluster.selfAddress)\ncluster.registerOnMemberUp {\n  cluster.subscribe(self, classOf[MemberEvent], classOf[UnreachableMember])\n} Java copysourceCluster cluster = Cluster.get(getContext().getSystem());\n  cluster.join(cluster.selfAddress());\ncluster.registerOnMemberUp(\n    () -> cluster.subscribe(getSelf(), MemberEvent.class, UnreachableMember.class));\nIf you find it inconvenient to handle the CurrentClusterState you can use ClusterEvent.InitialStateAsEvents ClusterEvent.initialStateAsEvents() as parameter to subscribesubscribe. That means that instead of receiving CurrentClusterState as the first message you will receive the events corresponding to the current state to mimic what you would have seen if you were listening to the events when they occurred in the past. Note that those initial events only correspond to the current state and it is not the full history of all changes that actually has occurred in the cluster.\nScala copysourcecluster.subscribe(self, initialStateMode = InitialStateAsEvents, classOf[MemberEvent], classOf[UnreachableMember]) Java copysourcecluster.subscribe(\n    getSelf(), ClusterEvent.initialStateAsEvents(), MemberEvent.class, UnreachableMember.class);","title":"Subscribe to Cluster Events"},{"location":"/cluster-usage.html#worker-dial-in-example","text":"Let’s take a look at an example that illustrates how workers, here named backend, can detect and register to new master nodes, here named frontend.\nThe example application provides a service to transform text. When some text is sent to one of the frontend services, it will be delegated to one of the backend workers, which performs the transformation job, and sends the result back to the original client. New backend nodes, as well as new frontend nodes, can be added or removed to the cluster dynamically.\nMessages:\nScala copysourcefinal case class TransformationJob(text: String)\nfinal case class TransformationResult(text: String)\nfinal case class JobFailed(reason: String, job: TransformationJob)\ncase object BackendRegistration Java copysourcepublic interface TransformationMessages {\n\n  public static class TransformationJob implements Serializable {\n    private final String text;\n\n    public TransformationJob(String text) {\n      this.text = text;\n    }\n\n    public String getText() {\n      return text;\n    }\n  }\n\n  public static class TransformationResult implements Serializable {\n    private final String text;\n\n    public TransformationResult(String text) {\n      this.text = text;\n    }\n\n    public String getText() {\n      return text;\n    }\n\n    @Override\n    public String toString() {\n      return \"TransformationResult(\" + text + \")\";\n    }\n  }\n\n  public static class JobFailed implements Serializable {\n    private final String reason;\n    private final TransformationJob job;\n\n    public JobFailed(String reason, TransformationJob job) {\n      this.reason = reason;\n      this.job = job;\n    }\n\n    public String getReason() {\n      return reason;\n    }\n\n    public TransformationJob getJob() {\n      return job;\n    }\n\n    @Override\n    public String toString() {\n      return \"JobFailed(\" + reason + \")\";\n    }\n  }\n\n  public static final String BACKEND_REGISTRATION = \"BackendRegistration\";\n}\nThe backend worker that performs the transformation job:\nScala copysourceclass TransformationBackend extends Actor {\n\n  val cluster = Cluster(context.system)\n\n  // subscribe to cluster changes, MemberUp\n  // re-subscribe when restart\n  override def preStart(): Unit = cluster.subscribe(self, classOf[MemberUp])\n  override def postStop(): Unit = cluster.unsubscribe(self)\n\n  def receive = {\n    case TransformationJob(text) => sender() ! TransformationResult(text.toUpperCase)\n    case state: CurrentClusterState =>\n      state.members.filter(_.status == MemberStatus.Up).foreach(register)\n    case MemberUp(m) => register(m)\n  }\n\n  def register(member: Member): Unit =\n    if (member.hasRole(\"frontend\"))\n      context.actorSelection(RootActorPath(member.address) / \"user\" / \"frontend\") !\n      BackendRegistration\n} Java copysourcepublic class TransformationBackend extends AbstractActor {\n\n  Cluster cluster = Cluster.get(getContext().getSystem());\n\n  // subscribe to cluster changes, MemberUp\n  @Override\n  public void preStart() {\n    cluster.subscribe(getSelf(), MemberUp.class);\n  }\n\n  // re-subscribe when restart\n  @Override\n  public void postStop() {\n    cluster.unsubscribe(getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            TransformationJob.class,\n            job -> {\n              getSender().tell(new TransformationResult(job.getText().toUpperCase()), getSelf());\n            })\n        .match(\n            CurrentClusterState.class,\n            state -> {\n              for (Member member : state.getMembers()) {\n                if (member.status().equals(MemberStatus.up())) {\n                  register(member);\n                }\n              }\n            })\n        .match(\n            MemberUp.class,\n            mUp -> {\n              register(mUp.member());\n            })\n        .build();\n  }\n\n  void register(Member member) {\n    if (member.hasRole(\"frontend\"))\n      getContext()\n          .actorSelection(member.address() + \"/user/frontend\")\n          .tell(BACKEND_REGISTRATION, getSelf());\n  }\n}\nNote that the TransformationBackend actor subscribes to cluster events to detect new, potential, frontend nodes, and send them a registration message so that they know that they can use the backend worker.\nThe frontend that receives user jobs and delegates to one of the registered backend workers:\nScala copysourceclass TransformationFrontend extends Actor {\n\n  var backends = IndexedSeq.empty[ActorRef]\n  var jobCounter = 0\n\n  def receive = {\n    case job: TransformationJob if backends.isEmpty =>\n      sender() ! JobFailed(\"Service unavailable, try again later\", job)\n\n    case job: TransformationJob =>\n      jobCounter += 1\n      backends(jobCounter % backends.size).forward(job)\n\n    case BackendRegistration if !backends.contains(sender()) =>\n      context.watch(sender())\n      backends = backends :+ sender()\n\n    case Terminated(a) =>\n      backends = backends.filterNot(_ == a)\n  }\n} Java copysourcepublic class TransformationFrontend extends AbstractActor {\n\n  List<ActorRef> backends = new ArrayList<ActorRef>();\n  int jobCounter = 0;\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            TransformationJob.class,\n            job -> backends.isEmpty(),\n            job -> {\n              getSender()\n                  .tell(new JobFailed(\"Service unavailable, try again later\", job), getSender());\n            })\n        .match(\n            TransformationJob.class,\n            job -> {\n              jobCounter++;\n              backends.get(jobCounter % backends.size()).forward(job, getContext());\n            })\n        .matchEquals(\n            BACKEND_REGISTRATION,\n            x -> {\n              getContext().watch(getSender());\n              backends.add(getSender());\n            })\n        .match(\n            Terminated.class,\n            terminated -> {\n              backends.remove(terminated.getActor());\n            })\n        .build();\n  }\n}\nNote that the TransformationFrontend actor watch the registered backend to be able to remove it from its list of available backend workers. Death watch uses the cluster failure detector for nodes in the cluster, i.e. it detects network failures and JVM crashes, in addition to graceful termination of watched actor. Death watch generates the TerminatedTerminated message to the watching actor when the unreachable cluster node has been downed and removed.","title":"Worker Dial-in Example"},{"location":"/cluster-usage.html#node-roles","text":"See Cluster Node Roles in the documentation of the new APIs.","title":"Node Roles"},{"location":"/cluster-usage.html#how-to-startup-when-cluster-size-reached","text":"See How to startup when a minimum number of members in the cluster is reached in the documentation of the new APIs.","title":"How To Startup when Cluster Size Reached"},{"location":"/cluster-usage.html#how-to-startup-when-member-is-up","text":"A common use case is to start actors after the cluster has been initialized, members have joined, and the cluster has reached a certain size.\nWith a configuration option you can define required number of members before the leader changes member status of ‘Joining’ members to ‘Up’.:\npekko.cluster.min-nr-of-members = 3\nIn a similar way you can define required number of members of a certain role before the leader changes member status of ‘Joining’ members to ‘Up’.:\npekko.cluster.role {\n  frontend.min-nr-of-members = 1\n  backend.min-nr-of-members = 2\n}\nYou can start actors or trigger any functions using the registerOnMemberUpregisterOnMemberUp callback, which will be invoked when the current member status is changed to ‘Up’. This can additionally be used with pekko.cluster.min-nr-of-members optional configuration to defer an action until the cluster has reached a certain size.\nScala copysourceCluster(system).registerOnMemberUp {\n  system.actorOf(Props(classOf[FactorialFrontend], upToN, true), name = \"factorialFrontend\")\n} Java copysourceCluster.get(system)\n    .registerOnMemberUp(\n        new Runnable() {\n          @Override\n          public void run() {\n            system.actorOf(\n                Props.create(FactorialFrontend.class, upToN, true), \"factorialFrontend\");\n          }\n        });","title":"How To Startup when Member is Up"},{"location":"/cluster-usage.html#how-to-cleanup-when-member-is-removed","text":"You can do some clean up in a registerOnMemberRemovedregisterOnMemberRemoved callback, which will be invoked when the current member status is changed to ‘Removed’ or the cluster have been shutdown.\nAn alternative is to register tasks to the Coordinated Shutdown.\nNote Register a OnMemberRemoved callback on a cluster that have been shutdown, the callback will be invoked immediately on the caller thread, otherwise it will be invoked later when the current member status changed to RemovedRemoved. You may want to install some cleanup handling after the cluster was started up, but the cluster might already be shutting down when you installing, and depending on the race is not healthy.","title":"How To Cleanup when Member is Removed"},{"location":"/cluster-usage.html#higher-level-cluster-tools","text":"","title":"Higher level Cluster tools"},{"location":"/cluster-usage.html#cluster-singleton","text":"For some use cases it is convenient or necessary to ensure only one actor of a certain type is running somewhere in the cluster. This can be implemented by subscribing to member events, but there are several corner cases to consider. Therefore, this specific use case is covered by the Cluster Singleton.\nSee Cluster Singleton.","title":"Cluster Singleton"},{"location":"/cluster-usage.html#cluster-sharding","text":"Distributes actors across several nodes in the cluster and supports interaction with the actors using their logical identifier, but without having to care about their physical location in the cluster.\nSee Cluster Sharding.","title":"Cluster Sharding"},{"location":"/cluster-usage.html#distributed-data","text":"Distributed Data is useful when you need to share data between nodes in an Pekko Cluster. The data is accessed with an actor providing a key-value store like API.\nSee Distributed Data.","title":"Distributed Data"},{"location":"/cluster-usage.html#distributed-publish-subscribe","text":"Publish-subscribe messaging between actors in the cluster based on a topic, i.e. the sender does not have to know on which node the destination actor is running.\nSee Cluster Distributed Publish Subscribe.","title":"Distributed Publish Subscribe"},{"location":"/cluster-usage.html#cluster-aware-routers","text":"All routers can be made aware of member nodes in the cluster, i.e. deploying new routees or looking up routees on nodes in the cluster. When a node becomes unreachable or leaves the cluster the routees of that node are automatically unregistered from the router. When new nodes join the cluster, additional routees are added to the router, according to the configuration.\nSee Cluster Aware Routers and Routers.","title":"Cluster Aware Routers"},{"location":"/cluster-usage.html#cluster-across-multiple-data-centers","text":"Pekko Cluster can be used across multiple data centers, availability zones or regions, so that one Cluster can span multiple data centers and still be tolerant to network partitions.\nSee Cluster Multi-DC.","title":"Cluster across multiple data centers"},{"location":"/cluster-usage.html#cluster-client","text":"Communication from an actor system that is not part of the cluster to actors running somewhere in the cluster. The client does not have to know on which node the destination actor is running.\nSee Cluster Client.","title":"Cluster Client"},{"location":"/cluster-usage.html#cluster-metrics","text":"The member nodes of the cluster can collect system health metrics and publish that to other cluster nodes and to the registered subscribers on the system event bus.\nSee Cluster Metrics.","title":"Cluster Metrics"},{"location":"/cluster-usage.html#failure-detector","text":"The nodes in the cluster monitor each other by sending heartbeats to detect if a node is unreachable from the rest of the cluster. Please see:\nFailure Detector specification Phi Accrual Failure Detector implementation Using the Failure Detector","title":"Failure Detector"},{"location":"/cluster-usage.html#how-to-test","text":"Multi Node Testing is useful for testing cluster applications. Set up your project according to the instructions in Multi Node Testing and Multi JVM Testing, i.e. add the sbt-multi-jvm plugin and the dependency to pekko-multi-node-testkit. First, as described in Multi Node Testing, we need some scaffolding to configure the MultiNodeSpec. Define the participating roles and their configuration in an object extending MultiNodeConfig: copysourceimport org.apache.pekko.remote.testkit.MultiNodeConfig\nimport com.typesafe.config.ConfigFactory\n\nobject StatsSampleSpecConfig extends MultiNodeConfig {\n  // register the named roles (nodes) of the test\n  val first = role(\"first\")\n  val second = role(\"second\")\n  val third = role(\"third\")\n\n  def nodeList = Seq(first, second, third)\n\n  // Extract individual sigar library for every node.\n  nodeList.foreach { role =>\n    nodeConfig(role) {\n      ConfigFactory.parseString(s\"\"\"\n      # Enable metrics extension in pekko-cluster-metrics.\n      pekko.extensions=[\"org.apache.pekko.cluster.metrics.ClusterMetricsExtension\"]\n      # Sigar native library extract location during tests.\n      pekko.cluster.metrics.native-library-extract-folder=target/native/${role.name}\n      \"\"\")\n    }\n  }\n\n  // this configuration will be used for all nodes\n  // note that no fixed host names and ports are used\n  commonConfig(ConfigFactory.parseString(\"\"\"\n    pekko.actor.provider = cluster\n    pekko.remote.classic.log-remote-lifecycle-events = off\n    pekko.cluster.roles = [compute]\n    pekko.actor.deployment {\n      /statsService/workerRouter {\n          router = consistent-hashing-group\n          routees.paths = [\"/user/statsWorker\"]\n          cluster {\n            enabled = on\n            allow-local-routees = on\n            use-roles = [\"compute\"]\n          }\n        }\n    }\n    \"\"\"))\n\n} Define one concrete test class for each role/node. These will be instantiated on the different nodes (JVMs). They can be implemented differently, but often they are the same and extend an abstract test class, as illustrated here. copysource// need one concrete test class per node\nclass StatsSampleSpecMultiJvmNode1 extends StatsSampleSpec\nclass StatsSampleSpecMultiJvmNode2 extends StatsSampleSpec\nclass StatsSampleSpecMultiJvmNode3 extends StatsSampleSpec Note the naming convention of these classes. The name of the classes must end with MultiJvmNode1, MultiJvmNode2 and so on. It is possible to define another suffix to be used by the sbt-multi-jvm, but the default should be fine in most cases. Then the abstract MultiNodeSpec, which takes the MultiNodeConfig as constructor parameter. copysourceimport org.apache.pekko\nimport pekko.remote.testkit.MultiNodeSpec\nimport pekko.testkit.ImplicitSender\nimport org.scalatest.BeforeAndAfterAll\nimport org.scalatest.matchers.should.Matchers\nimport org.scalatest.wordspec.AnyWordSpecLike\n\nabstract class StatsSampleSpec\n    extends MultiNodeSpec(StatsSampleSpecConfig)\n    with AnyWordSpecLike\n    with Matchers\n    with BeforeAndAfterAll\n    with ImplicitSender {\n\n  import StatsSampleSpecConfig._\n\n  override def initialParticipants = roles.size\n\n  override def beforeAll() = multiNodeSpecBeforeAll()\n\n  override def afterAll() = multiNodeSpecAfterAll()\n Most of this can be extracted to a separate trait to avoid repeating this in all your tests. Typically you begin your test by starting up the cluster and let the members join, and create some actors. That can be done like this: copysource\"illustrate how to startup cluster\" in within(15 seconds) {\n  Cluster(system).subscribe(testActor, classOf[MemberUp])\n  expectMsgClass(classOf[CurrentClusterState])\n\n  val firstAddress = node(first).address\n  val secondAddress = node(second).address\n  val thirdAddress = node(third).address\n\n  Cluster(system).join(firstAddress)\n\n  system.actorOf(Props[StatsWorker](), \"statsWorker\")\n  system.actorOf(Props[StatsService](), \"statsService\")\n\n  receiveN(3).collect { case MemberUp(m) => m.address }.toSet should be(\n    Set(firstAddress, secondAddress, thirdAddress))\n\n  Cluster(system).unsubscribe(testActor)\n\n  testConductor.enter(\"all-up\")\n} From the test you interact with the cluster using the Cluster extension, e.g. join. copysourceCluster(system).join(firstAddress) Notice how the testActor from testkit is added as subscriber to cluster changes and then waiting for certain events, such as in this case all members becoming ‘Up’. The above code was running for all roles (JVMs). runOn is a convenient utility to declare that a certain block of code should only run for a specific role. copysource\"show usage of the statsService from one node\" in within(15 seconds) {\n  runOn(second) {\n    assertServiceOk()\n  }\n\n  testConductor.enter(\"done-2\")\n}\n\ndef assertServiceOk(): Unit = {\n  val service = system.actorSelection(node(third) / \"user\" / \"statsService\")\n  // eventually the service should be ok,\n  // first attempts might fail because worker actors not started yet\n  awaitAssert {\n    service ! StatsJob(\"this is the text that will be analyzed\")\n    expectMsgType[StatsResult](1.second).meanWordLength should be(3.875 +- 0.001)\n  }\n\n} Once again we take advantage of the facilities in testkit to verify expected behavior. Here using testActor as sender (via ImplicitSender) and verifying the reply with expectMsgType. In the above code you can see node(third), which is useful facility to get the root actor reference of the actor system for a specific role. This can also be used to grab the actor.Address of that node. copysourceval firstAddress = node(first).address\nval secondAddress = node(second).address\nval thirdAddress = node(third).address\nCurrently testing with the sbt-multi-jvm plugin is only documented for Scala. Go to the corresponding Scala version of this page for details.","title":"How to Test"},{"location":"/cluster-usage.html#management","text":"There are several management tools for the cluster. Please refer to the Cluster Management for more information.","title":"Management"},{"location":"/cluster-usage.html#command-line","text":"Warning Deprecation warning - The command line script has been deprecated and is scheduled for removal in the next major version. Use the HTTP management API with curl or similar instead.\nThe cluster can be managed with the script pekko-cluster provided in the Pekko GitHub repository here. Place the script and the jmxsh-R5.jar library in the same directory.\nRun it without parameters to see instructions about how to use the script:\nUsage: ./pekko-cluster <node-hostname> <jmx-port> <command> ...\n\nSupported commands are:\n           join <node-url> - Sends request a JOIN node with the specified URL\n          leave <node-url> - Sends a request for node with URL to LEAVE the cluster\n           down <node-url> - Sends a request for marking node with URL as DOWN\n             member-status - Asks the member node for its current status\n                   members - Asks the cluster for addresses of current members\n               unreachable - Asks the cluster for addresses of unreachable members\n            cluster-status - Asks the cluster for its current status (member ring,\n                             unavailable nodes, meta data etc.)\n                    leader - Asks the cluster who the current leader is\n              is-singleton - Checks if the cluster is a singleton cluster (single\n                             node cluster)\n              is-available - Checks if the member node is available\nWhere the <node-url> should be on the format of\n  'pekko.<protocol>://<actor-system-name>@<hostname>:<port>'\n\nExamples: ./pekko-cluster localhost 9999 is-available\n          ./pekko-cluster localhost 9999 join pekko://MySystem@darkstar:2552\n          ./pekko-cluster localhost 9999 cluster-status\nTo be able to use the script you must enable remote monitoring and management when starting the JVMs of the cluster nodes, as described in Monitoring and Management Using JMX Technology. Make sure you understand the security implications of enabling remote monitoring and management.","title":"Command Line"},{"location":"/cluster-usage.html#configuration","text":"There are several configuration properties for the cluster, and the full reference configuration for complete information.","title":"Configuration"},{"location":"/cluster-routing.html","text":"","title":"Classic Cluster Aware Routers"},{"location":"/cluster-routing.html#classic-cluster-aware-routers","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see routers.\nAll routers can be made aware of member nodes in the cluster, i.e. deploying new routees or looking up routees on nodes in the cluster. When a node becomes unreachable or leaves the cluster the routees of that node are automatically unregistered from the router. When new nodes join the cluster, additional routees are added to the router, according to the configuration. Routees are also added when a node becomes reachable again, after having been unreachable.\nCluster aware routers make use of members with status WeaklyUp if that feature is enabled.\nThere are two distinct types of routers.\nGroup - router that sends messages to the specified path using actor selection The routees can be shared among routers running on different nodes in the cluster. One example of a use case for this type of router is a service running on some backend nodes in the cluster and used by routers running on front-end nodes in the cluster. Pool - router that creates routees as child actors and deploys them on remote nodes. Each router will have its own routee instances. For example, if you start a router on 3 nodes in a 10-node cluster, you will have 30 routees in total if the router is configured to use one instance per node. The routees created by the different routers will not be shared among the routers. One example of a use case for this type of router is a single master that coordinates jobs and delegates the actual work to routees running on other nodes in the cluster.","title":"Classic Cluster Aware Routers"},{"location":"/cluster-routing.html#dependency","text":"To use Cluster aware routers, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/cluster-routing.html#router-with-group-of-routees","text":"When using a Group you must start the routee actors on the cluster member nodes. That is not done by the router. The configuration for a group looks like this::\npekko.actor.deployment {\n  /statsService/workerRouter {\n      router = consistent-hashing-group\n      routees.paths = [\"/user/statsWorker\"]\n      cluster {\n        enabled = on\n        allow-local-routees = on\n        use-roles = [\"compute\"]\n      }\n    }\n}\nNote The routee actors should be started as early as possible when starting the actor system, because the router will try to use them as soon as the member status is changed to ‘Up’.\nThe actor paths that are defined in routees.paths are used for selecting the actors to which the messages will be forwarded to by the router. The path should not contain protocol and address information because they are retrieved dynamically from the cluster membership. Messages will be forwarded to the routees using ActorSelection, so the same delivery semantics should be expected. It is possible to limit the lookup of routees to member nodes tagged with a particular set of roles by specifying use-roles.\nmax-total-nr-of-instances defines total number of routees in the cluster. By default max-total-nr-of-instances is set to a high value (10000) that will result in new routees added to the router when nodes join the cluster. Set it to a lower value if you want to limit total number of routees.\nThe same type of router could also have been defined in code:\nScala copysourceimport org.apache.pekko\nimport pekko.cluster.routing.{ ClusterRouterGroup, ClusterRouterGroupSettings }\nimport pekko.routing.ConsistentHashingGroup\n\nval workerRouter = context.actorOf(\n  ClusterRouterGroup(\n    ConsistentHashingGroup(Nil),\n    ClusterRouterGroupSettings(\n      totalInstances = 100,\n      routeesPaths = List(\"/user/statsWorker\"),\n      allowLocalRoutees = true,\n      useRoles = Set(\"compute\"))).props(),\n  name = \"workerRouter2\") Java copysourceint totalInstances = 100;\nIterable<String> routeesPaths = Collections.singletonList(\"/user/statsWorker\");\nboolean allowLocalRoutees = true;\nSet<String> useRoles = new HashSet<>(Arrays.asList(\"compute\"));\nActorRef workerRouter =\n    getContext()\n        .actorOf(\n            new ClusterRouterGroup(\n                    new ConsistentHashingGroup(routeesPaths),\n                    new ClusterRouterGroupSettings(\n                        totalInstances, routeesPaths, allowLocalRoutees, useRoles))\n                .props(),\n            \"workerRouter2\");\nSee reference configuration for further descriptions of the settings.","title":"Router with Group of Routees"},{"location":"/cluster-routing.html#router-example-with-group-of-routees","text":"Let’s take a look at how to use a cluster aware router with a group of routees, i.e. router sending to the paths of the routees.\nThe example application provides a service to calculate statistics for a text. When some text is sent to the service it splits it into words, and delegates the task to count number of characters in each word to a separate worker, a routee of a router. The character count for each word is sent back to an aggregator that calculates the average number of characters per word when all results have been collected.\nMessages:\nScala copysourcefinal case class StatsJob(text: String) extends CborSerializable\nfinal case class StatsResult(meanWordLength: Double) extends CborSerializable\nfinal case class JobFailed(reason: String) extends CborSerializable Java copysourcepublic interface StatsMessages {\n\n  public static class StatsJob implements Serializable {\n    private final String text;\n\n    public StatsJob(String text) {\n      this.text = text;\n    }\n\n    public String getText() {\n      return text;\n    }\n  }\n\n  public static class StatsResult implements Serializable {\n    private final double meanWordLength;\n\n    public StatsResult(double meanWordLength) {\n      this.meanWordLength = meanWordLength;\n    }\n\n    public double getMeanWordLength() {\n      return meanWordLength;\n    }\n\n    @Override\n    public String toString() {\n      return \"meanWordLength: \" + meanWordLength;\n    }\n  }\n\n  public static class JobFailed implements Serializable {\n    private final String reason;\n\n    public JobFailed(String reason) {\n      this.reason = reason;\n    }\n\n    public String getReason() {\n      return reason;\n    }\n\n    @Override\n    public String toString() {\n      return \"JobFailed(\" + reason + \")\";\n    }\n  }\n}\nThe worker that counts number of characters in each word:\nScala copysourceclass StatsWorker extends Actor {\n  var cache = Map.empty[String, Int]\n  def receive = {\n    case word: String =>\n      val length = cache.get(word) match {\n        case Some(x) => x\n        case None =>\n          val x = word.length\n          cache += (word -> x)\n          x\n      }\n\n      sender() ! length\n  }\n} Java copysourcepublic class StatsWorker extends AbstractActor {\n\n  Map<String, Integer> cache = new HashMap<String, Integer>();\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            word -> {\n              Integer length = cache.get(word);\n              if (length == null) {\n                length = word.length();\n                cache.put(word, length);\n              }\n              getSender().tell(length, getSelf());\n            })\n        .build();\n  }\n}\nThe service that receives text from users and splits it up into words, delegates to workers and aggregates:\ncopysourceclass StatsService extends Actor {\n  // This router is used both with lookup and deploy of routees. If you\n  // have a router with only lookup of routees you can use Props.empty\n  // instead of Props[StatsWorker.class].\n  val workerRouter = context.actorOf(FromConfig.props(Props[StatsWorker]()), name = \"workerRouter\")\n\n  def receive = {\n    case StatsJob(text) if text != \"\" =>\n      val words = text.split(\" \")\n      val replyTo = sender() // important to not close over sender()\n      // create actor that collects replies from workers\n      val aggregator = context.actorOf(Props(classOf[StatsAggregator], words.size, replyTo))\n      words.foreach { word =>\n        workerRouter.tell(ConsistentHashableEnvelope(word, word), aggregator)\n      }\n  }\n}\n\nclass StatsAggregator(expectedResults: Int, replyTo: ActorRef) extends Actor {\n  var results = IndexedSeq.empty[Int]\n  context.setReceiveTimeout(3.seconds)\n\n  def receive = {\n    case wordCount: Int =>\n      results = results :+ wordCount\n      if (results.size == expectedResults) {\n        val meanWordLength = results.sum.toDouble / results.size\n        replyTo ! StatsResult(meanWordLength)\n        context.stop(self)\n      }\n    case ReceiveTimeout =>\n      replyTo ! JobFailed(\"Service unavailable, try again later\")\n      context.stop(self)\n  }\n}\ncopysourcepublic class StatsService extends AbstractActor {\n\n  // This router is used both with lookup and deploy of routees. If you\n  // have a router with only lookup of routees you can use Props.empty()\n  // instead of Props.create(StatsWorker.class).\n  ActorRef workerRouter =\n      getContext()\n          .actorOf(FromConfig.getInstance().props(Props.create(StatsWorker.class)), \"workerRouter\");\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            StatsJob.class,\n            job -> !job.getText().isEmpty(),\n            job -> {\n              String[] words = job.getText().split(\" \");\n              ActorRef replyTo = getSender();\n\n              // create actor that collects replies from workers\n              ActorRef aggregator =\n                  getContext().actorOf(Props.create(StatsAggregator.class, words.length, replyTo));\n\n              // send each word to a worker\n              for (String word : words) {\n                workerRouter.tell(new ConsistentHashableEnvelope(word, word), aggregator);\n              }\n            })\n        .build();\n  }\n} copysourcepublic class StatsAggregator extends AbstractActor {\n\n  final int expectedResults;\n  final ActorRef replyTo;\n  final List<Integer> results = new ArrayList<Integer>();\n\n  public StatsAggregator(int expectedResults, ActorRef replyTo) {\n    this.expectedResults = expectedResults;\n    this.replyTo = replyTo;\n  }\n\n  @Override\n  public void preStart() {\n    getContext().setReceiveTimeout(Duration.ofSeconds(3));\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Integer.class,\n            wordCount -> {\n              results.add(wordCount);\n              if (results.size() == expectedResults) {\n                int sum = 0;\n                for (int c : results) {\n                  sum += c;\n                }\n                double meanWordLength = ((double) sum) / results.size();\n                replyTo.tell(new StatsResult(meanWordLength), getSelf());\n                getContext().stop(getSelf());\n              }\n            })\n        .match(\n            ReceiveTimeout.class,\n            x -> {\n              replyTo.tell(new JobFailed(\"Service unavailable, try again later\"), getSelf());\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n}\nNote, nothing cluster specific so far, just plain actors.\nAll nodes start StatsService and StatsWorker actors. Remember, routees are the workers in this case. The router is configured with routees.paths::\npekko.actor.deployment {\n  /statsService/workerRouter {\n    router = consistent-hashing-group\n    routees.paths = [\"/user/statsWorker\"]\n    cluster {\n      enabled = on\n      allow-local-routees = on\n      use-roles = [\"compute\"]\n    }\n  }\n}\nThis means that user requests can be sent to StatsService on any node and it will use StatsWorker on all nodes.","title":"Router Example with Group of Routees"},{"location":"/cluster-routing.html#router-with-pool-of-remote-deployed-routees","text":"When using a Pool with routees created and deployed on the cluster member nodes the configuration for a router looks like this::\npekko.actor.deployment {\n  /statsService/singleton/workerRouter {\n      router = consistent-hashing-pool\n      cluster {\n        enabled = on\n        max-nr-of-instances-per-node = 3\n        allow-local-routees = on\n        use-roles = [\"compute\"]\n      }\n    }\n}\nIt is possible to limit the deployment of routees to member nodes tagged with a particular set of roles by specifying use-roles.\nmax-total-nr-of-instances defines total number of routees in the cluster, but the number of routees per node, max-nr-of-instances-per-node, will not be exceeded. By default max-total-nr-of-instances is set to a high value (10000) that will result in new routees added to the router when nodes join the cluster. Set it to a lower value if you want to limit total number of routees.\nThe same type of router could also have been defined in code:\nScala copysourceimport org.apache.pekko\nimport pekko.cluster.routing.{ ClusterRouterPool, ClusterRouterPoolSettings }\nimport pekko.routing.ConsistentHashingPool\n\nval workerRouter = context.actorOf(\n  ClusterRouterPool(\n    ConsistentHashingPool(0),\n    ClusterRouterPoolSettings(totalInstances = 100, maxInstancesPerNode = 3, allowLocalRoutees = false))\n    .props(Props[StatsWorker]()),\n  name = \"workerRouter3\") Java copysourceint totalInstances = 100;\nint maxInstancesPerNode = 3;\nboolean allowLocalRoutees = false;\nSet<String> useRoles = new HashSet<>(Arrays.asList(\"compute\"));\nActorRef workerRouter =\n    getContext()\n        .actorOf(\n            new ClusterRouterPool(\n                    new ConsistentHashingPool(0),\n                    new ClusterRouterPoolSettings(\n                        totalInstances, maxInstancesPerNode, allowLocalRoutees, useRoles))\n                .props(Props.create(StatsWorker.class)),\n            \"workerRouter3\");\nSee reference configuration for further descriptions of the settings.\nWhen using a pool of remote deployed routees you must ensure that all parameters of the Props can be serialized.","title":"Router with Pool of Remote Deployed Routees"},{"location":"/cluster-routing.html#router-example-with-pool-of-remote-deployed-routees","text":"Let’s take a look at how to use a cluster aware router on single master node that creates and deploys workers. To keep track of a single master we use the Cluster Singleton in the cluster-tools module. The ClusterSingletonManager is started on each node:\nScala system.actorOf(\n  ClusterSingletonManager.props(\n    singletonProps = Props[StatsService],\n    terminationMessage = PoisonPill,\n    settings = ClusterSingletonManagerSettings(system).withRole(\"compute\")),\n  name = \"statsService\")\n Java copysourceClusterSingletonManagerSettings settings =\n    ClusterSingletonManagerSettings.create(system).withRole(\"compute\");\nsystem.actorOf(\n    ClusterSingletonManager.props(\n        Props.create(StatsService.class), PoisonPill.getInstance(), settings),\n    \"statsService\");\nWe also need an actor on each node that keeps track of where current single master exists and delegates jobs to the StatsService. That is provided by the ClusterSingletonProxy:\nScala system.actorOf(\n  ClusterSingletonProxy.props(\n    singletonManagerPath = \"/user/statsService\",\n    settings = ClusterSingletonProxySettings(system).withRole(\"compute\")),\n  name = \"statsServiceProxy\")\n Java copysourceClusterSingletonProxySettings proxySettings =\n    ClusterSingletonProxySettings.create(system).withRole(\"compute\");\nsystem.actorOf(\n    ClusterSingletonProxy.props(\"/user/statsService\", proxySettings), \"statsServiceProxy\");\nThe ClusterSingletonProxy receives text from users and delegates to the current StatsService, the single master. It listens to cluster events to lookup the StatsService on the oldest node.\nAll nodes start ClusterSingletonProxy and the ClusterSingletonManager. The router is now configured like this::\npekko.actor.deployment {\n  /statsService/singleton/workerRouter {\n    router = consistent-hashing-pool\n    cluster {\n      enabled = on\n      max-nr-of-instances-per-node = 3\n      allow-local-routees = on\n      use-roles = [\"compute\"]\n    }\n  }\n}\nThe easiest way to run Router Example with Pool of Routees example yourself is to try the Pekko Cluster Sample with ScalaPekko Cluster Sample with Java. It contains instructions on how to run the Router Example with Pool of Routees sample.","title":"Router Example with Pool of Remote Deployed Routees"},{"location":"/cluster-singleton.html","text":"","title":"Classic Cluster Singleton"},{"location":"/cluster-singleton.html#classic-cluster-singleton","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see Cluster Singleton.","title":"Classic Cluster Singleton"},{"location":"/cluster-singleton.html#module-info","text":"To use Cluster Singleton, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-tools\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-tools_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-tools_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster Tools (classic) Artifact org.apache.pekko pekko-cluster-tools 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.tools License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/cluster-singleton.html#introduction","text":"For the full documentation of this feature and for new projects see Cluster Singleton - Introduction.\nThe cluster singleton pattern is implemented by org.apache.pekko.cluster.singleton.ClusterSingletonManager. It manages one singleton actor instance among all cluster nodes or a group of nodes tagged with a specific role. ClusterSingletonManager is an actor that is supposed to be started as early as possible on all nodes, or all nodes with specified role, in the cluster. The actual singleton actor is started by the ClusterSingletonManager on the oldest node by creating a child actor from supplied Props. ClusterSingletonManager makes sure that at most one singleton instance is running at any point in time.\nYou can access the singleton actor by using the provided org.apache.pekko.cluster.singleton.ClusterSingletonProxy, which will route all messages to the current instance of the singleton. The proxy will keep track of the oldest node in the cluster and resolve the singleton’s ActorRef by explicitly sending the singleton’s actorSelection the org.apache.pekko.actor.Identify message and waiting for it to reply. This is performed periodically if the singleton doesn’t reply within a certain (configurable) time. Given the implementation, there might be periods of time during which the ActorRef is unavailable, e.g., when a node leaves the cluster. In these cases, the proxy will buffer the messages sent to the singleton and then deliver them when the singleton is finally available. If the buffer is full the ClusterSingletonProxy will drop old messages when new messages are sent via the proxy. The size of the buffer is configurable and it can be disabled by using a buffer size of 0.\nSee Cluster Singleton - Potential problems to be aware of.","title":"Introduction"},{"location":"/cluster-singleton.html#an-example","text":"Assume that we need one single entry point to an external system. An actor that receives messages from a JMS queue with the strict requirement that only one JMS consumer must exist to make sure that the messages are processed in order. That is perhaps not how one would like to design things, but a typical real-world scenario when integrating with external systems.\nBefore explaining how to create a cluster singleton actor, let’s define message classes and their corresponding factory methods which will be used by the singleton.\nScala copysourceobject PointToPointChannel {\n  case object UnregistrationOk extends CborSerializable\n}\nobject Consumer {\n  case object End extends CborSerializable\n  case object GetCurrent extends CborSerializable\n  case object Ping extends CborSerializable\n  case object Pong extends CborSerializable\n} Java copysourcepublic class TestSingletonMessages {\n  public static class UnregistrationOk {}\n\n  public static class End {}\n\n  public static class Ping {}\n\n  public static class Pong {}\n\n  public static class GetCurrent {}\n\n  public static UnregistrationOk unregistrationOk() {\n    return new UnregistrationOk();\n  }\n\n  public static End end() {\n    return new End();\n  }\n\n  public static Ping ping() {\n    return new Ping();\n  }\n\n  public static Pong pong() {\n    return new Pong();\n  }\n\n  public static GetCurrent getCurrent() {\n    return new GetCurrent();\n  }\n}\nOn each node in the cluster you need to start the ClusterSingletonManager and supply the Props of the singleton actor, in this case the JMS queue consumer.\nScala copysourcesystem.actorOf(\n  ClusterSingletonManager.props(\n    singletonProps = Props(classOf[Consumer], queue, testActor),\n    terminationMessage = End,\n    settings = ClusterSingletonManagerSettings(system).withRole(\"worker\")),\n  name = \"consumer\") Java copysourcefinal ClusterSingletonManagerSettings settings =\n    ClusterSingletonManagerSettings.create(system).withRole(\"worker\");\n\nsystem.actorOf(\n    ClusterSingletonManager.props(\n        Props.create(Consumer.class, () -> new Consumer(queue, testActor)),\n        TestSingletonMessages.end(),\n        settings),\n    \"consumer\");\nHere we limit the singleton to nodes tagged with the \"worker\" role, but all nodes, independent of role, can be used by not specifying withRole.\nWe use an application specific terminationMessage (i.e. TestSingletonMessages.end() message) to be able to close the resources before actually stopping the singleton actor. Note that PoisonPill is a perfectly fine terminationMessage if you only need to stop the actor.\nHere is how the singleton actor handles the terminationMessage in this example.\nScala copysourcecase End =>\n  queue ! UnregisterConsumer\ncase UnregistrationOk =>\n  stoppedBeforeUnregistration = false\n  context.stop(self)\ncase Ping =>\n  sender() ! Pong Java copysource.match(End.class, message -> queue.tell(UnregisterConsumer.class, getSelf()))\n.match(\n    UnregistrationOk.class,\n    message -> {\n      stoppedBeforeUnregistration = false;\n      getContext().stop(getSelf());\n    })\n.match(Ping.class, message -> getSender().tell(TestSingletonMessages.pong(), getSelf()))\nWith the names given above, access to the singleton can be obtained from any cluster node using a properly configured proxy.\nScala copysourceval proxy = system.actorOf(\n  ClusterSingletonProxy.props(\n    singletonManagerPath = \"/user/consumer\",\n    settings = ClusterSingletonProxySettings(system).withRole(\"worker\")),\n  name = \"consumerProxy\") Java copysourceClusterSingletonProxySettings proxySettings =\n    ClusterSingletonProxySettings.create(system).withRole(\"worker\");\n\nActorRef proxy =\n    system.actorOf(\n        ClusterSingletonProxy.props(\"/user/consumer\", proxySettings), \"consumerProxy\");","title":"An Example"},{"location":"/cluster-singleton.html#configuration","text":"For the full documentation of this feature and for new projects see Cluster Singleton - configuration.","title":"Configuration"},{"location":"/cluster-singleton.html#supervision","text":"There are two actors that could potentially be supervised. For the consumer singleton created above these would be:\nCluster singleton manager e.g. /user/consumer which runs on every node in the cluster The user actor e.g. /user/consumer/singleton which the manager starts on the oldest node\nThe Cluster singleton manager actor should not have its supervision strategy changed as it should always be running. However it is sometimes useful to add supervision for the user actor. To accomplish this add a parent supervisor actor which will be used to create the ‘real’ singleton instance. Below is an example implementation (credit to this StackOverflow answer)\nScala copysourceimport org.apache.pekko.actor.{ Actor, Props, SupervisorStrategy }\nclass SupervisorActor(childProps: Props, override val supervisorStrategy: SupervisorStrategy) extends Actor {\n  val child = context.actorOf(childProps, \"supervised-child\")\n\n  def receive = {\n    case msg => child.forward(msg)\n  }\n} Java copysourceimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.actor.AbstractActor.Receive;\nimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.Props;\nimport org.apache.pekko.actor.SupervisorStrategy;\n\npublic class SupervisorActor extends AbstractActor {\n  final Props childProps;\n  final SupervisorStrategy supervisorStrategy;\n  final ActorRef child;\n\n  SupervisorActor(Props childProps, SupervisorStrategy supervisorStrategy) {\n    this.childProps = childProps;\n    this.supervisorStrategy = supervisorStrategy;\n    this.child = getContext().actorOf(childProps, \"supervised-child\");\n  }\n\n  @Override\n  public SupervisorStrategy supervisorStrategy() {\n    return supervisorStrategy;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().matchAny(msg -> child.forward(msg, getContext())).build();\n  }\n}\nAnd used here\nScala copysourceimport org.apache.pekko\nimport pekko.actor.{ PoisonPill, Props }\nimport pekko.cluster.singleton.{ ClusterSingletonManager, ClusterSingletonManagerSettings }\ncontext.system.actorOf(\n  ClusterSingletonManager.props(\n    singletonProps = Props(classOf[SupervisorActor], props, supervisorStrategy),\n    terminationMessage = PoisonPill,\n    settings = ClusterSingletonManagerSettings(context.system)),\n  name = name) Java copysourceimport org.apache.pekko.actor.PoisonPill;\nimport org.apache.pekko.actor.Props;\nimport org.apache.pekko.cluster.singleton.ClusterSingletonManager;\nimport org.apache.pekko.cluster.singleton.ClusterSingletonManagerSettings; copysourcereturn getContext()\n    .system()\n    .actorOf(\n        ClusterSingletonManager.props(\n            Props.create(\n                SupervisorActor.class, () -> new SupervisorActor(props, supervisorStrategy)),\n            PoisonPill.getInstance(),\n            ClusterSingletonManagerSettings.create(getContext().system())),\n        name = name);","title":"Supervision"},{"location":"/cluster-singleton.html#lease","text":"For the full documentation of this feature and for new projects see Cluster Singleton - Lease.","title":"Lease"},{"location":"/distributed-pub-sub.html","text":"","title":"Classic Distributed Publish Subscribe in Cluster"},{"location":"/distributed-pub-sub.html#classic-distributed-publish-subscribe-in-cluster","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the new API see Distributed Publish Subscribe in Cluster","title":"Classic Distributed Publish Subscribe in Cluster"},{"location":"/distributed-pub-sub.html#module-info","text":"To use Distributed Publish Subscribe you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-tools\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-tools_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-tools_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster Tools (classic) Artifact org.apache.pekko pekko-cluster-tools 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.tools License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/distributed-pub-sub.html#introduction","text":"How do I send a message to an actor without knowing which node it is running on?\nHow do I send messages to all actors in the cluster that have registered interest in a named topic?\nThis pattern provides a mediator actor, cluster.pubsub.DistributedPubSubMediatorcluster.pubsub.DistributedPubSubMediator, that manages a registry of actor references and replicates the entries to peer actors among all cluster nodes or a group of nodes tagged with a specific role.\nThe DistributedPubSubMediator actor is supposed to be started on all nodes, or all nodes with specified role, in the cluster. The mediator can be started with the DistributedPubSubDistributedPubSub extension or as an ordinary actor.\nThe registry is eventually consistent, i.e. changes are not immediately visible at other nodes, but typically they will be fully replicated to all other nodes after a few seconds. Changes are only performed in the own part of the registry and those changes are versioned. Deltas are disseminated in a scalable way to other nodes with a gossip protocol.\nCluster members with status WeaklyUp, will participate in Distributed Publish Subscribe, i.e. subscribers on nodes with WeaklyUp status will receive published messages if the publisher and subscriber are on same side of a network partition.\nYou can send messages via the mediator on any node to registered actors on any other node.\nThere a two different modes of message delivery, explained in the sections Publish and Send below.\nA more comprehensive sample is available in the tutorial named Pekko Clustered PubSub with Scala!.","title":"Introduction"},{"location":"/distributed-pub-sub.html#publish","text":"This is the true pub/sub mode. A typical usage of this mode is a chat room in an instant messaging application.\nActors are registered to a named topic. This enables many subscribers on each node. The message will be delivered to all subscribers of the topic.\nFor efficiency the message is sent over the wire only once per node (that has a matching topic), and then delivered to all subscribers of the local topic representation.\nYou register actors to the local mediator with DistributedPubSubMediator.Subscribe. Successful DistributedPubSubMediator.SubscribeDistributedPubSubMediator.Subscribe and DistributedPubSubMediator.UnsubscribeDistributedPubSubMediator.Unsubscribe is acknowledged with DistributedPubSubMediator.SubscribeAckDistributedPubSubMediator.SubscribeAck and DistributedPubSubMediator.UnsubscribeAckDistributedPubSubMediator.UnsubscribeAck replies. The acknowledgment means that the subscription is registered, but it can still take some time until it is replicated to other nodes.\nYou publish messages by sending DistributedPubSubMediator.PublishDistributedPubSubMediator.Publish message to the local mediator.\nActors are automatically removed from the registry when they are terminated, or you can explicitly remove entries with DistributedPubSubMediator.Unsubscribe.\nAn example of a subscriber actor:\nScala copysourceclass Subscriber extends Actor with ActorLogging {\n  import DistributedPubSubMediator.{ Subscribe, SubscribeAck }\n  val mediator = DistributedPubSub(context.system).mediator\n  // subscribe to the topic named \"content\"\n  mediator ! Subscribe(\"content\", self)\n\n  def receive = {\n    case s: String =>\n      log.info(\"Got {}\", s)\n    case SubscribeAck(Subscribe(\"content\", None, `self`)) =>\n      log.info(\"subscribing\")\n  }\n} Java copysourcestatic class Subscriber extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().system(), this);\n\n  public Subscriber() {\n    ActorRef mediator = DistributedPubSub.get(getContext().system()).mediator();\n    // subscribe to the topic named \"content\"\n    mediator.tell(new DistributedPubSubMediator.Subscribe(\"content\", getSelf()), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(String.class, msg -> log.info(\"Got: {}\", msg))\n        .match(DistributedPubSubMediator.SubscribeAck.class, msg -> log.info(\"subscribed\"))\n        .build();\n  }\n}\nSubscriber actors can be started on several nodes in the cluster, and all will receive messages published to the “content” topic.\nScala copysourcerunOn(first) {\n  system.actorOf(Props[Subscriber](), \"subscriber1\")\n}\nrunOn(second) {\n  system.actorOf(Props[Subscriber](), \"subscriber2\")\n  system.actorOf(Props[Subscriber](), \"subscriber3\")\n} Java copysourcesystem.actorOf(Props.create(Subscriber.class), \"subscriber1\");\n// another node\nsystem.actorOf(Props.create(Subscriber.class), \"subscriber2\");\nsystem.actorOf(Props.create(Subscriber.class), \"subscriber3\");\nA simple actor that publishes to this “content” topic:\nScala copysourceclass Publisher extends Actor {\n  import DistributedPubSubMediator.Publish\n  // activate the extension\n  val mediator = DistributedPubSub(context.system).mediator\n\n  def receive = {\n    case in: String =>\n      val out = in.toUpperCase\n      mediator ! Publish(\"content\", out)\n  }\n} Java copysourcestatic class Publisher extends AbstractActor {\n\n  // activate the extension\n  ActorRef mediator = DistributedPubSub.get(getContext().system()).mediator();\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            in -> {\n              String out = in.toUpperCase();\n              mediator.tell(new DistributedPubSubMediator.Publish(\"content\", out), getSelf());\n            })\n        .build();\n  }\n}\nIt can publish messages to the topic from anywhere in the cluster:\nScala copysourcerunOn(third) {\n  val publisher = system.actorOf(Props[Publisher](), \"publisher\")\n  later()\n  // after a while the subscriptions are replicated\n  publisher ! \"hello\"\n} Java copysource// somewhere else\nActorRef publisher = system.actorOf(Props.create(Publisher.class), \"publisher\");\n// after a while the subscriptions are replicated\npublisher.tell(\"hello\", null);","title":"Publish"},{"location":"/distributed-pub-sub.html#topic-groups","text":"Actors may also be subscribed to a named topic with a group id. If subscribing with a group id, each message published to a topic with the sendOneMessageToEachGroup flag set to true is delivered via the supplied RoutingLogicRoutingLogic (default random) to one actor within each subscribing group.\nIf all the subscribed actors have the same group id, then this works just like DistributedPubSubMediator.SendDistributedPubSubMediator.Send and each message is only delivered to one subscriber.\nIf all the subscribed actors have different group names, then this works like normal DistributedPubSubMediator.PublishDistributedPubSubMediator.Publish and each message is broadcasted to all subscribers.\nNote Note that if the group id is used it is part of the topic identifier. Messages published with sendOneMessageToEachGroup=false will not be delivered to subscribers that subscribed with a group id. Messages published with sendOneMessageToEachGroup=true will not be delivered to subscribers that subscribed without a group id.","title":"Topic Groups"},{"location":"/distributed-pub-sub.html#send","text":"This is a point-to-point mode where each message is delivered to one destination, but you still do not have to know where the destination is located. A typical usage of this mode is private chat to one other user in an instant messaging application. It can also be used for distributing tasks to registered workers, like a cluster aware router where the routees dynamically can register themselves.\nThe message will be delivered to one recipient with a matching path, if any such exists in the registry. If several entries match the path because it has been registered on several nodes the message will be sent via the supplied RoutingLogicRoutingLogic (default random) to one destination. The sender of the message can specify that local affinity is preferred, i.e. the message is sent to an actor in the same local actor system as the used mediator actor, if any such exists, otherwise route to any other matching entry.\nYou register actors to the local mediator with DistributedPubSubMediator.PutDistributedPubSubMediator.Put. The ActorRefActorRef in Put must belong to the same local actor system as the mediator. The path without address information is the key to which you send messages. On each node there can only be one actor for a given path, since the path is unique within one local actor system.\nYou send messages by sending DistributedPubSubMediator.SendDistributedPubSubMediator.Send message to the local mediator with the path (without address information) of the destination actors.\nActors are automatically removed from the registry when they are terminated, or you can explicitly remove entries with DistributedPubSubMediator.RemoveDistributedPubSubMediator.Remove.\nAn example of a destination actor:\nScala copysourceclass Destination extends Actor with ActorLogging {\n  import DistributedPubSubMediator.Put\n  val mediator = DistributedPubSub(context.system).mediator\n  // register to the path\n  mediator ! Put(self)\n\n  def receive = {\n    case s: String =>\n      log.info(\"Got {}\", s)\n  }\n} Java copysourcestatic class Destination extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().system(), this);\n\n  public Destination() {\n    ActorRef mediator = DistributedPubSub.get(getContext().system()).mediator();\n    // register to the path\n    mediator.tell(new DistributedPubSubMediator.Put(getSelf()), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder().match(String.class, msg -> log.info(\"Got: {}\", msg)).build();\n  }\n}\nDestination actors can be started on several nodes in the cluster, and all will receive messages sent to the path (without address information).\nScala copysourcerunOn(first) {\n  system.actorOf(Props[Destination](), \"destination\")\n}\nrunOn(second) {\n  system.actorOf(Props[Destination](), \"destination\")\n} Java copysourcesystem.actorOf(Props.create(Destination.class), \"destination\");\n// another node\nsystem.actorOf(Props.create(Destination.class), \"destination\");\nA simple actor that sends to the path:\nScala copysourceclass Sender extends Actor {\n  import DistributedPubSubMediator.Send\n  // activate the extension\n  val mediator = DistributedPubSub(context.system).mediator\n\n  def receive = {\n    case in: String =>\n      val out = in.toUpperCase\n      mediator ! Send(path = \"/user/destination\", msg = out, localAffinity = true)\n  }\n} Java copysourcestatic class Sender extends AbstractActor {\n\n  // activate the extension\n  ActorRef mediator = DistributedPubSub.get(getContext().system()).mediator();\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            in -> {\n              String out = in.toUpperCase();\n              boolean localAffinity = true;\n              mediator.tell(\n                  new DistributedPubSubMediator.Send(\"/user/destination\", out, localAffinity),\n                  getSelf());\n            })\n        .build();\n  }\n}\nIt can send messages to the path from anywhere in the cluster:\nScala copysourcerunOn(third) {\n  val sender = system.actorOf(Props[Sender](), \"sender\")\n  later()\n  // after a while the destinations are replicated\n  sender ! \"hello\"\n} Java copysource// somewhere else\nActorRef sender = system.actorOf(Props.create(Publisher.class), \"sender\");\n// after a while the destinations are replicated\nsender.tell(\"hello\", null);\nIt is also possible to broadcast messages to the actors that have been registered with DistributedPubSubMediator.PutDistributedPubSubMediator.Put. Send DistributedPubSubMediator.SendToAllDistributedPubSubMediator.SendToAll message to the local mediator and the wrapped message will then be delivered to all recipients with a matching path. Actors with the same path, without address information, can be registered on different nodes. On each node there can only be one such actor, since the path is unique within one local actor system.\nTypical usage of this mode is to broadcast messages to all replicas with the same path, e.g. 3 actors on different nodes that all perform the same actions, for redundancy. You can also optionally specify a property (allButSelf) deciding if the message should be sent to a matching path on the self node or not.","title":"Send"},{"location":"/distributed-pub-sub.html#distributedpubsub-extension","text":"In the example above the mediator is started and accessed with the cluster.pubsub.DistributedPubSubcluster.pubsub.DistributedPubSub extension. That is convenient and perfectly fine in most cases, but it can be good to know that it is possible to start the mediator actor as an ordinary actor and you can have several different mediators at the same time to be able to divide a large number of actors/topics to different mediators. For example you might want to use different cluster roles for different mediators.\nThe DistributedPubSub extension can be configured with the following properties:\ncopysource# Settings for the DistributedPubSub extension\npekko.cluster.pub-sub {\n  # Actor name of the mediator actor, /system/distributedPubSubMediator\n  name = distributedPubSubMediator\n\n  # Start the mediator on members tagged with this role.\n  # All members are used if undefined or empty.\n  role = \"\"\n\n  # The routing logic to use for 'Send'\n  # Possible values: random, round-robin, broadcast\n  routing-logic = random\n\n  # How often the DistributedPubSubMediator should send out gossip information\n  gossip-interval = 1s\n\n  # Removed entries are pruned after this duration\n  removed-time-to-live = 120s\n\n  # Maximum number of elements to transfer in one message when synchronizing the registries.\n  # Next chunk will be transferred in next round of gossip.\n  max-delta-elements = 3000\n\n  # When a message is published to a topic with no subscribers send it to the dead letters.\n  send-to-dead-letters-when-no-subscribers = on\n  \n  # The id of the dispatcher to use for DistributedPubSubMediator actors. \n  # If specified you need to define the settings of the actual dispatcher.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n}\nIt is recommended to load the extension when the actor system is started by defining it in pekko.extensions configuration property. Otherwise it will be activated when first used and then it takes a while for it to be populated.\npekko.extensions = [\"org.apache.pekko.cluster.pubsub.DistributedPubSub\"]","title":"DistributedPubSub Extension"},{"location":"/distributed-pub-sub.html#delivery-guarantee","text":"As in Message Delivery Reliability of Pekko, message delivery guarantee in distributed pub sub modes is at-most-once delivery. In other words, messages can be lost over the wire.\nIf you are looking for at-least-once delivery guarantee, we recommend Pekko Connectors.","title":"Delivery Guarantee"},{"location":"/cluster-client.html","text":"","title":"Classic Cluster Client"},{"location":"/cluster-client.html#classic-cluster-client","text":"Warning Cluster Client is deprecated in favor of using Pekko gRPC. It is not advised to build new applications with Cluster Client.\nNote Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.","title":"Classic Cluster Client"},{"location":"/cluster-client.html#module-info","text":"To use Cluster Client, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-tools\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-tools_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-tools_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster Tools (classic) Artifact org.apache.pekko pekko-cluster-tools 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.tools License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/cluster-client.html#introduction","text":"An actor system that is not part of the cluster can communicate with actors somewhere in the cluster via the ClusterClientClusterClient, the client can run in an ActorSystem that is part of another cluster. It only needs to know the location of one (or more) nodes to use as initial contact points. It will establish a connection to a ClusterReceptionistClusterReceptionist somewhere in the cluster. It will monitor the connection to the receptionist and establish a new connection if the link goes down. When looking for a new receptionist it uses fresh contact points retrieved from the previous establishment, or periodically refreshed contacts, i.e. not necessarily the initial contact points.\nUsing the ClusterClientClusterClient for communicating with a cluster from the outside requires that the system with the client can both connect and be connected with Pekko Remoting from all the nodes in the cluster with a receptionist. This creates a tight coupling in that the client and cluster systems may need to have the same version of both Pekko, libraries, message classes, serializers and potentially even the JVM. In many cases it is a better solution to use a more explicit and decoupling protocol such as HTTP or gRPC.\nAdditionally, since Pekko Remoting is primarily designed as a protocol for Pekko Cluster there is no explicit resource management, when a ClusterClientClusterClient has been used it will cause connections with the cluster until the ActorSystem is stopped (unlike other kinds of network clients).\nClusterClientClusterClient should not be used when sending messages to actors that run within the same cluster. Similar functionality as the ClusterClientClusterClient is provided more efficiently by Distributed Publish Subscribe in Cluster for actors that belong to the same cluster.\nThe connecting system must have its org.apache.pekko.actor.provider set to remote or cluster when using the cluster client.\nThe receptionist is supposed to be started on all nodes, or all nodes with a specified role, in the cluster. The receptionist can be started with the ClusterReceptionistClusterReceptionist extension or as an ordinary actor.\nYou can send messages via the ClusterClientClusterClient to any actor in the cluster that is registered in the DistributedPubSubMediatorDistributedPubSubMediator used by the ClusterReceptionistClusterReceptionist. The ClusterClientReceptionistClusterClientReceptionist provides methods for registration of actors that should be reachable from the client. Messages are wrapped in ClusterClient.Send, `ClusterClient.SendToAll`ClusterClient.SendToAll or `ClusterClient.Publish`ClusterClient.Publish.\nBoth the ClusterClientClusterClient and the ClusterClientReceptionistClusterClientReceptionist emit events that can be subscribed to. The ClusterClientClusterClient sends out notifications about the list of contact points received from the ClusterClientReceptionistClusterClientReceptionist. One use of this list might be for the client to record its contact points. A client that is restarted could then use this information to supersede any previously configured contact points.\nThe ClusterClientReceptionistClusterClientReceptionist sends out notifications in relation to having received a contact from a ClusterClientClusterClient. This notification enables the server containing the receptionist to become aware of what clients are connected to.\nClusterClient.Send\nThe message will be delivered to one recipient with a matching path if any such exists. If several entries match the path the message will be delivered to one random destination. The sender of the message can specify that local affinity is preferred, i.e. the message is sent to an actor in the same local actor system as the used receptionist actor, if any such exists, otherwise random to any other matching entry.\nClusterClient.SendToAll\nThe message will be delivered to all recipients with a matching path.\nClusterClient.Publish\nThe message will be delivered to all recipients Actors that have been registered as subscribers to the named topic.\nResponse messages from the destination actor are tunneled via the receptionist to avoid inbound connections from other cluster nodes to the client:\n`sender()``getSender()`, as seen by the destination actor, is not the client itself, but the receptionist `sender()` `getSender()` of the response messages, sent back from the destination and seen by the client, is deadLetters\nsince the client should normally send subsequent messages via the ClusterClientClusterClient. It is possible to pass the original sender inside the reply messages if the client is supposed to communicate directly to the actor in the cluster.\nWhile establishing a connection to a receptionist the ClusterClientClusterClient will buffer messages and send them when the connection is established. If the buffer is full the ClusterClientClusterClient will drop old messages when new messages are sent via the client. The size of the buffer is configurable and it can be disabled by using a buffer size of 0.\nIt’s worth noting that messages can always be lost because of the distributed nature of these actors. As always, additional logic should be implemented in the destination (acknowledgement) and in the client (retry) actors to ensure at-least-once message delivery.","title":"Introduction"},{"location":"/cluster-client.html#an-example","text":"On the cluster nodes, first start the receptionist. Note, it is recommended to load the extension when the actor system is started by defining it in the pekko.extensions configuration property:\npekko.extensions = [\"org.apache.pekko.cluster.client.ClusterClientReceptionist\"]\nNext, register the actors that should be available for the client.\nScala copysourcerunOn(host1) {\n  val serviceA = system.actorOf(Props[Service](), \"serviceA\")\n  ClusterClientReceptionist(system).registerService(serviceA)\n}\n\nrunOn(host2, host3) {\n  val serviceB = system.actorOf(Props[Service](), \"serviceB\")\n  ClusterClientReceptionist(system).registerService(serviceB)\n} Java copysourceActorRef serviceA = system.actorOf(Props.create(Service.class), \"serviceA\");\nClusterClientReceptionist.get(system).registerService(serviceA);\n\nActorRef serviceB = system.actorOf(Props.create(Service.class), \"serviceB\");\nClusterClientReceptionist.get(system).registerService(serviceB);\nOn the client, you create the ClusterClientClusterClient actor and use it as a gateway for sending messages to the actors identified by their path (without address information) somewhere in the cluster.\nScala copysourcerunOn(client) {\n  val c = system.actorOf(\n    ClusterClient.props(ClusterClientSettings(system).withInitialContacts(initialContacts)),\n    \"client\")\n  c ! ClusterClient.Send(\"/user/serviceA\", \"hello\", localAffinity = true)\n  c ! ClusterClient.SendToAll(\"/user/serviceB\", \"hi\")\n} Java copysourcefinal ActorRef c =\n    system.actorOf(\n        ClusterClient.props(\n            ClusterClientSettings.create(system).withInitialContacts(initialContacts())),\n        \"client\");\nc.tell(new ClusterClient.Send(\"/user/serviceA\", \"hello\", true), ActorRef.noSender());\nc.tell(new ClusterClient.SendToAll(\"/user/serviceB\", \"hi\"), ActorRef.noSender());\nThe initialContacts parameter is a Set[ActorPath]Set<ActorPath>, which can be created like this:\nScala copysourceval initialContacts = Set(\n  ActorPath.fromString(\"akka://OtherSys@host1:2552/system/receptionist\"),\n  ActorPath.fromString(\"akka://OtherSys@host2:2552/system/receptionist\"))\nval settings = ClusterClientSettings(system).withInitialContacts(initialContacts) Java copysourceSet<ActorPath> initialContacts() {\n  return new HashSet<ActorPath>(\n      Arrays.asList(\n          ActorPaths.fromString(\"akka://OtherSys@host1:2552/system/receptionist\"),\n          ActorPaths.fromString(\"akka://OtherSys@host2:2552/system/receptionist\")));\n}\nYou will probably define the address information of the initial contact points in configuration or system property. See also Configuration.","title":"An Example"},{"location":"/cluster-client.html#clusterclientreceptionist-extension","text":"In the example above the receptionist is started and accessed with the org.apache.pekko.cluster.client.ClusterClientReceptionist extension. That is convenient and perfectly fine in most cases, but it can be good to know that it is possible to start the org.apache.pekko.cluster.client.ClusterReceptionist actor as an ordinary actor and you can have several different receptionists at the same time, serving different types of clients.\nNote that the ClusterClientReceptionistClusterClientReceptionist uses the DistributedPubSubDistributedPubSub extension, which is described in Distributed Publish Subscribe in Cluster.\nIt is recommended to load the extension when the actor system is started by defining it in the pekko.extensions configuration property:\npekko.extensions = [\"pekko.cluster.client.ClusterClientReceptionist\"]","title":"ClusterClientReceptionist Extension"},{"location":"/cluster-client.html#events","text":"As mentioned earlier, both the ClusterClientClusterClient and ClusterClientReceptionistClusterClientReceptionist emit events that can be subscribed to. The following code snippet declares an actor that will receive notifications on contact points (addresses to the available receptionists), as they become available. The code illustrates subscribing to the events and receiving the ClusterClientClusterClient initial state.\nScala copysourceclass ClientListener(targetClient: ActorRef) extends Actor {\n  override def preStart(): Unit =\n    targetClient ! SubscribeContactPoints\n\n  def receive: Receive =\n    receiveWithContactPoints(Set.empty)\n\n  def receiveWithContactPoints(contactPoints: Set[ActorPath]): Receive = {\n    case ContactPoints(cps) =>\n      context.become(receiveWithContactPoints(cps))\n    // Now do something with the up-to-date \"cps\"\n    case ContactPointAdded(cp) =>\n      context.become(receiveWithContactPoints(contactPoints + cp))\n    // Now do something with an up-to-date \"contactPoints + cp\"\n    case ContactPointRemoved(cp) =>\n      context.become(receiveWithContactPoints(contactPoints - cp))\n    // Now do something with an up-to-date \"contactPoints - cp\"\n  }\n} Java copysourcepublic static class ClientListener extends AbstractActor {\n  private final ActorRef targetClient;\n  private final Set<ActorPath> contactPoints = new HashSet<>();\n\n  public ClientListener(ActorRef targetClient) {\n    this.targetClient = targetClient;\n  }\n\n  @Override\n  public void preStart() {\n    targetClient.tell(SubscribeContactPoints.getInstance(), sender());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            ContactPoints.class,\n            msg -> {\n              contactPoints.addAll(msg.getContactPoints());\n              // Now do something with an up-to-date \"contactPoints\"\n            })\n        .match(\n            ContactPointAdded.class,\n            msg -> {\n              contactPoints.add(msg.contactPoint());\n              // Now do something with an up-to-date \"contactPoints\"\n            })\n        .match(\n            ContactPointRemoved.class,\n            msg -> {\n              contactPoints.remove(msg.contactPoint());\n              // Now do something with an up-to-date \"contactPoints\"\n            })\n        .build();\n  }\n}\nSimilarly we can have an actor that behaves in a similar fashion for learning what cluster clients are connected to a ClusterClientReceptionistClusterClientReceptionist:\nScala copysourceclass ReceptionistListener(targetReceptionist: ActorRef) extends Actor {\n  override def preStart(): Unit =\n    targetReceptionist ! SubscribeClusterClients\n\n  def receive: Receive =\n    receiveWithClusterClients(Set.empty)\n\n  def receiveWithClusterClients(clusterClients: Set[ActorRef]): Receive = {\n    case ClusterClients(cs) =>\n      context.become(receiveWithClusterClients(cs))\n    // Now do something with the up-to-date \"c\"\n    case ClusterClientUp(c) =>\n      context.become(receiveWithClusterClients(clusterClients + c))\n    // Now do something with an up-to-date \"clusterClients + c\"\n    case ClusterClientUnreachable(c) =>\n      context.become(receiveWithClusterClients(clusterClients - c))\n    // Now do something with an up-to-date \"clusterClients - c\"\n  }\n} Java copysourcepublic static class ReceptionistListener extends AbstractActor {\n  private final ActorRef targetReceptionist;\n  private final Set<ActorRef> clusterClients = new HashSet<>();\n\n  public ReceptionistListener(ActorRef targetReceptionist) {\n    this.targetReceptionist = targetReceptionist;\n  }\n\n  @Override\n  public void preStart() {\n    targetReceptionist.tell(SubscribeClusterClients.getInstance(), sender());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            ClusterClients.class,\n            msg -> {\n              clusterClients.addAll(msg.getClusterClients());\n              // Now do something with an up-to-date \"clusterClients\"\n            })\n        .match(\n            ClusterClientUp.class,\n            msg -> {\n              clusterClients.add(msg.clusterClient());\n              // Now do something with an up-to-date \"clusterClients\"\n            })\n        .match(\n            ClusterClientUnreachable.class,\n            msg -> {\n              clusterClients.remove(msg.clusterClient());\n              // Now do something with an up-to-date \"clusterClients\"\n            })\n        .build();\n  }\n}","title":"Events"},{"location":"/cluster-client.html#configuration","text":"The ClusterClientReceptionistClusterClientReceptionist extension (or ClusterReceptionistSettingsClusterReceptionistSettings) can be configured with the following properties:\ncopysource# Settings for the ClusterClientReceptionist extension\npekko.cluster.client.receptionist {\n  # Actor name of the ClusterReceptionist actor, /system/receptionist\n  name = receptionist\n\n  # Start the receptionist on members tagged with this role.\n  # All members are used if undefined or empty.\n  role = \"\"\n\n  # The receptionist will send this number of contact points to the client\n  number-of-contacts = 3\n\n  # The actor that tunnel response messages to the client will be stopped\n  # after this time of inactivity.\n  response-tunnel-receive-timeout = 30s\n  \n  # The id of the dispatcher to use for ClusterReceptionist actors.\n  # If specified you need to define the settings of the actual dispatcher.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n  # How often failure detection heartbeat messages should be received for\n  # each ClusterClient\n  heartbeat-interval = 2s\n\n  # Number of potentially lost/delayed heartbeats that will be\n  # accepted before considering it to be an anomaly.\n  # The ClusterReceptionist is using the org.apache.pekko.remote.DeadlineFailureDetector, which\n  # will trigger if there are no heartbeats within the duration\n  # heartbeat-interval + acceptable-heartbeat-pause, i.e. 15 seconds with\n  # the default settings.\n  acceptable-heartbeat-pause = 13s\n\n  # Failure detection checking interval for checking all ClusterClients\n  failure-detection-interval = 2s\n}\nThe following configuration properties are read by the ClusterClientSettingsClusterClientSettings when created with a `ActorSystem``ActorSystem` parameter. It is also possible to amend the ClusterClientSettingsClusterClientSettings or create it from another config section with the same layout as below. ClusterClientSettingsClusterClientSettings is a parameter to the `ClusterClient.props``ClusterClient.props` factory method, i.e. each client can be configured with different settings if needed.\ncopysource# Settings for the ClusterClient\npekko.cluster.client {\n  # Actor paths of the ClusterReceptionist actors on the servers (cluster nodes)\n  # that the client will try to contact initially. It is mandatory to specify\n  # at least one initial contact. \n  # Comma separated full actor paths defined by a string on the form of\n  # \"akka://system@hostname:port/system/receptionist\"\n  initial-contacts = []\n  \n  # Interval at which the client retries to establish contact with one of \n  # ClusterReceptionist on the servers (cluster nodes)\n  establishing-get-contacts-interval = 3s\n  \n  # Interval at which the client will ask the ClusterReceptionist for\n  # new contact points to be used for next reconnect.\n  refresh-contacts-interval = 60s\n  \n  # How often failure detection heartbeat messages should be sent\n  heartbeat-interval = 2s\n  \n  # Number of potentially lost/delayed heartbeats that will be\n  # accepted before considering it to be an anomaly.\n  # The ClusterClient is using the org.apache.pekko.remote.DeadlineFailureDetector, which\n  # will trigger if there are no heartbeats within the duration \n  # heartbeat-interval + acceptable-heartbeat-pause, i.e. 15 seconds with\n  # the default settings.\n  acceptable-heartbeat-pause = 13s\n  \n  # If connection to the receptionist is not established the client will buffer\n  # this number of messages and deliver them the connection is established.\n  # When the buffer is full old messages will be dropped when new messages are sent\n  # via the client. Use 0 to disable buffering, i.e. messages will be dropped\n  # immediately if the location of the singleton is unknown.\n  # Maximum allowed buffer size is 10000.\n  buffer-size = 1000\n\n  # If connection to the receiptionist is lost and the client has not been\n  # able to acquire a new connection for this long the client will stop itself.\n  # This duration makes it possible to watch the cluster client and react on a more permanent\n  # loss of connection with the cluster, for example by accessing some kind of\n  # service registry for an updated set of initial contacts to start a new cluster client with.\n  # If this is not wanted it can be set to \"off\" to disable the timeout and retry\n  # forever.\n  reconnect-timeout = off\n}","title":"Configuration"},{"location":"/cluster-client.html#failure-handling","text":"When the cluster client is started it must be provided with a list of initial contacts which are cluster nodes where receptionists are running. It will then repeatedly (with an interval configurable by establishing-get-contacts-interval) try to contact those until it gets in contact with one of them. While running, the list of contacts is continuously updated with data from the receptionists (again, with an interval configurable with refresh-contacts-interval), so that if there are more receptionists in the cluster than the initial contacts provided to the client will learn about them.\nWhile the client is running it will detect failures in its connection to the receptionist by heartbeats if more than a configurable amount of heartbeats are missed the client will try to reconnect to its known set of contacts to find a receptionist it can access.","title":"Failure handling"},{"location":"/cluster-client.html#when-the-cluster-cannot-be-reached-at-all","text":"It is possible to make the cluster client stop entirely if it cannot find a receptionist it can talk to within a configurable interval. This is configured with the reconnect-timeout, which defaults to off. This can be useful when initial contacts are provided from some kind of service registry, cluster node addresses are entirely dynamic and the entire cluster might shut down or crash, be restarted on new addresses. Since the client will be stopped in that case a monitoring actor can watch it and upon Terminate a new set of initial contacts can be fetched and a new cluster client started.","title":"When the cluster cannot be reached at all"},{"location":"/cluster-client.html#migration-to-apache-pekko-grpc","text":"Cluster Client is deprecated and it is not advised to build new applications with it. As a replacement, we recommend using Pekko gRPC with an application-specific protocol. The benefits of this approach are:\nImproved security by using TLS for gRPC (HTTP/2) versus exposing Pekko Remoting outside the Pekko Cluster Easier to update clients and servers independent of each other Improved protocol definition between client and server Usage of Pekko gRPC Service Discovery Clients do not need to use Pekko See also gRPC versus Pekko Remoting","title":"Migration to Apache Pekko gRPC"},{"location":"/cluster-client.html#migrating-directly","text":"Existing users of Cluster Client may migrate directly to Pekko gRPC and use it as documented in its documentation.","title":"Migrating directly"},{"location":"/cluster-client.html#migrating-gradually","text":"If your application extensively uses Cluster Client, a more gradual migration might be desired that requires less re-write of the application. That migration step is described in this section. We recommend migration directly if feasible, though.\nAn example is provided to illustrate an approach to migrate from the deprecated Cluster Client to Pekko gRPC, with minimal changes to your existing code. The example is intended to be copied and adjusted to your needs. It will not be provided as a published artifact.\npekko-samples/pekko-bom-sample-cluster-cluster-client-grpc-scala implemented in Scala pekko-samples/pekko-bom-sample-cluster-cluster-client-grpc-java implemented in Java\nThe example is still using an actor on the client-side to have an API that is very close to the original Cluster Client. The messages this actor can handle correspond to the Distributed Pub Sub messages on the server-side, such as ClusterClient.Send and ClusterClient.Publish.\nThe ClusterClient actor delegates those messages to the gRPC client, and on the server-side those are translated and delegated to the destination actors that are registered via the ClusterClientReceptionist in the same way as in the original.\nPekko gRPC is used as the transport for the messages between client and server, instead of Pekko Remoting.\nThe application specific messages are wrapped and serialized with Pekko Serialization, which means that care must be taken to keep wire compatibility when changing any messages used between the client and server. The Pekko configuration of Pekko serializers must be the same (or being compatible) on the client and the server.","title":"Migrating gradually"},{"location":"/cluster-client.html#next-steps","text":"After this first migration step from Cluster Client to Pekko gRPC, you can start replacing calls to ClusterClientReceptionistService with new, application-specific gRPC endpoints.","title":"Next steps"},{"location":"/cluster-client.html#differences","text":"Aside from the underlying implementation using gRPC instead of Actor messages and Pekko Remoting it’s worth pointing out the following differences between the Cluster Client and the example emulating Cluster Client with Pekko gRPC as transport.","title":"Differences"},{"location":"/cluster-client.html#single-request-reply","text":"For request-reply interactions when there is only one reply message for each request it is more efficient to use the ClusterClient.AskSend message instead of ClusterClient.Send as illustrated in the example. Then it doesn’t have to setup a full bidirectional gRPC stream for each request but can use the FutureCompletionStage based API.","title":"Single request-reply"},{"location":"/cluster-client.html#initial-contact-points","text":"Instead of configured initial contact points the Pekko gRPC Service Discovery can be used.","title":"Initial contact points"},{"location":"/cluster-client.html#failure-detection","text":"Heartbeat messages and failure detection of the connections have been removed since that should be handled by the gRPC connections.","title":"Failure detection"},{"location":"/cluster-sharding.html","text":"","title":"Classic Cluster Sharding"},{"location":"/cluster-sharding.html#classic-cluster-sharding","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see Cluster Sharding.","title":"Classic Cluster Sharding"},{"location":"/cluster-sharding.html#module-info","text":"To use Cluster Sharding, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-sharding\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-sharding_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-sharding_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Cluster Sharding (classic) Artifact org.apache.pekko pekko-cluster-sharding 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.sharding License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/cluster-sharding.html#introduction","text":"For an introduction to Sharding concepts see Cluster Sharding.","title":"Introduction"},{"location":"/cluster-sharding.html#basic-example","text":"This is what an entity actor may look like:\nScala copysourcecase object Increment\ncase object Decrement\nfinal case class Get(counterId: Long)\nfinal case class EntityEnvelope(id: Long, payload: Any)\n\ncase object Stop\nfinal case class CounterChanged(delta: Int)\n\nclass Counter extends PersistentActor {\n  import ShardRegion.Passivate\n\n  context.setReceiveTimeout(120.seconds)\n\n  // self.path.name is the entity identifier (utf-8 URL-encoded)\n  override def persistenceId: String = \"Counter-\" + self.path.name\n\n  var count = 0\n\n  def updateState(event: CounterChanged): Unit =\n    count += event.delta\n\n  override def receiveRecover: Receive = {\n    case evt: CounterChanged => updateState(evt)\n  }\n\n  override def receiveCommand: Receive = {\n    case Increment      => persist(CounterChanged(+1))(updateState)\n    case Decrement      => persist(CounterChanged(-1))(updateState)\n    case Get(_)         => sender() ! count\n    case ReceiveTimeout => context.parent ! Passivate(stopMessage = Stop)\n    case Stop           => context.stop(self)\n  }\n} Java copysourcestatic class Counter extends AbstractPersistentActor {\n\n  public enum CounterOp {\n    INCREMENT,\n    DECREMENT\n  }\n\n  public static class Get {\n    public final long counterId;\n\n    public Get(long counterId) {\n      this.counterId = counterId;\n    }\n  }\n\n  public static class EntityEnvelope {\n    public final long id;\n    public final Object payload;\n\n    public EntityEnvelope(long id, Object payload) {\n      this.id = id;\n      this.payload = payload;\n    }\n  }\n\n  public static class CounterChanged {\n    public final int delta;\n\n    public CounterChanged(int delta) {\n      this.delta = delta;\n    }\n  }\n\n  int count = 0;\n\n  // getSelf().path().name() is the entity identifier (utf-8 URL-encoded)\n  @Override\n  public String persistenceId() {\n    return \"Counter-\" + getSelf().path().name();\n  }\n\n  @Override\n  public void preStart() throws Exception {\n    super.preStart();\n    getContext().setReceiveTimeout(Duration.ofSeconds(120));\n  }\n\n  void updateState(CounterChanged event) {\n    count += event.delta;\n  }\n\n  @Override\n  public Receive createReceiveRecover() {\n    return receiveBuilder().match(CounterChanged.class, this::updateState).build();\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(Get.class, this::receiveGet)\n        .matchEquals(CounterOp.INCREMENT, msg -> receiveIncrement())\n        .matchEquals(CounterOp.DECREMENT, msg -> receiveDecrement())\n        .matchEquals(ReceiveTimeout.getInstance(), msg -> passivate())\n        .build();\n  }\n\n  private void receiveGet(Get msg) {\n    getSender().tell(count, getSelf());\n  }\n\n  private void receiveIncrement() {\n    persist(new CounterChanged(+1), this::updateState);\n  }\n\n  private void receiveDecrement() {\n    persist(new CounterChanged(-1), this::updateState);\n  }\n\n  private void passivate() {\n    getContext().getParent().tell(new ShardRegion.Passivate(PoisonPill.getInstance()), getSelf());\n  }\n}\nThe above actor uses Event Sourcing and the support provided in PersistentActor AbstractPersistentActor to store its state. It does not have to be a persistent actor, but in case of failure or migration of entities between nodes it must be able to recover its state if it is valuable.\nNote how the persistenceId is defined. The name of the actor is the entity identifier (utf-8 URL-encoded). You may define it another way, but it must be unique.\nWhen using the sharding extension you are first, typically at system startup on each node in the cluster, supposed to register the supported entity types with the ClusterSharding.start method. ClusterSharding.start gives you the reference which you can pass along. Please note that ClusterSharding.start will start a ShardRegion in proxy only mode when there is no match between the roles of the current cluster node and the role specified in ClusterShardingSettings.\nScala copysourceval counterRegion: ActorRef = ClusterSharding(system).start(\n  typeName = \"Counter\",\n  entityProps = Props[Counter](),\n  settings = ClusterShardingSettings(system),\n  extractEntityId = extractEntityId,\n  extractShardId = extractShardId) Java copysourceimport org.apache.pekko.japi.Option;\nimport org.apache.pekko.cluster.sharding.ClusterSharding;\nimport org.apache.pekko.cluster.sharding.ClusterShardingSettings;\n\nOption<String> roleOption = Option.none();\nClusterShardingSettings settings = ClusterShardingSettings.create(system);\nActorRef startedCounterRegion =\n    ClusterSharding.get(system)\n        .start(\"Counter\", Props.create(Counter.class), settings, messageExtractor);\nThe extractEntityId and extractShardId are two messageExtractor defines application specific functions methods to extract the entity identifier and the shard identifier from incoming messages.\nScala copysourceval extractEntityId: ShardRegion.ExtractEntityId = {\n  case EntityEnvelope(id, payload) => (id.toString, payload)\n  case msg @ Get(id)               => (id.toString, msg)\n}\n\nval numberOfShards = 100\n\nval extractShardId: ShardRegion.ExtractShardId = {\n  case EntityEnvelope(id, _)       => (id % numberOfShards).toString\n  case Get(id)                     => (id % numberOfShards).toString\n  case ShardRegion.StartEntity(id) =>\n    // StartEntity is used by remembering entities feature\n    (id.toLong % numberOfShards).toString\n  case _ => throw new IllegalArgumentException()\n} Java copysourceimport org.apache.pekko.cluster.sharding.ShardRegion;\n\nShardRegion.MessageExtractor messageExtractor =\n    new ShardRegion.MessageExtractor() {\n\n      @Override\n      public String entityId(Object message) {\n        if (message instanceof Counter.EntityEnvelope)\n          return String.valueOf(((Counter.EntityEnvelope) message).id);\n        else if (message instanceof Counter.Get)\n          return String.valueOf(((Counter.Get) message).counterId);\n        else return null;\n      }\n\n      @Override\n      public Object entityMessage(Object message) {\n        if (message instanceof Counter.EntityEnvelope)\n          return ((Counter.EntityEnvelope) message).payload;\n        else return message;\n      }\n\n      @Override\n      public String shardId(Object message) {\n        int numberOfShards = 100;\n        if (message instanceof Counter.EntityEnvelope) {\n          long id = ((Counter.EntityEnvelope) message).id;\n          return String.valueOf(id % numberOfShards);\n        } else if (message instanceof Counter.Get) {\n          long id = ((Counter.Get) message).counterId;\n          return String.valueOf(id % numberOfShards);\n        } else {\n          return null;\n        }\n      }\n    };\nThis example illustrates two different ways to define the entity identifier in the messages:\nThe Get message includes the identifier itself. The EntityEnvelope holds the identifier, and the actual message that is sent to the entity actor is wrapped in the envelope.\nNote how these two messages types are handled in the extractEntityId function entityId and entityMessage methods shown above. The message sent to the entity actor is the second part of the tuple returned by the extractEntityId what entityMessage returns and that makes it possible to unwrap envelopes if needed.\nA shard is a group of entities that will be managed together. The grouping is defined by the extractShardId function shown above. For a specific entity identifier the shard identifier must always be the same. Otherwise the entity actor might accidentally be started in several places at the same time.\nCreating a good sharding algorithm is an interesting challenge in itself. Try to produce a uniform distribution, i.e. same amount of entities in each shard. As a rule of thumb, the number of shards should be a factor ten greater than the planned maximum number of cluster nodes. Fewer shards than number of nodes will result in that some nodes will not host any shards. Too many shards will result in less efficient management of the shards, e.g. rebalancing overhead, and increased latency because the coordinator is involved in the routing of the first message for each shard. The sharding algorithm must be the same on all nodes in a running cluster. It can be changed after stopping all nodes in the cluster.\nA simple sharding algorithm that works fine in most cases is to take the absolute value of the hashCode of the entity identifier modulo number of shards. As a convenience this is provided by the ShardRegion.HashCodeMessageExtractor.\nMessages to the entities are always sent via the local ShardRegion. The ShardRegion actor reference for a named entity type is returned by ClusterSharding.start and it can also be retrieved with ClusterSharding.shardRegion. The ShardRegion will lookup the location of the shard for the entity if it does not already know its location. It will delegate the message to the right node and it will create the entity actor on demand, i.e. when the first message for a specific entity is delivered.\nScala copysourceval counterRegion: ActorRef = ClusterSharding(system).shardRegion(\"Counter\")\ncounterRegion ! Get(123)\nexpectMsg(0)\n\ncounterRegion ! EntityEnvelope(123, Increment)\ncounterRegion ! Get(123)\nexpectMsg(1) Java copysourceActorRef counterRegion = ClusterSharding.get(system).shardRegion(\"Counter\");\ncounterRegion.tell(new Counter.Get(123), getSelf());\n\ncounterRegion.tell(new Counter.EntityEnvelope(123, Counter.CounterOp.INCREMENT), getSelf());\ncounterRegion.tell(new Counter.Get(123), getSelf());","title":"Basic example"},{"location":"/cluster-sharding.html#how-it-works","text":"See Cluster Sharding concepts in the documentation of the new APIs.","title":"How it works"},{"location":"/cluster-sharding.html#sharding-state-store-mode","text":"There are two cluster sharding states managed:\nShardCoordinator State - the Shard locations Remembering Entities - the entities in each Shard, which is optional, and disabled by default\nFor these, there are currently two modes which define how these states are stored:\nDistributed Data Mode - uses Pekko Distributed Data (CRDTs) (the default) Persistence Mode - (deprecated) uses Pekko Persistence (Event Sourcing)\nWarning Persistence for state store mode is deprecated. It is recommended to migrate to ddata for the coordinator state and if using replicated entities migrate to eventsourced for the replicated entities state. The data written by the deprecated persistence state store mode for remembered entities can be read by the new remember entities eventsourced mode. Once you’ve migrated you can not go back to persistence mode.\nChanging the mode requires a full cluster restart.","title":"Sharding State Store Mode"},{"location":"/cluster-sharding.html#distributed-data-mode","text":"The state of the ShardCoordinator is replicated across the cluster but is not durable, not stored to disk. The ShardCoordinator state replication is handled by Distributed Data with WriteMajority/ReadMajority consistency. When all nodes in the cluster have been stopped, the state is no longer needed and dropped.\nSee Distributed Data mode in the documentation of the new APIs.","title":"Distributed Data Mode"},{"location":"/cluster-sharding.html#persistence-mode","text":"See Persistence Mode in the documentation of the new APIs.","title":"Persistence Mode"},{"location":"/cluster-sharding.html#proxy-only-mode","text":"The ShardRegion actor can also be started in proxy only mode, i.e. it will not host any entities itself, but knows how to delegate messages to the right location. A ShardRegion is started in proxy only mode with the ClusterSharding.startProxy method. Also a ShardRegion is started in proxy only mode when there is no match between the roles of the current cluster node and the role specified in ClusterShardingSettings passed to the ClusterSharding.start method.","title":"Proxy Only Mode"},{"location":"/cluster-sharding.html#passivation","text":"If the state of the entities are persistent you may stop entities that are not used to reduce memory consumption. This is done by the application specific implementation of the entity actors for example by defining receive timeout (context.setReceiveTimeout). If a message is already enqueued to the entity when it stops itself the enqueued message in the mailbox will be dropped. To support graceful passivation without losing such messages the entity actor can send ShardRegion.Passivate to its parent Shard. The specified wrapped message in Passivate will be sent back to the entity, which is then supposed to stop itself. Incoming messages will be buffered by the Shard between reception of Passivate and termination of the entity. Such buffered messages are thereafter delivered to a new incarnation of the entity.\nSee Automatic Passivation in the documentation of the new APIs.","title":"Passivation"},{"location":"/cluster-sharding.html#remembering-entities","text":"See Remembering Entities in the documentation of the new APIs, including behavior when enabled and disabled.\nNote that the state of the entities themselves will not be restored unless they have been made persistent, for example with Event Sourcing.\nTo make the list of entities in each Shard persistent (durable), set the rememberEntities flag to true in ClusterShardingSettings when calling ClusterSharding.start and make sure the shardIdExtractor handles Shard.StartEntity(EntityId) which implies that a ShardId must be possible to extract from the EntityId.\nScala copysourceval extractShardId: ShardRegion.ExtractShardId = {\n  case EntityEnvelope(id, _)       => (id % numberOfShards).toString\n  case Get(id)                     => (id % numberOfShards).toString\n  case ShardRegion.StartEntity(id) =>\n    // StartEntity is used by remembering entities feature\n    (id.toLong % numberOfShards).toString\n  case _ => throw new IllegalArgumentException()\n} Java copysource@Override\npublic String shardId(Object message) {\n  int numberOfShards = 100;\n  if (message instanceof Counter.EntityEnvelope) {\n    long id = ((Counter.EntityEnvelope) message).id;\n    return String.valueOf(id % numberOfShards);\n  } else if (message instanceof Counter.Get) {\n    long id = ((Counter.Get) message).counterId;\n    return String.valueOf(id % numberOfShards);\n  } else if (message instanceof ShardRegion.StartEntity) {\n    long id = Long.valueOf(((ShardRegion.StartEntity) message).entityId());\n    return String.valueOf(id % numberOfShards);\n  } else {\n    return null;\n  }\n}","title":"Remembering Entities"},{"location":"/cluster-sharding.html#supervision","text":"If you need to use another supervisorStrategy for the entity actors than the default (restarting) strategy you need to create an intermediate parent actor that defines the supervisorStrategy to the child entity actor.\nScala copysourceclass CounterSupervisor extends Actor {\n  val counter = context.actorOf(Props[Counter](), \"theCounter\")\n\n  override val supervisorStrategy = OneForOneStrategy() {\n    case _: IllegalArgumentException     => SupervisorStrategy.Resume\n    case _: ActorInitializationException => SupervisorStrategy.Stop\n    case _: DeathPactException           => SupervisorStrategy.Stop\n    case _: Exception                    => SupervisorStrategy.Restart\n  }\n\n  def receive = {\n    case msg => counter.forward(msg)\n  }\n} Java copysourcestatic class CounterSupervisor extends AbstractActor {\n\n  private final ActorRef counter =\n      getContext().actorOf(Props.create(Counter.class), \"theCounter\");\n\n  private static final SupervisorStrategy strategy =\n      new OneForOneStrategy(\n          DeciderBuilder.match(IllegalArgumentException.class, e -> SupervisorStrategy.resume())\n              .match(ActorInitializationException.class, e -> SupervisorStrategy.stop())\n              .match(Exception.class, e -> SupervisorStrategy.restart())\n              .matchAny(o -> SupervisorStrategy.escalate())\n              .build());\n\n  @Override\n  public SupervisorStrategy supervisorStrategy() {\n    return strategy;\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(Object.class, msg -> counter.forward(msg, getContext()))\n        .build();\n  }\n}\nYou start such a supervisor in the same way as if it was the entity actor.\nScala copysourceClusterSharding(system).start(\n  typeName = \"SupervisedCounter\",\n  entityProps = Props[CounterSupervisor](),\n  settings = ClusterShardingSettings(system),\n  extractEntityId = extractEntityId,\n  extractShardId = extractShardId) Java copysourceClusterSharding.get(system)\n    .start(\n        \"SupervisedCounter\", Props.create(CounterSupervisor.class), settings, messageExtractor);\nNote that stopped entities will be started again when a new message is targeted to the entity.\nIf ‘on stop’ backoff supervision strategy is used, a final termination message must be set and used for passivation, see Backoff supervisor and sharding","title":"Supervision"},{"location":"/cluster-sharding.html#graceful-shutdown","text":"You can send the ShardRegion.GracefulShutdown ShardRegion.gracefulShutdownInstance message to the ShardRegion actor to hand off all shards that are hosted by that ShardRegion and then the ShardRegion actor will be stopped. You can watch the ShardRegion actor to know when it is completed. During this period other regions will buffer messages for those shards in the same way as when a rebalance is triggered by the coordinator. When the shards have been stopped the coordinator will allocate these shards elsewhere.\nThis is performed automatically by the Coordinated Shutdown and is therefore part of the graceful leaving process of a cluster member.","title":"Graceful Shutdown"},{"location":"/cluster-sharding.html#removal-of-internal-cluster-sharding-data","text":"See removal of Internal Cluster Sharding Data in the documentation of the new APIs.","title":"Removal of Internal Cluster Sharding Data"},{"location":"/cluster-sharding.html#inspecting-cluster-sharding-state","text":"Two requests to inspect the cluster state are available:\nShardRegion.GetShardRegionState ShardRegion.getShardRegionStateInstance which will return a ShardRegion.CurrentShardRegionState ShardRegion.ShardRegionState that contains the identifiers of the shards running in a Region and what entities are alive for each of them.\nShardRegion.GetClusterShardingStats which will query all the regions in the cluster and return a ShardRegion.ClusterShardingStats containing the identifiers of the shards running in each region and a count of entities that are alive in each shard.\nIf any shard queries failed, for example due to timeout if a shard was too busy to reply within the configured pekko.cluster.sharding.shard-region-query-timeout, ShardRegion.CurrentShardRegionState and ShardRegion.ClusterShardingStats will also include the set of shard identifiers by region that failed.\nThe type names of all started shards can be acquired via ClusterSharding.shardTypeNames ClusterSharding.getShardTypeNames.\nThe purpose of these messages is testing and monitoring, they are not provided to give access to directly sending messages to the individual entities.","title":"Inspecting cluster sharding state"},{"location":"/cluster-sharding.html#lease","text":"A lease can be used as an additional safety measure to ensure a shard does not run on two nodes. See Lease in the documentation of the new APIs.","title":"Lease"},{"location":"/cluster-sharding.html#configuration","text":"ClusterShardingSettings is a parameter to the start method of the ClusterSharding extension, i.e. each each entity type can be configured with different settings if needed.\nSee configuration for more information.","title":"Configuration"},{"location":"/cluster-metrics.html","text":"","title":"Classic Cluster Metrics Extension"},{"location":"/cluster-metrics.html#classic-cluster-metrics-extension","text":"","title":"Classic Cluster Metrics Extension"},{"location":"/cluster-metrics.html#module-info","text":"To use Cluster Metrics Extension, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-cluster-metrics\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-cluster-metrics_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-cluster-metrics_${versions.ScalaBinary}\"\n}\nand add the following configuration stanza to your application.conf :\npekko.extensions = [ \"pekko.cluster.metrics.ClusterMetricsExtension\" ]\nProject Info: Pekko Cluster Metrics (classic) Artifact org.apache.pekko pekko-cluster-metrics 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.cluster.metrics License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/cluster-metrics.html#introduction","text":"The member nodes of the cluster can collect system health metrics and publish that to other cluster nodes and to the registered subscribers on the system event bus with the help of Cluster Metrics Extension.\nCluster metrics information is primarily used for load-balancing routers, and can also be used to implement advanced metrics-based node life cycles, such as “Node Let-it-crash” when CPU steal time becomes excessive.\nCluster members with status WeaklyUp, if that feature is enabled, will participate in Cluster Metrics collection and dissemination.","title":"Introduction"},{"location":"/cluster-metrics.html#metrics-collector","text":"Metrics collection is delegated to an implementation of org.apache.pekko.cluster.metrics.MetricsCollector.\nDifferent collector implementations provide different subsets of metrics published to the cluster. Certain message routing and let-it-crash functions may not work when Sigar is not provisioned.\nCluster metrics extension comes with two built-in collector implementations:\norg.apache.pekko.cluster.metrics.SigarMetricsCollector, which requires Sigar provisioning, and is more rich/precise org.apache.pekko.cluster.metrics.JmxMetricsCollector, which is used as fall back, and is less rich/precise\nYou can also plug-in your own metrics collector implementation.\nBy default, metrics extension will use collector provider fall back and will try to load them in this order:\nconfigured user-provided collector built-in org.apache.pekko.cluster.metrics.SigarMetricsCollector and finally org.apache.pekko.cluster.metrics.JmxMetricsCollector","title":"Metrics Collector"},{"location":"/cluster-metrics.html#metrics-events","text":"Metrics extension periodically publishes current snapshot of the cluster metrics to the node system event bus.\nThe publication interval is controlled by the pekko.cluster.metrics.collector.sample-interval setting.\nThe payload of the org.apache.pekko.cluster.metrics.ClusterMetricsChanged event will contain latest metrics of the node as well as other cluster member nodes metrics gossip which was received during the collector sample interval.\nYou can subscribe your metrics listener actors to these events in order to implement custom node lifecycle:\nScala ClusterMetricsExtension(system).subscribe(metricsListenerActor)\n Java ClusterMetricsExtension.get(system).subscribe(metricsListenerActor);","title":"Metrics Events"},{"location":"/cluster-metrics.html#hyperic-sigar-provisioning","text":"Both user-provided and built-in metrics collectors can optionally use Hyperic Sigar for a wider and more accurate range of metrics compared to what can be retrieved from ordinary JMX MBeans.\nSigar is using a native o/s library, and requires library provisioning, i.e. deployment, extraction and loading of the o/s native library into JVM at runtime.\nUser can provision Sigar classes and native library in one of the following ways:\nUse Kamon sigar-loader as a project dependency for the user project. Metrics extension will extract and load sigar library on demand with help of Kamon sigar provisioner. Use Kamon sigar-loader as java agent: java -javaagent:/path/to/sigar-loader.jar. Kamon sigar loader agent will extract and load sigar library during JVM start. Place sigar.jar on the classpath and Sigar native library for the o/s on the java.library.path. User is required to manage both project dependency and library deployment manually.\nWarning When using Kamon sigar-loader and running multiple instances of the same application on the same host, you have to make sure that sigar library is extracted to a unique per instance directory. You can control the extract directory with the pekko.cluster.metrics.native-library-extract-folder configuration setting.\nTo enable usage of Sigar you can add the following dependency to the user project:\nsbt libraryDependencies += \"io.kamon\" % \"sigar-loader\" % \"1.6.6-rev002\" Maven <dependencies>\n  <dependency>\n    <groupId>io.kamon</groupId>\n    <artifactId>sigar-loader</artifactId>\n    <version>1.6.6-rev002</version>\n  </dependency>\n</dependencies> Gradle dependencies {\n  implementation \"io.kamon:sigar-loader:1.6.6-rev002\"\n}\nYou can download Kamon sigar-loader from Maven Central","title":"Hyperic Sigar Provisioning"},{"location":"/cluster-metrics.html#adaptive-load-balancing","text":"The AdaptiveLoadBalancingPool / AdaptiveLoadBalancingGroup performs load balancing of messages to cluster nodes based on the cluster metrics data. It uses random selection of routees with probabilities derived from the remaining capacity of the corresponding node. It can be configured to use a specific MetricsSelector to produce the probabilities, a.k.a. weights:\nheap / HeapMetricsSelector - Used and max JVM heap memory. Weights based on remaining heap capacity; (max - used) / max load / SystemLoadAverageMetricsSelector - System load average for the past 1 minute, corresponding value can be found in top of Linux systems. The system is possibly nearing a bottleneck if the system load average is nearing number of cpus/cores. Weights based on remaining load capacity; 1 - (load / processors) cpu / CpuMetricsSelector - CPU utilization in percentage, sum of User + Sys + Nice + Wait. Weights based on remaining cpu capacity; 1 - utilization mix / MixMetricsSelector - Combines heap, cpu and load. Weights based on mean of remaining capacity of the combined selectors. Any custom implementation of org.apache.pekko.cluster.metrics.MetricsSelector\nThe collected metrics values are smoothed with exponential weighted moving average. In the Cluster configuration you can adjust how quickly past data is decayed compared to new data.\nLet’s take a look at this router in action. What can be more demanding than calculating factorials?\nThe backend worker that performs the factorial calculation:\nScala copysourceclass FactorialBackend extends Actor with ActorLogging {\n\n  import context.dispatcher\n\n  def receive = {\n    case (n: Int) =>\n      Future(factorial(n))\n        .map { result =>\n          (n, result)\n        }\n        .pipeTo(sender())\n  }\n\n  def factorial(n: Int): BigInt = {\n    @tailrec def factorialAcc(acc: BigInt, n: Int): BigInt = {\n      if (n <= 1) acc\n      else factorialAcc(acc * n, n - 1)\n    }\n    factorialAcc(BigInt(1), n)\n  }\n\n} Java copysourcepublic class FactorialBackend extends AbstractActor {\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Integer.class,\n            n -> {\n              CompletableFuture<FactorialResult> result =\n                  CompletableFuture.supplyAsync(() -> factorial(n))\n                      .thenApply((factorial) -> new FactorialResult(n, factorial));\n\n              pipe(result, getContext().dispatcher()).to(getSender());\n            })\n        .build();\n  }\n\n  BigInteger factorial(int n) {\n    BigInteger acc = BigInteger.ONE;\n    for (int i = 1; i <= n; ++i) {\n      acc = acc.multiply(BigInteger.valueOf(i));\n    }\n    return acc;\n  }\n}\nThe frontend that receives user jobs and delegates to the backends via the router:\nScala copysourceclass FactorialFrontend(upToN: Int, repeat: Boolean) extends Actor with ActorLogging {\n\n  val backend = context.actorOf(FromConfig.props(), name = \"factorialBackendRouter\")\n\n  override def preStart(): Unit = {\n    sendJobs()\n    if (repeat) {\n      context.setReceiveTimeout(10.seconds)\n    }\n  }\n\n  def receive = {\n    case (n: Int, factorial: BigInt) =>\n      if (n == upToN) {\n        log.debug(\"{}! = {}\", n, factorial)\n        if (repeat) sendJobs()\n        else context.stop(self)\n      }\n    case ReceiveTimeout =>\n      log.info(\"Timeout\")\n      sendJobs()\n  }\n\n  def sendJobs(): Unit = {\n    log.info(\"Starting batch of factorials up to [{}]\", upToN)\n    (1 to upToN).foreach { backend ! _ }\n  }\n} Java copysourcepublic class FactorialFrontend extends AbstractActor {\n  final int upToN;\n  final boolean repeat;\n\n  LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  ActorRef backend =\n      getContext().actorOf(FromConfig.getInstance().props(), \"factorialBackendRouter\");\n\n  public FactorialFrontend(int upToN, boolean repeat) {\n    this.upToN = upToN;\n    this.repeat = repeat;\n  }\n\n  @Override\n  public void preStart() {\n    sendJobs();\n    getContext().setReceiveTimeout(Duration.ofSeconds(10));\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            FactorialResult.class,\n            result -> {\n              if (result.n == upToN) {\n                log.debug(\"{}! = {}\", result.n, result.factorial);\n                if (repeat) sendJobs();\n                else getContext().stop(getSelf());\n              }\n            })\n        .match(\n            ReceiveTimeout.class,\n            x -> {\n              log.info(\"Timeout\");\n              sendJobs();\n            })\n        .build();\n  }\n\n  void sendJobs() {\n    log.info(\"Starting batch of factorials up to [{}]\", upToN);\n    for (int n = 1; n <= upToN; n++) {\n      backend.tell(n, getSelf());\n    }\n  }\n}\nAs you can see, the router is defined in the same way as other routers, and in this case it is configured as follows:\npekko.actor.deployment {\n  /factorialFrontend/factorialBackendRouter = {\n    # Router type provided by metrics extension.\n    router = cluster-metrics-adaptive-group\n    # Router parameter specific for metrics extension.\n    # metrics-selector = heap\n    # metrics-selector = load\n    # metrics-selector = cpu\n    metrics-selector = mix\n    #\n    routees.paths = [\"/user/factorialBackend\"]\n    cluster {\n      enabled = on\n      use-roles = [\"backend\"]\n      allow-local-routees = off\n    }\n  }\n}\nIt is only router type and the metrics-selector parameter that is specific to this router, other things work in the same way as other routers.\nThe same type of router could also have been defined in code:\nScala copysourceimport org.apache.pekko\nimport pekko.cluster.routing.ClusterRouterGroup\nimport pekko.cluster.routing.ClusterRouterGroupSettings\nimport pekko.cluster.metrics.AdaptiveLoadBalancingGroup\nimport pekko.cluster.metrics.HeapMetricsSelector\n\nval backend = context.actorOf(\n  ClusterRouterGroup(\n    AdaptiveLoadBalancingGroup(HeapMetricsSelector),\n    ClusterRouterGroupSettings(\n      totalInstances = 100,\n      routeesPaths = List(\"/user/factorialBackend\"),\n      allowLocalRoutees = true,\n      useRoles = Set(\"backend\"))).props(),\n  name = \"factorialBackendRouter2\")\n\nimport pekko.cluster.routing.ClusterRouterPool\nimport pekko.cluster.routing.ClusterRouterPoolSettings\nimport pekko.cluster.metrics.AdaptiveLoadBalancingPool\nimport pekko.cluster.metrics.SystemLoadAverageMetricsSelector\n\nval backend = context.actorOf(\n  ClusterRouterPool(\n    AdaptiveLoadBalancingPool(SystemLoadAverageMetricsSelector),\n    ClusterRouterPoolSettings(\n      totalInstances = 100,\n      maxInstancesPerNode = 3,\n      allowLocalRoutees = false,\n      useRoles = Set(\"backend\"))).props(Props[FactorialBackend]()),\n  name = \"factorialBackendRouter3\") Java copysourceint totalInstances = 100;\nIterable<String> routeesPaths = Arrays.asList(\"/user/factorialBackend\", \"\");\nboolean allowLocalRoutees = true;\nSet<String> useRoles = new HashSet<>(Arrays.asList(\"backend\"));\nActorRef backend =\n    getContext()\n        .actorOf(\n            new ClusterRouterGroup(\n                    new AdaptiveLoadBalancingGroup(\n                        HeapMetricsSelector.getInstance(), Collections.<String>emptyList()),\n                    new ClusterRouterGroupSettings(\n                        totalInstances, routeesPaths, allowLocalRoutees, useRoles))\n                .props(),\n            \"factorialBackendRouter2\");\n\nint totalInstances = 100;\nint maxInstancesPerNode = 3;\nboolean allowLocalRoutees = false;\nSet<String> useRoles = new HashSet<>(Arrays.asList(\"backend\"));\nActorRef backend =\n    getContext()\n        .actorOf(\n            new ClusterRouterPool(\n                    new AdaptiveLoadBalancingPool(\n                        SystemLoadAverageMetricsSelector.getInstance(), 0),\n                    new ClusterRouterPoolSettings(\n                        totalInstances, maxInstancesPerNode, allowLocalRoutees, useRoles))\n                .props(Props.create(FactorialBackend.class)),\n            \"factorialBackendRouter3\");","title":"Adaptive Load Balancing"},{"location":"/cluster-metrics.html#subscribe-to-metrics-events","text":"It is possible to subscribe to the metrics events directly to implement other functionality.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.ActorLogging\nimport pekko.actor.Actor\nimport pekko.cluster.Cluster\nimport pekko.cluster.metrics.ClusterMetricsEvent\nimport pekko.cluster.metrics.ClusterMetricsChanged\nimport pekko.cluster.ClusterEvent.CurrentClusterState\nimport pekko.cluster.metrics.NodeMetrics\nimport pekko.cluster.metrics.StandardMetrics.HeapMemory\nimport pekko.cluster.metrics.StandardMetrics.Cpu\nimport pekko.cluster.metrics.ClusterMetricsExtension\n\nclass MetricsListener extends Actor with ActorLogging {\n  val selfAddress = Cluster(context.system).selfAddress\n  val extension = ClusterMetricsExtension(context.system)\n\n  // Subscribe unto ClusterMetricsEvent events.\n  override def preStart(): Unit = extension.subscribe(self)\n\n  // Unsubscribe from ClusterMetricsEvent events.\n  override def postStop(): Unit = extension.unsubscribe(self)\n\n  def receive = {\n    case ClusterMetricsChanged(clusterMetrics) =>\n      clusterMetrics.filter(_.address == selfAddress).foreach { nodeMetrics =>\n        logHeap(nodeMetrics)\n        logCpu(nodeMetrics)\n      }\n    case state: CurrentClusterState => // Ignore.\n  }\n\n  def logHeap(nodeMetrics: NodeMetrics): Unit = nodeMetrics match {\n    case HeapMemory(address, timestamp, used, committed, max) =>\n      log.info(\"Used heap: {} MB\", used.doubleValue / 1024 / 1024)\n    case _ => // No heap info.\n  }\n\n  def logCpu(nodeMetrics: NodeMetrics): Unit = nodeMetrics match {\n    case Cpu(address, timestamp, Some(systemLoadAverage), cpuCombined, cpuStolen, processors) =>\n      log.info(\"Load: {} ({} processors)\", systemLoadAverage, processors)\n    case _ => // No cpu info.\n  }\n} Java copysourceimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.cluster.Cluster;\nimport org.apache.pekko.cluster.ClusterEvent.CurrentClusterState;\nimport org.apache.pekko.cluster.metrics.ClusterMetricsChanged;\nimport org.apache.pekko.cluster.metrics.NodeMetrics;\nimport org.apache.pekko.cluster.metrics.StandardMetrics;\nimport org.apache.pekko.cluster.metrics.StandardMetrics.HeapMemory;\nimport org.apache.pekko.cluster.metrics.StandardMetrics.Cpu;\nimport org.apache.pekko.cluster.metrics.ClusterMetricsExtension;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\n\npublic class MetricsListener extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  Cluster cluster = Cluster.get(getContext().getSystem());\n\n  ClusterMetricsExtension extension = ClusterMetricsExtension.get(getContext().getSystem());\n\n  // Subscribe unto ClusterMetricsEvent events.\n  @Override\n  public void preStart() {\n    extension.subscribe(getSelf());\n  }\n\n  // Unsubscribe from ClusterMetricsEvent events.\n  @Override\n  public void postStop() {\n    extension.unsubscribe(getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            ClusterMetricsChanged.class,\n            clusterMetrics -> {\n              for (NodeMetrics nodeMetrics : clusterMetrics.getNodeMetrics()) {\n                if (nodeMetrics.address().equals(cluster.selfAddress())) {\n                  logHeap(nodeMetrics);\n                  logCpu(nodeMetrics);\n                }\n              }\n            })\n        .match(\n            CurrentClusterState.class,\n            message -> {\n              // Ignore.\n            })\n        .build();\n  }\n\n  void logHeap(NodeMetrics nodeMetrics) {\n    HeapMemory heap = StandardMetrics.extractHeapMemory(nodeMetrics);\n    if (heap != null) {\n      log.info(\"Used heap: {} MB\", ((double) heap.used()) / 1024 / 1024);\n    }\n  }\n\n  void logCpu(NodeMetrics nodeMetrics) {\n    Cpu cpu = StandardMetrics.extractCpu(nodeMetrics);\n    if (cpu != null && cpu.systemLoadAverage().isDefined()) {\n      log.info(\"Load: {} ({} processors)\", cpu.systemLoadAverage().get(), cpu.processors());\n    }\n  }\n}","title":"Subscribe to Metrics Events"},{"location":"/cluster-metrics.html#custom-metrics-collector","text":"Metrics collection is delegated to the implementation of org.apache.pekko.cluster.metrics.MetricsCollector\nYou can plug-in your own metrics collector instead of built-in org.apache.pekko.cluster.metrics.SigarMetricsCollector or org.apache.pekko.cluster.metrics.JmxMetricsCollector.\nLook at those two implementations for inspiration.\nCustom metrics collector implementation class must be specified in the pekko.cluster.metrics.collector.provider configuration property.","title":"Custom Metrics Collector"},{"location":"/cluster-metrics.html#configuration","text":"The Cluster metrics extension can be configured with the following properties:\ncopysource##############################################\n# Pekko Cluster Metrics Reference Config File #\n##############################################\n\n# This is the reference config file that contains all the default settings.\n# Make your edits in your application.conf in order to override these settings.\n\n# Sigar provisioning:\n#\n#  User can provision sigar classes and native library in one of the following ways:\n# \n#  1) Use https://github.com/kamon-io/sigar-loader Kamon sigar-loader as a project dependency for the user project.\n#  Metrics extension will extract and load sigar library on demand with help of Kamon sigar provisioner.\n# \n#  2) Use https://github.com/kamon-io/sigar-loader Kamon sigar-loader as java agent: `java -javaagent:/path/to/sigar-loader.jar`\n#  Kamon sigar loader agent will extract and load sigar library during JVM start.\n# \n#  3) Place `sigar.jar` on the `classpath` and sigar native library for the o/s on the `java.library.path`\n#  User is required to manage both project dependency and library deployment manually.\n\n# Cluster metrics extension.\n# Provides periodic statistics collection and publication throughout the cluster.\npekko.cluster.metrics {\n  # Full path of dispatcher configuration key.\n  dispatcher = \"pekko.actor.default-dispatcher\"\n  # How long should any actor wait before starting the periodic tasks.\n  periodic-tasks-initial-delay = 1s\n  # Sigar native library extract location.\n  # Use per-application-instance scoped location, such as program working directory.\n  native-library-extract-folder = ${user.dir}\"/native\"\n  # Metrics supervisor actor.\n  supervisor {\n    # Actor name. Example name space: /system/cluster-metrics\n    name = \"cluster-metrics\"\n    # Supervision strategy.\n    strategy {\n      #\n      # FQCN of class providing `org.apache.pekko.actor.SupervisorStrategy`.\n      # Must have a constructor with signature `<init>(com.typesafe.config.Config)`.\n      # Default metrics strategy provider is a configurable extension of `OneForOneStrategy`.\n      provider = \"org.apache.pekko.cluster.metrics.ClusterMetricsStrategy\"\n      #\n      # Configuration of the default strategy provider.\n      # Replace with custom settings when overriding the provider.\n      configuration = {\n        # Log restart attempts.\n        loggingEnabled = true\n        # Child actor restart-on-failure window.\n        withinTimeRange = 3s\n        # Maximum number of restart attempts before child actor is stopped.\n        maxNrOfRetries = 3\n      }\n    }\n  }\n  # Metrics collector actor.\n  collector {\n    # Enable or disable metrics collector for load-balancing nodes.\n    # Metrics collection can also be controlled at runtime by sending control messages\n    # to /system/cluster-metrics actor: `org.apache.pekko.cluster.metrics.{CollectionStartMessage,CollectionStopMessage}`\n    enabled = on\n    # FQCN of the metrics collector implementation.\n    # It must implement `org.apache.pekko.cluster.metrics.MetricsCollector` and\n    # have public constructor with org.apache.pekko.actor.ActorSystem parameter.\n    # Will try to load in the following order of priority:\n    # 1) configured custom collector 2) internal `SigarMetricsCollector` 3) internal `JmxMetricsCollector`\n    provider = \"\"\n    # Try all 3 available collector providers, or else fail on the configured custom collector provider.\n    fallback = true\n    # How often metrics are sampled on a node.\n    # Shorter interval will collect the metrics more often.\n    # Also controls frequency of the metrics publication to the node system event bus.\n    sample-interval = 3s\n    # How often a node publishes metrics information to the other nodes in the cluster.\n    # Shorter interval will publish the metrics gossip more often.\n    gossip-interval = 3s\n    # How quickly the exponential weighting of past data is decayed compared to\n    # new data. Set lower to increase the bias toward newer values.\n    # The relevance of each data sample is halved for every passing half-life\n    # duration, i.e. after 4 times the half-life, a data sample’s relevance is\n    # reduced to 6% of its original relevance. The initial relevance of a data\n    # sample is given by 1 – 0.5 ^ (collect-interval / half-life).\n    # See https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\n    moving-average-half-life = 12s\n  }\n}\n\n# Cluster metrics extension serializers and routers.\npekko.actor {\n  # Protobuf serializer for remote cluster metrics messages.\n  serializers {\n    pekko-cluster-metrics = \"org.apache.pekko.cluster.metrics.protobuf.MessageSerializer\"\n  }\n  # Interface binding for remote cluster metrics messages.\n  serialization-bindings {\n    \"org.apache.pekko.cluster.metrics.ClusterMetricsMessage\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.AdaptiveLoadBalancingPool\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.MixMetricsSelector\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.CpuMetricsSelector$\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.HeapMetricsSelector$\" = pekko-cluster-metrics\n    \"org.apache.pekko.cluster.metrics.SystemLoadAverageMetricsSelector$\" = pekko-cluster-metrics\n  }\n  # Globally unique metrics extension serializer identifier.\n  serialization-identifiers {\n    \"org.apache.pekko.cluster.metrics.protobuf.MessageSerializer\" = 10\n  }\n  #  Provide routing of messages based on cluster metrics.\n  router.type-mapping {\n    cluster-metrics-adaptive-pool  = \"org.apache.pekko.cluster.metrics.AdaptiveLoadBalancingPool\"\n    cluster-metrics-adaptive-group = \"org.apache.pekko.cluster.metrics.AdaptiveLoadBalancingGroup\"\n  }\n}","title":"Configuration"},{"location":"/distributed-data.html","text":"","title":"Classic Distributed Data"},{"location":"/distributed-data.html#classic-distributed-data","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the full documentation of this feature and for new projects see Distributed Data.","title":"Classic Distributed Data"},{"location":"/distributed-data.html#dependency","text":"To use Pekko Distributed Data, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-distributed-data\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-distributed-data_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-distributed-data_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/distributed-data.html#introduction","text":"For the full documentation of this feature and for new projects see Distributed Data - Introduction.","title":"Introduction"},{"location":"/distributed-data.html#using-the-replicator","text":"The ReplicatorReplicator actor provides the API for interacting with the data. The Replicator actor must be started on each node in the cluster, or group of nodes tagged with a specific role. It communicates with other Replicator instances with the same path (without address) that are running on other nodes . For convenience it can be used with the DistributedDataDistributedData extension but it can also be started as an ordinary actor using the Replicator.propsReplicator.props. If it is started as an ordinary actor it is important that it is given the same name, started on same path, on all nodes.\nCluster members with status WeaklyUp, will participate in Distributed Data. This means that the data will be replicated to the WeaklyUp nodes with the background gossip protocol. Note that it will not participate in any actions where the consistency mode is to read/write from all nodes or the majority of nodes. The WeaklyUp node is not counted as part of the cluster. So 3 nodes + 5 WeaklyUp is essentially a 3 node cluster as far as consistent actions are concerned.\nBelow is an example of an actor that schedules tick messages to itself and for each tick adds or removes elements from a ORSetORSet (observed-remove set). It also subscribes to changes of this.\nScala copysourceimport java.util.concurrent.ThreadLocalRandom\nimport org.apache.pekko\nimport pekko.actor.Actor\nimport pekko.actor.ActorLogging\nimport pekko.cluster.Cluster\nimport pekko.cluster.ddata.DistributedData\nimport pekko.cluster.ddata.ORSet\nimport pekko.cluster.ddata.ORSetKey\nimport pekko.cluster.ddata.Replicator\nimport pekko.cluster.ddata.Replicator._\n\nobject DataBot {\n  private case object Tick\n}\n\nclass DataBot extends Actor with ActorLogging {\n  import DataBot._\n\n  val replicator = DistributedData(context.system).replicator\n  implicit val node: SelfUniqueAddress = DistributedData(context.system).selfUniqueAddress\n\n  import context.dispatcher\n  val tickTask = context.system.scheduler.scheduleWithFixedDelay(5.seconds, 5.seconds, self, Tick)\n\n  val DataKey = ORSetKey[String](\"key\")\n\n  replicator ! Subscribe(DataKey, self)\n\n  def receive = {\n    case Tick =>\n      val s = ThreadLocalRandom.current().nextInt(97, 123).toChar.toString\n      if (ThreadLocalRandom.current().nextBoolean()) {\n        // add\n        log.info(\"Adding: {}\", s)\n        replicator ! Update(DataKey, ORSet.empty[String], WriteLocal)(_ :+ s)\n      } else {\n        // remove\n        log.info(\"Removing: {}\", s)\n        replicator ! Update(DataKey, ORSet.empty[String], WriteLocal)(_.remove(s))\n      }\n\n    case _: UpdateResponse[_] => // ignore\n    case c @ Changed(DataKey) =>\n      val data = c.get(DataKey)\n      log.info(\"Current elements: {}\", data.elements)\n  }\n\n  override def postStop(): Unit = tickTask.cancel()\n\n} Java copysourceimport java.time.Duration;\nimport java.util.concurrent.ThreadLocalRandom;\n\nimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.Cancellable;\nimport org.apache.pekko.cluster.Cluster;\nimport org.apache.pekko.cluster.ddata.DistributedData;\nimport org.apache.pekko.cluster.ddata.Key;\nimport org.apache.pekko.cluster.ddata.ORSet;\nimport org.apache.pekko.cluster.ddata.ORSetKey;\nimport org.apache.pekko.cluster.ddata.Replicator;\nimport org.apache.pekko.cluster.ddata.Replicator.Changed;\nimport org.apache.pekko.cluster.ddata.Replicator.Subscribe;\nimport org.apache.pekko.cluster.ddata.Replicator.Update;\nimport org.apache.pekko.cluster.ddata.Replicator.UpdateResponse;\nimport org.apache.pekko.cluster.ddata.SelfUniqueAddress;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\n\npublic class DataBot extends AbstractActor {\n\n  private static final String TICK = \"tick\";\n\n  private final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  private final ActorRef replicator = DistributedData.get(getContext().getSystem()).replicator();\n  private final SelfUniqueAddress node =\n      DistributedData.get(getContext().getSystem()).selfUniqueAddress();\n\n  private final Cancellable tickTask =\n      getContext()\n          .getSystem()\n          .scheduler()\n          .scheduleWithFixedDelay(\n              Duration.ofSeconds(5),\n              Duration.ofSeconds(5),\n              getSelf(),\n              TICK,\n              getContext().getDispatcher(),\n              getSelf());\n\n  private final Key<ORSet<String>> dataKey = ORSetKey.create(\"key\");\n\n  @SuppressWarnings(\"unchecked\")\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(String.class, a -> a.equals(TICK), a -> receiveTick())\n        .match(\n            Changed.class,\n            c -> c.key().equals(dataKey),\n            c -> receiveChanged((Changed<ORSet<String>>) c))\n        .match(UpdateResponse.class, r -> receiveUpdateResponse())\n        .build();\n  }\n\n  private void receiveTick() {\n    String s = String.valueOf((char) ThreadLocalRandom.current().nextInt(97, 123));\n    if (ThreadLocalRandom.current().nextBoolean()) {\n      // add\n      log.info(\"Adding: {}\", s);\n      Update<ORSet<String>> update =\n          new Update<>(dataKey, ORSet.create(), Replicator.writeLocal(), curr -> curr.add(node, s));\n      replicator.tell(update, getSelf());\n    } else {\n      // remove\n      log.info(\"Removing: {}\", s);\n      Update<ORSet<String>> update =\n          new Update<>(\n              dataKey, ORSet.create(), Replicator.writeLocal(), curr -> curr.remove(node, s));\n      replicator.tell(update, getSelf());\n    }\n  }\n\n  private void receiveChanged(Changed<ORSet<String>> c) {\n    ORSet<String> data = c.dataValue();\n    log.info(\"Current elements: {}\", data.getElements());\n  }\n\n  private void receiveUpdateResponse() {\n    // ignore\n  }\n\n  @Override\n  public void preStart() {\n    Subscribe<ORSet<String>> subscribe = new Subscribe<>(dataKey, getSelf());\n    replicator.tell(subscribe, ActorRef.noSender());\n  }\n\n  @Override\n  public void postStop() {\n    tickTask.cancel();\n  }\n}","title":"Using the Replicator"},{"location":"/distributed-data.html#update","text":"For the full documentation of this feature and for new projects see Distributed Data - Update.\nTo modify and replicate a data value you send a Replicator.UpdateReplicator.Update message to the local ReplicatorReplicator.\nThe current data value for the key of the Update is passed as parameter to the modifymodify() function of the Update. The function is supposed to return the new value of the data, which will then be replicated according to the given consistency level.\nThe modify function is called by the Replicator actor and must therefore be a pure function that only uses the data parameter and stable fields from enclosing scope. It must for example not access the sender (sender()getSender()) reference of an enclosing actor.\nUpdate is intended to only be sent from an actor running in same local ActorSystemActorSystem as the Replicator, because the modify function is typically not serializable.\nScala copysourceimplicit val node: SelfUniqueAddress = DistributedData(system).selfUniqueAddress\nval replicator = DistributedData(system).replicator\n\nval Counter1Key = PNCounterKey(\"counter1\")\nval Set1Key = GSetKey[String](\"set1\")\nval Set2Key = ORSetKey[String](\"set2\")\nval ActiveFlagKey = FlagKey(\"active\")\n\nreplicator ! Update(Counter1Key, PNCounter(), WriteLocal)(_ :+ 1)\n\nval writeTo3 = WriteTo(n = 3, timeout = 1.second)\nreplicator ! Update(Set1Key, GSet.empty[String], writeTo3)(_ + \"hello\")\n\nval writeMajority = WriteMajority(timeout = 5.seconds)\nreplicator ! Update(Set2Key, ORSet.empty[String], writeMajority)(_ :+ \"hello\")\n\nval writeAll = WriteAll(timeout = 5.seconds)\nreplicator ! Update(ActiveFlagKey, Flag.Disabled, writeAll)(_.switchOn) Java copysourceclass DemonstrateUpdate extends AbstractActor {\n  final SelfUniqueAddress node =\n      DistributedData.get(getContext().getSystem()).selfUniqueAddress();\n  final ActorRef replicator = DistributedData.get(getContext().getSystem()).replicator();\n\n  final Key<PNCounter> counter1Key = PNCounterKey.create(\"counter1\");\n  final Key<GSet<String>> set1Key = GSetKey.create(\"set1\");\n  final Key<ORSet<String>> set2Key = ORSetKey.create(\"set2\");\n  final Key<Flag> activeFlagKey = FlagKey.create(\"active\");\n\n  @Override\n  public Receive createReceive() {\n    ReceiveBuilder b = receiveBuilder();\n\n    b.matchEquals(\n        \"demonstrate update\",\n        msg -> {\n          replicator.tell(\n              new Replicator.Update<PNCounter>(\n                  counter1Key,\n                  PNCounter.create(),\n                  Replicator.writeLocal(),\n                  curr -> curr.increment(node, 1)),\n              getSelf());\n\n          final WriteConsistency writeTo3 = new WriteTo(3, Duration.ofSeconds(1));\n          replicator.tell(\n              new Replicator.Update<GSet<String>>(\n                  set1Key, GSet.create(), writeTo3, curr -> curr.add(\"hello\")),\n              getSelf());\n\n          final WriteConsistency writeMajority = new WriteMajority(Duration.ofSeconds(5));\n          replicator.tell(\n              new Replicator.Update<ORSet<String>>(\n                  set2Key, ORSet.create(), writeMajority, curr -> curr.add(node, \"hello\")),\n              getSelf());\n\n          final WriteConsistency writeAll = new WriteAll(Duration.ofSeconds(5));\n          replicator.tell(\n              new Replicator.Update<Flag>(\n                  activeFlagKey, Flag.create(), writeAll, curr -> curr.switchOn()),\n              getSelf());\n        });\n    return b.build();\n  }\n}\nAs reply of the Update a Replicator.UpdateSuccessReplicator.UpdateSuccess is sent to the sender of the Update if the value was successfully replicated according to the supplied write consistency level within the supplied timeout. Otherwise a Replicator.UpdateFailureReplicator.UpdateFailure subclass is sent back. Note that a Replicator.UpdateTimeoutReplicator.UpdateTimeout reply does not mean that the update completely failed or was rolled back. It may still have been replicated to some nodes, and will eventually be replicated to all nodes with the gossip protocol.\nScala copysourcecase UpdateSuccess(Counter1Key, req) => // ok Java copysourceb.match(\n    UpdateSuccess.class,\n    a -> a.key().equals(counter1Key),\n    a -> {\n      // ok\n    });\nScala copysourcecase UpdateSuccess(Set1Key, req) => // ok\ncase UpdateTimeout(Set1Key, req) =>\n// write to 3 nodes failed within 1.second Java copysourceb.match(\n        UpdateSuccess.class,\n        a -> a.key().equals(set1Key),\n        a -> {\n          // ok\n        })\n    .match(\n        UpdateTimeout.class,\n        a -> a.key().equals(set1Key),\n        a -> {\n          // write to 3 nodes failed within 1.second\n        });\nYou will always see your own writes. For example if you send two Replicator.UpdateReplicator.Update messages changing the value of the same key, the modify function of the second message will see the change that was performed by the first Update message.\nIt is possible to abort the Update when inspecting the state parameter that is passed in to the modify function by throwing an exception. That happens before the update is performed and a Replicator.ModifyFailureReplicator.ModifyFailure is sent back as reply.\nIn the Update message you can pass an optional request context, which the ReplicatorReplicator does not care about, but is included in the reply messages. This is a convenient way to pass contextual information (e.g. original sender) without having to use ask or maintain local correlation data structures.\nScala copysourceimplicit val node = DistributedData(system).selfUniqueAddress\nval replicator = DistributedData(system).replicator\nval writeTwo = WriteTo(n = 2, timeout = 3.second)\nval Counter1Key = PNCounterKey(\"counter1\")\n\ndef receive: Receive = {\n  case \"increment\" =>\n    // incoming command to increase the counter\n    val upd = Update(Counter1Key, PNCounter(), writeTwo, request = Some(sender()))(_ :+ 1)\n    replicator ! upd\n\n  case UpdateSuccess(Counter1Key, Some(replyTo: ActorRef)) =>\n    replyTo ! \"ack\"\n  case UpdateTimeout(Counter1Key, Some(replyTo: ActorRef)) =>\n    replyTo ! \"nack\"\n} Java copysourceclass DemonstrateUpdateWithRequestContext extends AbstractActor {\n  final SelfUniqueAddress node = DistributedData.get(system).selfUniqueAddress();\n  final ActorRef replicator = DistributedData.get(getContext().getSystem()).replicator();\n\n  final WriteConsistency writeTwo = new WriteTo(2, Duration.ofSeconds(3));\n  final Key<PNCounter> counter1Key = PNCounterKey.create(\"counter1\");\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            a -> a.equals(\"increment\"),\n            a -> {\n              // incoming command to increase the counter\n              Optional<Object> reqContext = Optional.of(getSender());\n              Replicator.Update<PNCounter> upd =\n                  new Replicator.Update<PNCounter>(\n                      counter1Key,\n                      PNCounter.create(),\n                      writeTwo,\n                      reqContext,\n                      curr -> curr.increment(node, 1));\n              replicator.tell(upd, getSelf());\n            })\n        .match(\n            UpdateSuccess.class,\n            a -> a.key().equals(counter1Key),\n            a -> {\n              ActorRef replyTo = (ActorRef) a.getRequest().get();\n              replyTo.tell(\"ack\", getSelf());\n            })\n        .match(\n            UpdateTimeout.class,\n            a -> a.key().equals(counter1Key),\n            a -> {\n              ActorRef replyTo = (ActorRef) a.getRequest().get();\n              replyTo.tell(\"nack\", getSelf());\n            })\n        .build();\n  }\n}","title":"Update"},{"location":"/distributed-data.html#get","text":"For the full documentation of this feature and for new projects see Distributed Data - Get.\nTo retrieve the current value of a data you send Replicator.GetReplicator.Get message to the Replicator. You supply a consistency level which has the following meaning:\nScala copysourceval replicator = DistributedData(system).replicator\nval Counter1Key = PNCounterKey(\"counter1\")\nval Set1Key = GSetKey[String](\"set1\")\nval Set2Key = ORSetKey[String](\"set2\")\nval ActiveFlagKey = FlagKey(\"active\")\n\nreplicator ! Get(Counter1Key, ReadLocal)\n\nval readFrom3 = ReadFrom(n = 3, timeout = 1.second)\nreplicator ! Get(Set1Key, readFrom3)\n\nval readMajority = ReadMajority(timeout = 5.seconds)\nreplicator ! Get(Set2Key, readMajority)\n\nval readAll = ReadAll(timeout = 5.seconds)\nreplicator ! Get(ActiveFlagKey, readAll) Java copysourceclass DemonstrateGet extends AbstractActor {\n  final ActorRef replicator = DistributedData.get(getContext().getSystem()).replicator();\n\n  final Key<PNCounter> counter1Key = PNCounterKey.create(\"counter1\");\n  final Key<GSet<String>> set1Key = GSetKey.create(\"set1\");\n  final Key<ORSet<String>> set2Key = ORSetKey.create(\"set2\");\n  final Key<Flag> activeFlagKey = FlagKey.create(\"active\");\n\n  @Override\n  public Receive createReceive() {\n    ReceiveBuilder b = receiveBuilder();\n\n    b.matchEquals(\n        \"demonstrate get\",\n        msg -> {\n          replicator.tell(\n              new Replicator.Get<PNCounter>(counter1Key, Replicator.readLocal()), getSelf());\n\n          final ReadConsistency readFrom3 = new ReadFrom(3, Duration.ofSeconds(1));\n          replicator.tell(new Replicator.Get<GSet<String>>(set1Key, readFrom3), getSelf());\n\n          final ReadConsistency readMajority = new ReadMajority(Duration.ofSeconds(5));\n          replicator.tell(new Replicator.Get<ORSet<String>>(set2Key, readMajority), getSelf());\n\n          final ReadConsistency readAll = new ReadAll(Duration.ofSeconds(5));\n          replicator.tell(new Replicator.Get<Flag>(activeFlagKey, readAll), getSelf());\n        });\n    return b.build();\n  }\n}\nAs reply of the Get a Replicator.GetSuccessReplicator.GetSuccess is sent to the sender of the Get if the value was successfully retrieved according to the supplied read consistency level within the supplied timeout. Otherwise a Replicator.GetFailureReplicator.GetFailure is sent. If the key does not exist the reply will be Replicator.NotFoundReplicator.NotFound.\nScala copysourcecase g @ GetSuccess(Counter1Key, req) =>\n  val value = g.get(Counter1Key).value\ncase NotFound(Counter1Key, req) => // key counter1 does not exist Java copysourceb.match(\n        GetSuccess.class,\n        a -> a.key().equals(counter1Key),\n        a -> {\n          GetSuccess<PNCounter> g = a;\n          BigInteger value = g.dataValue().getValue();\n        })\n    .match(\n        NotFound.class,\n        a -> a.key().equals(counter1Key),\n        a -> {\n          // key counter1 does not exist\n        });\nScala copysourcecase g @ GetSuccess(Set1Key, req) =>\n  val elements = g.get(Set1Key).elements\ncase GetFailure(Set1Key, req) =>\n// read from 3 nodes failed within 1.second\ncase NotFound(Set1Key, req) => // key set1 does not exist Java copysourceb.match(\n        GetSuccess.class,\n        a -> a.key().equals(set1Key),\n        a -> {\n          GetSuccess<GSet<String>> g = a;\n          Set<String> value = g.dataValue().getElements();\n        })\n    .match(\n        GetFailure.class,\n        a -> a.key().equals(set1Key),\n        a -> {\n          // read from 3 nodes failed within 1.second\n        })\n    .match(\n        NotFound.class,\n        a -> a.key().equals(set1Key),\n        a -> {\n          // key set1 does not exist\n        });\nIn the Replicator.GetReplicator.Get message you can pass an optional request context in the same way as for the Replicator.UpdateReplicator.Update message, described above. For example the original sender can be passed and replied to after receiving and transforming Replicator.GetSuccessReplicator.GetSuccess.\nScala copysourceimplicit val node = DistributedData(system).selfUniqueAddress\nval replicator = DistributedData(system).replicator\nval readTwo = ReadFrom(n = 2, timeout = 3.second)\nval Counter1Key = PNCounterKey(\"counter1\")\n\ndef receive: Receive = {\n  case \"get-count\" =>\n    // incoming request to retrieve current value of the counter\n    replicator ! Get(Counter1Key, readTwo, request = Some(sender()))\n\n  case g @ GetSuccess(Counter1Key, Some(replyTo: ActorRef)) =>\n    val value = g.get(Counter1Key).value.longValue\n    replyTo ! value\n  case GetFailure(Counter1Key, Some(replyTo: ActorRef)) =>\n    replyTo ! -1L\n  case NotFound(Counter1Key, Some(replyTo: ActorRef)) =>\n    replyTo ! 0L\n} Java copysourceclass DemonstrateGetWithRequestContext extends AbstractActor {\n  final ActorRef replicator = DistributedData.get(getContext().getSystem()).replicator();\n\n  final ReadConsistency readTwo = new ReadFrom(2, Duration.ofSeconds(3));\n  final Key<PNCounter> counter1Key = PNCounterKey.create(\"counter1\");\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            String.class,\n            a -> a.equals(\"get-count\"),\n            a -> {\n              // incoming request to retrieve current value of the counter\n              Optional<Object> reqContext = Optional.of(getSender());\n              replicator.tell(new Replicator.Get<PNCounter>(counter1Key, readTwo), getSelf());\n            })\n        .match(\n            GetSuccess.class,\n            a -> a.key().equals(counter1Key),\n            a -> {\n              ActorRef replyTo = (ActorRef) a.getRequest().get();\n              GetSuccess<PNCounter> g = a;\n              long value = g.dataValue().getValue().longValue();\n              replyTo.tell(value, getSelf());\n            })\n        .match(\n            GetFailure.class,\n            a -> a.key().equals(counter1Key),\n            a -> {\n              ActorRef replyTo = (ActorRef) a.getRequest().get();\n              replyTo.tell(-1L, getSelf());\n            })\n        .match(\n            NotFound.class,\n            a -> a.key().equals(counter1Key),\n            a -> {\n              ActorRef replyTo = (ActorRef) a.getRequest().get();\n              replyTo.tell(0L, getSelf());\n            })\n        .build();\n  }\n}","title":"Get"},{"location":"/distributed-data.html#subscribe","text":"For the full documentation of this feature and for new projects see Distributed Data - Subscribe.\nYou may also register interest in change notifications by sending Replicator.SubscribeReplicator.Subscribe message to the Replicator. It will send Replicator.ChangedReplicator.Changed messages to the registered subscriber when the data for the subscribed key is updated. Subscribers will be notified periodically with the configured notify-subscribers-interval, and it is also possible to send an explicit Replicator.FlushChanges message to the Replicator to notify the subscribers immediately.\nThe subscriber is automatically removed if the subscriber is terminated. A subscriber can also be deregistered with the Replicator.UnsubscribeReplicator.Unsubscribe message.\nScala copysourceval replicator = DistributedData(system).replicator\nval Counter1Key = PNCounterKey(\"counter1\")\n// subscribe to changes of the Counter1Key value\nreplicator ! Subscribe(Counter1Key, self)\nvar currentValue = BigInt(0)\n\ndef receive: Receive = {\n  case c @ Changed(Counter1Key) =>\n    currentValue = c.get(Counter1Key).value\n  case \"get-count\" =>\n    // incoming request to retrieve current value of the counter\n    sender() ! currentValue\n} Java copysourceclass DemonstrateSubscribe extends AbstractActor {\n  final ActorRef replicator = DistributedData.get(getContext().getSystem()).replicator();\n  final Key<PNCounter> counter1Key = PNCounterKey.create(\"counter1\");\n\n  BigInteger currentValue = BigInteger.valueOf(0);\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Changed.class,\n            a -> a.key().equals(counter1Key),\n            a -> {\n              Changed<PNCounter> g = a;\n              currentValue = g.dataValue().getValue();\n            })\n        .match(\n            String.class,\n            a -> a.equals(\"get-count\"),\n            a -> {\n              // incoming request to retrieve current value of the counter\n              getSender().tell(currentValue, getSender());\n            })\n        .build();\n  }\n\n  @Override\n  public void preStart() {\n    // subscribe to changes of the Counter1Key value\n    replicator.tell(new Subscribe<PNCounter>(counter1Key, getSelf()), ActorRef.noSender());\n  }\n}","title":"Subscribe"},{"location":"/distributed-data.html#consistency","text":"For the full documentation of this feature and for new projects see Distributed Data Consistency.\nHere is an example of using Replicator.WriteMajorityReplicator.WriteMajority and Replicator.ReadMajorityReplicator.ReadMajority:\nScala copysourceprivate val timeout = 3.seconds\nprivate val readMajority = ReadMajority(timeout)\nprivate val writeMajority = WriteMajority(timeout) Java copysourceprivate final WriteConsistency writeMajority = new WriteMajority(Duration.ofSeconds(3));\nprivate static final ReadConsistency readMajority = new ReadMajority(Duration.ofSeconds(3));\nScala copysourcedef receiveGetCart: Receive = {\n  case GetCart =>\n    replicator ! Get(DataKey, readMajority, Some(sender()))\n\n  case g @ GetSuccess(DataKey, Some(replyTo: ActorRef)) =>\n    val data = g.get(DataKey)\n    val cart = Cart(data.entries.values.toSet)\n    replyTo ! cart\n\n  case NotFound(DataKey, Some(replyTo: ActorRef)) =>\n    replyTo ! Cart(Set.empty)\n\n  case GetFailure(DataKey, Some(replyTo: ActorRef)) =>\n    // ReadMajority failure, try again with local read\n    replicator ! Get(DataKey, ReadLocal, Some(replyTo))\n} Java copysourceprivate Receive matchGetCart() {\n  return receiveBuilder()\n      .matchEquals(GET_CART, s -> receiveGetCart())\n      .match(\n          GetSuccess.class,\n          this::isResponseToGetCart,\n          g -> receiveGetSuccess((GetSuccess<LWWMap<String, LineItem>>) g))\n      .match(\n          NotFound.class,\n          this::isResponseToGetCart,\n          n -> receiveNotFound((NotFound<LWWMap<String, LineItem>>) n))\n      .match(\n          GetFailure.class,\n          this::isResponseToGetCart,\n          f -> receiveGetFailure((GetFailure<LWWMap<String, LineItem>>) f))\n      .build();\n}\n\nprivate void receiveGetCart() {\n  Optional<Object> ctx = Optional.of(getSender());\n  replicator.tell(\n      new Replicator.Get<LWWMap<String, LineItem>>(dataKey, readMajority, ctx), getSelf());\n}\n\nprivate boolean isResponseToGetCart(GetResponse<?> response) {\n  return response.key().equals(dataKey)\n      && (response.getRequest().orElse(null) instanceof ActorRef);\n}\n\nprivate void receiveGetSuccess(GetSuccess<LWWMap<String, LineItem>> g) {\n  Set<LineItem> items = new HashSet<>(g.dataValue().getEntries().values());\n  ActorRef replyTo = (ActorRef) g.getRequest().get();\n  replyTo.tell(new Cart(items), getSelf());\n}\n\nprivate void receiveNotFound(NotFound<LWWMap<String, LineItem>> n) {\n  ActorRef replyTo = (ActorRef) n.getRequest().get();\n  replyTo.tell(new Cart(new HashSet<>()), getSelf());\n}\n\nprivate void receiveGetFailure(GetFailure<LWWMap<String, LineItem>> f) {\n  // ReadMajority failure, try again with local read\n  Optional<Object> ctx = Optional.of(getSender());\n  replicator.tell(\n      new Replicator.Get<LWWMap<String, LineItem>>(dataKey, Replicator.readLocal(), ctx),\n      getSelf());\n}\nScala copysourcedef receiveAddItem: Receive = {\n  case cmd @ AddItem(item) =>\n    val update = Update(DataKey, LWWMap.empty[String, LineItem], writeMajority, Some(cmd)) { cart =>\n      updateCart(cart, item)\n    }\n    replicator ! update\n} Java copysourceprivate Receive matchAddItem() {\n  return receiveBuilder().match(AddItem.class, this::receiveAddItem).build();\n}\n\nprivate void receiveAddItem(AddItem add) {\n  Update<LWWMap<String, LineItem>> update =\n      new Update<>(dataKey, LWWMap.create(), writeMajority, cart -> updateCart(cart, add.item));\n  replicator.tell(update, getSelf());\n}\nIn some rare cases, when performing an Replicator.UpdateReplicator.Update it is needed to first try to fetch latest data from other nodes. That can be done by first sending a Replicator.GetReplicator.Get with Replicator.ReadMajorityReplicator.ReadMajority and then continue with the Replicator.UpdateReplicator.Update when the Replicator.GetSuccessReplicator.GetSuccess, Replicator.GetFailureReplicator.GetFailure or Replicator.NotFoundReplicator.NotFound reply is received. This might be needed when you need to base a decision on latest information or when removing entries from an ORSetORSet or ORMapORMap. If an entry is added to an ORSet or ORMap from one node and removed from another node the entry will only be removed if the added entry is visible on the node where the removal is performed (hence the name observed-removed set).\nThe following example illustrates how to do that:\nScala copysourcedef receiveRemoveItem: Receive = {\n  case cmd @ RemoveItem(productId) =>\n    // Try to fetch latest from a majority of nodes first, since ORMap\n    // remove must have seen the item to be able to remove it.\n    replicator ! Get(DataKey, readMajority, Some(cmd))\n\n  case GetSuccess(DataKey, Some(RemoveItem(productId))) =>\n    replicator ! Update(DataKey, LWWMap(), writeMajority, None) {\n      _.remove(node, productId)\n    }\n\n  case GetFailure(DataKey, Some(RemoveItem(productId))) =>\n    // ReadMajority failed, fall back to best effort local value\n    replicator ! Update(DataKey, LWWMap(), writeMajority, None) {\n      _.remove(node, productId)\n    }\n\n  case NotFound(DataKey, Some(RemoveItem(productId))) =>\n  // nothing to remove\n} Java copysourceprivate void receiveRemoveItem(RemoveItem rm) {\n  // Try to fetch latest from a majority of nodes first, since ORMap\n  // remove must have seen the item to be able to remove it.\n  Optional<Object> ctx = Optional.of(rm);\n  replicator.tell(\n      new Replicator.Get<LWWMap<String, LineItem>>(dataKey, readMajority, ctx), getSelf());\n}\n\nprivate void receiveRemoveItemGetSuccess(GetSuccess<LWWMap<String, LineItem>> g) {\n  RemoveItem rm = (RemoveItem) g.getRequest().get();\n  removeItem(rm.productId);\n}\n\nprivate void receiveRemoveItemGetFailure(GetFailure<LWWMap<String, LineItem>> f) {\n  // ReadMajority failed, fall back to best effort local value\n  RemoveItem rm = (RemoveItem) f.getRequest().get();\n  removeItem(rm.productId);\n}\n\nprivate void removeItem(String productId) {\n  Update<LWWMap<String, LineItem>> update =\n      new Update<>(dataKey, LWWMap.create(), writeMajority, cart -> cart.remove(node, productId));\n  replicator.tell(update, getSelf());\n}\n\nprivate boolean isResponseToRemoveItem(GetResponse<?> response) {\n  return response.key().equals(dataKey)\n      && (response.getRequest().orElse(null) instanceof RemoveItem);\n}\nWarning Caveat: Even if you use WriteMajority and ReadMajority there is small risk that you may read stale data if the cluster membership has changed between the Update and the Get. For example, in cluster of 5 nodes when you Update and that change is written to 3 nodes: n1, n2, n3. Then 2 more nodes are added and a Get request is reading from 4 nodes, which happens to be n4, n5, n6, n7, i.e. the value on n1, n2, n3 is not seen in the response of the Get request.","title":"Consistency"},{"location":"/distributed-data.html#delete","text":"For the full documentation of this feature and for new projects see Distributed Data - Delete.\nScala copysourceval replicator = DistributedData(system).replicator\nval Counter1Key = PNCounterKey(\"counter1\")\nval Set2Key = ORSetKey[String](\"set2\")\n\nreplicator ! Delete(Counter1Key, WriteLocal)\n\nval writeMajority = WriteMajority(timeout = 5.seconds)\nreplicator ! Delete(Set2Key, writeMajority) Java copysourceclass DemonstrateDelete extends AbstractActor {\n  final ActorRef replicator = DistributedData.get(getContext().getSystem()).replicator();\n\n  final Key<PNCounter> counter1Key = PNCounterKey.create(\"counter1\");\n  final Key<ORSet<String>> set2Key = ORSetKey.create(\"set2\");\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            \"demonstrate delete\",\n            msg -> {\n              replicator.tell(\n                  new Delete<PNCounter>(counter1Key, Replicator.writeLocal()), getSelf());\n\n              final WriteConsistency writeMajority = new WriteMajority(Duration.ofSeconds(5));\n              replicator.tell(new Delete<PNCounter>(counter1Key, writeMajority), getSelf());\n            })\n        .build();\n  }\n}\nWarning As deleted keys continue to be included in the stored data on each node as well as in gossip messages, a continuous series of updates and deletes of top-level entities will result in growing memory usage until an ActorSystem runs out of memory. To use Pekko Distributed Data where frequent adds and removes are required, you should use a fixed number of top-level data types that support both updates and removals, for example ORMapORMap or ORSetORSet.","title":"Delete"},{"location":"/distributed-data.html#replicated-data-types","text":"Pekko contains a set of useful replicated data types and it is fully possible to implement custom replicated data types. For the full documentation of this feature and for new projects see Distributed Data Replicated data types.","title":"Replicated data types"},{"location":"/distributed-data.html#delta-crdt","text":"For the full documentation of this feature and for new projects see Distributed Data Delta CRDT.","title":"Delta-CRDT"},{"location":"/distributed-data.html#custom-data-type","text":"You can implement your own data types. For the full documentation of this feature and for new projects see Distributed Data custom data type.","title":"Custom Data Type"},{"location":"/distributed-data.html#durable-storage","text":"For the full documentation of this feature and for new projects see Durable Storage.","title":"Durable Storage"},{"location":"/distributed-data.html#limitations","text":"For the full documentation of this feature and for new projects see Limitations.","title":"Limitations"},{"location":"/distributed-data.html#learn-more-about-crdts","text":"Strong Eventual Consistency and Conflict-free Replicated Data Types (video) talk by Mark Shapiro A comprehensive study of Convergent and Commutative Replicated Data Types paper by Mark Shapiro et. al.","title":"Learn More about CRDTs"},{"location":"/distributed-data.html#configuration","text":"The DistributedDataDistributedData extension can be configured with the following properties:\ncopysource# Settings for the DistributedData extension\npekko.cluster.distributed-data {\n  # Actor name of the Replicator actor, /system/ddataReplicator\n  name = ddataReplicator\n\n  # Replicas are running on members tagged with this role.\n  # All members are used if undefined or empty.\n  role = \"\"\n\n  # How often the Replicator should send out gossip information\n  gossip-interval = 2 s\n  \n  # How often the subscribers will be notified of changes, if any\n  notify-subscribers-interval = 500 ms\n\n  # Logging of data with payload size in bytes larger than\n  # this value. Maximum detected size per key is logged once,\n  # with an increase threshold of 10%.\n  # It can be disabled by setting the property to off.\n  log-data-size-exceeding = 10 KiB\n\n  # Maximum number of entries to transfer in one round of gossip exchange when\n  # synchronizing the replicas. Next chunk will be transferred in next round of gossip.\n  # The actual number of data entries in each Gossip message is dynamically\n  # adjusted to not exceed the maximum remote message size (maximum-frame-size).\n  max-delta-elements = 500\n  \n  # The id of the dispatcher to use for Replicator actors.\n  # If specified you need to define the settings of the actual dispatcher.\n  use-dispatcher = \"pekko.actor.internal-dispatcher\"\n\n  # How often the Replicator checks for pruning of data associated with\n  # removed cluster nodes. If this is set to 'off' the pruning feature will\n  # be completely disabled.\n  pruning-interval = 120 s\n  \n  # How long time it takes to spread the data to all other replica nodes.\n  # This is used when initiating and completing the pruning process of data associated\n  # with removed cluster nodes. The time measurement is stopped when any replica is \n  # unreachable, but it's still recommended to configure this with certain margin.\n  # It should be in the magnitude of minutes even though typical dissemination time\n  # is shorter (grows logarithmic with number of nodes). There is no advantage of \n  # setting this too low. Setting it to large value will delay the pruning process.\n  max-pruning-dissemination = 300 s\n  \n  # The markers of that pruning has been performed for a removed node are kept for this\n  # time and thereafter removed. If and old data entry that was never pruned is somehow\n  # injected and merged with existing data after this time the value will not be correct.\n  # This would be possible (although unlikely) in the case of a long network partition.\n  # It should be in the magnitude of hours. For durable data it is configured by \n  # 'pekko.cluster.distributed-data.durable.pruning-marker-time-to-live'.\n pruning-marker-time-to-live = 6 h\n  \n  # Serialized Write and Read messages are cached when they are sent to \n  # several nodes. If no further activity they are removed from the cache\n  # after this duration.\n  serializer-cache-time-to-live = 10s\n\n  # Update and Get operations are sent to oldest nodes first.\n  # This is useful together with Cluster Singleton, which is running on oldest nodes.\n  prefer-oldest = off\n  \n  # Settings for delta-CRDT\n  delta-crdt {\n    # enable or disable delta-CRDT replication\n    enabled = on\n    \n    # Some complex deltas grow in size for each update and above this\n    # threshold such deltas are discarded and sent as full state instead.\n    # This is number of elements or similar size hint, not size in bytes.\n    max-delta-size = 50\n  }\n  \n  durable {\n    # List of keys that are durable. Prefix matching is supported by using * at the\n    # end of a key.  \n    keys = []\n    \n    # The markers of that pruning has been performed for a removed node are kept for this\n    # time and thereafter removed. If and old data entry that was never pruned is\n    # injected and merged with existing data after this time the value will not be correct.\n    # This would be possible if replica with durable data didn't participate in the pruning\n    # (e.g. it was shutdown) and later started after this time. A durable replica should not \n    # be stopped for longer time than this duration and if it is joining again after this\n    # duration its data should first be manually removed (from the lmdb directory).\n    # It should be in the magnitude of days. Note that there is a corresponding setting\n    # for non-durable data: 'pekko.cluster.distributed-data.pruning-marker-time-to-live'.\n    pruning-marker-time-to-live = 10 d\n    \n    # Fully qualified class name of the durable store actor. It must be a subclass\n    # of pekko.actor.Actor and handle the protocol defined in \n    # org.apache.pekko.cluster.ddata.DurableStore. The class must have a constructor with\n    # com.typesafe.config.Config parameter.\n    store-actor-class = org.apache.pekko.cluster.ddata.LmdbDurableStore\n    \n    use-dispatcher = pekko.cluster.distributed-data.durable.pinned-store\n    \n    pinned-store {\n      executor = thread-pool-executor\n      type = PinnedDispatcher\n    }\n    \n    # Config for the LmdbDurableStore\n    lmdb {\n      # Directory of LMDB file. There are two options:\n      # 1. A relative or absolute path to a directory that ends with 'ddata'\n      #    the full name of the directory will contain name of the ActorSystem\n      #    and its remote port.\n      # 2. Otherwise the path is used as is, as a relative or absolute path to\n      #    a directory.\n      #\n      # When running in production you may want to configure this to a specific\n      # path (alt 2), since the default directory contains the remote port of the\n      # actor system to make the name unique. If using a dynamically assigned \n      # port (0) it will be different each time and the previously stored data \n      # will not be loaded.\n      dir = \"ddata\"\n      \n      # Size in bytes of the memory mapped file.\n      map-size = 100 MiB\n      \n      # Accumulate changes before storing improves performance with the\n      # risk of losing the last writes if the JVM crashes.\n      # The interval is by default set to 'off' to write each update immediately.\n      # Enabling write behind by specifying a duration, e.g. 200ms, is especially \n      # efficient when performing many writes to the same key, because it is only \n      # the last value for each key that will be serialized and stored.  \n      # write-behind-interval = 200 ms\n      write-behind-interval = off\n    }\n  }\n  \n}","title":"Configuration"},{"location":"/cluster-dc.html","text":"","title":"Classic Multi-DC Cluster"},{"location":"/cluster-dc.html#classic-multi-dc-cluster","text":"This chapter describes how Pekko Cluster can be used across multiple data centers, availability zones or regions.\nFor the full documentation of this feature and for new projects see Multi-DC Cluster.","title":"Classic Multi-DC Cluster"},{"location":"/cluster-dc.html#membership","text":"You can retrieve information about what data center a member belongs to:\nScala copysourceval cluster = Cluster(system)\n// this node's data center\nval dc = cluster.selfDataCenter\n// all known data centers\nval allDc = cluster.state.allDataCenters\n// a specific member's data center\nval aMember = cluster.state.members.head\nval aDc = aMember.dataCenter Java copysourcefinal Cluster cluster = Cluster.get(system);\n// this node's data center\nString dc = cluster.selfDataCenter();\n// all known data centers\nSet<String> allDc = cluster.state().getAllDataCenters();\n// a specific member's data center\nMember aMember = cluster.state().getMembers().iterator().next();\nString aDc = aMember.dataCenter();\nFor the full documentation of this feature and for new projects see Multi-DC Cluster.","title":"Membership"},{"location":"/cluster-dc.html#cluster-singleton","text":"This is how to create a singleton proxy for a specific data center:\nScala copysourceval proxyDcB = system.actorOf(\n  ClusterSingletonProxy.props(\n    singletonManagerPath = \"/user/consumer\",\n    settings = ClusterSingletonProxySettings(system).withDataCenter(\"B\")),\n  name = \"consumerProxyDcB\") Java copysourceActorRef proxyDcB =\n    system.actorOf(\n        ClusterSingletonProxy.props(\n            \"/user/consumer\",\n            ClusterSingletonProxySettings.create(system)\n                .withRole(\"worker\")\n                .withDataCenter(\"B\")),\n        \"consumerProxyDcB\");\nIf using the own data center as the withDataCenter parameter that would be a proxy for the singleton in the own data center, which is also the default if withDataCenter is not given.\nFor the full documentation of this feature and for new projects see Multi-DC Cluster.","title":"Cluster Singleton"},{"location":"/cluster-dc.html#cluster-sharding","text":"This is how to create a sharding proxy for a specific data center:\nScala copysourceval counterProxyDcB: ActorRef = ClusterSharding(system).startProxy(\n  typeName = \"Counter\",\n  role = None,\n  dataCenter = Some(\"B\"),\n  extractEntityId = extractEntityId,\n  extractShardId = extractShardId) Java copysourceActorRef counterProxyDcB =\n    ClusterSharding.get(system)\n        .startProxy(\n            \"Counter\",\n            Optional.empty(),\n            Optional.of(\"B\"), // data center name\n            messageExtractor);\nFor the full documentation of this feature and for new projects see Multi-DC Cluster.","title":"Cluster Sharding"},{"location":"/serialization-classic.html","text":"","title":"Classic Serialization"},{"location":"/serialization-classic.html#classic-serialization","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nSerialization is the same for Classic and Typed actors. It is described in Serialization, aside from serialization of ActorRef that is described here.","title":"Classic Serialization"},{"location":"/serialization-classic.html#dependency","text":"To use Serialization, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/serialization-classic.html#serializing-actorrefs","text":"All ActorRefs are serializable when using Serialization with Jackson, but in case you are writing your own serializer, you might want to know how to serialize and deserialize them properly. In the general case, the local address to be used depends on the type of remote address which shall be the recipient of the serialized information. Use Serialization.serializedActorPath(actorRef) like this:\nScala copysourceimport org.apache.pekko\nimport pekko.actor._\nimport pekko.actor.typed.scaladsl.Behaviors\nimport pekko.cluster.Cluster\nimport pekko.serialization._\n Java copysourceimport org.apache.pekko.actor.*;\nimport org.apache.pekko.serialization.*;\n\nimport java.nio.charset.Charset;\nimport java.nio.charset.StandardCharsets;\nScala copysource// Serialize\n// (beneath toBinary)\nval serializedRef: String = Serialization.serializedActorPath(theActorRef)\n\n// Then serialize the identifier however you like\n\n// Deserialize\n// (beneath fromBinary)\nval deserializedRef = extendedSystem.provider.resolveActorRef(serializedRef)\n// Then use the ActorRef Java copysource// Serialize\n// (beneath toBinary)\nString serializedRef = Serialization.serializedActorPath(theActorRef);\n\n// Then just serialize the identifier however you like\n\n// Deserialize\n// (beneath fromBinary)\nfinal ActorRef deserializedRef = extendedSystem.provider().resolveActorRef(serializedRef);\n// Then just use the ActorRef\nThis assumes that serialization happens in the context of sending a message through the remote transport. There are other uses of serialization, though, e.g. storing actor references outside of an actor application (database, etc.). In this case, it is important to keep in mind that the address part of an actor’s path determines how that actor is communicated with. Storing a local actor path might be the right choice if the retrieval happens in the same logical context, but it is not enough when deserializing it on a different network host: for that it would need to include the system’s remote transport address.\nScala copysourceval selfAddress = Cluster(system).selfAddress\n\nval serializedRef: String =\n  theActorRef.path.toSerializationFormatWithAddress(selfAddress) Java copysourceAddress selfAddress = Cluster.get(system).selfAddress();\n\nString serializedRef = theActorRef.path().toSerializationFormatWithAddress(selfAddress);\nNote ActorPath.toSerializationFormatWithAddress differs from toString if the address does not already have host and port components, i.e. it only inserts address information for local addresses. toSerializationFormatWithAddress also adds the unique id of the actor, which will change when the actor is stopped and then created again with the same name. Sending messages to a reference pointing the old actor will not be delivered to the new actor. If you don’t want this behavior, e.g. in case of long term storage of the reference, you can use toStringWithAddress, which doesn’t include the unique id.\nThere is also a default remote address which is the one used by cluster support (and typical systems have just this one); you can get it like this:\nScala copysourceval selfAddress = Cluster(system).selfAddress\n\nval serializedRef: String =\n  theActorRef.path.toSerializationFormatWithAddress(selfAddress) Java copysourceAddress selfAddress = Cluster.get(system).selfAddress();\n\nString serializedRef = theActorRef.path().toSerializationFormatWithAddress(selfAddress);","title":"Serializing ActorRefs"},{"location":"/index-network.html","text":"","title":"Classic Networking"},{"location":"/index-network.html#classic-networking","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nI/O Dependency Introduction Terminology, Concepts Architecture in-depth I/O Layer Design Using TCP Dependency Introduction Connecting Accepting connections Closing connections Writing to a connection Throttling Reads and Writes ACK-Based Write Back-Pressure NACK-Based Write Back-Pressure with Suspending Read Back-Pressure with Pull Mode Using UDP Dependency Introduction Unconnected UDP Connected UDP UDP Multicast DNS Extension SRV Records","title":"Classic Networking"},{"location":"/io.html","text":"","title":"I/O"},{"location":"/io.html#i-o","text":"","title":"I/O"},{"location":"/io.html#dependency","text":"To use I/O, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/io.html#introduction","text":"The pekko.io package design combines experiences from the spray-io module with improvements that were jointly developed for more general consumption as an actor-based service.\nThe guiding design goal for this I/O implementation was to reach extreme scalability, make no compromises in providing an API correctly matching the underlying transport mechanism and to be fully event-driven, non-blocking and asynchronous. The API is meant to be a solid foundation for the implementation of network protocols and building higher abstractions; it is not meant to be a full-service high-level NIO wrapper for end users.","title":"Introduction"},{"location":"/io.html#terminology-concepts","text":"The I/O API is completely actor based, meaning that all operations are implemented with message passing instead of direct method calls. Every I/O driver (TCP, UDP) has a special actor, called a manager that serves as an entry point for the API. I/O is broken into several drivers. The manager for a particular driver is accessible through the IO entry pointby querying an ActorSystem. For example the following code looks up the TCP manager and returns its ActorRefActorRef:\nScala copysourceimport org.apache.pekko.io.{ IO, Tcp }\nimport context.system // implicitly used by IO(Tcp)\n\nval manager = IO(Tcp) Java copysourcefinal ActorRef tcpManager = Tcp.get(getContext().getSystem()).manager();\nThe manager receives I/O command messages and instantiates worker actors in response. The worker actors present themselves to the API user in the reply to the command that was sent. For example after a ConnectConnect command sent to the TCP manager the manager creates an actor representing the TCP connection. All operations related to the given TCP connections can be invoked by sending messages to the connection actor which announces itself by sending a ConnectedConnected message.","title":"Terminology, Concepts"},{"location":"/io.html#deathwatch-and-resource-management","text":"I/O worker actors receive commands and also send out events. They usually need a user-side counterpart actor listening for these events (such events could be inbound connections, incoming bytes or acknowledgements for writes). These worker actors watch their listener counterparts. If the listener stops then the worker will automatically release any resources that it holds. This design makes the API more robust against resource leaks.\nThanks to the completely actor based approach of the I/O API the opposite direction works as well: a user actor responsible for handling a connection can watch the connection actor to be notified if it unexpectedly terminates.","title":"DeathWatch and Resource Management"},{"location":"/io.html#write-models-ack-nack-","text":"I/O devices have a maximum throughput which limits the frequency and size of writes. When an application tries to push more data than a device can handle, the driver has to buffer bytes until the device is able to write them. With buffering it is possible to handle short bursts of intensive writes — but no buffer is infinite. “Flow control” is needed to avoid overwhelming device buffers.\nPekko supports two types of flow control:\nAck-based, where the driver notifies the writer when writes have succeeded. Nack-based, where the driver notifies the writer when writes have failed.\nEach of these models is available in both the TCP and the UDP implementations of Pekko I/O.\nIndividual writes can be acknowledged by providing an ack object in the write message (WriteWrite in the case of TCP and SendSend for UDP). When the write is complete the worker will send the ack object to the writing actor. This can be used to implement ack-based flow control; sending new data only when old data has been acknowledged.\nIf a write (or any other command) fails, the driver notifies the actor that sent the command with a special message (CommandFailed in the case of UDP and TCP). This message will also notify the writer of a failed write, serving as a nack for that write. Please note, that in a nack-based flow-control setting the writer has to be prepared for the fact that the failed write might not be the most recent write it sent. For example, the failure notification for a write W1 might arrive after additional write commands W2 and W3 have been sent. If the writer wants to resend any nacked messages it may need to keep a buffer of pending messages.\nWarning An acknowledged write does not mean acknowledged delivery or storage; receiving an ack for a write signals that the I/O driver has successfully processed the write. The Ack/Nack protocol described here is a means of flow control not error handling. In other words, data may still be lost, even if every write is acknowledged.","title":"Write models (Ack, Nack)"},{"location":"/io.html#bytestring","text":"To maintain isolation, actors should communicate with immutable objects only. ByteStringByteString is an immutable container for bytes. It is used by Pekko’s I/O system as an efficient, immutable alternative the traditional byte containers used for I/O on the JVM, such as Array[Byte]byte[] and ByteBuffer.\nByteString is a rope-like data structure that is immutable and provides fast concatenation and slicing operations (perfect for I/O). When two ByteStrings are concatenated together they are both stored within the resulting ByteString instead of copying both to a new Arrayarray. Operations such as dropdrop and taketake return ByteStrings that still reference the original Arrayarray, but just change the offset and length that is visible. Great care has also been taken to make sure that the internal Arrayarray cannot be modified. Whenever a potentially unsafe Arrayarray is used to create a new ByteString a defensive copy is created. If you require a ByteString that only blocks as much memory as necessary for its content, use the compactcompact method to get a CompactByteStringCompactByteString instance. If the ByteString represented only a slice of the original array, this will result in copying all bytes in that slice.\nByteString inherits all methods from IndexedSeq, and it also has some new ones. For more information, look up the util.ByteStringutil.ByteString class and its companion object in the ScalaDoc.\nByteString also comes with its own optimized builder and iterator classes ByteStringBuilderByteStringBuilder and ByteIteratorByteIterator which provide extra features in addition to those of normal builders and iterators.","title":"ByteString"},{"location":"/io.html#compatibility-with-java-io","text":"A ByteStringBuilderByteStringBuilder can be wrapped in a java.io.OutputStream via the asOutputStreamasOutputStream method. Likewise, ByteIteratorByteIterator can be wrapped in a java.io.InputStream via asInputStreamasInputStream. Using these, pekko.io applications can integrate legacy code based on java.io streams.","title":"Compatibility with java.io"},{"location":"/io.html#architecture-in-depth","text":"For further details on the design and internal architecture see I/O Layer Design.\nI/O Layer Design Requirements Basic Architecture Design Benefits How to go about Adding a New Transport","title":"Architecture in-depth"},{"location":"/common/io-layer.html","text":"","title":"I/O Layer Design"},{"location":"/common/io-layer.html#i-o-layer-design","text":"The org.apache.pekko.io package design incorporates the experiences with the spray-io module along with improvements that were jointly developed for more general consumption as an actor-based service. Spray has been deprecated in favour of Akka HTTP and Apache Pekko HTTP.","title":"I/O Layer Design"},{"location":"/common/io-layer.html#requirements","text":"In order to form a general and extensible IO layer basis for a wide range of applications, with Apache Pekko remoting and Spray HTTP being the initial ones, the following requirements were established as key drivers for the design:\nscalability to millions of concurrent connections lowest possible latency in getting data from an input channel into the target actor’s mailbox maximal throughput optional back-pressure in both directions (i.e. throttling local senders as well as allowing local readers to throttle remote senders, where allowed by the protocol) a purely actor-based API with immutable data representation extensibility for integrating new transports by way of a very lean SPI; the goal is to not force I/O mechanisms into a lowest common denominator but instead allow completely protocol-specific user-level APIs.","title":"Requirements"},{"location":"/common/io-layer.html#basic-architecture","text":"Each transport implementation will be made available as a separate Pekko extension, offering an ActorRefActorRef representing the initial point of contact for client code. This “manager” accepts requests for establishing a communications channel (e.g. connect or listen on a TCP socket). Each communications channel is represented by one dedicated actor, which is exposed to client code for all interaction with this channel over its entire lifetime.\nThe central element of the implementation is the transport-specific “selector” actor; in the case of TCP this would wrap a java.nio.channels.Selector. The channel actors register their interest in readability or writability of their channel by sending corresponding messages to their assigned selector actor. However, the actual channel reading and writing is performed by the channel actors themselves, which frees the selector actors from time-consuming tasks and thereby ensures low latency. The selector actor’s only responsibility is the management of the underlying selector’s key set and the actual select operation, which is the only operation to typically block.\nThe assignment of channels to selectors is performed by the manager actor and remains unchanged for the entire lifetime of a channel. Thereby the management actor “stripes” new channels across one or more selector actors based on some implementation-specific distribution logic. This logic may be delegated (in part) to the selectors actors, which could, for example, choose to reject the assignment of a new channel when they consider themselves to be at capacity.\nThe manager actor creates (and therefore supervises) the selector actors, which in turn create and supervise their channel actors. The actor hierarchy of one single transport implementation therefore consists of three distinct actor levels, with the management actor at the top-, the channel actors at the leaf- and the selector actors at the mid-level.\nBack-pressure for output is enabled by allowing the user to specify within its Write messages whether it wants to receive an acknowledgement for enqueuing that write to the O/S kernel. Back-pressure for input is enabled by sending the channel actor a message which temporarily disables read interest for the channel until reading is re-enabled with a corresponding resume command. In the case of transports with flow control—like TCP—the act of not consuming data at the receiving end (thereby causing them to remain in the kernels read buffers) is propagated back to the sender, linking these two mechanisms across the network.","title":"Basic Architecture"},{"location":"/common/io-layer.html#design-benefits","text":"Staying within the actor model for the whole implementation allows us to remove the need for explicit thread handling logic, and it also means that there are no locks involved (besides those which are part of the underlying transport library). Writing only actor code results in a cleaner implementation, while Pekko’s efficient actor messaging does not impose a high tax for this benefit. In fact the event-based nature of I/O maps so well to the actor model that we expect clear performance and especially scalability benefits over traditional solutions with explicit thread management and synchronization.\nAnother benefit of supervision hierarchies is that clean-up of resources comes naturally: shutting down a selector actor will automatically clean up all channel actors, allowing proper closing of the channels and sending the appropriate messages to user-level client actors. DeathWatch allows the channel actors to notice the demise of their user-level handler actors and terminate in an orderly fashion in that case as well; this naturally reduces the chances of leaking open channels.\nThe choice of using ActorRefActorRef for exposing all functionality entails that these references can be distributed or delegated freely and in general handled as the user sees fit, including the use of remoting and life-cycle monitoring (just to name two).","title":"Design Benefits"},{"location":"/common/io-layer.html#how-to-go-about-adding-a-new-transport","text":"The best start is to study the TCP reference implementation to get a good grip on the basic working principle and then design an implementation, which is similar in spirit, but adapted to the new protocol in question. There are vast differences between I/O mechanisms (e.g. compare file I/O to a message broker) and the goal of this I/O layer is explicitly not to shoehorn all of them into a uniform API, which is why only the basic architecture ideas are documented here.","title":"How to go about Adding a New Transport"},{"location":"/io-tcp.html","text":"","title":"Using TCP"},{"location":"/io-tcp.html#using-tcp","text":"","title":"Using TCP"},{"location":"/io-tcp.html#dependency","text":"To use TCP, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/io-tcp.html#introduction","text":"The code snippets through-out this section assume the following imports:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.{ Actor, ActorRef, Props }\nimport pekko.io.{ IO, Tcp }\nimport pekko.util.ByteString\nimport java.net.InetSocketAddress Java copysourceimport java.net.InetSocketAddress;\nimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.actor.Props;\nimport org.apache.pekko.actor.AbstractActor;\nimport org.apache.pekko.io.Tcp;\nimport org.apache.pekko.io.Tcp.Bound;\nimport org.apache.pekko.io.Tcp.CommandFailed;\nimport org.apache.pekko.io.Tcp.Connected;\nimport org.apache.pekko.io.Tcp.ConnectionClosed;\nimport org.apache.pekko.io.Tcp.Received;\nimport org.apache.pekko.io.TcpMessage;\nimport org.apache.pekko.util.ByteString;\nAll of the Pekko I/O APIs are accessed through manager objects. When using an I/O API, the first step is to acquire a reference to the appropriate manager. The code below shows how to acquire a reference to the Tcp manager.\nScala copysourceimport org.apache.pekko.io.{ IO, Tcp }\nimport context.system // implicitly used by IO(Tcp)\n\nval manager = IO(Tcp) Java copysourcefinal ActorRef tcpManager = Tcp.get(getContext().getSystem()).manager();\nThe manager is an actor that handles the underlying low level I/O resources (selectors, channels) and instantiates workers for specific tasks, such as listening to incoming connections.","title":"Introduction"},{"location":"/io-tcp.html#connecting","text":"Scala copysourceobject Client {\n  def props(remote: InetSocketAddress, replies: ActorRef) =\n    Props(classOf[Client], remote, replies)\n}\n\nclass Client(remote: InetSocketAddress, listener: ActorRef) extends Actor {\n\n  import Tcp._\n  import context.system\n\n  IO(Tcp) ! Connect(remote)\n\n  def receive = {\n    case CommandFailed(_: Connect) =>\n      listener ! \"connect failed\"\n      context.stop(self)\n\n    case c @ Connected(remote, local) =>\n      listener ! c\n      val connection = sender()\n      connection ! Register(self)\n      context.become {\n        case data: ByteString =>\n          connection ! Write(data)\n        case CommandFailed(w: Write) =>\n          // O/S buffer was full\n          listener ! \"write failed\"\n        case Received(data) =>\n          listener ! data\n        case \"close\" =>\n          connection ! Close\n        case _: ConnectionClosed =>\n          listener ! \"connection closed\"\n          context.stop(self)\n      }\n  }\n} Java copysourcestatic class Client extends AbstractActor {\n\n  final InetSocketAddress remote;\n  final ActorRef listener;\n\n  public static Props props(InetSocketAddress remote, ActorRef listener) {\n    return Props.create(Client.class, remote, listener);\n  }\n\n  public Client(InetSocketAddress remote, ActorRef listener) {\n    this.remote = remote;\n    this.listener = listener;\n\n    final ActorRef tcp = Tcp.get(getContext().getSystem()).manager();\n    tcp.tell(TcpMessage.connect(remote), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            CommandFailed.class,\n            msg -> {\n              listener.tell(\"failed\", getSelf());\n              getContext().stop(getSelf());\n            })\n        .match(\n            Connected.class,\n            msg -> {\n              listener.tell(msg, getSelf());\n              getSender().tell(TcpMessage.register(getSelf()), getSelf());\n              getContext().become(connected(getSender()));\n            })\n        .build();\n  }\n\n  private Receive connected(final ActorRef connection) {\n    return receiveBuilder()\n        .match(\n            ByteString.class,\n            msg -> {\n              connection.tell(TcpMessage.write((ByteString) msg), getSelf());\n            })\n        .match(\n            CommandFailed.class,\n            msg -> {\n              // OS kernel socket buffer was full\n            })\n        .match(\n            Received.class,\n            msg -> {\n              listener.tell(msg.data(), getSelf());\n            })\n        .matchEquals(\n            \"close\",\n            msg -> {\n              connection.tell(TcpMessage.close(), getSelf());\n            })\n        .match(\n            ConnectionClosed.class,\n            msg -> {\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n}\nThe first step of connecting to a remote address is sending a Connect messagemessage by the TcpMessage.connect method to the TCP manager; in addition to the simplest form shown above there is also the possibility to specify a local InetSocketAddress to bind to and a list of socket options to apply.\nNote The SO_NODELAY (TCP_NODELAY on Windows) socket option defaults to true in Pekko, independently of the OS default settings. This setting disables Nagle’s algorithm, considerably improving latency for most applications. This setting could be overridden by passing SO.TcpNoDelay(false) in the list of socket options of the Connect messagemessage by the TcpMessage.connect method.\nThe TCP manager will then reply either with a CommandFailed or it will spawn an internal actor representing the new connection. This new actor will then send a Connected message to the original sender of the Connect messagemessage by the TcpMessage.connect method.\nIn order to activate the new connection a Register messagemessage by the TcpMessage.register method must be sent to the connection actor, informing that one about who shall receive data from the socket. Before this step is done the connection cannot be used, and there is an internal timeout after which the connection actor will shut itself down if no Register messagemessage by the TcpMessage.register method message is received.\nThe connection actor watches the registered handler and closes the connection when that one terminates, thereby cleaning up all internal resources associated with that connection.\nThe actor in the example above uses become to switch from unconnected to connected operation, demonstrating the commands and events which are observed in that state. For a discussion on CommandFailed see Throttling Reads and Writes below. ConnectionClosed is a trait, which marks the different connection close events. The last line handles all connection close events in the same way. It is possible to listen for more fine-grained connection close events, see Closing Connections below.","title":"Connecting"},{"location":"/io-tcp.html#accepting-connections","text":"Scala copysourceclass Server extends Actor {\n\n  import Tcp._\n  import context.system\n\n  IO(Tcp) ! Bind(self, new InetSocketAddress(\"localhost\", 0))\n\n  def receive = {\n    case b @ Bound(localAddress) =>\n      context.parent ! b\n\n    case CommandFailed(_: Bind) => context.stop(self)\n\n    case c @ Connected(remote, local) =>\n      val handler = context.actorOf(Props[SimplisticHandler]())\n      val connection = sender()\n      connection ! Register(handler)\n  }\n\n} Java copysourcestatic class Server extends AbstractActor {\n\n  final ActorRef manager;\n\n  public Server(ActorRef manager) {\n    this.manager = manager;\n  }\n\n  public static Props props(ActorRef manager) {\n    return Props.create(Server.class, manager);\n  }\n\n  @Override\n  public void preStart() throws Exception {\n    final ActorRef tcp = Tcp.get(getContext().getSystem()).manager();\n    tcp.tell(TcpMessage.bind(getSelf(), new InetSocketAddress(\"localhost\", 0), 100), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Bound.class,\n            msg -> {\n              manager.tell(msg, getSelf());\n            })\n        .match(\n            CommandFailed.class,\n            msg -> {\n              getContext().stop(getSelf());\n            })\n        .match(\n            Connected.class,\n            conn -> {\n              manager.tell(conn, getSelf());\n              final ActorRef handler =\n                  getContext().actorOf(Props.create(SimplisticHandler.class));\n              getSender().tell(TcpMessage.register(handler), getSelf());\n            })\n        .build();\n  }\n}\nTo create a TCP server and listen for inbound connections, a Bind commandmessage by the TcpMessage.bind method has to be sent to the TCP manager. This will instruct the TCP manager to listen for TCP connections on a particular InetSocketAddress; the port may be specified as 0 in order to bind to a random port.\nThe actor sending the Bind messagemessage by the TcpMessage.bind method will receive a Bound message signaling that the server is ready to accept incoming connections; this message also contains the InetSocketAddress to which the socket was actually bound (i.e. resolved IP address and correct port number).\nFrom this point forward the process of handling connections is the same as for outgoing connections. The example demonstrates that handling the reads from a certain connection can be delegated to another actor by naming it as the handler when sending the Register messagemessage by the TcpMessage.register method. Writes can be sent from any actor in the system to the connection actor (i.e. the actor which sent the Connected message). The simplistic handler is defined as:\nScala copysourceclass SimplisticHandler extends Actor {\n  import Tcp._\n  def receive = {\n    case Received(data) => sender() ! Write(data)\n    case PeerClosed     => context.stop(self)\n  }\n} Java copysourcestatic class SimplisticHandler extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Received.class,\n            msg -> {\n              final ByteString data = msg.data();\n              System.out.println(data);\n              getSender().tell(TcpMessage.write(data), getSelf());\n            })\n        .match(\n            ConnectionClosed.class,\n            msg -> {\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n}\nFor a more complete sample which also takes into account the possibility of failures when sending please see Throttling Reads and Writes below.\nThe only difference to outgoing connections is that the internal actor managing the listen port—the sender of the Bound message—watches the actor which was named as the recipient for Connected messages in the Bind messageTcpMessage.bind method. When that actor terminates the listen port will be closed and all resources associated with it will be released; existing connections will not be terminated at this point.","title":"Accepting connections"},{"location":"/io-tcp.html#closing-connections","text":"A connection can be closed by sending one of the commands Close, ConfirmedClose or Abort a message by one of the methods TcpMessage.close, TcpMessage.confirmedClose or TcpMessage.abort to the connection actor.\nCloseTcpMessage.close will close the connection by sending a FIN message, but without waiting for confirmation from the remote endpoint. Pending writes will be flushed. If the close is successful, the listener will be notified with Closed.\nConfirmedCloseTcpMessage.confirmedClose will close the sending direction of the connection by sending a FIN message, but data will continue to be received until the remote endpoint closes the connection, too. Pending writes will be flushed. If the close is successful, the listener will be notified with ConfirmedClosed.\nAbortTcpMessage.abort will immediately terminate the connection by sending a RST message to the remote endpoint. Pending writes will be not flushed. If the close is successful, the listener will be notified with Aborted.\nPeerClosed will be sent to the listener if the connection has been closed by the remote endpoint. Per default, the connection will then automatically be closed from this endpoint as well. To support half-closed connections set the keepOpenOnPeerClosed member of the Register messageTcpMessage.register method to true in which case the connection stays open until it receives one of the above close commands.\nErrorClosed will be sent to the listener whenever an error happened that forced the connection to be closed.\nAll close notifications are sub-types of ConnectionClosed so listeners who do not need fine-grained close events may handle all close events in the same way.","title":"Closing connections"},{"location":"/io-tcp.html#writing-to-a-connection","text":"Once a connection has been established data can be sent to it from any actor in the form of a Tcp.WriteCommand. Tcp.WriteCommand is an abstract class with three concrete implementations:\nTcp.Write The simplest WriteCommand implementation which wraps a ByteString instance and an “ack” event. A ByteString (as explained in this section) models one or more chunks of immutable in-memory data with a maximum (total) size of 2 GB (2^31 bytes). Tcp.WriteFile If you want to send “raw” data from a file you can do so efficiently with the Tcp.WriteFile command. This allows you do designate a (contiguous) chunk of on-disk bytes for sending across the connection without the need to first load them into the JVM memory. As such Tcp.WriteFile can “hold” more than 2GB of data and an “ack” event if required. Tcp.CompoundWrite Sometimes, you might want to group (or interleave) several Tcp.Write and/or Tcp.WriteFile commands into one atomic write command which gets written to the connection in one go. The Tcp.CompoundWrite allows you to do just that and offers three benefits:\nAs explained in the following section the TCP connection actor can only handle one single write command at a time. By combining several writes into one CompoundWrite you can have them be sent across the connection with minimum overhead and without the need to spoon feed them to the connection actor via an ACK-based message protocol. Because a WriteCommand is atomic you can be sure that no other actor can “inject” other writes into your series of writes if you combine them into one single CompoundWrite. In scenarios where several actors write to the same connection this can be an important feature which can be somewhat hard to achieve otherwise. The “sub writes” of a CompoundWrite are regular Write or WriteFile commandsmessages by TcpMessage.write or TcpMessage.writeFile methods that themselves can request “ack” events. These ACKs are sent out as soon as the respective “sub write” has been completed. This allows you to attach more than one ACK to a Write or WriteFilemessage by TcpMessage.write or TcpMessage.writeFile (by combining it with an empty write that itself requests an ACK) or to have the connection actor acknowledge the progress of transmitting the CompoundWrite by sending out intermediate ACKs at arbitrary points.","title":"Writing to a connection"},{"location":"/io-tcp.html#throttling-reads-and-writes","text":"The basic model of the TCP connection actor is that it has no internal buffering (i.e. it can only process one write at a time, meaning it can buffer one write until it has been passed on to the O/S kernel in full). Congestion needs to be handled at the user level, for both writes and reads.\nFor back-pressuring writes there are three modes of operation\nACK-based: every Write command carries an arbitrary object, and if this object is not Tcp.NoAck then it will be returned to the sender of the Write upon successfully writing all contained data to the socket. If no other write is initiated before having received this acknowledgement then no failures can happen due to buffer overrun. NACK-based: every write which arrives while a previous write is not yet completed will be replied to with a CommandFailed message containing the failed write. Just relying on this mechanism requires the implemented protocol to tolerate skipping writes (e.g. if each write is a valid message on its own and it is not required that all are delivered). This mode is enabled by setting the useResumeWriting flag to false within the Register messagemessage by the TcpMessage.register method during connection activation. NACK-based with write suspending: this mode is very similar to the NACK-based one, but once a single write has failed no further writes will succeed until a ResumeWriting messagemessage by the TcpMessage.resumeWriting method is received. This message will be answered with a WritingResumed message once the last accepted write has completed. If the actor driving the connection implements buffering and resends the NACK’ed messages after having awaited the WritingResumed signal then every message is delivered exactly once to the network socket.\nThese write back-pressure models (with the exception of the second which is rather specialised) are demonstrated in complete examples below. The full and contiguous source is available on GitHubon GitHub.\nFor back-pressuring reads there are two modes of operation\nPush-reading: in this mode the connection actor sends the registered reader actor incoming data as soon as available as Received events. Whenever the reader actor wants to signal back-pressure to the remote TCP endpoint it can send a SuspendReading messagemessage by the TcpMessage.suspendReading method to the connection actor to indicate that it wants to suspend the reception of new data. No Received events will arrive until a corresponding ResumeReading is sent indicating that the receiver actor is ready again. Pull-reading: after sending a Received event the connection actor automatically suspends accepting data from the socket until the reader actor signals with a ResumeReading message that it is ready to process more input data. Hence new data is “pulled” from the connection by sending ResumeReading messages.\nNote It should be obvious that all these flow control schemes only work between one writer/reader and one connection actor; as soon as multiple actors send write commands to a single connection no consistent result can be achieved.","title":"Throttling Reads and Writes"},{"location":"/io-tcp.html#ack-based-write-back-pressure","text":"For proper function of the following example it is important to configure the connection to remain half-open when the remote side closed its writing end: this allows the example EchoHandler to write all outstanding data back to the client before fully closing the connection. This is enabled using a flag upon connection activation (observe the Register messageTcpMessage.register method):\nScala copysourcecase Connected(remote, local) =>\n  log.info(\"received connection from {}\", remote)\n  val handler = context.actorOf(Props(handlerClass, sender(), remote))\n  sender() ! Register(handler, keepOpenOnPeerClosed = true) Java copysourceconnection.tell(\n    TcpMessage.register(\n        handler, true, // <-- keepOpenOnPeerClosed flag\n        true),\n    getSelf());\nWith this preparation let us dive into the handler itself:\nScala copysourceclass SimpleEchoHandler(connection: ActorRef, remote: InetSocketAddress) extends Actor with ActorLogging {\n\n  import Tcp._\n\n  // sign death pact: this actor terminates when connection breaks\n  context.watch(connection)\n\n  case object Ack extends Event\n\n  def receive = {\n    case Received(data) =>\n      buffer(data)\n      connection ! Write(data, Ack)\n\n      context.become({\n          case Received(data) => buffer(data)\n          case Ack            => acknowledge()\n          case PeerClosed     => closing = true\n        }, discardOld = false)\n\n    case PeerClosed => context.stop(self)\n  }\n\n  override def postStop(): Unit = {\n    log.info(s\"transferred $transferred bytes from/to [$remote]\")\n  }\n\n  var storage = Vector.empty[ByteString]\n  var stored = 0L\n  var transferred = 0L\n  var closing = false\n\n  val maxStored = 100000000L\n  val highWatermark = maxStored * 5 / 10\n  val lowWatermark = maxStored * 3 / 10\n  var suspended = false\n\n  private def buffer(data: ByteString): Unit = {\n    storage :+= data\n    stored += data.size\n\n    if (stored > maxStored) {\n      log.warning(s\"drop connection to [$remote] (buffer overrun)\")\n      context.stop(self)\n\n    } else if (stored > highWatermark) {\n      log.debug(s\"suspending reading\")\n      connection ! SuspendReading\n      suspended = true\n    }\n  }\n\n  private def acknowledge(): Unit = {\n    require(storage.nonEmpty, \"storage was empty\")\n\n    val size = storage(0).size\n    stored -= size\n    transferred += size\n\n    storage = storage.drop(1)\n\n    if (suspended && stored < lowWatermark) {\n      log.debug(\"resuming reading\")\n      connection ! ResumeReading\n      suspended = false\n    }\n\n    if (storage.isEmpty) {\n      if (closing) context.stop(self)\n      else context.unbecome()\n    } else connection ! Write(storage(0), Ack)\n  }\n} Java copysourcepublic class SimpleEchoHandler extends AbstractActor {\n\n  final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), getSelf());\n\n  final ActorRef connection;\n  final InetSocketAddress remote;\n\n  public static final long maxStored = 100000000;\n  public static final long highWatermark = maxStored * 5 / 10;\n  public static final long lowWatermark = maxStored * 2 / 10;\n\n  public SimpleEchoHandler(ActorRef connection, InetSocketAddress remote) {\n    this.connection = connection;\n    this.remote = remote;\n\n    // sign death pact: this actor stops when the connection is closed\n    getContext().watch(connection);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Received.class,\n            msg -> {\n              final ByteString data = msg.data();\n              buffer(data);\n              connection.tell(TcpMessage.write(data, ACK), getSelf());\n              // now switch behavior to “waiting for acknowledgement”\n              getContext().become(buffering(), false);\n            })\n        .match(\n            ConnectionClosed.class,\n            msg -> {\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n\n  private Receive buffering() {\n    return receiveBuilder()\n        .match(\n            Received.class,\n            msg -> {\n              buffer(msg.data());\n            })\n        .match(\n            Event.class,\n            msg -> msg == ACK,\n            msg -> {\n              acknowledge();\n            })\n        .match(\n            ConnectionClosed.class,\n            msg -> {\n              if (msg.isPeerClosed()) {\n                closing = true;\n              } else {\n                // could also be ErrorClosed, in which case we just give up\n                getContext().stop(getSelf());\n              }\n            })\n        .build();\n  }\n\n  public void postStop() {\n    log.info(\"transferred {} bytes from/to [{}]\", transferred, remote);\n  }\n\n  private long transferred;\n  private long stored = 0;\n  private Queue<ByteString> storage = new LinkedList<>();\n\n  private boolean suspended = false;\n  private boolean closing = false;\n\n  private final Event ACK = new Event() {};\n\n  protected void buffer(ByteString data) {\n    storage.add(data);\n    stored += data.size();\n\n    if (stored > maxStored) {\n      log.warning(\"drop connection to [{}] (buffer overrun)\", remote);\n      getContext().stop(getSelf());\n\n    } else if (stored > highWatermark) {\n      log.debug(\"suspending reading\");\n      connection.tell(TcpMessage.suspendReading(), getSelf());\n      suspended = true;\n    }\n  }\n\n  protected void acknowledge() {\n    final ByteString acked = storage.remove();\n    stored -= acked.size();\n    transferred += acked.size();\n\n    if (suspended && stored < lowWatermark) {\n      log.debug(\"resuming reading\");\n      connection.tell(TcpMessage.resumeReading(), getSelf());\n      suspended = false;\n    }\n\n    if (storage.isEmpty()) {\n      if (closing) {\n        getContext().stop(getSelf());\n      } else {\n        getContext().unbecome();\n      }\n    } else {\n      connection.tell(TcpMessage.write(storage.peek(), ACK), getSelf());\n    }\n  }\n}\nThe principle is simple: when having written a chunk always wait for the Ack to come back before sending the next chunk. While waiting we switch behavior such that new incoming data are buffered. The helper functions used are a bit lengthy but not complicated:\nScala copysourceprivate def buffer(data: ByteString): Unit = {\n  storage :+= data\n  stored += data.size\n\n  if (stored > maxStored) {\n    log.warning(s\"drop connection to [$remote] (buffer overrun)\")\n    context.stop(self)\n\n  } else if (stored > highWatermark) {\n    log.debug(s\"suspending reading\")\n    connection ! SuspendReading\n    suspended = true\n  }\n}\n\nprivate def acknowledge(): Unit = {\n  require(storage.nonEmpty, \"storage was empty\")\n\n  val size = storage(0).size\n  stored -= size\n  transferred += size\n\n  storage = storage.drop(1)\n\n  if (suspended && stored < lowWatermark) {\n    log.debug(\"resuming reading\")\n    connection ! ResumeReading\n    suspended = false\n  }\n\n  if (storage.isEmpty) {\n    if (closing) context.stop(self)\n    else context.unbecome()\n  } else connection ! Write(storage(0), Ack)\n} Java copysourceprotected void buffer(ByteString data) {\n  storage.add(data);\n  stored += data.size();\n\n  if (stored > maxStored) {\n    log.warning(\"drop connection to [{}] (buffer overrun)\", remote);\n    getContext().stop(getSelf());\n\n  } else if (stored > highWatermark) {\n    log.debug(\"suspending reading\");\n    connection.tell(TcpMessage.suspendReading(), getSelf());\n    suspended = true;\n  }\n}\n\nprotected void acknowledge() {\n  final ByteString acked = storage.remove();\n  stored -= acked.size();\n  transferred += acked.size();\n\n  if (suspended && stored < lowWatermark) {\n    log.debug(\"resuming reading\");\n    connection.tell(TcpMessage.resumeReading(), getSelf());\n    suspended = false;\n  }\n\n  if (storage.isEmpty()) {\n    if (closing) {\n      getContext().stop(getSelf());\n    } else {\n      getContext().unbecome();\n    }\n  } else {\n    connection.tell(TcpMessage.write(storage.peek(), ACK), getSelf());\n  }\n}\nThe most interesting part is probably the last: an Ack removes the oldest data chunk from the buffer, and if that was the last chunk then we either close the connection (if the peer closed its half already) or return to the idle behavior; otherwise we send the next buffered chunk and stay waiting for the next Ack.\nBack-pressure can be propagated also across the reading side back to the writer on the other end of the connection by sending the SuspendReading commandmessage by the TcpMessage.suspendReading method to the connection actor. This will lead to no data being read from the socket anymore (although this does happen after a delay because it takes some time until the connection actor processes this command, hence appropriate head-room in the buffer should be present), which in turn will lead to the O/S kernel buffer filling up on our end, then the TCP window mechanism will stop the remote side from writing, filling up its write buffer, until finally the writer on the other side cannot push any data into the socket anymore. This is how end-to-end back-pressure is realized across a TCP connection.","title":"ACK-Based Write Back-Pressure"},{"location":"/io-tcp.html#nack-based-write-back-pressure-with-suspending","text":"Scala copysourceobject EchoHandler {\n  final case class Ack(offset: Int) extends Tcp.Event\n\n  def props(connection: ActorRef, remote: InetSocketAddress): Props =\n    Props(classOf[EchoHandler], connection, remote)\n}\n\nclass EchoHandler(connection: ActorRef, remote: InetSocketAddress) extends Actor with ActorLogging {\n\n  import Tcp._\n  import EchoHandler._\n\n  // sign death pact: this actor terminates when connection breaks\n  context.watch(connection)\n\n  // start out in optimistic write-through mode\n  def receive = writing\n\n  def writing: Receive = {\n    case Received(data) =>\n      connection ! Write(data, Ack(currentOffset))\n      buffer(data)\n\n    case Ack(ack) =>\n      acknowledge(ack)\n\n    case CommandFailed(Write(_, Ack(ack))) =>\n      connection ! ResumeWriting\n      context.become(buffering(ack))\n\n    case PeerClosed =>\n      if (storage.isEmpty) context.stop(self)\n      else context.become(closing)\n  }\n\n  def buffering(nack: Int): Receive = {\n    var toAck = 10\n    var peerClosed = false\n\n    {\n      case Received(data)         => buffer(data)\n      case WritingResumed         => writeFirst()\n      case PeerClosed             => peerClosed = true\n      case Ack(ack) if ack < nack => acknowledge(ack)\n      case Ack(ack) =>\n        acknowledge(ack)\n        if (storage.nonEmpty) {\n          if (toAck > 0) {\n            // stay in ACK-based mode for a while\n            writeFirst()\n            toAck -= 1\n          } else {\n            // then return to NACK-based again\n            writeAll()\n            context.become(if (peerClosed) closing else writing)\n          }\n        } else if (peerClosed) context.stop(self)\n        else context.become(writing)\n    }\n  }\n\n  def closing: Receive = {\n    case CommandFailed(_: Write) =>\n      connection ! ResumeWriting\n      context.become({\n\n          case WritingResumed =>\n            writeAll()\n            context.unbecome()\n\n          case ack: Int => acknowledge(ack)\n\n        }, discardOld = false)\n\n    case Ack(ack) =>\n      acknowledge(ack)\n      if (storage.isEmpty) context.stop(self)\n  }\n\n  override def postStop(): Unit = {\n    log.info(s\"transferred $transferred bytes from/to [$remote]\")\n  }\n\n  private var storageOffset = 0\n  private var storage = Vector.empty[ByteString]\n  private var stored = 0L\n  private var transferred = 0L\n\n  val maxStored = 100000000L\n  val highWatermark = maxStored * 5 / 10\n  val lowWatermark = maxStored * 3 / 10\n  private var suspended = false\n\n  private def currentOffset = storageOffset + storage.size\n\n  private def buffer(data: ByteString): Unit = {\n    storage :+= data\n    stored += data.size\n\n    if (stored > maxStored) {\n      log.warning(s\"drop connection to [$remote] (buffer overrun)\")\n      context.stop(self)\n\n    } else if (stored > highWatermark) {\n      log.debug(s\"suspending reading at $currentOffset\")\n      connection ! SuspendReading\n      suspended = true\n    }\n  }\n\n  private def acknowledge(ack: Int): Unit = {\n    require(ack == storageOffset, s\"received ack $ack at $storageOffset\")\n    require(storage.nonEmpty, s\"storage was empty at ack $ack\")\n\n    val size = storage(0).size\n    stored -= size\n    transferred += size\n\n    storageOffset += 1\n    storage = storage.drop(1)\n\n    if (suspended && stored < lowWatermark) {\n      log.debug(\"resuming reading\")\n      connection ! ResumeReading\n      suspended = false\n    }\n  }\n\n  private def writeFirst(): Unit = {\n    connection ! Write(storage(0), Ack(storageOffset))\n  }\n\n  private def writeAll(): Unit = {\n    for ((data, i) <- storage.zipWithIndex) {\n      connection ! Write(data, Ack(storageOffset + i))\n    }\n  }\n\n} Java copysourcepublic class EchoHandler extends AbstractActor {\n\n  final LoggingAdapter log = Logging.getLogger(getContext().getSystem(), getSelf());\n\n  final ActorRef connection;\n  final InetSocketAddress remote;\n\n  public static final long MAX_STORED = 100000000;\n  public static final long HIGH_WATERMARK = MAX_STORED * 5 / 10;\n  public static final long LOW_WATERMARK = MAX_STORED * 2 / 10;\n\n  private long transferred;\n  private int storageOffset = 0;\n  private long stored = 0;\n  private Queue<ByteString> storage = new LinkedList<>();\n\n  private boolean suspended = false;\n\n  private static class Ack implements Event {\n    public final int ack;\n\n    public Ack(int ack) {\n      this.ack = ack;\n    }\n  }\n\n  public EchoHandler(ActorRef connection, InetSocketAddress remote) {\n    this.connection = connection;\n    this.remote = remote;\n\n    writing = writing();\n\n    // sign death pact: this actor stops when the connection is closed\n    getContext().watch(connection);\n\n    // start out in optimistic write-through mode\n    getContext().become(writing);\n  }\n\n  @Override\n  public Receive createReceive() {\n    return writing;\n  }\n\n  private final Receive writing;\n\n  private Receive writing() {\n    return receiveBuilder()\n        .match(\n            Received.class,\n            msg -> {\n              final ByteString data = msg.data();\n              connection.tell(TcpMessage.write(data, new Ack(currentOffset())), getSelf());\n              buffer(data);\n            })\n        .match(\n            Integer.class,\n            msg -> {\n              acknowledge(msg);\n            })\n        .match(\n            CommandFailed.class,\n            msg -> {\n              final Write w = (Write) msg.cmd();\n              connection.tell(TcpMessage.resumeWriting(), getSelf());\n              getContext().become(buffering((Ack) w.ack()));\n            })\n        .match(\n            ConnectionClosed.class,\n            msg -> {\n              if (msg.isPeerClosed()) {\n                if (storage.isEmpty()) {\n                  getContext().stop(getSelf());\n                } else {\n                  getContext().become(closing());\n                }\n              }\n            })\n        .build();\n  }\n\n\n  static final class BufferingState {\n    int toAck = 10;\n    boolean peerClosed = false;\n  }\n\n  protected Receive buffering(final Ack nack) {\n    final BufferingState state = new BufferingState();\n\n    return receiveBuilder()\n        .match(\n            Received.class,\n            msg -> {\n              buffer(msg.data());\n            })\n        .match(\n            WritingResumed.class,\n            msg -> {\n              writeFirst();\n            })\n        .match(\n            ConnectionClosed.class,\n            msg -> {\n              if (msg.isPeerClosed()) state.peerClosed = true;\n              else getContext().stop(getSelf());\n            })\n        .match(\n            Integer.class,\n            ack -> {\n              acknowledge(ack);\n\n              if (ack >= nack.ack) {\n                // otherwise it was the ack of the last successful write\n\n                if (storage.isEmpty()) {\n                  if (state.peerClosed) getContext().stop(getSelf());\n                  else getContext().become(writing);\n\n                } else {\n                  if (state.toAck > 0) {\n                    // stay in ACK-based mode for a short while\n                    writeFirst();\n                    --state.toAck;\n                  } else {\n                    // then return to NACK-based again\n                    writeAll();\n                    if (state.peerClosed) getContext().become(closing());\n                    else getContext().become(writing);\n                  }\n                }\n              }\n            })\n        .build();\n  }\n\n  protected Receive closing() {\n    return receiveBuilder()\n        .match(\n            CommandFailed.class,\n            msg -> {\n              // the command can only have been a Write\n              connection.tell(TcpMessage.resumeWriting(), getSelf());\n              getContext().become(closeResend(), false);\n            })\n        .match(\n            Integer.class,\n            msg -> {\n              acknowledge(msg);\n              if (storage.isEmpty()) getContext().stop(getSelf());\n            })\n        .build();\n  }\n\n  protected Receive closeResend() {\n    return receiveBuilder()\n        .match(\n            WritingResumed.class,\n            msg -> {\n              writeAll();\n              getContext().unbecome();\n            })\n        .match(\n            Integer.class,\n            msg -> {\n              acknowledge(msg);\n            })\n        .build();\n  }\n\n\n  @Override\n  public void postStop() {\n    log.info(\"transferred {} bytes from/to [{}]\", transferred, remote);\n  }\n\n  protected void buffer(ByteString data) {\n    storage.add(data);\n    stored += data.size();\n\n    if (stored > MAX_STORED) {\n      log.warning(\"drop connection to [{}] (buffer overrun)\", remote);\n      getContext().stop(getSelf());\n\n    } else if (stored > HIGH_WATERMARK) {\n      log.debug(\"suspending reading at {}\", currentOffset());\n      connection.tell(TcpMessage.suspendReading(), getSelf());\n      suspended = true;\n    }\n  }\n\n  protected void acknowledge(int ack) {\n    assertEquals(storageOffset, ack);\n    assertFalse(storage.isEmpty());\n\n    final ByteString acked = storage.remove();\n    stored -= acked.size();\n    transferred += acked.size();\n    storageOffset += 1;\n\n    if (suspended && stored < LOW_WATERMARK) {\n      log.debug(\"resuming reading\");\n      connection.tell(TcpMessage.resumeReading(), getSelf());\n      suspended = false;\n    }\n  }\n\n  protected int currentOffset() {\n    return storageOffset + storage.size();\n  }\n\n  protected void writeAll() {\n    int i = 0;\n    for (ByteString data : storage) {\n      connection.tell(TcpMessage.write(data, new Ack(storageOffset + i++)), getSelf());\n    }\n  }\n\n  protected void writeFirst() {\n    connection.tell(TcpMessage.write(storage.peek(), new Ack(storageOffset)), getSelf());\n  }\n\n}\nThe principle here is to keep writing until a CommandFailed is received, using acknowledgements only to prune the resend buffer. When a such a failure was received, transition into a different state for handling and handle resending of all queued data:\nScala copysourcedef buffering(nack: Int): Receive = {\n  var toAck = 10\n  var peerClosed = false\n\n  {\n    case Received(data)         => buffer(data)\n    case WritingResumed         => writeFirst()\n    case PeerClosed             => peerClosed = true\n    case Ack(ack) if ack < nack => acknowledge(ack)\n    case Ack(ack) =>\n      acknowledge(ack)\n      if (storage.nonEmpty) {\n        if (toAck > 0) {\n          // stay in ACK-based mode for a while\n          writeFirst()\n          toAck -= 1\n        } else {\n          // then return to NACK-based again\n          writeAll()\n          context.become(if (peerClosed) closing else writing)\n        }\n      } else if (peerClosed) context.stop(self)\n      else context.become(writing)\n  }\n} Java copysource static final class BufferingState {\n  int toAck = 10;\n  boolean peerClosed = false;\n}\n\nprotected Receive buffering(final Ack nack) {\n  final BufferingState state = new BufferingState();\n\n  return receiveBuilder()\n      .match(\n          Received.class,\n          msg -> {\n            buffer(msg.data());\n          })\n      .match(\n          WritingResumed.class,\n          msg -> {\n            writeFirst();\n          })\n      .match(\n          ConnectionClosed.class,\n          msg -> {\n            if (msg.isPeerClosed()) state.peerClosed = true;\n            else getContext().stop(getSelf());\n          })\n      .match(\n          Integer.class,\n          ack -> {\n            acknowledge(ack);\n\n            if (ack >= nack.ack) {\n              // otherwise it was the ack of the last successful write\n\n              if (storage.isEmpty()) {\n                if (state.peerClosed) getContext().stop(getSelf());\n                else getContext().become(writing);\n\n              } else {\n                if (state.toAck > 0) {\n                  // stay in ACK-based mode for a short while\n                  writeFirst();\n                  --state.toAck;\n                } else {\n                  // then return to NACK-based again\n                  writeAll();\n                  if (state.peerClosed) getContext().become(closing());\n                  else getContext().become(writing);\n                }\n              }\n            }\n          })\n      .build();\n}\nIt should be noted that all writes which are currently buffered have also been sent to the connection actor upon entering this state, which means that the ResumeWriting messagemessage by the TcpMessage.resumeWriting method is enqueued after those writes, leading to the reception of all outstanding CommandFailed messages (which are ignored in this state) before receiving the WritingResumed signal. That latter message is sent by the connection actor only once the internally queued write has been fully completed, meaning that a subsequent write will not fail. This is exploited by the EchoHandler to switch to an ACK-based approach for the first ten writes after a failure before resuming the optimistic write-through behavior.\nScala copysourcedef closing: Receive = {\n  case CommandFailed(_: Write) =>\n    connection ! ResumeWriting\n    context.become({\n\n        case WritingResumed =>\n          writeAll()\n          context.unbecome()\n\n        case ack: Int => acknowledge(ack)\n\n      }, discardOld = false)\n\n  case Ack(ack) =>\n    acknowledge(ack)\n    if (storage.isEmpty) context.stop(self)\n} Java copysourceprotected Receive closing() {\n  return receiveBuilder()\n      .match(\n          CommandFailed.class,\n          msg -> {\n            // the command can only have been a Write\n            connection.tell(TcpMessage.resumeWriting(), getSelf());\n            getContext().become(closeResend(), false);\n          })\n      .match(\n          Integer.class,\n          msg -> {\n            acknowledge(msg);\n            if (storage.isEmpty()) getContext().stop(getSelf());\n          })\n      .build();\n}\n\nprotected Receive closeResend() {\n  return receiveBuilder()\n      .match(\n          WritingResumed.class,\n          msg -> {\n            writeAll();\n            getContext().unbecome();\n          })\n      .match(\n          Integer.class,\n          msg -> {\n            acknowledge(msg);\n          })\n      .build();\n}\nClosing the connection while still sending all data is a bit more involved than in the ACK-based approach: the idea is to always send all outstanding messages and acknowledge all successful writes, and if a failure happens then switch behavior to await the WritingResumed event and start over.\nThe helper functions are very similar to the ACK-based case:\nScala copysourceprivate def buffer(data: ByteString): Unit = {\n  storage :+= data\n  stored += data.size\n\n  if (stored > maxStored) {\n    log.warning(s\"drop connection to [$remote] (buffer overrun)\")\n    context.stop(self)\n\n  } else if (stored > highWatermark) {\n    log.debug(s\"suspending reading at $currentOffset\")\n    connection ! SuspendReading\n    suspended = true\n  }\n}\n\nprivate def acknowledge(ack: Int): Unit = {\n  require(ack == storageOffset, s\"received ack $ack at $storageOffset\")\n  require(storage.nonEmpty, s\"storage was empty at ack $ack\")\n\n  val size = storage(0).size\n  stored -= size\n  transferred += size\n\n  storageOffset += 1\n  storage = storage.drop(1)\n\n  if (suspended && stored < lowWatermark) {\n    log.debug(\"resuming reading\")\n    connection ! ResumeReading\n    suspended = false\n  }\n} Java copysourceprotected void buffer(ByteString data) {\n  storage.add(data);\n  stored += data.size();\n\n  if (stored > MAX_STORED) {\n    log.warning(\"drop connection to [{}] (buffer overrun)\", remote);\n    getContext().stop(getSelf());\n\n  } else if (stored > HIGH_WATERMARK) {\n    log.debug(\"suspending reading at {}\", currentOffset());\n    connection.tell(TcpMessage.suspendReading(), getSelf());\n    suspended = true;\n  }\n}\n\nprotected void acknowledge(int ack) {\n  assertEquals(storageOffset, ack);\n  assertFalse(storage.isEmpty());\n\n  final ByteString acked = storage.remove();\n  stored -= acked.size();\n  transferred += acked.size();\n  storageOffset += 1;\n\n  if (suspended && stored < LOW_WATERMARK) {\n    log.debug(\"resuming reading\");\n    connection.tell(TcpMessage.resumeReading(), getSelf());\n    suspended = false;\n  }\n}","title":"NACK-Based Write Back-Pressure with Suspending"},{"location":"/io-tcp.html#read-back-pressure-with-pull-mode","text":"When using push based reading, data coming from the socket is sent to the actor as soon as it is available. In the case of the previous Echo server example this meant that we needed to maintain a buffer of incoming data to keep it around since the rate of writing might be slower than the rate of the arrival of new data.\nWith the Pull mode this buffer can be completely eliminated as the following snippet demonstrates:\nScala copysourceoverride def preStart(): Unit = connection ! ResumeReading\n\ndef receive = {\n  case Received(data) => connection ! Write(data, Ack)\n  case Ack            => connection ! ResumeReading\n} Java copysource@Override\npublic void preStart() throws Exception {\n  connection.tell(TcpMessage.resumeReading(), getSelf());\n}\n\n@Override\npublic Receive createReceive() {\n  return receiveBuilder()\n      .match(\n          Tcp.Received.class,\n          message -> {\n            ByteString data = message.data();\n            connection.tell(TcpMessage.write(data, new Ack()), getSelf());\n          })\n      .match(\n          Ack.class,\n          message -> {\n            connection.tell(TcpMessage.resumeReading(), getSelf());\n          })\n      .build();\n}\nThe idea here is that reading is not resumed until the previous write has been completely acknowledged by the connection actor. Every pull mode connection actor starts from suspended state. To start the flow of data we send a ResumeReadingmessage by the TcpMessage.resumeReading method in the preStart method to tell the connection actor that we are ready to receive the first chunk of data. Since we only resume reading when the previous data chunk has been completely written there is no need for maintaining a buffer.\nTo enable pull reading on an outbound connection the pullMode parameter of the ConnectTcpMessage.connect method should be set to true:\nScala copysourceIO(Tcp) ! Connect(listenAddress, pullMode = true) Java copysourcefinal List<Inet.SocketOption> options = new ArrayList<Inet.SocketOption>();\nDuration timeout = null;\ntcp.tell(\n    TcpMessage.connect(\n        new InetSocketAddress(\"localhost\", 3000), null, options, timeout, true),\n    getSelf());","title":"Read Back-Pressure with Pull Mode"},{"location":"/io-tcp.html#pull-mode-reading-for-inbound-connections","text":"The previous section demonstrated how to enable pull reading mode for outbound connections but it is possible to create a listener actor with this mode of reading by setting the pullMode parameter of the Bind commandTcpMessage.bind method to true:\nScala copysourceIO(Tcp) ! Bind(self, new InetSocketAddress(\"localhost\", 0), pullMode = true) Java copysourcetcp = Tcp.get(getContext().getSystem()).manager();\nfinal List<Inet.SocketOption> options = new ArrayList<Inet.SocketOption>();\ntcp.tell(\n    TcpMessage.bind(getSelf(), new InetSocketAddress(\"localhost\", 0), 100, options, true),\n    getSelf());\nOne of the effects of this setting is that all connections accepted by this listener actor will use pull mode reading.\nAnother effect of this setting is that in addition of setting all inbound connections to pull mode, accepting connections becomes pull based, too. This means that after handling one (or more) Connected events the listener actor has to be resumed by sending it a ResumeAccepting messagemessage by the TcpMessage.resumeAccepting method.\nListener actors with pull mode start suspended so to start accepting connections a ResumeAccepting commandmessage by the TcpMessage.resumeAccepting method has to be sent to the listener actor after binding was successful:\nScala copysourcecase Bound(localAddress) =>\n  // Accept connections one by one\n  sender() ! ResumeAccepting(batchSize = 1)\n  context.become(listening(sender()))\ndef listening(listener: ActorRef): Receive = {\n  case Connected(remote, local) =>\n    val handler = context.actorOf(Props(classOf[PullEcho], sender()))\n    sender() ! Register(handler, keepOpenOnPeerClosed = true)\n    listener ! ResumeAccepting(batchSize = 1)\n} Java copysourcepublic Receive createReceive() {\n  return receiveBuilder()\n      .match(\n          Tcp.Bound.class,\n          x -> {\n            listener = getSender();\n            // Accept connections one by one\n            listener.tell(TcpMessage.resumeAccepting(1), getSelf());\n          })\n      .match(\n          Tcp.Connected.class,\n          x -> {\n            ActorRef handler = getContext().actorOf(Props.create(PullEcho.class, getSender()));\n            getSender().tell(TcpMessage.register(handler), getSelf());\n            // Resume accepting connections\n            listener.tell(TcpMessage.resumeAccepting(1), getSelf());\n          })\n      .build();\n}\nAs shown in the example, after handling an incoming connection we need to resume accepting again.\nThe ResumeAcceptingTcpMessage.resumeAccepting method accepts a batchSize parameter that specifies how many new connections are accepted before a next ResumeAccepting message is needed to resume handling of new connections.","title":"Pull Mode Reading for Inbound Connections"},{"location":"/io-udp.html","text":"","title":"Using UDP"},{"location":"/io-udp.html#using-udp","text":"","title":"Using UDP"},{"location":"/io-udp.html#dependency","text":"To use UDP, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/io-udp.html#introduction","text":"UDP is a connectionless datagram protocol which offers two different ways of communication on the JDK level:\nsockets which are free to send datagrams to any destination and receive datagrams from any origin sockets which are restricted to communication with one specific remote socket address\nIn the low-level API the distinction is made—confusingly—by whether or not connect has been called on the socket (even when connect has been called the protocol is still connectionless). These two forms of UDP usage are offered using distinct IO extensions described below.","title":"Introduction"},{"location":"/io-udp.html#unconnected-udp","text":"","title":"Unconnected UDP"},{"location":"/io-udp.html#simple-send","text":"Scala copysourceclass SimpleSender(remote: InetSocketAddress) extends Actor {\n  import context.system\n  IO(Udp) ! Udp.SimpleSender\n\n  def receive = {\n    case Udp.SimpleSenderReady =>\n      context.become(ready(sender()))\n  }\n\n  def ready(send: ActorRef): Receive = {\n    case msg: String =>\n      send ! Udp.Send(ByteString(msg), remote)\n  }\n} Java copysourcepublic static class SimpleSender extends AbstractActor {\n  final InetSocketAddress remote;\n\n  public SimpleSender(InetSocketAddress remote) {\n    this.remote = remote;\n\n    // request creation of a SimpleSender\n    final ActorRef mgr = Udp.get(getContext().getSystem()).getManager();\n    mgr.tell(UdpMessage.simpleSender(), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Udp.SimpleSenderReady.class,\n            message -> {\n              getContext().become(ready(getSender()));\n            })\n        .build();\n  }\n\n  private Receive ready(final ActorRef send) {\n    return receiveBuilder()\n        .match(\n            String.class,\n            message -> {\n              send.tell(UdpMessage.send(ByteString.fromString(message), remote), getSelf());\n            })\n        .build();\n  }\n}\nThe simplest form of UDP usage is to just send datagrams without the need of getting a reply. To this end a “simple sender” facility is provided as demonstrated above. The UDP extension is queried using the SimpleSenderUdpMessage.simpleSender message, which is answered by a SimpleSenderReady notification. The sender of this message is the newly created sender actor which from this point onward can be used to send datagrams to arbitrary destinations; in this example it will send any UTF-8 encoded String it receives to a predefined remote address.\nNote The simple sender will not shut itself down because it cannot know when you are done with it. You will need to send it a PoisonPill when you want to close the ephemeral port the sender is bound to.","title":"Simple Send"},{"location":"/io-udp.html#bind-and-send-","text":"Scala copysourceclass Listener(nextActor: ActorRef) extends Actor {\n  import context.system\n  IO(Udp) ! Udp.Bind(self, new InetSocketAddress(\"localhost\", 0))\n\n  def receive = {\n    case Udp.Bound(local) =>\n      context.become(ready(sender()))\n  }\n\n  def ready(socket: ActorRef): Receive = {\n    case Udp.Received(data, remote) =>\n      val processed = // parse data etc., e.g. using PipelineStage\n      socket ! Udp.Send(data, remote) // example server echoes back\n      nextActor ! processed\n    case Udp.Unbind  => socket ! Udp.Unbind\n    case Udp.Unbound => context.stop(self)\n  }\n} Java copysourcepublic static class Listener extends AbstractActor {\n  final ActorRef nextActor;\n\n  public Listener(ActorRef nextActor) {\n    this.nextActor = nextActor;\n\n    // request creation of a bound listen socket\n    final ActorRef mgr = Udp.get(getContext().getSystem()).getManager();\n    mgr.tell(UdpMessage.bind(getSelf(), new InetSocketAddress(\"localhost\", 0)), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Udp.Bound.class,\n            bound -> {\n              getContext().become(ready(getSender()));\n            })\n        .build();\n  }\n\n  private Receive ready(final ActorRef socket) {\n    return receiveBuilder()\n        .match(\n            Udp.Received.class,\n            r -> {\n              // echo server example: send back the data\n              socket.tell(UdpMessage.send(r.data(), r.sender()), getSelf());\n              // or do some processing and forward it on\n              final Object processed = // parse data etc., e.g. using PipelineStage\n              nextActor.tell(processed, getSelf());\n            })\n        .matchEquals(\n            UdpMessage.unbind(),\n            message -> {\n              socket.tell(message, getSelf());\n            })\n        .match(\n            Udp.Unbound.class,\n            message -> {\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n}\nIf you want to implement a UDP server which listens on a socket for incoming datagrams then you need to use the BindUdpMessage.bind message as shown above. The local address specified may have a zero port in which case the operating system will automatically choose a free port and assign it to the new socket. Which port was actually bound can be found out by inspecting the Bound message.\nThe sender of the Bound message is the actor which manages the new socket. Sending datagrams is achieved by using the SendUdpMessage.send message and the socket can be closed by sending a UnbindUdpMessage.unbind message, in which case the socket actor will reply with a Unbound notification.\nReceived datagrams are sent to the actor designated in the Bind message, whereas the Bound message will be sent to the sender of the BindUdpMessage.bind.","title":"Bind (and Send)"},{"location":"/io-udp.html#connected-udp","text":"The service provided by the connection based UDP API is similar to the bind-and-send service we saw earlier, but the main difference is that a connection is only able to send to the remoteAddress it was connected to, and will receive datagrams only from that address.\nScala copysourceclass Connected(remote: InetSocketAddress) extends Actor {\n  import context.system\n  IO(UdpConnected) ! UdpConnected.Connect(self, remote)\n\n  def receive = {\n    case UdpConnected.Connected =>\n      context.become(ready(sender()))\n  }\n\n  def ready(connection: ActorRef): Receive = {\n    case UdpConnected.Received(data) =>\n      // process data, send it on, etc.\n    case msg: String =>\n      connection ! UdpConnected.Send(ByteString(msg))\n    case UdpConnected.Disconnect =>\n      connection ! UdpConnected.Disconnect\n    case UdpConnected.Disconnected => context.stop(self)\n  }\n} Java copysourcepublic static class Connected extends AbstractActor {\n  final InetSocketAddress remote;\n\n  public Connected(InetSocketAddress remote) {\n    this.remote = remote;\n\n    // create a restricted a.k.a. “connected” socket\n    final ActorRef mgr = UdpConnected.get(getContext().getSystem()).getManager();\n    mgr.tell(UdpConnectedMessage.connect(getSelf(), remote), getSelf());\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            UdpConnected.Connected.class,\n            message -> {\n              getContext().become(ready(getSender()));\n            })\n        .build();\n  }\n\n  private Receive ready(final ActorRef connection) {\n    return receiveBuilder()\n        .match(\n            UdpConnected.Received.class,\n            r -> {\n              // process data, send it on, etc.\n            })\n        .match(\n            String.class,\n            str -> {\n              connection.tell(UdpConnectedMessage.send(ByteString.fromString(str)), getSelf());\n            })\n        .matchEquals(\n            UdpConnectedMessage.disconnect(),\n            message -> {\n              connection.tell(message, getSelf());\n            })\n        .match(\n            UdpConnected.Disconnected.class,\n            x -> {\n              getContext().stop(getSelf());\n            })\n        .build();\n  }\n}\nConsequently the example shown here looks quite similar to the previous one, the biggest difference is the absence of remote address information in SendUdpMessage.send and Received messages.\nNote There is a small performance benefit in using connection based UDP API over the connectionless one. If there is a SecurityManager enabled on the system, every connectionless message send has to go through a security check, while in the case of connection-based UDP the security check is cached after connect, thus writes do not suffer an additional performance penalty.","title":"Connected UDP"},{"location":"/io-udp.html#udp-multicast","text":"Pekko provides a way to control various options of DatagramChannel through the org.apache.pekko.io.Inet.SocketOption interface. The example below shows how to setup a receiver of multicast messages using IPv6 protocol.\nTo select a Protocol Family you must extend org.apache.pekko.io.Inet.DatagramChannelCreator class which extendsimplements org.apache.pekko.io.Inet.SocketOption. Provide custom logic for opening a datagram channel by overriding create method.\nScala copysourcefinal case class Inet6ProtocolFamily() extends DatagramChannelCreator {\n  override def create() =\n    DatagramChannel.open(StandardProtocolFamily.INET6)\n} Java copysourcepublic static class Inet6ProtocolFamily extends Inet.DatagramChannelCreator {\n  @Override\n  public DatagramChannel create() throws Exception {\n    return DatagramChannel.open(StandardProtocolFamily.INET6);\n  }\n}\nAnother socket option will be needed to join a multicast group.\nScala copysourcefinal case class MulticastGroup(address: String, interface: String) extends SocketOptionV2 {\n  override def afterBind(s: DatagramSocket): Unit = {\n    val group = InetAddress.getByName(address)\n    val networkInterface = NetworkInterface.getByName(interface)\n    s.getChannel.join(group, networkInterface)\n  }\n} Java copysourcepublic static class MulticastGroup extends Inet.AbstractSocketOptionV2 {\n  private String address;\n  private String interf;\n\n  public MulticastGroup(String address, String interf) {\n    this.address = address;\n    this.interf = interf;\n  }\n\n  @Override\n  public void afterBind(DatagramSocket s) {\n    try {\n      InetAddress group = InetAddress.getByName(address);\n      NetworkInterface networkInterface = NetworkInterface.getByName(interf);\n      s.getChannel().join(group, networkInterface);\n    } catch (Exception ex) {\n      System.out.println(\"Unable to join multicast group.\");\n    }\n  }\n}\nSocket options must be provided to UdpMessage.BindUdpMessage.bind message.\nScala copysourceimport context.system\nval opts = List(Inet6ProtocolFamily(), MulticastGroup(group, iface))\nIO(Udp) ! Udp.Bind(self, new InetSocketAddress(port), opts) Java copysourceList<Inet.SocketOption> options = new ArrayList<>();\noptions.add(new Inet6ProtocolFamily());\noptions.add(new MulticastGroup(group, iface));\n\nfinal ActorRef mgr = Udp.get(getContext().getSystem()).getManager();\n// listen for datagrams on this address\nInetSocketAddress endpoint = new InetSocketAddress(port);\nmgr.tell(UdpMessage.bind(getSelf(), endpoint, options), getSelf());","title":"UDP Multicast"},{"location":"/io-dns.html","text":"","title":"DNS Extension"},{"location":"/io-dns.html#dns-extension","text":"Warning async-dns does not support: Local hosts file e.g. /etc/hosts on Unix systems The nsswitch.conf file (no plan to support) Additionally, while search domains are supported through configuration, detection of the system configured search domains is only supported on systems that provide this configuration through a /etc/resolv.conf file, i.e. it isn’t supported on Windows or OSX, and none of the environment variables that are usually supported on most *nix OSes are supported.\nNote The async-dns API is marked as ApiMayChange as more information is expected to be added to the protocol.\nWarning The ability to plugin in a custom DNS implementation is expected to be removed in future versions of Pekko. Users should pick one of the built in extensions.\nPekko DNS is a pluggable way to interact with DNS. Implementations much implement org.apache.pekko.io.DnsProvider and provide a configuration block that specifies the implementation via provider-object.\nDNS via Pekko Discovery Pekko Discovery can be backed by the Pekko DNS implementation and provides a more general API for service lookups which is not limited to domain name lookup.\nTo select which DnsProvider to use set pekko.io.dns.resolver to the location of the configuration.\nThere are currently two implementations:\ninet-address - Based on the JDK’s InetAddress. Using this will be subject to both the JVM’s DNS cache and its built in one. async-dns - A native implemention of the DNS protocol that does not use any JDK classes or caches.\ninet-address is the default implementation as it pre-dates async-dns, async-dns will likely become the default in the next major release.\nDNS lookups can be done via the DNS extension:\nScala copysourceval initial: Option[Dns.Resolved] = Dns(system).cache.resolve(\"google.com\")(system, actorRef)\nval cached: Option[Dns.Resolved] = Dns(system).cache.cached(\"google.com\") Java copysourceOption<DnsProtocol.Resolved> initial =\n    Dns.get(system)\n        .cache()\n        .resolve(\n            new DnsProtocol.Resolve(\"google.com\", DnsProtocol.ipRequestType()),\n            system,\n            actorRef);\nOption<DnsProtocol.Resolved> cached =\n    Dns.get(system)\n        .cache()\n        .cached(new DnsProtocol.Resolve(\"google.com\", DnsProtocol.ipRequestType()));\nAlternatively the IO(Dns) actor can be interacted with directly. However this exposes the different protocols of the DNS provider. inet-adddress uses Dns.Resolve and Dns.Resolved where as the async-dns uses DnsProtocol.Resolve and DnsProtocol.Resolved. The reason for the difference is inet-address predates async-dns and async-dns exposes additional information such as SRV records and it wasn’t possible to evolve the original API in a backward compatible way.\nInet-Address API:\nScala copysourceval resolved: Future[Dns.Resolved] = (IO(Dns) ? Dns.Resolve(\"google.com\")).mapTo[Dns.Resolved] Java copysourcefinal ActorRef dnsManager = Dns.get(system).manager();\nCompletionStage<Object> resolved =\n    ask(\n        dnsManager,\n        new DnsProtocol.Resolve(\"google.com\", DnsProtocol.ipRequestType()),\n        timeout);\nAsync-DNS API:\nScala copysourceval resolved: Future[DnsProtocol.Resolved] =\n  (IO(Dns) ? DnsProtocol.Resolve(\"google.com\")).mapTo[DnsProtocol.Resolved] Java copysourcefinal ActorRef dnsManager = Dns.get(system).manager();\nCompletionStage<Object> resolved =\n    ask(dnsManager, DnsProtocol.resolve(\"google.com\"), timeout);\nThe Async DNS provider has the following advantages:\nNo JVM DNS caching. It is expected that future versions will expose more caching related information. No blocking. InetAddress resolving is a blocking operation. Exposes SRV, A and AAAA records.","title":"DNS Extension"},{"location":"/io-dns.html#srv-records","text":"To get DNS SRV records pekko.io.dns.resolver must be set to async-dns and DnsProtocol.Resolve’s requestType must be set to DnsProtocol.Srv\nScala copysourceval resolved: Future[DnsProtocol.Resolved] =\n  (IO(Dns) ? DnsProtocol.Resolve(\"your-service\", Srv)).mapTo[DnsProtocol.Resolved] Java copysourcefinal ActorRef dnsManager = Dns.get(system).manager();\nCompletionStage<Object> resolved =\n    ask(dnsManager, DnsProtocol.resolve(\"google.com\", DnsProtocol.srvRequestType()), timeout);\nThe DnsProtocol.Resolved will contain org.apache.pekko.io.dns.SRVRecords.","title":"SRV Records"},{"location":"/index-utilities-classic.html","text":"","title":"Classic Utilities"},{"location":"/index-utilities-classic.html#classic-utilities","text":"","title":"Classic Utilities"},{"location":"/index-utilities-classic.html#dependency","text":"To use Utilities, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion,\n  \"org.apache.pekko\" %% \"pekko-testkit\" % PekkoVersion % Test\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-testkit_${scala.binary.version}</artifactId>\n    <scope>test</scope>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n  testImplementation \"org.apache.pekko:pekko-testkit_${versions.ScalaBinary}\"\n}\nClassic Event Bus Classifiers Event Stream Classic Logging Module info Introduction How to Log Loggers Logging to stdout during startup and shutdown SLF4J Classic Scheduler Dependency Introduction Some examples Schedule periodically The Scheduler interface The Cancellable interface Classic Apache Pekko Extensions Building an Extension Loading from Configuration Applicability Library extensions","title":"Dependency"},{"location":"/event-bus.html","text":"","title":"Classic Event Bus"},{"location":"/event-bus.html#classic-event-bus","text":"Originally conceived as a way to send messages to groups of actors, the EventBusEventBus has been generalized into a set of composable traits abstract base classes implementing a simple interface:\nScala copysource/**\n * Attempts to register the subscriber to the specified Classifier\n * @return true if successful and false if not (because it was already\n *   subscribed to that Classifier, or otherwise)\n */\ndef subscribe(subscriber: Subscriber, to: Classifier): Boolean\n\n/**\n * Attempts to deregister the subscriber from the specified Classifier\n * @return true if successful and false if not (because it wasn't subscribed\n *   to that Classifier, or otherwise)\n */\ndef unsubscribe(subscriber: Subscriber, from: Classifier): Boolean\n\n/**\n * Attempts to deregister the subscriber from all Classifiers it may be subscribed to\n */\ndef unsubscribe(subscriber: Subscriber): Unit\n\n/**\n * Publishes the specified Event to this bus\n */\ndef publish(event: Event): Unit Java copysource/**\n * Attempts to register the subscriber to the specified Classifier\n *\n * @return true if successful and false if not (because it was already subscribed to that\n *     Classifier, or otherwise)\n */\npublic boolean subscribe(Subscriber subscriber, Classifier to);\n\n/**\n * Attempts to deregister the subscriber from the specified Classifier\n *\n * @return true if successful and false if not (because it wasn't subscribed to that Classifier,\n *     or otherwise)\n */\npublic boolean unsubscribe(Subscriber subscriber, Classifier from);\n\n/** Attempts to deregister the subscriber from all Classifiers it may be subscribed to */\npublic void unsubscribe(Subscriber subscriber);\n\n/** Publishes the specified Event to this bus */\npublic void publish(Event event);\nNote Please note that the EventBus does not preserve the sender of the published messages. If you need a reference to the original sender you have to provide it inside the message.\nThis mechanism is used in different places within Pekko, e.g. the Event Stream. Implementations can make use of the specific building blocks presented below.\nAn event bus must define the following three abstract typestype parameters:\nEvent is the type of all events published on that bus Subscriber is the type of subscribers allowed to register on that event bus Classifier defines the classifier to be used in selecting subscribers for dispatching events\nThe traits below are still generic in these types, but they need to be defined for any concrete implementation.","title":"Classic Event Bus"},{"location":"/event-bus.html#classifiers","text":"The classifiers presented here are part of the Pekko distribution, but rolling your own in case you do not find a perfect match is not difficult, check the implementation of the existing ones on github","title":"Classifiers"},{"location":"/event-bus.html#lookup-classification","text":"The simplest classification is just to extract an arbitrary classifier from each event and maintaining a set of subscribers for each possible classifier. This can be compared to tuning in on a radio station. The trait LookupClassificationabstract class LookupEventBus is still generic in that it abstracts over how to compare subscribers and how exactly to classify them.\nThe necessary methods to be implemented are illustrated with the following example:\nScala copysourceimport org.apache.pekko\nimport pekko.event.EventBus\nimport pekko.event.LookupClassification\n\nfinal case class MsgEnvelope(topic: String, payload: Any)\n\n/**\n * Publishes the payload of the MsgEnvelope when the topic of the\n * MsgEnvelope equals the String specified when subscribing.\n */\nclass LookupBusImpl extends EventBus with LookupClassification {\n  type Event = MsgEnvelope\n  type Classifier = String\n  type Subscriber = ActorRef\n\n  // is used for extracting the classifier from the incoming events\n  override protected def classify(event: Event): Classifier = event.topic\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for the event’s classifier\n  override protected def publish(event: Event, subscriber: Subscriber): Unit = {\n    subscriber ! event.payload\n  }\n\n  // must define a full order over the subscribers, expressed as expected from\n  // `java.lang.Comparable.compare`\n  override protected def compareSubscribers(a: Subscriber, b: Subscriber): Int =\n    a.compareTo(b)\n\n  // determines the initial size of the index data structure\n  // used internally (i.e. the expected number of different classifiers)\n  override protected def mapSize(): Int = 128\n\n}\n Java copysourceimport org.apache.pekko.event.japi.LookupEventBus;\n\nstatic class MsgEnvelope {\n  public final String topic;\n  public final Object payload;\n\n  public MsgEnvelope(String topic, Object payload) {\n    this.topic = topic;\n    this.payload = payload;\n  }\n}\n\n/**\n * Publishes the payload of the MsgEnvelope when the topic of the MsgEnvelope equals the String\n * specified when subscribing.\n */\nstatic class LookupBusImpl extends LookupEventBus<MsgEnvelope, ActorRef, String> {\n\n  // is used for extracting the classifier from the incoming events\n  @Override\n  public String classify(MsgEnvelope event) {\n    return event.topic;\n  }\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for the event’s classifier\n  @Override\n  public void publish(MsgEnvelope event, ActorRef subscriber) {\n    subscriber.tell(event.payload, ActorRef.noSender());\n  }\n\n  // must define a full order over the subscribers, expressed as expected from\n  // `java.lang.Comparable.compare`\n  @Override\n  public int compareSubscribers(ActorRef a, ActorRef b) {\n    return a.compareTo(b);\n  }\n\n  // determines the initial size of the index data structure\n  // used internally (i.e. the expected number of different classifiers)\n  @Override\n  public int mapSize() {\n    return 128;\n  }\n}\nA test for this implementation may look like this:\nScala copysourceval lookupBus = new LookupBusImpl\nlookupBus.subscribe(testActor, \"greetings\")\nlookupBus.publish(MsgEnvelope(\"time\", System.currentTimeMillis()))\nlookupBus.publish(MsgEnvelope(\"greetings\", \"hello\"))\nexpectMsg(\"hello\") Java copysourceLookupBusImpl lookupBus = new LookupBusImpl();\nlookupBus.subscribe(getTestActor(), \"greetings\");\nlookupBus.publish(new MsgEnvelope(\"time\", System.currentTimeMillis()));\nlookupBus.publish(new MsgEnvelope(\"greetings\", \"hello\"));\nexpectMsgEquals(\"hello\");\nThis classifier is efficient in case no subscribers exist for a particular event.","title":"Lookup Classification"},{"location":"/event-bus.html#subchannel-classification","text":"If classifiers form a hierarchy and it is desired that subscription be possible not only at the leaf nodes, this classification may be just the right one. It can be compared to tuning in on (possibly multiple) radio channels by genre. This classification has been developed for the case where the classifier is just the JVM class of the event and subscribers may be interested in subscribing to all subclasses of a certain class, but it may be used with any classifier hierarchy.\nThe necessary methods to be implemented are illustrated with the following example:\nScala copysourceimport org.apache.pekko.util.Subclassification\n\nclass StartsWithSubclassification extends Subclassification[String] {\n  override def isEqual(x: String, y: String): Boolean =\n    x == y\n\n  override def isSubclass(x: String, y: String): Boolean =\n    x.startsWith(y)\n}\n\nimport pekko.event.SubchannelClassification\n\n/**\n * Publishes the payload of the MsgEnvelope when the topic of the\n * MsgEnvelope starts with the String specified when subscribing.\n */\nclass SubchannelBusImpl extends EventBus with SubchannelClassification {\n  type Event = MsgEnvelope\n  type Classifier = String\n  type Subscriber = ActorRef\n\n  // Subclassification is an object providing `isEqual` and `isSubclass`\n  // to be consumed by the other methods of this classifier\n  override protected val subclassification: Subclassification[Classifier] =\n    new StartsWithSubclassification\n\n  // is used for extracting the classifier from the incoming events\n  override protected def classify(event: Event): Classifier = event.topic\n\n  // will be invoked for each event for all subscribers which registered\n  // themselves for the event’s classifier\n  override protected def publish(event: Event, subscriber: Subscriber): Unit = {\n    subscriber ! event.payload\n  }\n} Java copysourceimport org.apache.pekko.event.japi.SubchannelEventBus;\n\nstatic class StartsWithSubclassification implements Subclassification<String> {\n  @Override\n  public boolean isEqual(String x, String y) {\n    return x.equals(y);\n  }\n\n  @Override\n  public boolean isSubclass(String x, String y) {\n    return x.startsWith(y);\n  }\n}\n\n/**\n * Publishes the payload of the MsgEnvelope when the topic of the MsgEnvelope starts with the\n * String specified when subscribing.\n */\nstatic class SubchannelBusImpl extends SubchannelEventBus<MsgEnvelope, ActorRef, String> {\n\n  // Subclassification is an object providing `isEqual` and `isSubclass`\n  // to be consumed by the other methods of this classifier\n  @Override\n  public Subclassification<String> subclassification() {\n    return new StartsWithSubclassification();\n  }\n\n  // is used for extracting the classifier from the incoming events\n  @Override\n  public String classify(MsgEnvelope event) {\n    return event.topic;\n  }\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for the event’s classifier\n  @Override\n  public void publish(MsgEnvelope event, ActorRef subscriber) {\n    subscriber.tell(event.payload, ActorRef.noSender());\n  }\n}\nA test for this implementation may look like this:\nScala copysourceval subchannelBus = new SubchannelBusImpl\nsubchannelBus.subscribe(testActor, \"abc\")\nsubchannelBus.publish(MsgEnvelope(\"xyzabc\", \"x\"))\nsubchannelBus.publish(MsgEnvelope(\"bcdef\", \"b\"))\nsubchannelBus.publish(MsgEnvelope(\"abc\", \"c\"))\nexpectMsg(\"c\")\nsubchannelBus.publish(MsgEnvelope(\"abcdef\", \"d\"))\nexpectMsg(\"d\") Java copysourceSubchannelBusImpl subchannelBus = new SubchannelBusImpl();\nsubchannelBus.subscribe(getTestActor(), \"abc\");\nsubchannelBus.publish(new MsgEnvelope(\"xyzabc\", \"x\"));\nsubchannelBus.publish(new MsgEnvelope(\"bcdef\", \"b\"));\nsubchannelBus.publish(new MsgEnvelope(\"abc\", \"c\"));\nexpectMsgEquals(\"c\");\nsubchannelBus.publish(new MsgEnvelope(\"abcdef\", \"d\"));\nexpectMsgEquals(\"d\");\nThis classifier is also efficient in case no subscribers are found for an event, but it uses conventional locking to synchronize an internal classifier cache, hence it is not well-suited to use cases in which subscriptions change with very high frequency (keep in mind that “opening” a classifier by sending the first message will also have to re-check all previous subscriptions).","title":"Subchannel Classification"},{"location":"/event-bus.html#scanning-classification","text":"The previous classifier was built for multi-classifier subscriptions which are strictly hierarchical, this classifier is useful if there are overlapping classifiers which cover various parts of the event space without forming a hierarchy. It can be compared to tuning in on (possibly multiple) radio stations by geographical reachability (for old-school radio-wave transmission).\nThe necessary methods to be implemented are illustrated with the following example:\nScala copysourceimport org.apache.pekko.event.ScanningClassification\n\n/**\n * Publishes String messages with length less than or equal to the length\n * specified when subscribing.\n */\nclass ScanningBusImpl extends EventBus with ScanningClassification {\n  type Event = String\n  type Classifier = Int\n  type Subscriber = ActorRef\n\n  // is needed for determining matching classifiers and storing them in an\n  // ordered collection\n  override protected def compareClassifiers(a: Classifier, b: Classifier): Int =\n    if (a < b) -1 else if (a == b) 0 else 1\n\n  // is needed for storing subscribers in an ordered collection\n  override protected def compareSubscribers(a: Subscriber, b: Subscriber): Int =\n    a.compareTo(b)\n\n  // determines whether a given classifier shall match a given event; it is invoked\n  // for each subscription for all received events, hence the name of the classifier\n  override protected def matches(classifier: Classifier, event: Event): Boolean =\n    event.length <= classifier\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for a classifier matching this event\n  override protected def publish(event: Event, subscriber: Subscriber): Unit = {\n    subscriber ! event\n  }\n} Java copysourceimport org.apache.pekko.event.japi.ScanningEventBus;\n\n/**\n * Publishes String messages with length less than or equal to the length specified when\n * subscribing.\n */\nstatic class ScanningBusImpl extends ScanningEventBus<String, ActorRef, Integer> {\n\n  // is needed for determining matching classifiers and storing them in an\n  // ordered collection\n  @Override\n  public int compareClassifiers(Integer a, Integer b) {\n    return a.compareTo(b);\n  }\n\n  // is needed for storing subscribers in an ordered collection\n  @Override\n  public int compareSubscribers(ActorRef a, ActorRef b) {\n    return a.compareTo(b);\n  }\n\n  // determines whether a given classifier shall match a given event; it is invoked\n  // for each subscription for all received events, hence the name of the classifier\n  @Override\n  public boolean matches(Integer classifier, String event) {\n    return event.length() <= classifier;\n  }\n\n  // will be invoked for each event for all subscribers which registered themselves\n  // for the event’s classifier\n  @Override\n  public void publish(String event, ActorRef subscriber) {\n    subscriber.tell(event, ActorRef.noSender());\n  }\n}\nA test for this implementation may look like this:\nScala copysourceval scanningBus = new ScanningBusImpl\nscanningBus.subscribe(testActor, 3)\nscanningBus.publish(\"xyzabc\")\nscanningBus.publish(\"ab\")\nexpectMsg(\"ab\")\nscanningBus.publish(\"abc\")\nexpectMsg(\"abc\") Java copysourceScanningBusImpl scanningBus = new ScanningBusImpl();\nscanningBus.subscribe(getTestActor(), 3);\nscanningBus.publish(\"xyzabc\");\nscanningBus.publish(\"ab\");\nexpectMsgEquals(\"ab\");\nscanningBus.publish(\"abc\");\nexpectMsgEquals(\"abc\");\nThis classifier takes always a time which is proportional to the number of subscriptions, independent of how many actually match.","title":"Scanning Classification"},{"location":"/event-bus.html#actor-classification","text":"This classification was originally developed specifically for implementing DeathWatch: subscribers as well as classifiers are of type ActorRefActorRef.\nThis classification requires an ActorSystemActorSystem in order to perform book-keeping operations related to the subscribers being Actors, which can terminate without first unsubscribing from the EventBus. ManagedActorClassification maintains a system Actor which takes care of unsubscribing terminated actors automatically.\nThe necessary methods to be implemented are illustrated with the following example:\nScala copysourceimport org.apache.pekko\nimport pekko.event.ActorEventBus\nimport pekko.event.ManagedActorClassification\nimport pekko.event.ActorClassifier\n\nfinal case class Notification(ref: ActorRef, id: Int)\n\nclass ActorBusImpl(val system: ActorSystem)\n    extends ActorEventBus\n    with ActorClassifier\n    with ManagedActorClassification {\n  type Event = Notification\n\n  // is used for extracting the classifier from the incoming events\n  override protected def classify(event: Event): ActorRef = event.ref\n\n  // determines the initial size of the index data structure\n  // used internally (i.e. the expected number of different classifiers)\n  override protected def mapSize: Int = 128\n} Java copysourceimport org.apache.pekko.event.japi.ManagedActorEventBus;\n\nstatic class Notification {\n  public final ActorRef ref;\n  public final int id;\n\n  public Notification(ActorRef ref, int id) {\n    this.ref = ref;\n    this.id = id;\n  }\n}\n\nstatic class ActorBusImpl extends ManagedActorEventBus<Notification> {\n\n  // the ActorSystem will be used for book-keeping operations, such as subscribers terminating\n  public ActorBusImpl(ActorSystem system) {\n    super(system);\n  }\n\n  // is used for extracting the classifier from the incoming events\n  @Override\n  public ActorRef classify(Notification event) {\n    return event.ref;\n  }\n\n  // determines the initial size of the index data structure\n  // used internally (i.e. the expected number of different classifiers)\n  @Override\n  public int mapSize() {\n    return 128;\n  }\n}\nA test for this implementation may look like this:\nScala copysourceval observer1 = TestProbe().ref\nval observer2 = TestProbe().ref\nval probe1 = TestProbe()\nval probe2 = TestProbe()\nval subscriber1 = probe1.ref\nval subscriber2 = probe2.ref\nval actorBus = new ActorBusImpl(system)\nactorBus.subscribe(subscriber1, observer1)\nactorBus.subscribe(subscriber2, observer1)\nactorBus.subscribe(subscriber2, observer2)\nactorBus.publish(Notification(observer1, 100))\nprobe1.expectMsg(Notification(observer1, 100))\nprobe2.expectMsg(Notification(observer1, 100))\nactorBus.publish(Notification(observer2, 101))\nprobe2.expectMsg(Notification(observer2, 101))\nprobe1.expectNoMessage(500.millis) Java copysourceActorRef observer1 = new TestKit(system).getRef();\nActorRef observer2 = new TestKit(system).getRef();\nTestKit probe1 = new TestKit(system);\nTestKit probe2 = new TestKit(system);\nActorRef subscriber1 = probe1.getRef();\nActorRef subscriber2 = probe2.getRef();\nActorBusImpl actorBus = new ActorBusImpl(system);\nactorBus.subscribe(subscriber1, observer1);\nactorBus.subscribe(subscriber2, observer1);\nactorBus.subscribe(subscriber2, observer2);\nNotification n1 = new Notification(observer1, 100);\nactorBus.publish(n1);\nprobe1.expectMsgEquals(n1);\nprobe2.expectMsgEquals(n1);\nNotification n2 = new Notification(observer2, 101);\nactorBus.publish(n2);\nprobe2.expectMsgEquals(n2);\nprobe1.expectNoMessage(Duration.ofMillis(500));\nThis classifier is still is generic in the event type, and it is efficient for all use cases.","title":"Actor Classification"},{"location":"/event-bus.html#event-stream","text":"The event stream is the main event bus of each actor system: it is used for carrying log messages and Dead Letters and may be used by the user code for other purposes as well. It uses Subchannel Classification which enables registering to related sets of channels (as is used for RemotingLifecycleEventRemotingLifecycleEvent). The following example demonstrates how a simple subscription works. Given a simple actor:\ncopysourceimport org.apache.pekko.actor.{ Actor, DeadLetter, Props }\n\nclass DeadLetterListener extends Actor {\n  def receive = {\n    case d: DeadLetter => println(d)\n  }\n}\n\nval listener = system.actorOf(Props[DeadLetterListener]())\nsystem.eventStream.subscribe(listener, classOf[DeadLetter])\ncopysourceimport org.apache.pekko.actor.ActorRef;\nimport org.apache.pekko.actor.ActorSystem; copysourcestatic class DeadLetterActor extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            DeadLetter.class,\n            msg -> {\n              System.out.println(msg);\n            })\n        .build();\n  }\n} it can be subscribed like this: copysourcefinal ActorSystem system = ActorSystem.create(\"DeadLetters\");\nfinal ActorRef actor = system.actorOf(Props.create(DeadLetterActor.class));\nsystem.getEventStream().subscribe(actor, DeadLetter.class);\nIt is also worth pointing out that thanks to the way the subchannel classification is implemented in the event stream, it is possible to subscribe to a group of events, by subscribing to their common superclass as demonstrated in the following example:\nScala copysourceabstract class AllKindsOfMusic { def artist: String }\ncase class Jazz(artist: String) extends AllKindsOfMusic\ncase class Electronic(artist: String) extends AllKindsOfMusic\n\nclass Listener extends Actor {\n  def receive = {\n    case m: Jazz       => println(s\"${self.path.name} is listening to: ${m.artist}\")\n    case m: Electronic => println(s\"${self.path.name} is listening to: ${m.artist}\")\n  }\n}\n\nval jazzListener = system.actorOf(Props[Listener]())\nval musicListener = system.actorOf(Props[Listener]())\nsystem.eventStream.subscribe(jazzListener, classOf[Jazz])\nsystem.eventStream.subscribe(musicListener, classOf[AllKindsOfMusic])\n\n// only musicListener gets this message, since it listens to *all* kinds of music:\nsystem.eventStream.publish(Electronic(\"Parov Stelar\"))\n\n// jazzListener and musicListener will be notified about Jazz:\nsystem.eventStream.publish(Jazz(\"Sonny Rollins\")) Java copysourceinterface AllKindsOfMusic {}\n\nclass Jazz implements AllKindsOfMusic {\n  public final String artist;\n\n  public Jazz(String artist) {\n    this.artist = artist;\n  }\n}\n\nclass Electronic implements AllKindsOfMusic {\n  public final String artist;\n\n  public Electronic(String artist) {\n    this.artist = artist;\n  }\n}\n\nstatic class Listener extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            Jazz.class,\n            msg -> System.out.printf(\"%s is listening to: %s%n\", getSelf().path().name(), msg))\n        .match(\n            Electronic.class,\n            msg -> System.out.printf(\"%s is listening to: %s%n\", getSelf().path().name(), msg))\n        .build();\n  }\n}\n  final ActorRef actor = system.actorOf(Props.create(DeadLetterActor.class));\n  system.getEventStream().subscribe(actor, DeadLetter.class);\n\n  final ActorRef jazzListener = system.actorOf(Props.create(Listener.class));\n  final ActorRef musicListener = system.actorOf(Props.create(Listener.class));\n  system.getEventStream().subscribe(jazzListener, Jazz.class);\n  system.getEventStream().subscribe(musicListener, AllKindsOfMusic.class);\n\n  // only musicListener gets this message, since it listens to *all* kinds of music:\n  system.getEventStream().publish(new Electronic(\"Parov Stelar\"));\n\n  // jazzListener and musicListener will be notified about Jazz:\n  system.getEventStream().publish(new Jazz(\"Sonny Rollins\"));\nSimilarly to Actor Classification, EventStreamEventStream will automatically remove subscribers when they terminate.\nNote The event stream is a local facility, meaning that it will not distribute events to other nodes in a clustered environment (unless you subscribe a Remote Actor to the stream explicitly). If you need to broadcast events in a Pekko cluster, without knowing your recipients explicitly (i.e. obtaining their ActorRefs), you may want to look into: Distributed Publish Subscribe in Cluster.","title":"Event Stream"},{"location":"/event-bus.html#default-handlers","text":"Upon start-up the actor system creates and subscribes actors to the event stream for logging: these are the handlers which are configured for example in application.conf:\npekko {\n  loggers = [\"org.apache.pekko.event.Logging$DefaultLogger\"]\n}\nThe handlers listed here by fully-qualified class name will be subscribed to all log event classes with priority higher than or equal to the configured log-level and their subscriptions are kept in sync when changing the log-level at runtime:\nScala system.eventStream.setLogLevel(Logging.DebugLevel)\n Java system.eventStream.setLogLevel(Logging.DebugLevel());\nThis means that log events for a level which will not be logged are typically not dispatched at all (unless manual subscriptions to the respective event class have been done)","title":"Default Handlers"},{"location":"/event-bus.html#dead-letters","text":"As described at Stopping actors, messages queued when an actor terminates or sent after its death are re-routed to the dead letter mailbox, which by default will publish the messages wrapped in DeadLetterDeadLetter. This wrapper holds the original sender, receiver and message of the envelope which was redirected.\nSome internal messages (marked with the DeadLetterSuppressionDeadLetterSuppression traitinterface) will not end up as dead letters like normal messages. These are by design safe and expected to sometimes arrive at a terminated actor and since they are nothing to worry about, they are suppressed from the default dead letters logging mechanism.\nHowever, in case you find yourself in need of debugging these kinds of low level suppressed dead letters, it’s still possible to subscribe to them explicitly:\nScala copysourceimport org.apache.pekko.actor.SuppressedDeadLetter\nsystem.eventStream.subscribe(listener, classOf[SuppressedDeadLetter]) Java copysourcesystem.getEventStream().subscribe(actor, SuppressedDeadLetter.class);\nor all dead letters (including the suppressed ones):\nScala copysourceimport org.apache.pekko.actor.AllDeadLetters\nsystem.eventStream.subscribe(listener, classOf[AllDeadLetters]) Java copysourcesystem.getEventStream().subscribe(actor, AllDeadLetters.class);","title":"Dead Letters"},{"location":"/event-bus.html#other-uses","text":"The event stream is always there and ready to be used, you can publish your own events (it accepts AnyRefObject) and subscribe listeners to the corresponding JVM classes.","title":"Other Uses"},{"location":"/logging.html","text":"","title":"Classic Logging"},{"location":"/logging.html#classic-logging","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the new API see Logging.","title":"Classic Logging"},{"location":"/logging.html#module-info","text":"To use Logging, you must at least use the Pekko actors dependency in your project, and will most likely want to configure logging via the SLF4J module (see below).\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}\nProject Info: Pekko Logging (classic) Artifact org.apache.pekko pekko-slf4j 2.6.20+81-523134c3+20230202-1514-SNAPSHOT Snapshots are available JDK versions Adopt OpenJDK 8 Adopt OpenJDK 11 Scala versions 2.13.8, 2.12.16, 3.1.2 JPMS module name pekko.slf4j License Apache-2.0 Home page https://pekko.apache.org/ API documentation API (Scaladoc) API (Javadoc) Forums Apache Pekko Dev mailing list apache/incubator-pekko discussion Release notes akka.io blog Issues Github issues Sources https://github.com/apache/incubator-pekko","title":"Module info"},{"location":"/logging.html#introduction","text":"Logging in Pekko is not tied to a specific logging backend. By default log messages are printed to STDOUT, but you can plug-in a SLF4J logger or your own logger. Logging is performed asynchronously to ensure that logging has minimal performance impact. Logging generally means IO and locks, which can slow down the operations of your code if it was performed synchronously.","title":"Introduction"},{"location":"/logging.html#how-to-log","text":"Create a LoggingAdapterLoggingAdapter and use the error, warning, info, or debug methods, as illustrated in this example:\nScala copysourceimport org.apache.pekko.event.Logging\n\nclass MyActor extends Actor {\n  val log = Logging(context.system, this)\n  override def preStart() = {\n    log.debug(\"Starting\")\n  }\n  override def preRestart(reason: Throwable, message: Option[Any]): Unit = {\n    log.error(reason, \"Restarting due to [{}] when processing [{}]\", reason.getMessage, message.getOrElse(\"\"))\n  }\n  def receive = {\n    case \"test\" => log.info(\"Received test\")\n    case x      => log.warning(\"Received unknown message: {}\", x)\n  }\n} Java copysourceimport org.apache.pekko.actor.*;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\n copysourceclass MyActor extends AbstractActor {\n  LoggingAdapter log = Logging.getLogger(getContext().getSystem(), this);\n\n  @Override\n  public void preStart() {\n    log.debug(\"Starting\");\n  }\n\n  @Override\n  public void preRestart(Throwable reason, Optional<Object> message) {\n    log.error(\n        reason,\n        \"Restarting due to [{}] when processing [{}]\",\n        reason.getMessage(),\n        message.isPresent() ? message.get() : \"\");\n  }\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\"test\", msg -> log.info(\"Received test\"))\n        .matchAny(msg -> log.warning(\"Received unknown message: {}\", msg))\n        .build();\n  }\n}\nFor convenience, you can mix in the log member into actors, instead of defining it as above. class MyActor extends Actor with org.apache.pekko.actor.ActorLogging {\n  ...\n}\nThe first parameter to Logging Logging.getLogger could also be any LoggingBusLoggingBus, specifically system.eventStream system.getEventStream(). In the demonstrated case, the actor system’s address is included in the pekkoSource representation of the log source (see Logging Thread, Pekko Source and Actor System in MDC), while in the second case this is not automatically done. The second parameter to Logging Logging.getLogger is the source of this logging channel. The source object is translated to a String according to the following rules:\nif it is an Actor or ActorRef, its path is used in case of a String it is used as is in case of a Class an approximation of its simpleName is used in all other cases a compile error occurs unless an implicit LogSource[T] is in scope for the type in question the simpleName of its class is used\nThe log message may contain argument placeholders {}, which will be substituted if the log level is enabled. Compared to constructing a full string for the log message this has the advantage of avoiding superfluous string concatenation and object allocations when the log level is disabled. Giving more arguments than placeholders results in a warning being appended to the log statement (i.e. on the same line with the same severity). You may pass an array as the only substitution argument to have its elements be treated individually:\nScala copysourceval args = Array(\"The\", \"brown\", \"fox\", \"jumps\", 42)\nsystem.log.debug(\"five parameters: {}, {}, {}, {}, {}\", args) Java copysourcefinal Object[] args = new Object[] {\"The\", \"brown\", \"fox\", \"jumps\", 42};\nsystem.log().debug(\"five parameters: {}, {}, {}, {}, {}\", args);\nThe Java Class of the log source is also included in the generated Logging.LogEventLogging.LogEvent. In case of a simple string this is replaced with a “marker” class DummyClassForStringSourcesDummyClassForStringSources in order to allow special treatment of this case, e.g. in the SLF4J event listener which will then use the string instead of the class’ name for looking up the logger instance to use.","title":"How to Log"},{"location":"/logging.html#logging-of-dead-letters","text":"By default messages sent to dead letters are logged at INFO level. Existence of dead letters does not necessarily indicate a problem, but they are logged by default for the sake of caution. After a few messages this logging is turned off, to avoid flooding the logs. You can disable this logging completely or adjust how many dead letters are logged. During system shutdown it is likely that you see dead letters, since pending messages in the actor mailboxes are sent to dead letters. You can also disable logging of dead letters during shutdown.\npekko {\n  log-dead-letters = 10\n  log-dead-letters-during-shutdown = on\n}\nTo customize the logging further or take other actions for dead letters you can subscribe to the Event Stream.","title":"Logging of Dead Letters"},{"location":"/logging.html#auxiliary-logging-options","text":"Pekko has a few configuration options for very low level debugging. These make more sense in development than in production.\nYou almost definitely need to have logging set to DEBUG to use any of the options below:\npekko {\n  loglevel = \"DEBUG\"\n}\nThis config option is very good if you want to know what config settings are loaded by Pekko:\npekko {\n  # Log the complete configuration at INFO level when the actor system is started.\n  # This is useful when you are uncertain of what configuration is used.\n  log-config-on-start = on\n}\nIf you want very detailed logging of user-level messages then wrap your actors’ behaviors with LoggingReceive and enable the receive option: pekko {\n  actor {\n    debug {\n      # enable function of LoggingReceive, which is to log any received message at\n      # DEBUG level\n      receive = on\n    }\n  }\n}\nIf you want very detailed logging of all automatically received messages that are processed by Actors:\npekko {\n  actor {\n    debug {\n      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill etc.)\n      autoreceive = on\n    }\n  }\n}\nIf you want very detailed logging of all lifecycle changes of Actors (restarts, deaths etc.):\npekko {\n  actor {\n    debug {\n      # enable DEBUG logging of actor lifecycle changes\n      lifecycle = on\n    }\n  }\n}\nIf you want unhandled messages logged at DEBUG:\npekko {\n  actor {\n    debug {\n      # enable DEBUG logging of unhandled messages\n      unhandled = on\n    }\n  }\n}\nIf you want very detailed logging of all events, transitions and timers of FSM Actors that extend LoggingFSM:\npekko {\n  actor {\n    debug {\n      # enable DEBUG logging of all LoggingFSMs for events, transitions and timers\n      fsm = on\n    }\n  }\n}\nIf you want to monitor subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream:\npekko {\n  actor {\n    debug {\n      # enable DEBUG logging of subscription changes on the eventStream\n      event-stream = on\n    }\n  }\n}","title":"Auxiliary logging options"},{"location":"/logging.html#auxiliary-remote-logging-options","text":"If you want to see all messages that are sent through remoting at DEBUG log level, use the following config option. Note that this logs the messages as they are sent by the transport layer, not by an actor.\npekko.remote.artery {\n  # If this is \"on\", Pekko will log all outbound messages at DEBUG level,\n  # if off then they are not logged\n  log-sent-messages = on\n}\nIf you want to see all messages that are received through remoting at DEBUG log level, use the following config option. Note that this logs the messages as they are received by the transport layer, not by an actor.\npekko.remote.artery {\n  # If this is \"on\", Pekko will log all inbound messages at DEBUG level,\n  # if off then they are not logged\n  log-received-messages = on\n}\nAlso see the logging options for TestKit.\nTranslating Log Source to String and Class The rules for translating the source object to the source string and class which are inserted into the LogEvent during runtime are implemented using implicit parameters and thus fully customizable: create your own instance of LogSource[T] and have it in scope when creating the logger. copysourceimport org.apache.pekko\nimport pekko.actor.ActorSystem\nimport pekko.event.LogSource\n\nobject MyType {\n  implicit val logSource: LogSource[AnyRef] = new LogSource[AnyRef] {\n    def genString(o: AnyRef): String = o.getClass.getName\n    override def getClazz(o: AnyRef): Class[_] = o.getClass\n  }\n}\n\nclass MyType(system: ActorSystem) {\n  import MyType._\n  import pekko.event.Logging\n\n  val log = Logging(system, this)\n} This example creates a log source which mimics traditional usage of Java loggers, which are based upon the originating object’s class name as log category. The override of getClazz is only included for demonstration purposes as it contains exactly the default behavior.\nNote You may also create the string representation up front and pass that in as the log source, but be aware that then the Class[_] which will be put in the LogEvent is event.DummyClassForStringSources. The SLF4J event listener treats this case specially (using the actual string to look up the logger instance to use instead of the class’ name), and you might want to do this also in case you implement your own logging adapter.","title":"Auxiliary remote logging options"},{"location":"/logging.html#turn-off-logging","text":"To turn off logging you can configure the log levels to be OFF like this.\npekko {\n  stdout-loglevel = \"OFF\"\n  loglevel = \"OFF\"\n}\nThe stdout-loglevel is only in effect during system startup and shutdown, and setting it to OFF as well, ensures that nothing gets logged during system startup or shutdown.","title":"Turn Off Logging"},{"location":"/logging.html#loggers","text":"Logging is performed asynchronously through an event bus. Log events are processed by an event handler actor that receives the log events in the same order they were emitted.\nNote The event handler actor does not have a bounded inbox and is run on the default dispatcher. This means that logging extreme amounts of data may affect your application badly. This can be somewhat mitigated by using an async logging backend though. (See Using the SLF4J API directly)\nYou can configure which event handlers are created at system start-up and listen to logging events. That is done using the loggers element in the configuration. Here you can also define the log level. More fine grained filtering based on the log source can be implemented in a custom LoggingFilterLoggingFilter, which can be defined in the logging-filter configuration property.\npekko {\n  # Loggers to register at boot time (org.apache.pekko.event.Logging$DefaultLogger logs\n  # to STDOUT)\n  loggers = [\"org.apache.pekko.event.Logging$DefaultLogger\"]\n  # Options: OFF, ERROR, WARNING, INFO, DEBUG\n  loglevel = \"DEBUG\"\n}\nThe default one logs to STDOUT and is registered by default. It is not intended to be used for production. There is also an SLF4J logger available in the ‘pekko-slf4j’ module.\nNote If pekko-actor-typed is available on your classpath, logging will automatically switch to SLF4J instead of the default logger. See the Pekko typed logging docs for more details.\nExample of creating a listener:\nScala copysourceimport org.apache.pekko\nimport pekko.event.Logging.Debug\nimport pekko.event.Logging.Error\nimport pekko.event.Logging.Info\nimport pekko.event.Logging.InitializeLogger\nimport pekko.event.Logging.LoggerInitialized\nimport pekko.event.Logging.Warning\n\nclass MyEventListener extends Actor {\n  def receive = {\n    case InitializeLogger(_)                        => sender() ! LoggerInitialized\n    case Error(cause, logSource, logClass, message) => // ...\n    case Warning(logSource, logClass, message)      => // ...\n    case Info(logSource, logClass, message)         => // ...\n    case Debug(logSource, logClass, message)        => // ...\n  }\n} Java copysourceimport org.apache.pekko.actor.*;\nimport org.apache.pekko.event.Logging;\nimport org.apache.pekko.event.LoggingAdapter;\n\nimport org.apache.pekko.event.Logging.InitializeLogger;\nimport org.apache.pekko.event.Logging.Error;\nimport org.apache.pekko.event.Logging.Warning;\nimport org.apache.pekko.event.Logging.Info;\nimport org.apache.pekko.event.Logging.Debug;\n copysourceclass MyEventListener extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .match(\n            InitializeLogger.class,\n            msg -> {\n              getSender().tell(Logging.loggerInitialized(), getSelf());\n            })\n        .match(\n            Error.class,\n            msg -> {\n              // ...\n            })\n        .match(\n            Warning.class,\n            msg -> {\n              // ...\n            })\n        .match(\n            Info.class,\n            msg -> {\n              // ...\n            })\n        .match(\n            Debug.class,\n            msg -> {\n              // ...\n            })\n        .build();\n  }\n}","title":"Loggers"},{"location":"/logging.html#logging-to-stdout-during-startup-and-shutdown","text":"When the actor system is starting up and shutting down the configured loggers are not used. Instead log messages are printed to stdout (System.out). The default log level for this stdout logger is WARNING and it can be silenced completely by setting pekko.stdout-loglevel=OFF.","title":"Logging to stdout during startup and shutdown"},{"location":"/logging.html#slf4j","text":"Pekko provides a logger for SLF4J. This module is available in the ‘pekko-slf4j.jar’. It has a single dependency: the slf4j-api jar. In your runtime, you also need a SLF4J backend. We recommend Logback:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies ++= Seq(\n  \"org.apache.pekko\" %% \"pekko-slf4j\" % PekkoVersion,\n  \"ch.qos.logback\" % \"logback-classic\" % \"1.2.11\"\n) Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-slf4j_${scala.binary.version}</artifactId>\n  </dependency>\n  <dependency>\n    <groupId>ch.qos.logback</groupId>\n    <artifactId>logback-classic</artifactId>\n    <version>1.2.11</version>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-slf4j_${versions.ScalaBinary}\"\n  implementation \"ch.qos.logback:logback-classic:1.2.11\"\n}\nYou need to enable the Slf4jLogger in the loggers element in the configuration. Here you can also define the log level of the event bus. More fine grained log levels can be defined in the configuration of the SLF4J backend (e.g. logback.xml). You should also define Slf4jLoggingFilterSlf4jLoggingFilter in the logging-filter configuration property. It will filter the log events using the backend configuration (e.g. logback.xml) before they are published to the event bus.\nWarning If you set the loglevel to a higher level than DEBUG, any DEBUG events will be filtered out already at the source and will never reach the logging backend, regardless of how the backend is configured. You can enable DEBUG level for pekko.loglevel and control the actual level in the SLF4J backend without any significant overhead, also for production.\npekko {\n  loggers = [\"org.apache.pekko.event.slf4j.Slf4jLogger\"]\n  loglevel = \"DEBUG\"\n  logging-filter = \"org.apache.pekko.event.slf4j.Slf4jLoggingFilter\"\n}\nOne gotcha is that the timestamp is attributed in the event handler, not when actually doing the logging.\nThe SLF4J logger selected for each log event is chosen based on the Class[_] Class of the log source specified when creating the LoggingAdapterLoggingAdapter, unless that was given directly as a string in which case that string is used (i.e. LoggerFactory.getLogger(c: Class[_ ]) LoggerFactory.getLogger(Class c) is used in the first case and LoggerFactory.getLogger(s: String) LoggerFactory.getLogger(String s) in the second).\nNote Beware that the actor system’s name is appended to a String log source if the LoggingAdapter was created giving an ActorSystemActorSystem to the factory. If this is not intended, give a LoggingBusLoggingBus instead as shown below:\nScala val log = Logging(system.eventStream, \"my.nice.string\")\n Java final LoggingAdapter log = Logging.getLogger(system.eventStream(), \"my.string\");","title":"SLF4J"},{"location":"/logging.html#using-the-slf4j-api-directly","text":"If you use the SLF4J API directly in your application, remember that the logging operations will block while the underlying infrastructure writes the log statements.\nThis can be avoided by configuring the logging implementation to use a non-blocking appender. Logback provides AsyncAppender that does this.","title":"Using the SLF4J API directly"},{"location":"/logging.html#logback-configuration","text":"Logback has flexible configuration options and details can be found in the Logback manual and other external resources.\nOne part that is important to highlight is the importance of configuring an AsyncAppender, because it offloads rendering of logging events to a background thread, increasing performance. It doesn’t block the threads of the ActorSystemActorSystem while the underlying infrastructure writes the log messages to disk or other configured destination. It also contains a feature which will drop INFO and DEBUG messages if the logging load is high.\nA starting point for configuration of logback.xml for production:\ncopysource<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n\n    <appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <file>myapp.log</file>\n        <immediateFlush>false</immediateFlush>\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n            <fileNamePattern>myapp_%d{yyyy-MM-dd}.log</fileNamePattern>\n        </rollingPolicy>\n        <encoder>\n            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n        </encoder>\n    </appender>\n\n    <appender name=\"ASYNC\" class=\"ch.qos.logback.classic.AsyncAppender\">\n        <queueSize>8192</queueSize>\n        <neverBlock>true</neverBlock>\n        <appender-ref ref=\"FILE\" />\n    </appender>\n\n    <root level=\"INFO\">\n        <appender-ref ref=\"ASYNC\"/>\n    </root>\n</configuration>\nFor development you might want to log to standard out, but also have all DEBUG level logging to file, like in this example:\ncopysource<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n\n    <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\">\n            <level>INFO</level>\n        </filter>\n        <encoder>\n            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n        </encoder>\n    </appender>\n\n    <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\">\n        <file>target/myapp-dev.log</file>\n        <encoder>\n            <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n        </encoder>\n    </appender>\n\n    <root level=\"DEBUG\">\n        <appender-ref ref=\"STDOUT\"/>\n        <appender-ref ref=\"FILE\"/>\n    </root>\n</configuration>\nPlace the logback.xml file in src/main/resources/logback.xml. For tests you can define different logging configuration in src/test/resources/logback-test.xml.\nMDC properties can be included in the Logback output with for example %X{pekkoSource} specifier within the pattern layout configuration:\n<encoder>\n    <pattern>%date{ISO8601} %-5level %logger{36} %X{pekkoSource} - %msg%n</pattern>\n  </encoder>\nAll MDC properties as key-value entries can be included with %mdc:\n<encoder>\n    <pattern>%date{ISO8601} %-5level %logger{36} - %msg MDC: {%mdc}%n</pattern>\n  </encoder>","title":"Logback configuration"},{"location":"/logging.html#logging-thread-pekko-source-and-actor-system-in-mdc","text":"Since the logging is done asynchronously the thread in which the logging was performed is captured in Mapped Diagnostic Context (MDC) with attribute name sourceThread.\nNote It will probably be a good idea to use the sourceThread MDC value also in non-Pekko parts of the application in order to have this property consistently available in the logs.\nAnother helpful facility is that Pekko captures the actor’s address when instantiating a logger within it, meaning that the full instance identification is available for associating log messages e.g. with members of a router. This information is available in the MDC with attribute name pekkoSource.\nThe address of the actor system, containing host and port if the system is using cluster, is available through pekkoAddress.\nFinally, the actor system in which the logging was performed is available in the MDC with attribute name sourceActorSystem.\nFor more details on what this attribute contains—also for non-actors—please see How to Log.","title":"Logging Thread, Pekko Source and Actor System in MDC"},{"location":"/logging.html#more-accurate-timestamps-for-log-output-in-mdc","text":"Pekko’s logging is asynchronous which means that the timestamp of a log entry is taken from when the underlying logger implementation is called, which can be surprising at first. If you want to more accurately output the timestamp, use the MDC attribute pekkoTimestamp.","title":"More accurate timestamps for log output in MDC"},{"location":"/logging.html#mdc-values-defined-by-the-application","text":"One useful feature available in Slf4j is MDC, Pekko has a way to let the application specify custom values, for this you need to use a specialized LoggingAdapterLoggingAdapter, the DiagnosticLoggingAdapterDiagnosticLoggingAdapter. In order to get it you can use the factory, providing an Actor AbstractActor as logSource:\nScala // Within your Actor\nval log: DiagnosticLoggingAdapter = Logging(this);\n Java // Within your AbstractActor\nfinal DiagnosticLoggingAdapter log = Logging.getLogger(this);\nOnce you have the logger, you need to add the custom values before you log something. This way, the values will be put in the SLF4J MDC right before appending the log and removed after.\nNote The cleanup (removal) should be done in the actor at the end, otherwise, next message will log with same MDC values, if it is not set to a new map. Use log.clearMDC()log.clearMDC().\nScala copysourceval mdc = Map(\"requestId\" -> 1234, \"visitorId\" -> 5678)\nlog.mdc(mdc)\n\n// Log something\nlog.info(\"Starting new request\")\n\nlog.clearMDC() Java copysourceimport org.apache.pekko.event.DiagnosticLoggingAdapter;\nimport java.util.HashMap;\nimport java.util.Map; copysourceclass MdcActor extends AbstractActor {\n\n  final DiagnosticLoggingAdapter log = Logging.getLogger(this);\n\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchAny(\n            msg -> {\n              Map<String, Object> mdc;\n              mdc = new HashMap<String, Object>();\n              mdc.put(\"requestId\", 1234);\n              mdc.put(\"visitorId\", 5678);\n              log.setMDC(mdc);\n\n              log.info(\"Starting new request\");\n\n              log.clearMDC();\n            })\n        .build();\n  }\n}\nFor convenience, you can mix in the log member into actors, instead of defining it as above. This trait also lets you override mdc(msg: Any) for specifying MDC values depending on current message and lets you forget about the cleanup as well, since it already does it for you. copysourceimport Logging.MDC\n\nfinal case class Req(work: String, visitorId: Int)\n\nclass MdcActorMixin extends Actor with pekko.actor.DiagnosticActorLogging {\n  var reqId = 0\n\n  override def mdc(currentMessage: Any): MDC = {\n    reqId += 1\n    val always = Map(\"requestId\" -> reqId)\n    val perMessage = currentMessage match {\n      case r: Req => Map(\"visitorId\" -> r.visitorId)\n      case _      => Map()\n    }\n    always ++ perMessage\n  }\n\n  def receive: Receive = {\n    case r: Req => {\n      log.info(s\"Starting new request: ${r.work}\")\n    }\n  }\n}\nNow, the values will be available in the MDC, so you can use them in the layout pattern:\n<appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n  <encoder>\n    <pattern>\n      %-5level %logger{36} [req: %X{requestId}, visitor: %X{visitorId}] - %msg%n\n    </pattern>\n  </encoder>\n</appender>\nAll MDC properties as key-value entries can be included with %mdc:\n<encoder>\n    <pattern>%date{ISO8601} %-5level %logger{36} - %msg MDC: {%mdc}%n</pattern>\n  </encoder>","title":"MDC values defined by the application"},{"location":"/logging.html#using-markers","text":"Some logging libraries allow, in addition to MDC data, attaching so called “markers” to log statements. These are used to filter out rare and special events, for example you might want to mark logs that detect some malicious activity and mark them with a SECURITY tag, and in your appender configuration make these trigger emails and other notifications immediately.\nMarkers are available through the LoggingAdapters, when obtained via Logging.withMarkerLogging.withMarker. The first argument passed into all log calls then should be a LogMarkerLogMarker.\nThe slf4j bridge provided by Pekko in pekko-slf4j will automatically pick up this marker value and make it available to SLF4J.\nPekko is logging some events with markers. Some of these events also include structured MDC properties.\nThe “SECURITY” marker is used for highlighting security related events or incidents. Pekko Actor is using the markers defined in ActorLogMarkerActorLogMarker. Pekko Cluster is using the markers defined in ClusterLogMarkerClusterLogMarker. Pekko Remoting is using the markers defined in RemoteLogMarkerRemoteLogMarker. Pekko Cluster Sharding is using the markers defined in ShardingLogMarkerShardingLogMarker.\nMarkers and MDC properties are automatically picked up by the Logstash Logback encoder.\nThe marker can be included in the Logback output with %marker and all MDC properties as key-value entries with %mdc.\n<encoder>\n    <pattern>[%date{ISO8601}] [%level] [%logger] [%marker] [%thread] - %msg MDC: {%mdc}%n</pattern>\n  </encoder>","title":"Using Markers"},{"location":"/logging.html#using-slf4js-markers","text":"It is also possible to use the org.slf4j.Marker with the LoggingAdapterLoggingAdapter when using slf4j.\nSince the pekko-actor library avoids depending on any specific logging library, the support for this is included in pekko-slf4j, which provides the Slf4jLogMarkerSlf4jLogMarker type which can be passed in as first argument instead of the logging framework agnostic LogMarker type from pekko-actor. The most notable difference between the two is that slf4j’s Markers can have child markers, so one can rely more information using them rather than just a single string.","title":"Using SLF4J’s Markers"},{"location":"/scheduler.html","text":"","title":"Classic Scheduler"},{"location":"/scheduler.html#classic-scheduler","text":"Note Pekko Classic pertains to the original Actor APIs, which have been improved by more type safe and guided Actor APIs. Pekko Classic is still fully supported and existing applications can continue to use the classic APIs. It is also possible to use the new Actor APIs together with classic actors in the same ActorSystem, see coexistence. For new projects we recommend using the new Actor API.\nFor the new API see typed scheduling.","title":"Classic Scheduler"},{"location":"/scheduler.html#dependency","text":"To use Scheduler, you must add the following dependency in your project:\nsbt val PekkoVersion = \"2.6.20+81-523134c3+20230202-1514-SNAPSHOT\"\nlibraryDependencies += \"org.apache.pekko\" %% \"pekko-actor\" % PekkoVersion Maven <properties>\n  <scala.binary.version>2.13</scala.binary.version>\n</properties>\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.pekko</groupId>\n      <artifactId>pekko-bom_${scala.binary.version}</artifactId>\n      <version>2.6.20+81-523134c3+20230202-1514-SNAPSHOT</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.pekko</groupId>\n    <artifactId>pekko-actor_${scala.binary.version}</artifactId>\n  </dependency>\n</dependencies> Gradle def versions = [\n  ScalaBinary: \"2.13\"\n]\ndependencies {\n  implementation platform(\"org.apache.pekko:pekko-bom_${versions.ScalaBinary}:2.6.20+81-523134c3+20230202-1514-SNAPSHOT\")\n\n  implementation \"org.apache.pekko:pekko-actor_${versions.ScalaBinary}\"\n}","title":"Dependency"},{"location":"/scheduler.html#introduction","text":"Sometimes, the need for making things happen in the future arises, and where do you go look then? Look no further than ActorSystemActorSystem! There you find the schedulergetScheduler() method that returns an instance of SchedulerScheduler, this instance is unique per ActorSystem and is used internally for scheduling things to happen at specific points in time.\nYou can schedule sending of messages to actors and execution of tasks (functions or Runnable). You will get a CancellableCancellable back that you can call cancelcancel() on to cancel the execution of the scheduled operation.\nWhen scheduling periodic or single messages in an actor to itself, it is recommended to use the Actor Timers instead of using the SchedulerScheduler directly.\nThe scheduler in Pekko is designed for high-throughput of thousands up to millions of triggers. The prime use-case being triggering Actor receive timeouts, Future timeouts, circuit breakers and other time dependent events which happen all-the-time and in many instances at the same time. The implementation is based on a Hashed Wheel Timer, which is a known datastructure and algorithm for handling such use cases, refer to the Hashed and Hierarchical Timing Wheels whitepaper by Varghese and Lauck if you’d like to understand its inner workings.\nThe Pekko scheduler is not designed for long-term scheduling (see akka-quartz-scheduler instead for this use case) nor is it to be used for highly precise firing of the events. The maximum amount of time into the future you can schedule an event to trigger is around 8 months, which in practice is too much to be useful since this would assume the system never went down during that period. If you need long-term scheduling we highly recommend looking into alternative schedulers, as this is not the use-case the Pekko scheduler is implemented for.\nWarning The default implementation of SchedulerScheduler used by Pekko is based on job buckets which are emptied according to a fixed schedule. It does not execute tasks at the exact time, but on every tick, it will run everything that is (over)due. The accuracy of the default Scheduler can be modified by the pekko.scheduler.tick-duration configuration property.","title":"Introduction"},{"location":"/scheduler.html#some-examples","text":"Scala copysourceimport org.apache.pekko\nimport pekko.actor.Actor\nimport pekko.actor.Props\nimport scala.concurrent.duration._\n Java copysourceimport java.time.Duration;\nSchedule to send the “foo”-message to the testActor after 50ms:\nScala copysource// Use the system's dispatcher as ExecutionContext\nimport system.dispatcher\n\n// Schedules to send the \"foo\"-message to the testActor after 50ms\nsystem.scheduler.scheduleOnce(50 milliseconds, testActor, \"foo\") Java copysourcesystem\n    .scheduler()\n    .scheduleOnce(\n        Duration.ofMillis(50), testActor, \"foo\", system.dispatcher(), ActorRef.noSender());\nSchedule a functionRunnable, that sends the current time to the testActor, to be executed after 50ms:\nScala copysource// Schedules a function to be executed (send a message to the testActor) after 50ms\nsystem.scheduler.scheduleOnce(50 milliseconds) {\n  testActor ! System.currentTimeMillis\n} Java copysourcesystem\n    .scheduler()\n    .scheduleOnce(\n        Duration.ofMillis(50),\n        new Runnable() {\n          @Override\n          public void run() {\n            testActor.tell(System.currentTimeMillis(), ActorRef.noSender());\n          }\n        },\n        system.dispatcher());\nSchedule to send the “Tick”-message to the tickActor after 0ms repeating every 50ms:\nScala copysourceval Tick = \"tick\"\nclass TickActor extends Actor {\n  def receive = {\n    case Tick => // Do something\n  }\n}\nval tickActor = system.actorOf(Props(classOf[TickActor], this))\n// Use system's dispatcher as ExecutionContext\nimport system.dispatcher\n\n// This will schedule to send the Tick-message\n// to the tickActor after 0ms repeating every 50ms\nval cancellable =\n  system.scheduler.scheduleWithFixedDelay(Duration.Zero, 50.milliseconds, tickActor, Tick)\n\n// This cancels further Ticks to be sent\ncancellable.cancel() Java copysourceclass Ticker extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchEquals(\n            \"Tick\",\n            m -> {\n              // Do something\n            })\n        .build();\n  }\n}\n\nActorRef tickActor = system.actorOf(Props.create(Ticker.class, this));\n\n// This will schedule to send the Tick-message\n// to the tickActor after 0ms repeating every 50ms\nCancellable cancellable =\n    system\n        .scheduler()\n        .scheduleWithFixedDelay(\n            Duration.ZERO,\n            Duration.ofMillis(50),\n            tickActor,\n            \"Tick\",\n            system.dispatcher(),\n            ActorRef.noSender());\n\n// This cancels further Ticks to be sent\ncancellable.cancel();\nWarning If you schedule functions or Runnable instances you should be extra careful to not close over unstable references. In practice this means not using this inside the closure in the scope of an Actor instance, not accessing sendersender() directly and not calling the methods of the Actor instance directly. If you need to schedule an invocation schedule a message to selfself() instead (containing the necessary parameters) and then call the method when the message is received.\nWarning All scheduled task will be executed when the ActorSystemActorSystem is terminated, i.e. the task may execute before its timeout.","title":"Some examples"},{"location":"/scheduler.html#schedule-periodically","text":"Scheduling of recurring tasks or messages can have two different characteristics:\nfixed-delay - The delay between subsequent execution will always be (at least) the given delay. Use scheduleWithFixedDelay. fixed-rate - The frequency of execution over time will meet the given interval. Use scheduleAtFixedRate.\nIf you are uncertain of which one to use you should pick scheduleWithFixedDelay.\nWhen using fixed-delay it will not compensate the delay between tasks or messages if the execution takes long time or if scheduling is delayed longer than specified for some reason. The delay between subsequent execution will always be (at least) the given delay. In the long run, the frequency of execution will generally be slightly lower than the reciprocal of the specified delay.\nFixed-delay execution is appropriate for recurring activities that require “smoothness.” In other words, it is appropriate for activities where it is more important to keep the frequency accurate in the short run than in the long run.\nWhen using fixed-rate it will compensate the delay for a subsequent task if the previous tasks took too long to execute. For example, if the given interval is 1000 milliseconds and a task takes 200 milliseconds to execute the next task will be scheduled to run after 800 milliseconds. In such cases, the actual execution interval will differ from the interval passed to the scheduleAtFixedRate method.\nIf the execution of the tasks takes longer than the interval, the subsequent execution will start immediately after the prior one completes (there will be no overlap of executions). This also has the consequence that after long garbage collection pauses or other reasons when the JVM was suspended all “missed” tasks will execute when the process wakes up again. For example, scheduleAtFixedRate with an interval of 1 second and the process is suspended for 30 seconds will result in 30 tasks (or messages) being executed in rapid succession to catch up. In the long run, the frequency of execution will be exactly the reciprocal of the specified interval.\nFixed-rate execution is appropriate for recurring activities that are sensitive to absolute time or where the total time to perform a fixed number of executions is important, such as a countdown timer that ticks once every second for ten seconds.\nWarning scheduleAtFixedRate can result in bursts of scheduled tasks or messages after long garbage collection pauses, which may in worst case cause undesired load on the system. scheduleWithFixedDelay is often preferred.","title":"Schedule periodically"},{"location":"/scheduler.html#the-scheduler-interface","text":"The actual scheduler implementation is loaded reflectively upon ActorSystemActorSystem start-up, which means that it is possible to provide a different one using the pekko.scheduler.implementation configuration property. The referenced class must implement the SchedulerSchedulerAbstractSchedulerAbstractScheduler interface.","title":"The Scheduler interface"},{"location":"/scheduler.html#the-cancellable-interface","text":"Scheduling a task will result in a CancellableCancellable (or throw an IllegalStateException if attempted after the scheduler’s shutdown). This allows you to cancel something that has been scheduled for execution.\nWarning This does not abort the execution of the task, if it had already been started. Check the return value of cancelcancel() to detect whether the scheduled task was canceled or will (eventually) have run.","title":"The Cancellable interface"},{"location":"/extending-pekko.html","text":"","title":"Classic Apache Pekko Extensions"},{"location":"/extending-pekko.html#classic-apache-pekko-extensions","text":"If you want to add features to Apache Pekko, there is a very elegant, but powerful mechanism for doing so. It’s called Apache Pekko Extensions and comprises 2 basic components: an ExtensionExtension and an ExtensionIdExtensionId.\nExtensions will only be loaded once per ActorSystemActorSystem, which will be managed by Pekko. You can choose to have your Extension loaded on-demand or at ActorSystemActorSystem creation time through the Pekko configuration. Details on how to make that happens are below, in the Loading from Configuration section.\nWarning Since an extension is a way to hook into Pekko itself, the implementor of the extension needs to ensure the thread safety of his/her extension.","title":"Classic Apache Pekko Extensions"},{"location":"/extending-pekko.html#building-an-extension","text":"So let’s create a sample extension that lets us count the number of times something has happened.\nFirst, we define what our ExtensionExtension should do:\nScala copysourceimport org.apache.pekko.actor.Extension\n\nclass CountExtensionImpl extends Extension {\n  // Since this Extension is a shared instance\n  // per ActorSystem we need to be threadsafe\n  private val counter = new AtomicLong(0)\n\n  // This is the operation this Extension provides\n  def increment() = counter.incrementAndGet()\n} Java copysourceimport org.apache.pekko.actor.*;\nimport java.util.concurrent.atomic.AtomicLong;\n\nstatic class CountExtensionImpl implements Extension {\n  // Since this Extension is a shared instance\n  // per ActorSystem we need to be threadsafe\n  private final AtomicLong counter = new AtomicLong(0);\n\n  // This is the operation this Extension provides\n  public long increment() {\n    return counter.incrementAndGet();\n  }\n}\nThen we need to create an ExtensionIdExtensionId for our extension so we can grab a hold of it.\nScala copysourceimport org.apache.pekko\nimport pekko.actor.ActorSystem\nimport pekko.actor.ExtensionId\nimport pekko.actor.ExtensionIdProvider\nimport pekko.actor.ExtendedActorSystem\n\nobject CountExtension extends ExtensionId[CountExtensionImpl] with ExtensionIdProvider {\n  // The lookup method is required by ExtensionIdProvider,\n  // so we return ourselves here, this allows us\n  // to configure our extension to be loaded when\n  // the ActorSystem starts up\n  override def lookup = CountExtension\n\n  // This method will be called by Pekko\n  // to instantiate our Extension\n  override def createExtension(system: ExtendedActorSystem) = new CountExtensionImpl\n\n  /**\n   * Java API: retrieve the Count extension for the given system.\n   */\n  override def get(system: ActorSystem): CountExtensionImpl = super.get(system)\n  override def get(system: ClassicActorSystemProvider): CountExtensionImpl = super.get(system)\n} Java copysourceimport org.apache.pekko.actor.*;\nimport java.util.concurrent.atomic.AtomicLong;\n\nstatic class CountExtension extends AbstractExtensionId<CountExtensionImpl>\n    implements ExtensionIdProvider {\n  // This will be the identifier of our CountExtension\n  public static final CountExtension CountExtensionProvider = new CountExtension();\n\n  private CountExtension() {}\n\n  // The lookup method is required by ExtensionIdProvider,\n  // so we return ourselves here, this allows us\n  // to configure our extension to be loaded when\n  // the ActorSystem starts up\n  public CountExtension lookup() {\n    return CountExtension.CountExtensionProvider; // The public static final\n  }\n\n  // This method will be called by Pekko\n  // to instantiate our Extension\n  public CountExtensionImpl createExtension(ExtendedActorSystem system) {\n    return new CountExtensionImpl();\n  }\n}\nWicked! Now all we need to do is to actually use it:\nScala copysourceCountExtension(system).increment() Java copysource// typically you would use static import of the\n// CountExtension.CountExtensionProvider field\nCountExtension.CountExtensionProvider.get(system).increment();\nOr from inside of a Pekko Actor:\nScala copysource class MyActor extends Actor {\n  def receive = {\n    case someMessage =>\n      CountExtension(context.system).increment()\n  }\n} Java copysourcestatic class MyActor extends AbstractActor {\n  @Override\n  public Receive createReceive() {\n    return receiveBuilder()\n        .matchAny(\n            msg -> {\n              // typically you would use static import of the\n              // CountExtension.CountExtensionProvider field\n              CountExtension.CountExtensionProvider.get(getContext().getSystem()).increment();\n            })\n        .build();\n  }\n}\nYou can also hide extension behind traits: copysource trait Counting { self: Actor =>\n  def increment() = CountExtension(context.system).increment()\n}\nclass MyCounterActor extends Actor with Counting {\n  def receive = {\n    case someMessage => increment()\n  }\n}\nThat’s all there is to it!","title":"Building an Extension"},{"location":"/extending-pekko.html#loading-from-configuration","text":"To be able to load extensions from your Pekko configuration you must add FQCNs of implementations of either ExtensionIdExtensionId or ExtensionIdProviderExtensionIdProvider in the pekko.extensions section of the config you provide to your ActorSystemActorSystem.\nScala copysourcepekko {\n  extensions = [\"docs.extension.CountExtension\"]\n} Java pekko {\n  extensions = [\"docs.extension.ExtensionDocTest.CountExtension\"]\n}","title":"Loading from Configuration"},{"location":"/extending-pekko.html#applicability","text":"The sky is the limit! By the way, did you know that Pekko Cluster, Serialization and other features are implemented as Pekko Extensions?","title":"Applicability"},{"location":"/extending-pekko.html#application-specific-settings","text":"The configuration can be used for application specific settings. A good practice is to place those settings in an Extension.\nSample configuration:\ncopysourcemyapp {\n  db {\n    uri = \"mongodb://example1.com:27017,example2.com:27017\"\n  }\n  circuit-breaker {\n    timeout = 30 seconds\n  }\n}\nThe ExtensionExtension:\nScala copysourceimport org.apache.pekko\nimport pekko.actor.ActorSystem\nimport pekko.actor.Extension\nimport pekko.actor.ExtensionId\nimport pekko.actor.ExtensionIdProvider\nimport pekko.actor.ExtendedActorSystem\n\nimport scala.concurrent.duration.Duration\nimport com.typesafe.config.Config\nimport java.util.concurrent.TimeUnit\n\nimport pekko.actor.ClassicActorSystemProvider\n\nclass SettingsImpl(config: Config) extends Extension {\n  val DbUri: String = config.getString(\"myapp.db.uri\")\n  val CircuitBreakerTimeout: Duration =\n    Duration(config.getDuration(\"myapp.circuit-breaker.timeout\", TimeUnit.MILLISECONDS), TimeUnit.MILLISECONDS)\n}\nobject Settings extends ExtensionId[SettingsImpl] with ExtensionIdProvider {\n\n  override def lookup = Settings\n\n  override def createExtension(system: ExtendedActorSystem) =\n    new SettingsImpl(system.settings.config)\n\n  /**\n   * Java API: retrieve the Settings extension for the given system.\n   */\n  override def get(system: ActorSystem): SettingsImpl = super.get(system)\n  override def get(system: ClassicActorSystemProvider): SettingsImpl = super.get(system)\n} Java copysourceimport org.apache.pekko.actor.Extension;\nimport org.apache.pekko.actor.AbstractExtensionId;\nimport org.apache.pekko.actor.ExtensionIdProvider;\nimport org.apache.pekko.actor.ActorSystem;\nimport org.apache.pekko.actor.ExtendedActorSystem;\nimport com.typesafe.config.Config;\nimport java.util.concurrent.TimeUnit;\nimport java.time.Duration;\n\nstatic class SettingsImpl implements Extension {\n\n  public final String DB_URI;\n  public final Duration CIRCUIT_BREAKER_TIMEOUT;\n\n  public SettingsImpl(Config config) {\n    DB_URI = config.getString(\"myapp.db.uri\");\n    CIRCUIT_BREAKER_TIMEOUT =\n        Duration.ofMillis(\n            config.getDuration(\"myapp.circuit-breaker.timeout\", TimeUnit.MILLISECONDS));\n  }\n}\n\nstatic class Settings extends AbstractExtensionId<SettingsImpl> implements ExtensionIdProvider {\n  public static final Settings SettingsProvider = new Settings();\n\n  private Settings() {}\n\n  public Settings lookup() {\n    return Settings.SettingsProvider;\n  }\n\n  public SettingsImpl createExtension(ExtendedActorSystem system) {\n    return new SettingsImpl(system.settings().config());\n  }\n}\nUse it:\nScala copysource class MyActor extends Actor {\n  val settings = Settings(context.system)\n  val connection = connect(settings.DbUri, settings.CircuitBreakerTimeout)\n Java copysourcestatic class MyActor extends AbstractActor {\n  // typically you would use static import of the Settings.SettingsProvider field\n  final SettingsImpl settings = Settings.SettingsProvider.get(getContext().getSystem());\n  Connection connection = connect(settings.DB_URI, settings.CIRCUIT_BREAKER_TIMEOUT);\n\n}","title":"Application specific settings"},{"location":"/extending-pekko.html#library-extensions","text":"A third part library may register its extension for auto-loading on actor system startup by appending it to pekko.library-extensions in its reference.conf.\npekko.library-extensions += \"docs.extension.ExampleExtension\"\nAs there is no way to selectively remove such extensions, it should be used with care and only when there is no case where the user would ever want it disabled or have specific support for disabling such sub-features. One example where this could be important is in tests.\nWarning Thepekko.library-extensions must never be assigned (= [\"Extension\"]) instead of appending as this will break the library-extension mechanism and make behavior depend on class path ordering.","title":"Library extensions"},{"location":"/fault-tolerance-sample.html","text":"","title":"Diagrams of the Fault Tolerance Sample"},{"location":"/fault-tolerance-sample.html#diagrams-of-the-fault-tolerance-sample","text":"The above diagram illustrates the normal message flow.\nNormal flow:\nStep Description 1 The progress Listener starts the work. 2 The Worker schedules work by sending Do messages periodically to itself 3, 4, 5 When receiving Do the Worker tells the CounterService to increment the counter, three times. The Increment message is forwarded to the Counter, which updates its counter variable and sends current value to the Storage. 6, 7 The Worker asks the CounterService of current value of the counter and pipes the result back to the Listener.\nThe above diagram illustrates what happens in case of storage failure.\nFailure flow:\nStep Description 1 The Storage throws StorageException. 2 The CounterService is supervisor of the Storage and restarts the Storage when StorageException is thrown. 3, 4, 5, 6 The Storage continues to fail and is restarted. 7 After 3 failures and restarts within 5 seconds the Storage is stopped by its supervisor, i.e. the CounterService. 8 The CounterService is also watching the Storage for termination and receives the Terminated message when the Storage has been stopped … 9, 10, 11 and tells the Counter that there is no Storage. 12 The CounterService schedules a Reconnect message to itself. 13, 14 When it receives the Reconnect message it creates a new Storage … 15, 16 and tells the Counter to use the new Storage","title":"Diagrams of the Fault Tolerance Sample"},{"location":"/fault-tolerance-sample.html#full-source-code-of-the-fault-tolerance-sample","text":"Scala copysourceimport org.apache.pekko\nimport pekko.actor._\nimport pekko.actor.SupervisorStrategy._\nimport scala.concurrent.duration._\nimport pekko.util.Timeout\nimport pekko.event.LoggingReceive\nimport pekko.pattern.{ ask, pipe }\nimport com.typesafe.config.ConfigFactory\n\n/**\n * Runs the sample\n */\nobject FaultHandlingDocSample extends App {\n  import Worker._\n\n  val config = ConfigFactory.parseString(\"\"\"\n    pekko.loglevel = \"DEBUG\"\n    pekko.actor.debug {\n      receive = on\n      lifecycle = on\n    }\n    \"\"\")\n\n  val system = ActorSystem(\"FaultToleranceSample\", config)\n  val worker = system.actorOf(Props[Worker](), name = \"worker\")\n  val listener = system.actorOf(Props[Listener](), name = \"listener\")\n  // start the work and listen on progress\n  // note that the listener is used as sender of the tell,\n  // i.e. it will receive replies from the worker\n  worker.tell(Start, sender = listener)\n}\n\n/**\n * Listens on progress from the worker and shuts down the system when enough\n * work has been done.\n */\nclass Listener extends Actor with ActorLogging {\n  import Worker._\n  // If we don't get any progress within 15 seconds then the service is unavailable\n  context.setReceiveTimeout(15 seconds)\n\n  def receive = {\n    case Progress(percent) =>\n      log.info(\"Current progress: {} %\", percent)\n      if (percent >= 100.0) {\n        log.info(\"That's all, shutting down\")\n        context.system.terminate()\n      }\n\n    case ReceiveTimeout =>\n      // No progress within 15 seconds, ServiceUnavailable\n      log.error(\"Shutting down due to unavailable service\")\n      context.system.terminate()\n  }\n}\n\nobject Worker {\n  case object Start\n  case object Do\n  final case class Progress(percent: Double)\n}\n\n/**\n * Worker performs some work when it receives the `Start` message.\n * It will continuously notify the sender of the `Start` message\n * of current ``Progress``. The `Worker` supervise the `CounterService`.\n */\nclass Worker extends Actor with ActorLogging {\n  import Worker._\n  import CounterService._\n  implicit val askTimeout: Timeout = Timeout(5 seconds)\n\n  // Stop the CounterService child if it throws ServiceUnavailable\n  override val supervisorStrategy = OneForOneStrategy() {\n    case _: CounterService.ServiceUnavailable => Stop\n  }\n\n  // The sender of the initial Start message will continuously be notified\n  // about progress\n  var progressListener: Option[ActorRef] = None\n  val counterService = context.actorOf(Props[CounterService](), name = \"counter\")\n  val totalCount = 51\n  import context.dispatcher // Use this Actors' Dispatcher as ExecutionContext\n\n  def receive = LoggingReceive {\n    case Start if progressListener.isEmpty =>\n      progressListener = Some(sender())\n      context.system.scheduler.scheduleWithFixedDelay(Duration.Zero, 1 second, self, Do)\n\n    case Do =>\n      counterService ! Increment(1)\n      counterService ! Increment(1)\n      counterService ! Increment(1)\n\n      // Send current progress to the initial sender\n      (counterService ? GetCurrentCount)\n        .map {\n          case CurrentCount(_, count) => Progress(100.0 * count / totalCount)\n        }\n        .pipeTo(progressListener.get)\n  }\n}\n\nobject CounterService {\n  final case class Increment(n: Int)\n  sealed abstract class GetCurrentCount\n  case object GetCurrentCount extends GetCurrentCount\n  final case class CurrentCount(key: String, count: Long)\n  class ServiceUnavailable(msg: String) extends RuntimeException(msg)\n\n  private case object Reconnect\n}\n\n/**\n * Adds the value received in `Increment` message to a persistent\n * counter. Replies with `CurrentCount` when it is asked for `CurrentCount`.\n * `CounterService` supervise `Storage` and `Counter`.\n */\nclass CounterService extends Actor {\n  import CounterService._\n  import Counter._\n  import Storage._\n\n  // Restart the storage child when StorageException is thrown.\n  // After 3 restarts within 5 seconds it will be stopped.\n  override val supervisorStrategy = OneForOneStrategy(maxNrOfRetries = 3, withinTimeRange = 5 seconds) {\n    case _: Storage.StorageException => Restart\n  }\n\n  val key = self.path.name\n  var storage: Option[ActorRef] = None\n  var counter: Option[ActorRef] = None\n  var backlog = IndexedSeq.empty[(ActorRef, Any)]\n  val MaxBacklog = 10000\n\n  import context.dispatcher // Use this Actors' Dispatcher as ExecutionContext\n\n  override def preStart(): Unit = {\n    initStorage()\n  }\n\n  /**\n   * The child storage is restarted in case of failure, but after 3 restarts,\n   * and still failing it will be stopped. Better to back-off than continuously\n   * failing. When it has been stopped we will schedule a Reconnect after a delay.\n   * Watch the child so we receive Terminated message when it has been terminated.\n   */\n  def initStorage(): Unit = {\n    storage = Some(context.watch(context.actorOf(Props[Storage](), name = \"storage\")))\n    // Tell the counter, if any, to use the new storage\n    counter.foreach { _ ! UseStorage(storage) }\n    // We need the initial value to be able to operate\n    storage.get ! Get(key)\n  }\n\n  def receive = LoggingReceive {\n\n    case Entry(k, v) if k == key && counter == None =>\n      // Reply from Storage of the initial value, now we can create the Counter\n      val c = context.actorOf(Props(classOf[Counter], key, v))\n      counter = Some(c)\n      // Tell the counter to use current storage\n      c ! UseStorage(storage)\n      // and send the buffered backlog to the counter\n      for ((replyTo, msg) <- backlog) c.tell(msg, sender = replyTo)\n      backlog = IndexedSeq.empty\n\n    case msg: Increment => forwardOrPlaceInBacklog(msg)\n\n    case msg: GetCurrentCount => forwardOrPlaceInBacklog(msg)\n\n    case Terminated(actorRef) if Some(actorRef) == storage =>\n      // After 3 restarts the storage child is stopped.\n      // We receive Terminated because we watch the child, see initStorage.\n      storage = None\n      // Tell the counter that there is no storage for the moment\n      counter.foreach { _ ! UseStorage(None) }\n      // Try to re-establish storage after while\n      context.system.scheduler.scheduleOnce(10 seconds, self, Reconnect)\n\n    case Reconnect =>\n      // Re-establish storage after the scheduled delay\n      initStorage()\n  }\n\n  def forwardOrPlaceInBacklog(msg: Any): Unit = {\n    // We need the initial value from storage before we can start delegate to\n    // the counter. Before that we place the messages in a backlog, to be sent\n    // to the counter when it is initialized.\n    counter match {\n      case Some(c) => c.forward(msg)\n      case None =>\n        if (backlog.size >= MaxBacklog)\n          throw new ServiceUnavailable(\"CounterService not available, lack of initial value\")\n        backlog :+= (sender() -> msg)\n    }\n  }\n\n}\n\nobject Counter {\n  final case class UseStorage(storage: Option[ActorRef])\n}\n\n/**\n * The in memory count variable that will send current\n * value to the `Storage`, if there is any storage\n * available at the moment.\n */\nclass Counter(key: String, initialValue: Long) extends Actor {\n  import Counter._\n  import CounterService._\n  import Storage._\n\n  var count = initialValue\n  var storage: Option[ActorRef] = None\n\n  def receive = LoggingReceive {\n    case UseStorage(s) =>\n      storage = s\n      storeCount()\n\n    case Increment(n) =>\n      count += n\n      storeCount()\n\n    case GetCurrentCount =>\n      sender() ! CurrentCount(key, count)\n\n  }\n\n  def storeCount(): Unit = {\n    // Delegate dangerous work, to protect our valuable state.\n    // We can continue without storage.\n    storage.foreach { _ ! Store(Entry(key, count)) }\n  }\n\n}\n\nobject Storage {\n  final case class Store(entry: Entry)\n  final case class Get(key: String)\n  final case class Entry(key: String, value: Long)\n  class StorageException(msg: String) extends RuntimeException(msg)\n}\n\n/**\n * Saves key/value pairs to persistent storage when receiving `Store` message.\n * Replies with current value when receiving `Get` message.\n * Will throw StorageException if the underlying data store is out of order.\n */\nclass Storage extends Actor {\n  import Storage._\n\n  val db = DummyDB\n\n  def receive = LoggingReceive {\n    case Store(Entry(key, count)) => db.save(key, count)\n    case Get(key)                 => sender() ! Entry(key, db.load(key).getOrElse(0L))\n  }\n}\n\nobject DummyDB {\n  import Storage.StorageException\n  private var db = Map[String, Long]()\n\n  @throws(classOf[StorageException])\n  def save(key: String, value: Long): Unit = synchronized {\n    if (11 <= value && value <= 14)\n      throw new StorageException(\"Simulated store failure \" + value)\n    db += (key -> value)\n  }\n\n  @throws(classOf[StorageException])\n  def load(key: String): Option[Long] = synchronized {\n    db.get(key)\n  }\n} Java copysourceimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.time.Duration;\n\nimport org.apache.pekko.actor.*;\nimport org.apache.pekko.dispatch.Mapper;\nimport org.apache.pekko.event.LoggingReceive;\nimport org.apache.pekko.japi.pf.DeciderBuilder;\nimport org.apache.pekko.pattern.Patterns;\nimport org.apache.pekko.util.Timeout;\nimport com.typesafe.config.Config;\nimport com.typesafe.config.ConfigFactory;\n\nimport static org.apache.pekko.japi.Util.classTag;\nimport static org.apache.pekko.actor.SupervisorStrategy.restart;\nimport static org.apache.pekko.actor.SupervisorStrategy.stop;\nimport static org.apache.pekko.actor.SupervisorStrategy.escalate;\n\nimport static org.apache.pekko.pattern.Patterns.pipe;\n\nimport static jdocs.actor.FaultHandlingDocSample.WorkerApi.*;\nimport static jdocs.actor.FaultHandlingDocSample.CounterServiceApi.*;\nimport static jdocs.actor.FaultHandlingDocSample.CounterApi.*;\nimport static jdocs.actor.FaultHandlingDocSample.StorageApi.*;\n\n\npublic class FaultHandlingDocSample {\n\n  /** Runs the sample */\n  public static void main(String[] args) {\n    Config config =\n        ConfigFactory.parseString(\n            \"pekko.loglevel = \\\"DEBUG\\\"\\n\"\n                + \"pekko.actor.debug {\\n\"\n                + \"  receive = on\\n\"\n                + \"  lifecycle = on\\n\"\n                + \"}\\n\");\n\n    ActorSystem system = ActorSystem.create(\"FaultToleranceSample\", config);\n    ActorRef worker = system.actorOf(Props.create(Worker.class), \"worker\");\n    ActorRef listener = system.actorOf(Props.create(Listener.class), \"listener\");\n    // start the work and listen on progress\n    // note that the listener is used as sender of the tell,\n    // i.e. it will receive replies from the worker\n    worker.tell(Start, listener);\n  }\n\n  /**\n   * Listens on progress from the worker and shuts down the system when enough work has been done.\n   */\n  public static class Listener extends AbstractLoggingActor {\n\n    @Override\n    public void preStart() {\n      // If we don't get any progress within 15 seconds then the service\n      // is unavailable\n      getContext().setReceiveTimeout(Duration.ofSeconds(15));\n    }\n\n    @Override\n    public Receive createReceive() {\n      return LoggingReceive.create(\n          receiveBuilder()\n              .match(\n                  Progress.class,\n                  progress -> {\n                    log().info(\"Current progress: {} %\", progress.percent);\n                    if (progress.percent >= 100.0) {\n                      log().info(\"That's all, shutting down\");\n                      getContext().getSystem().terminate();\n                    }\n                  })\n              .matchEquals(\n                  ReceiveTimeout.getInstance(),\n                  x -> {\n                    // No progress within 15 seconds, ServiceUnavailable\n                    log().error(\"Shutting down due to unavailable service\");\n                    getContext().getSystem().terminate();\n                  })\n              .build(),\n          getContext());\n    }\n  }\n\n  public interface WorkerApi {\n    public static final Object Start = \"Start\";\n    public static final Object Do = \"Do\";\n\n    public static class Progress {\n      public final double percent;\n\n      public Progress(double percent) {\n        this.percent = percent;\n      }\n\n      public String toString() {\n        return String.format(\"%s(%s)\", getClass().getSimpleName(), percent);\n      }\n    }\n  }\n\n\n  /**\n   * Worker performs some work when it receives the Start message. It will continuously notify the\n   * sender of the Start message of current Progress. The Worker supervise the CounterService.\n   */\n  public static class Worker extends AbstractLoggingActor {\n    final Timeout askTimeout = Timeout.create(Duration.ofSeconds(5));\n\n    // The sender of the initial Start message will continuously be notified\n    // about progress\n    ActorRef progressListener;\n    final ActorRef counterService =\n        getContext().actorOf(Props.create(CounterService.class), \"counter\");\n    final int totalCount = 51;\n\n    // Stop the CounterService child if it throws ServiceUnavailable\n    private static final SupervisorStrategy strategy =\n        new OneForOneStrategy(\n            DeciderBuilder.match(ServiceUnavailable.class, e -> stop())\n                .matchAny(o -> escalate())\n                .build());\n\n    @Override\n    public SupervisorStrategy supervisorStrategy() {\n      return strategy;\n    }\n\n    @Override\n    public Receive createReceive() {\n      return LoggingReceive.create(\n          receiveBuilder()\n              .matchEquals(\n                  Start,\n                  x -> progressListener == null,\n                  x -> {\n                    progressListener = getSender();\n                    getContext()\n                        .getSystem()\n                        .scheduler()\n                        .scheduleWithFixedDelay(\n                            Duration.ZERO,\n                            Duration.ofSeconds(1L),\n                            getSelf(),\n                            Do,\n                            getContext().getDispatcher(),\n                            null);\n                  })\n              .matchEquals(\n                  Do,\n                  x -> {\n                    counterService.tell(new Increment(1), getSelf());\n                    counterService.tell(new Increment(1), getSelf());\n                    counterService.tell(new Increment(1), getSelf());\n                    // Send current progress to the initial sender\n                    pipe(\n                            Patterns.ask(counterService, GetCurrentCount, askTimeout)\n                                .mapTo(classTag(CurrentCount.class))\n                                .map(\n                                    new Mapper<CurrentCount, Progress>() {\n                                      public Progress apply(CurrentCount c) {\n                                        return new Progress(100.0 * c.count / totalCount);\n                                      }\n                                    },\n                                    getContext().dispatcher()),\n                            getContext().dispatcher())\n                        .to(progressListener);\n                  })\n              .build(),\n          getContext());\n    }\n  }\n\n  public interface CounterServiceApi {\n\n    public static final Object GetCurrentCount = \"GetCurrentCount\";\n\n    public static class CurrentCount {\n      public final String key;\n      public final long count;\n\n      public CurrentCount(String key, long count) {\n        this.key = key;\n        this.count = count;\n      }\n\n      public String toString() {\n        return String.format(\"%s(%s, %s)\", getClass().getSimpleName(), key, count);\n      }\n    }\n\n    public static class Increment {\n      public final long n;\n\n      public Increment(long n) {\n        this.n = n;\n      }\n\n      public String toString() {\n        return String.format(\"%s(%s)\", getClass().getSimpleName(), n);\n      }\n    }\n\n    public static class ServiceUnavailable extends RuntimeException {\n      private static final long serialVersionUID = 1L;\n\n      public ServiceUnavailable(String msg) {\n        super(msg);\n      }\n    }\n  }\n\n\n  /**\n   * Adds the value received in Increment message to a persistent counter. Replies with CurrentCount\n   * when it is asked for CurrentCount. CounterService supervise Storage and Counter.\n   */\n  public static class CounterService extends AbstractLoggingActor {\n\n    // Reconnect message\n    static final Object Reconnect = \"Reconnect\";\n\n    private static class SenderMsgPair {\n      final ActorRef sender;\n      final Object msg;\n\n      SenderMsgPair(ActorRef sender, Object msg) {\n        this.msg = msg;\n        this.sender = sender;\n      }\n    }\n\n    final String key = getSelf().path().name();\n    ActorRef storage;\n    ActorRef counter;\n    final List<SenderMsgPair> backlog = new ArrayList<>();\n    final int MAX_BACKLOG = 10000;\n\n    // Restart the storage child when StorageException is thrown.\n    // After 3 restarts within 5 seconds it will be stopped.\n    private static final SupervisorStrategy strategy =\n        new OneForOneStrategy(\n            3,\n            Duration.ofSeconds(5),\n            DeciderBuilder.match(StorageException.class, e -> restart())\n                .matchAny(o -> escalate())\n                .build());\n\n    @Override\n    public SupervisorStrategy supervisorStrategy() {\n      return strategy;\n    }\n\n    @Override\n    public void preStart() {\n      initStorage();\n    }\n\n    /**\n     * The child storage is restarted in case of failure, but after 3 restarts, and still failing it\n     * will be stopped. Better to back-off than continuously failing. When it has been stopped we\n     * will schedule a Reconnect after a delay. Watch the child so we receive Terminated message\n     * when it has been terminated.\n     */\n    void initStorage() {\n      storage = getContext().watch(getContext().actorOf(Props.create(Storage.class), \"storage\"));\n      // Tell the counter, if any, to use the new storage\n      if (counter != null) counter.tell(new UseStorage(storage), getSelf());\n      // We need the initial value to be able to operate\n      storage.tell(new Get(key), getSelf());\n    }\n\n    @Override\n    public Receive createReceive() {\n      return LoggingReceive.create(\n          receiveBuilder()\n              .match(\n                  Entry.class,\n                  entry -> entry.key.equals(key) && counter == null,\n                  entry -> {\n                    // Reply from Storage of the initial value, now we can create the Counter\n                    final long value = entry.value;\n                    counter = getContext().actorOf(Props.create(Counter.class, key, value));\n                    // Tell the counter to use current storage\n                    counter.tell(new UseStorage(storage), getSelf());\n                    // and send the buffered backlog to the counter\n                    for (SenderMsgPair each : backlog) {\n                      counter.tell(each.msg, each.sender);\n                    }\n                    backlog.clear();\n                  })\n              .match(\n                  Increment.class,\n                  increment -> {\n                    forwardOrPlaceInBacklog(increment);\n                  })\n              .matchEquals(\n                  GetCurrentCount,\n                  gcc -> {\n                    forwardOrPlaceInBacklog(gcc);\n                  })\n              .match(\n                  Terminated.class,\n                  o -> {\n                    // After 3 restarts the storage child is stopped.\n                    // We receive Terminated because we watch the child, see initStorage.\n                    storage = null;\n                    // Tell the counter that there is no storage for the moment\n                    counter.tell(new UseStorage(null), getSelf());\n                    // Try to re-establish storage after while\n                    getContext()\n                        .getSystem()\n                        .scheduler()\n                        .scheduleOnce(\n                            Duration.ofSeconds(10),\n                            getSelf(),\n                            Reconnect,\n                            getContext().getDispatcher(),\n                            null);\n                  })\n              .matchEquals(\n                  Reconnect,\n                  o -> {\n                    // Re-establish storage after the scheduled delay\n                    initStorage();\n                  })\n              .build(),\n          getContext());\n    }\n\n    void forwardOrPlaceInBacklog(Object msg) {\n      // We need the initial value from storage before we can start delegate to\n      // the counter. Before that we place the messages in a backlog, to be sent\n      // to the counter when it is initialized.\n      if (counter == null) {\n        if (backlog.size() >= MAX_BACKLOG)\n          throw new ServiceUnavailable(\"CounterService not available,\" + \" lack of initial value\");\n        backlog.add(new SenderMsgPair(getSender(), msg));\n      } else {\n        counter.forward(msg, getContext());\n      }\n    }\n  }\n\n  public interface CounterApi {\n    public static class UseStorage {\n      public final ActorRef storage;\n\n      public UseStorage(ActorRef storage) {\n        this.storage = storage;\n      }\n\n      public String toString() {\n        return String.format(\"%s(%s)\", getClass().getSimpleName(), storage);\n      }\n    }\n  }\n\n\n  /**\n   * The in memory count variable that will send current value to the Storage, if there is any\n   * storage available at the moment.\n   */\n  public static class Counter extends AbstractLoggingActor {\n    final String key;\n    long count;\n    ActorRef storage;\n\n    public Counter(String key, long initialValue) {\n      this.key = key;\n      this.count = initialValue;\n    }\n\n    @Override\n    public Receive createReceive() {\n      return LoggingReceive.create(\n          receiveBuilder()\n              .match(\n                  UseStorage.class,\n                  useStorage -> {\n                    storage = useStorage.storage;\n                    storeCount();\n                  })\n              .match(\n                  Increment.class,\n                  increment -> {\n                    count += increment.n;\n                    storeCount();\n                  })\n              .matchEquals(\n                  GetCurrentCount,\n                  gcc -> {\n                    getSender().tell(new CurrentCount(key, count), getSelf());\n                  })\n              .build(),\n          getContext());\n    }\n\n    void storeCount() {\n      // Delegate dangerous work, to protect our valuable state.\n      // We can continue without storage.\n      if (storage != null) {\n        storage.tell(new Store(new Entry(key, count)), getSelf());\n      }\n    }\n  }\n\n  public interface StorageApi {\n\n    public static class Store {\n      public final Entry entry;\n\n      public Store(Entry entry) {\n        this.entry = entry;\n      }\n\n      public String toString() {\n        return String.format(\"%s(%s)\", getClass().getSimpleName(), entry);\n      }\n    }\n\n    public static class Entry {\n      public final String key;\n      public final long value;\n\n      public Entry(String key, long value) {\n        this.key = key;\n        this.value = value;\n      }\n\n      public String toString() {\n        return String.format(\"%s(%s, %s)\", getClass().getSimpleName(), key, value);\n      }\n    }\n\n    public static class Get {\n      public final String key;\n\n      public Get(String key) {\n        this.key = key;\n      }\n\n      public String toString() {\n        return String.format(\"%s(%s)\", getClass().getSimpleName(), key);\n      }\n    }\n\n    public static class StorageException extends RuntimeException {\n      private static final long serialVersionUID = 1L;\n\n      public StorageException(String msg) {\n        super(msg);\n      }\n    }\n  }\n\n\n  /**\n   * Saves key/value pairs to persistent storage when receiving Store message. Replies with current\n   * value when receiving Get message. Will throw StorageException if the underlying data store is\n   * out of order.\n   */\n  public static class Storage extends AbstractLoggingActor {\n\n    final DummyDB db = DummyDB.instance;\n\n    @Override\n    public Receive createReceive() {\n      return LoggingReceive.create(\n          receiveBuilder()\n              .match(\n                  Store.class,\n                  store -> {\n                    db.save(store.entry.key, store.entry.value);\n                  })\n              .match(\n                  Get.class,\n                  get -> {\n                    Long value = db.load(get.key);\n                    getSender()\n                        .tell(\n                            new Entry(get.key, value == null ? Long.valueOf(0L) : value),\n                            getSelf());\n                  })\n              .build(),\n          getContext());\n    }\n  }\n\n  public static class DummyDB {\n    public static final DummyDB instance = new DummyDB();\n    private final Map<String, Long> db = new HashMap<String, Long>();\n\n    private DummyDB() {}\n\n    public synchronized void save(String key, Long value) throws StorageException {\n      if (11 <= value && value <= 14)\n        throw new StorageException(\"Simulated store failure \" + value);\n      db.put(key, value);\n    }\n\n    public synchronized Long load(String key) throws StorageException {\n      return db.get(key);\n    }\n  }\n}","title":"Full Source Code of the Fault Tolerance Sample"}]}